{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA4NTE3NDg0", "number": 1558, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMzoyOTozNlrOD2_9Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwMDo1OToxM1rOENps1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4OTk3NTExOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMzoyOTozNlrOGNA7tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOToyMDoyNFrOGNK11Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI5OTk1Ng==", "bodyText": "Let us modify help string of dryrun, statements are inaccurate :)", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416299956", "createdAt": "2020-04-28T03:29:36Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,11 +64,15 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          mandatory = true) final String sparkPropertiesPath,\n+      @CliOption(key = {\"useCommitTimeForDedupe\"}, help = \"Set it to true if duplicates have never been updated\",\n+        unspecifiedDefaultValue = \"true\") final boolean useCommitTimeForDedupe,\n+      @CliOption(key = {\"dryrun\"}, help = \"Should we actually add or just print what would be done\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2MjI5Mw==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416462293", "createdAt": "2020-04-28T09:20:24Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,11 +64,15 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          mandatory = true) final String sparkPropertiesPath,\n+      @CliOption(key = {\"useCommitTimeForDedupe\"}, help = \"Set it to true if duplicates have never been updated\",\n+        unspecifiedDefaultValue = \"true\") final boolean useCommitTimeForDedupe,\n+      @CliOption(key = {\"dryrun\"}, help = \"Should we actually add or just print what would be done\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI5OTk1Ng=="}, "originalCommit": {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MDA4MzUzOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoxODozOFrOGNB1SA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwOTozMDo1NFrOGNLQcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNDY5Ng==", "bodyText": "It's better not use break here, rows.init also can get the rows will be delete.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416314696", "createdAt": "2020-04-28T04:18:38Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -103,24 +105,51 @@ class DedupeSparkJob(basePath: String,\n     // Mark all files except the one with latest commits for deletion\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n+\n+      if (useCommitTimeForDedupe) {\n+        /*\n+        This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+         */\n+        var maxCommit = -1L\n+\n+        rows.foreach(r => {\n+          val c = r(3).asInstanceOf[String].toLong\n+          if (c > maxCommit)\n+            maxCommit = c\n+        })\n+        rows.foreach(r => {\n+          val c = r(3).asInstanceOf[String].toLong\n+          if (c != maxCommit) {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n           }\n-          fileToDeleteKeyMap(f).add(key)\n+        })\n+      } else {\n+        /*\n+        This corresponds to the case where duplicates have been updated at least once.\n+        Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+         */\n+        val size = rows.size - 1\n+        var i = 0\n+        val loop = new Breaks\n+        loop.breakable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2OTEwNw==", "bodyText": "Right. Thank you for suggesting this, I am not hands on at scala properly. :)", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416469107", "createdAt": "2020-04-28T09:30:54Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -103,24 +105,51 @@ class DedupeSparkJob(basePath: String,\n     // Mark all files except the one with latest commits for deletion\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n+\n+      if (useCommitTimeForDedupe) {\n+        /*\n+        This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+         */\n+        var maxCommit = -1L\n+\n+        rows.foreach(r => {\n+          val c = r(3).asInstanceOf[String].toLong\n+          if (c > maxCommit)\n+            maxCommit = c\n+        })\n+        rows.foreach(r => {\n+          val c = r(3).asInstanceOf[String].toLong\n+          if (c != maxCommit) {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n           }\n-          fileToDeleteKeyMap(f).add(key)\n+        })\n+      } else {\n+        /*\n+        This corresponds to the case where duplicates have been updated at least once.\n+        Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+         */\n+        val size = rows.size - 1\n+        var i = 0\n+        val loop = new Breaks\n+        loop.breakable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNDY5Ng=="}, "originalCommit": {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTk1Mzc2OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDoyNTo1M1rOGYOe-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yN1QyMTozMzoxNVrOGp4cCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ==", "bodyText": "Can use DeDupeType.withName(\"insertType\") instead?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428056315", "createdAt": "2020-05-20T14:25:53Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -263,13 +265,26 @@ private static int compact(JavaSparkContext jsc, String basePath, String tableNa\n   }\n \n   private static int deduplicatePartitionPath(JavaSparkContext jsc, String duplicatedPartitionPath,\n-      String repairedOutputPath, String basePath, String dryRun) {\n+      String repairedOutputPath, String basePath, boolean dryRun, String dedupeType) {\n     DedupeSparkJob job = new DedupeSparkJob(basePath, duplicatedPartitionPath, repairedOutputPath, new SQLContext(jsc),\n-        FSUtils.getFs(basePath, jsc.hadoopConfiguration()));\n-    job.fixDuplicates(Boolean.parseBoolean(dryRun));\n+        FSUtils.getFs(basePath, jsc.hadoopConfiguration()), getDedupeType(dedupeType));\n+    job.fixDuplicates(dryRun);\n     return 0;\n   }\n \n+  private static Enumeration.Value getDedupeType(String type) {\n+    switch (type) {\n+      case \"insertType\":\n+        return DeDupeType.insertType();\n+      case \"updateType\":\n+        return DeDupeType.updateType();\n+      case \"upsertType\":\n+        return DeDupeType.upsertType();\n+      default:\n+        throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3MTk5MA==", "bodyText": "But what difference does it create?\nDeDupeType.insertType() and DeDupeType.withName(\"insertType\") - both return the same Value.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429171990", "createdAt": "2020-05-22T10:36:17Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -263,13 +265,26 @@ private static int compact(JavaSparkContext jsc, String basePath, String tableNa\n   }\n \n   private static int deduplicatePartitionPath(JavaSparkContext jsc, String duplicatedPartitionPath,\n-      String repairedOutputPath, String basePath, String dryRun) {\n+      String repairedOutputPath, String basePath, boolean dryRun, String dedupeType) {\n     DedupeSparkJob job = new DedupeSparkJob(basePath, duplicatedPartitionPath, repairedOutputPath, new SQLContext(jsc),\n-        FSUtils.getFs(basePath, jsc.hadoopConfiguration()));\n-    job.fixDuplicates(Boolean.parseBoolean(dryRun));\n+        FSUtils.getFs(basePath, jsc.hadoopConfiguration()), getDedupeType(dedupeType));\n+    job.fixDuplicates(dryRun);\n     return 0;\n   }\n \n+  private static Enumeration.Value getDedupeType(String type) {\n+    switch (type) {\n+      case \"insertType\":\n+        return DeDupeType.insertType();\n+      case \"updateType\":\n+        return DeDupeType.updateType();\n+      case \"upsertType\":\n+        return DeDupeType.upsertType();\n+      default:\n+        throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ=="}, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTI1MTEyNg==", "bodyText": "@pratyakshsharma : I mean that we can use DeDupeType.withName(\"insertType\") to convert String to Enum.  getDedupeType Function may not need here.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429251126", "createdAt": "2020-05-22T13:39:03Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -263,13 +265,26 @@ private static int compact(JavaSparkContext jsc, String basePath, String tableNa\n   }\n \n   private static int deduplicatePartitionPath(JavaSparkContext jsc, String duplicatedPartitionPath,\n-      String repairedOutputPath, String basePath, String dryRun) {\n+      String repairedOutputPath, String basePath, boolean dryRun, String dedupeType) {\n     DedupeSparkJob job = new DedupeSparkJob(basePath, duplicatedPartitionPath, repairedOutputPath, new SQLContext(jsc),\n-        FSUtils.getFs(basePath, jsc.hadoopConfiguration()));\n-    job.fixDuplicates(Boolean.parseBoolean(dryRun));\n+        FSUtils.getFs(basePath, jsc.hadoopConfiguration()), getDedupeType(dedupeType));\n+    job.fixDuplicates(dryRun);\n     return 0;\n   }\n \n+  private static Enumeration.Value getDedupeType(String type) {\n+    switch (type) {\n+      case \"insertType\":\n+        return DeDupeType.insertType();\n+      case \"updateType\":\n+        return DeDupeType.updateType();\n+      case \"upsertType\":\n+        return DeDupeType.upsertType();\n+      default:\n+        throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ=="}, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjU2OTQ4Mg==", "bodyText": "Taken care. :)", "url": "https://github.com/apache/hudi/pull/1558#discussion_r446569482", "createdAt": "2020-06-27T21:33:15Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -263,13 +265,26 @@ private static int compact(JavaSparkContext jsc, String basePath, String tableNa\n   }\n \n   private static int deduplicatePartitionPath(JavaSparkContext jsc, String duplicatedPartitionPath,\n-      String repairedOutputPath, String basePath, String dryRun) {\n+      String repairedOutputPath, String basePath, boolean dryRun, String dedupeType) {\n     DedupeSparkJob job = new DedupeSparkJob(basePath, duplicatedPartitionPath, repairedOutputPath, new SQLContext(jsc),\n-        FSUtils.getFs(basePath, jsc.hadoopConfiguration()));\n-    job.fixDuplicates(Boolean.parseBoolean(dryRun));\n+        FSUtils.getFs(basePath, jsc.hadoopConfiguration()), getDedupeType(dedupeType));\n+    job.fixDuplicates(dryRun);\n     return 0;\n   }\n \n+  private static Enumeration.Value getDedupeType(String type) {\n+    switch (type) {\n+      case \"insertType\":\n+        return DeDupeType.insertType();\n+      case \"updateType\":\n+        return DeDupeType.updateType();\n+      case \"upsertType\":\n+        return DeDupeType.upsertType();\n+      default:\n+        throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ=="}, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjA0NTk4OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo0NToyNlrOGYPaZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxMDo0MToyMVrOGZStZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MTUyNQ==", "bodyText": "It's better to show the three types in help string and have a type check at first line of command.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428071525", "createdAt": "2020-05-20T14:45:26Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,7 +77,9 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Check DeDupeType.scala for valid values\",\n+          unspecifiedDefaultValue = \"insertType\") final String dedupeType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3NDExOA==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429174118", "createdAt": "2020-05-22T10:41:21Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,7 +77,9 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Check DeDupeType.scala for valid values\",\n+          unspecifiedDefaultValue = \"insertType\") final String dedupeType)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MTUyNQ=="}, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjA5MDAwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DeDupeType.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo1NDozN1rOGYP3bA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxMDo0NzoyOFrOGZS28g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3ODk1Ng==", "bodyText": "Can we make it all uppercase to keep the format uniform\nhttps://github.com/apache/incubator-hudi/blob/74ecc27e920c70fa4598d8e5a696954203a5b127/hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java#L30-L34", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428078956", "createdAt": "2020-05-20T14:54:37Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DeDupeType.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli\n+\n+object DeDupeType extends Enumeration {\n+\n+  type dedupeType = Value\n+\n+  val insertType = Value(\"insertType\")\n+  val updateType = Value(\"updateType\")\n+  val upsertType = Value(\"upsertType\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3NjU2Mg==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429176562", "createdAt": "2020-05-22T10:47:28Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DeDupeType.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli\n+\n+object DeDupeType extends Enumeration {\n+\n+  type dedupeType = Value\n+\n+  val insertType = Value(\"insertType\")\n+  val updateType = Value(\"updateType\")\n+  val upsertType = Value(\"upsertType\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3ODk1Ng=="}, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjA5OTcyOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo1NjozM1rOGYP9sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxMDo0OTo0OFrOGZS6wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA4MDU2MQ==", "bodyText": "Can we use $ to get value? like:\nhttps://github.com/apache/incubator-hudi/blob/74ecc27e920c70fa4598d8e5a696954203a5b127/hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala#L144", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428080561", "createdAt": "2020-05-20T14:56:33Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.updateType =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.insertType =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            }\n+          })\n+\n+        case DeDupeType.upsertType =>\n+          /*\n+          This corresponds to the case where duplicates got created as a result of inserts as well as updates,\n+          i.e few duplicate records have been updated, while others were never updated.\n+           */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          val rowsWithMaxCommit = new ListBuffer[Row]()\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            } else {\n+              rowsWithMaxCommit += r\n+            }\n+          })\n+\n+          rowsWithMaxCommit.toList.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+\n+        case _ => throw new IllegalArgumentException(\"Please provide valid type for deduping!\")\n+      }\n     })\n+    LOG.debug(\"fileToDeleteKeyMap size : \" + fileToDeleteKeyMap.size + \", map: \" + fileToDeleteKeyMap)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3NzUzNw==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429177537", "createdAt": "2020-05-22T10:49:48Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.updateType =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.insertType =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            }\n+          })\n+\n+        case DeDupeType.upsertType =>\n+          /*\n+          This corresponds to the case where duplicates got created as a result of inserts as well as updates,\n+          i.e few duplicate records have been updated, while others were never updated.\n+           */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          val rowsWithMaxCommit = new ListBuffer[Row]()\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            } else {\n+              rowsWithMaxCommit += r\n+            }\n+          })\n+\n+          rowsWithMaxCommit.toList.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+\n+        case _ => throw new IllegalArgumentException(\"Please provide valid type for deduping!\")\n+      }\n     })\n+    LOG.debug(\"fileToDeleteKeyMap size : \" + fileToDeleteKeyMap.size + \", map: \" + fileToDeleteKeyMap)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA4MDU2MQ=="}, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4OTczODQ5OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMToxNjozMVrOGq3pWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMToxNjozMVrOGq3pWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTA4MQ==", "bodyText": "Can we get the representation of the dedupeType by the DeDupeType enum?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447605081", "createdAt": "2020-06-30T11:16:31Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,16 +77,21 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(\"insert_type\") && !dedupeType.equals(\"update_type\") && !dedupeType.equals(\"upsert_type\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4OTc0Mjc3OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMToxNzo0NVrOGq3r2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNzo0ODoxN1rOGrvrpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTcyMw==", "bodyText": "Can we append this parameter into the list of the parameters?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447605723", "createdAt": "2020-06-30T11:17:45Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,16 +77,21 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(\"insert_type\") && !dedupeType.equals(\"update_type\") && !dedupeType.equals(\"upsert_type\")) {\n+      throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n     if (StringUtils.isNullOrEmpty(sparkPropertiesPath)) {\n       sparkPropertiesPath =\n           Utils.getDefaultPropertiesFile(JavaConverters.mapAsScalaMapConverter(System.getenv()).asScala());\n     }\n \n     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);\n     sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,\n-        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(),\n+        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(), dedupeType,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyMjM1Nw==", "bodyText": "I did not get this. Which list are you referring to?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r448522357", "createdAt": "2020-07-01T17:46:42Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,16 +77,21 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(\"insert_type\") && !dedupeType.equals(\"update_type\") && !dedupeType.equals(\"upsert_type\")) {\n+      throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n     if (StringUtils.isNullOrEmpty(sparkPropertiesPath)) {\n       sparkPropertiesPath =\n           Utils.getDefaultPropertiesFile(JavaConverters.mapAsScalaMapConverter(System.getenv()).asScala());\n     }\n \n     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);\n     sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,\n-        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(),\n+        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(), dedupeType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTcyMw=="}, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyMzE3Mw==", "bodyText": "Ok, you mean I should append it at the end of the already existing parameters? Sure will do that.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r448523173", "createdAt": "2020-07-01T17:48:17Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,16 +77,21 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(\"insert_type\") && !dedupeType.equals(\"update_type\") && !dedupeType.equals(\"upsert_type\")) {\n+      throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n     if (StringUtils.isNullOrEmpty(sparkPropertiesPath)) {\n       sparkPropertiesPath =\n           Utils.getDefaultPropertiesFile(JavaConverters.mapAsScalaMapConverter(System.getenv()).asScala());\n     }\n \n     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);\n     sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,\n-        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(),\n+        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(), dedupeType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTcyMw=="}, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4OTc0NTMxOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMToxODoyNlrOGq3tVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMToxODoyNlrOGq3tVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNjEwMA==", "bodyText": "ditto, move to the last of the method's parameter list?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447606100", "createdAt": "2020-06-30T11:18:26Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -75,8 +76,8 @@ public static void main(String[] args) throws Exception {\n         returnCode = rollback(jsc, args[3], args[4]);\n         break;\n       case DEDUPLICATE:\n-        assert (args.length == 7);\n-        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], args[6]);\n+        assert (args.length == 8);\n+        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], Boolean.parseBoolean(args[7]), args[6]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4OTc1NTQwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMToyMToxNVrOGq3zTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMToyMToxNVrOGq3zTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNzYzMA==", "bodyText": "split via a new empty line?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447607630", "createdAt": "2020-06-30T11:21:15Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDcwOTIwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTowNzowMVrOGrA-7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTowNzowMVrOGrA-7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1ODA2Mg==", "bodyText": "split via empty line", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447758062", "createdAt": "2020-06-30T15:07:01Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.INSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDcxMzMwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTowNzo1N1rOGrBBnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTowNzo1N1rOGrBBnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1ODc0OA==", "bodyText": "why do you add new line here?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447758748", "createdAt": "2020-06-30T15:07:57Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.INSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            }\n+          })\n+\n+        case DeDupeType.UPSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created as a result of inserts as well as updates,\n+          i.e few duplicate records have been updated, while others were never updated.\n+           */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          val rowsWithMaxCommit = new ListBuffer[Row]()\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            } else {\n+              rowsWithMaxCommit += r\n+            }\n+          })\n+\n+          rowsWithMaxCommit.toList.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+\n+        case _ => throw new IllegalArgumentException(\"Please provide valid type for deduping!\")\n+      }\n     })\n+    LOG.debug(s\"fileToDeleteKeyMap size: ${fileToDeleteKeyMap.size}, map: $fileToDeleteKeyMap\")\n     fileToDeleteKeyMap\n   }\n \n \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDcxOTczOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTowOToyNVrOGrBFzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNToxNDozMlrOGwNZhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1OTgyMA==", "bodyText": "Can you explain this change?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447759820", "createdAt": "2020-06-30T15:09:25Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -152,7 +209,7 @@ class DedupeSparkJob(basePath: String,\n       val newFilePath = new Path(s\"$repairOutputPath/${fileNameToPathMap(fileName).getName}\")\n       LOG.info(\" Skipping and writing new file for : \" + fileName)\n       SparkHelpers.skipKeysAndWriteNewFile(instantTime, fs, badFilePath, newFilePath, dupeFixPlan(fileName))\n-      fs.delete(badFilePath, false)\n+      fs.delete(badFilePath, true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNDM1Ng==", "bodyText": "We need to set it to true because somehow xyz.parquet.bad is getting considered as a directory and not a file. Hence to be able to delete, we need to set recursive to true. @yanghua", "url": "https://github.com/apache/hudi/pull/1558#discussion_r453204356", "createdAt": "2020-07-11T15:14:32Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -152,7 +209,7 @@ class DedupeSparkJob(basePath: String,\n       val newFilePath = new Path(s\"$repairOutputPath/${fileNameToPathMap(fileName).getName}\")\n       LOG.info(\" Skipping and writing new file for : \" + fileName)\n       SparkHelpers.skipKeysAndWriteNewFile(instantTime, fs, badFilePath, newFilePath, dupeFixPlan(fileName))\n-      fs.delete(badFilePath, false)\n+      fs.delete(badFilePath, true)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1OTgyMA=="}, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 143}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDczODE2OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNToxMzoxOFrOGrBRJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNToxMzoxOFrOGrBRJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MjcyNg==", "bodyText": "Can we rename the exists testDeduplicate  to testDeduplicateWithDefault or testDeduplicateWithInserts?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447762726", "createdAt": "2020-06-30T15:13:18Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -145,6 +166,60 @@ public void testDeduplicate() throws IOException {\n     assertEquals(200, result.count());\n   }\n \n+  @Test\n+  public void testDeduplicateWithUpdates() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzUwMTY2OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwMDo1OToxM1rOGwYj6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxOTo0OTo0N1rOHTB3Cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzM4NzI0Mw==", "bodyText": "Can we change it to be:\n      if (!DeDupeType.values().contains(DeDupeType.withName(dedupeType))) {}", "url": "https://github.com/apache/hudi/pull/1558#discussion_r453387243", "createdAt": "2020-07-13T00:59:13Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,8 +78,14 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(DeDupeType.INSERT_TYPE().toString()) && !dedupeType.equals(DeDupeType.UPDATE_TYPE().toString())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "59eb45ce254e1952a03ee8023604c13c96082862"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTcxNTQ2Nw==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r489715467", "createdAt": "2020-09-16T19:49:47Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,8 +78,14 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(DeDupeType.INSERT_TYPE().toString()) && !dedupeType.equals(DeDupeType.UPDATE_TYPE().toString())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzM4NzI0Mw=="}, "originalCommit": {"oid": "59eb45ce254e1952a03ee8023604c13c96082862"}, "originalPosition": 17}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4587, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}