{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ1MTY5MTE5", "number": 2374, "reviewThreads": {"totalCount": 108, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxODoxOVrOFht7jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMzowNjo0OVrOFiXhlA==", "hasNextPage": false, "hasPreviousPage": true}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk5ODU0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxODoxOVrOIwhagw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMToyMTozMVrOIwmjfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc1MDAxOQ==", "bodyText": "should we recreate? again, we need to revisit this whole passing of metadatawriter to resolve conflict. Don't understand this part", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587750019", "createdAt": "2021-03-04T19:18:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -401,13 +441,25 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   public void syncTableMetadata() {\n     // Open up the metadata table again, for syncing\n-    try (HoodieTableMetadataWriter writer = SparkHoodieBackedTableMetadataWriter.create(hadoopConf, config, context)) {\n+    try {\n+      HoodieTableMetadataWriter writer =\n+          SparkHoodieBackedTableMetadataWriter.create(hadoopConf, config, context);\n       LOG.info(\"Successfully synced to metadata table\");\n     } catch (Exception e) {\n       throw new HoodieMetadataException(\"Error syncing to metadata table.\", e);\n     }\n   }\n \n+  @Override\n+  protected void preCommit(String instantTime, HoodieCommitMetadata metadata) {\n+    // Create a Hoodie table after startTxn which encapsulated the commits and files visible.\n+    // Important to create this after the lock to ensure latest commits show up in the timeline without need for reload\n+    HoodieTable table = createTable(config, hadoopConf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgzNDIzOA==", "bodyText": "removed the metadtawriter.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587834238", "createdAt": "2021-03-04T21:21:31Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -401,13 +441,25 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   public void syncTableMetadata() {\n     // Open up the metadata table again, for syncing\n-    try (HoodieTableMetadataWriter writer = SparkHoodieBackedTableMetadataWriter.create(hadoopConf, config, context)) {\n+    try {\n+      HoodieTableMetadataWriter writer =\n+          SparkHoodieBackedTableMetadataWriter.create(hadoopConf, config, context);\n       LOG.info(\"Successfully synced to metadata table\");\n     } catch (Exception e) {\n       throw new HoodieMetadataException(\"Error syncing to metadata table.\", e);\n     }\n   }\n \n+  @Override\n+  protected void preCommit(String instantTime, HoodieCommitMetadata metadata) {\n+    // Create a Hoodie table after startTxn which encapsulated the commits and files visible.\n+    // Important to create this after the lock to ensure latest commits show up in the timeline without need for reload\n+    HoodieTable table = createTable(config, hadoopConf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc1MDAxOQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwOTAwNTMwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToyMDowMFrOIwhevA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQxOTowMjo0MVrOIxothQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc1MTEwMA==", "bodyText": "I think all this can be removed. from all action executors? can't we take the lock in post commit/write from the write client level?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587751100", "createdAt": "2021-03-04T19:20:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "diffHunk": "@@ -222,6 +225,17 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta\n     LOG.info(\"Committing metadata bootstrap !!\");\n   }\n \n+  @Override\n+  protected void syncTableMetadata() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzg3MDI3Ng==", "bodyText": "This is required due to autoCommit code in BaseCommitActionExecutor. We already have to take a lock in BaseCommitActionExecutor for committing the data, instead of taking a lock again in the write client level, I have moved this sync into the same critical section as commit.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587870276", "createdAt": "2021-03-04T22:24:19Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "diffHunk": "@@ -222,6 +225,17 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta\n     LOG.info(\"Committing metadata bootstrap !!\");\n   }\n \n+  @Override\n+  protected void syncTableMetadata() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc1MTEwMA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODc3MTY5MQ==", "bodyText": "can we please file a ticket for removing this auto commit stuff. it's kind of messy.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588771691", "createdAt": "2021-03-05T22:56:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "diffHunk": "@@ -222,6 +225,17 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta\n     LOG.info(\"Committing metadata bootstrap !!\");\n   }\n \n+  @Override\n+  protected void syncTableMetadata() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc1MTEwMA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODkxODE0OQ==", "bodyText": "https://issues.apache.org/jira/browse/HUDI-1665", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588918149", "createdAt": "2021-03-06T19:02:41Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "diffHunk": "@@ -222,6 +225,17 @@ protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMeta\n     LOG.info(\"Committing metadata bootstrap !!\");\n   }\n \n+  @Override\n+  protected void syncTableMetadata() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc1MTEwMA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxNTA1MTkwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDoxOTo0N1rOIxZGcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQxODo1OTozNVrOIxosWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY2MjM4Nw==", "bodyText": "lets file a JIRA for clustering/concurrent updates.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588662387", "createdAt": "2021-03-05T20:19:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.transaction;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getCandidateInstants(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    // NOTE that any commits from table services such as compaction, clustering or cleaning since the\n+    // overlapping of files is handled using MVCC.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : HoodieTimeline.INIT_INSTANT_TS)\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.warn(\"Found conflicting writes between first operation = \" + thisOperation\n+          + \", second operation = \" + otherOperation + \" , intersecting file ids \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Option<HoodieCommitMetadata> resolveConflict(HoodieTable table,\n+      HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // Since compaction is eventually written as commit, we need to ensure\n+    // we handle this during conflict resolution and not treat the commit from compaction operation\n+    // as a regular commit. Regular commit, deltacommits and replace are candidates for conflict\n+    // replace is used for a) clustering without update support b) insert_overwrite both of which are\n+    // candidates for conflict. We need to add CLUSTER here once it supports concurrent updates", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODkxNzg1MA==", "bodyText": "There is already one -> https://issues.apache.org/jira/browse/HUDI-1042", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588917850", "createdAt": "2021-03-06T18:59:35Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.transaction;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getCandidateInstants(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    // NOTE that any commits from table services such as compaction, clustering or cleaning since the\n+    // overlapping of files is handled using MVCC.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : HoodieTimeline.INIT_INSTANT_TS)\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.warn(\"Found conflicting writes between first operation = \" + thisOperation\n+          + \", second operation = \" + otherOperation + \" , intersecting file ids \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Option<HoodieCommitMetadata> resolveConflict(HoodieTable table,\n+      HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // Since compaction is eventually written as commit, we need to ensure\n+    // we handle this during conflict resolution and not treat the commit from compaction operation\n+    // as a regular commit. Regular commit, deltacommits and replace are candidates for conflict\n+    // replace is used for a) clustering without update support b) insert_overwrite both of which are\n+    // candidates for conflict. We need to add CLUSTER here once it supports concurrent updates", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY2MjM4Nw=="}, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxNTExMjQzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDoyODo1NFrOIxZqYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQxODo1Nzo0MVrOIxoriw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY3MTU4NQ==", "bodyText": "what about async clustering? would it raise an exception? may be it wont today, given its always picks non conflicting files alreayd?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588671585", "createdAt": "2021-03-05T20:28:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.transaction;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getCandidateInstants(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    // NOTE that any commits from table services such as compaction, clustering or cleaning since the\n+    // overlapping of files is handled using MVCC.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : HoodieTimeline.INIT_INSTANT_TS)\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.warn(\"Found conflicting writes between first operation = \" + thisOperation\n+          + \", second operation = \" + otherOperation + \" , intersecting file ids \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Option<HoodieCommitMetadata> resolveConflict(HoodieTable table,\n+      HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // Since compaction is eventually written as commit, we need to ensure\n+    // we handle this during conflict resolution and not treat the commit from compaction operation\n+    // as a regular commit. Regular commit, deltacommits and replace are candidates for conflict\n+    // replace is used for a) clustering without update support b) insert_overwrite both of which are\n+    // candidates for conflict. We need to add CLUSTER here once it supports concurrent updates\n+    // like COMPACT.\n+    if (otherOperation.getOperationType() == WriteOperationType.COMPACT", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY3MjUyNg==", "bodyText": "I just want to ensure that async clustering can happen, as long as the the files don't overlap.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588672526", "createdAt": "2021-03-05T20:29:52Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.transaction;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getCandidateInstants(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    // NOTE that any commits from table services such as compaction, clustering or cleaning since the\n+    // overlapping of files is handled using MVCC.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : HoodieTimeline.INIT_INSTANT_TS)\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.warn(\"Found conflicting writes between first operation = \" + thisOperation\n+          + \", second operation = \" + otherOperation + \" , intersecting file ids \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Option<HoodieCommitMetadata> resolveConflict(HoodieTable table,\n+      HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // Since compaction is eventually written as commit, we need to ensure\n+    // we handle this during conflict resolution and not treat the commit from compaction operation\n+    // as a regular commit. Regular commit, deltacommits and replace are candidates for conflict\n+    // replace is used for a) clustering without update support b) insert_overwrite both of which are\n+    // candidates for conflict. We need to add CLUSTER here once it supports concurrent updates\n+    // like COMPACT.\n+    if (otherOperation.getOperationType() == WriteOperationType.COMPACT", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY3MTU4NQ=="}, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODkxNzY0Mw==", "bodyText": "Yes, they will. I have a test case for it as well.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588917643", "createdAt": "2021-03-06T18:57:41Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.transaction;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getCandidateInstants(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    // NOTE that any commits from table services such as compaction, clustering or cleaning since the\n+    // overlapping of files is handled using MVCC.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : HoodieTimeline.INIT_INSTANT_TS)\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.warn(\"Found conflicting writes between first operation = \" + thisOperation\n+          + \", second operation = \" + otherOperation + \" , intersecting file ids \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Option<HoodieCommitMetadata> resolveConflict(HoodieTable table,\n+      HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // Since compaction is eventually written as commit, we need to ensure\n+    // we handle this during conflict resolution and not treat the commit from compaction operation\n+    // as a regular commit. Regular commit, deltacommits and replace are candidates for conflict\n+    // replace is used for a) clustering without update support b) insert_overwrite both of which are\n+    // candidates for conflict. We need to add CLUSTER here once it supports concurrent updates\n+    // like COMPACT.\n+    if (otherOperation.getOperationType() == WriteOperationType.COMPACT", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY3MTU4NQ=="}, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxNTE2MzM0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/ZookeeperBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDozNzoxMlrOIxaI3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDozNzoxMlrOIxaI3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY3OTM5MA==", "bodyText": "a better validation message, than just the enum name?  (applicable for any such usage)", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588679390", "createdAt": "2021-03-05T20:37:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.transaction.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private final CuratorFramework curatorFrameworkClient;\n+  private volatile InterProcessMutex lock = null;\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final Configuration conf) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+    this.curatorFrameworkClient = CuratorFrameworkFactory.builder()\n+        .connectString(lockConfiguration.getConfig().getString(ZK_CONNECT_URL_PROP))\n+        .retryPolicy(new BoundedExponentialBackoffRetry(lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP),\n+            5000, lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP)))\n+        .sessionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_SESSION_TIMEOUT_MS_PROP, DEFAULT_ZK_SESSION_TIMEOUT_MS))\n+        .connectionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_CONNECTION_TIMEOUT_MS_PROP, DEFAULT_ZK_CONNECTION_TIMEOUT_MS))\n+        .build();\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  // Only used for testing\n+  public ZookeeperBasedLockProvider(\n+      final LockConfiguration lockConfiguration, final CuratorFramework curatorFrameworkClient) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+    this.curatorFrameworkClient = curatorFrameworkClient;\n+    synchronized (this.curatorFrameworkClient) {\n+      if (this.curatorFrameworkClient.getState() != CuratorFrameworkState.STARTED) {\n+        this.curatorFrameworkClient.start();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    LOG.info(generateLogStatement(LockState.ACQUIRING, generateLogSuffixString()));\n+    try {\n+      acquireLock(time, unit);\n+      LOG.info(generateLogStatement(LockState.ACQUIRED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_ACQUIRE, generateLogSuffixString()), e);\n+    }\n+    return lock != null && lock.isAcquiredInThisProcess();\n+  }\n+\n+  @Override\n+  public void unlock() {\n+    try {\n+      LOG.info(generateLogStatement(LockState.RELEASING, generateLogSuffixString()));\n+      if (lock == null || !lock.isAcquiredInThisProcess()) {\n+        return;\n+      }\n+      lock.release();\n+      lock = null;\n+      LOG.info(generateLogStatement(LockState.RELEASED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_RELEASE, generateLogSuffixString()), e);\n+    }\n+  }\n+\n+  @Override\n+  public void close() {\n+    try {\n+      if (lock != null) {\n+        lock.release();\n+        lock = null;\n+      }\n+      this.curatorFrameworkClient.close();\n+    } catch (Exception e) {\n+      LOG.error(generateLogStatement(LockState.FAILED_TO_RELEASE, generateLogSuffixString()));\n+    }\n+  }\n+\n+  @Override\n+  public InterProcessMutex getLock() {\n+    return this.lock;\n+  }\n+\n+  private void acquireLock(long time, TimeUnit unit) throws Exception {\n+    ValidationUtils.checkArgument(this.lock == null, LockState.ALREADY_ACQUIRED.name());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxNTE3NzMzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/ZookeeperBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDozOToyOFrOIxaRUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDo0MDowOFrOIxaTww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY4MTU1NA==", "bodyText": "this will lead to a LockException being wrapped inside another LockException in tryLock()?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588681554", "createdAt": "2021-03-05T20:39:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.transaction.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private final CuratorFramework curatorFrameworkClient;\n+  private volatile InterProcessMutex lock = null;\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final Configuration conf) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+    this.curatorFrameworkClient = CuratorFrameworkFactory.builder()\n+        .connectString(lockConfiguration.getConfig().getString(ZK_CONNECT_URL_PROP))\n+        .retryPolicy(new BoundedExponentialBackoffRetry(lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP),\n+            5000, lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP)))\n+        .sessionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_SESSION_TIMEOUT_MS_PROP, DEFAULT_ZK_SESSION_TIMEOUT_MS))\n+        .connectionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_CONNECTION_TIMEOUT_MS_PROP, DEFAULT_ZK_CONNECTION_TIMEOUT_MS))\n+        .build();\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  // Only used for testing\n+  public ZookeeperBasedLockProvider(\n+      final LockConfiguration lockConfiguration, final CuratorFramework curatorFrameworkClient) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+    this.curatorFrameworkClient = curatorFrameworkClient;\n+    synchronized (this.curatorFrameworkClient) {\n+      if (this.curatorFrameworkClient.getState() != CuratorFrameworkState.STARTED) {\n+        this.curatorFrameworkClient.start();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    LOG.info(generateLogStatement(LockState.ACQUIRING, generateLogSuffixString()));\n+    try {\n+      acquireLock(time, unit);\n+      LOG.info(generateLogStatement(LockState.ACQUIRED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_ACQUIRE, generateLogSuffixString()), e);\n+    }\n+    return lock != null && lock.isAcquiredInThisProcess();\n+  }\n+\n+  @Override\n+  public void unlock() {\n+    try {\n+      LOG.info(generateLogStatement(LockState.RELEASING, generateLogSuffixString()));\n+      if (lock == null || !lock.isAcquiredInThisProcess()) {\n+        return;\n+      }\n+      lock.release();\n+      lock = null;\n+      LOG.info(generateLogStatement(LockState.RELEASED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_RELEASE, generateLogSuffixString()), e);\n+    }\n+  }\n+\n+  @Override\n+  public void close() {\n+    try {\n+      if (lock != null) {\n+        lock.release();\n+        lock = null;\n+      }\n+      this.curatorFrameworkClient.close();\n+    } catch (Exception e) {\n+      LOG.error(generateLogStatement(LockState.FAILED_TO_RELEASE, generateLogSuffixString()));\n+    }\n+  }\n+\n+  @Override\n+  public InterProcessMutex getLock() {\n+    return this.lock;\n+  }\n+\n+  private void acquireLock(long time, TimeUnit unit) throws Exception {\n+    ValidationUtils.checkArgument(this.lock == null, LockState.ALREADY_ACQUIRED.name());\n+    InterProcessMutex newLock = new InterProcessMutex(\n+        this.curatorFrameworkClient, lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP) + \"/\"\n+        + this.lockConfiguration.getConfig().getString(ZK_LOCK_KEY_PROP));\n+    boolean acquired = newLock.acquire(time, unit);\n+    if (!acquired) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_ACQUIRE, generateLogSuffixString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY4MjE3OQ==", "bodyText": "Can we handle this specially and just rethrow the lock exception without the wrapping.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588682179", "createdAt": "2021-03-05T20:40:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/transaction/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.transaction.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private final CuratorFramework curatorFrameworkClient;\n+  private volatile InterProcessMutex lock = null;\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final Configuration conf) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+    this.curatorFrameworkClient = CuratorFrameworkFactory.builder()\n+        .connectString(lockConfiguration.getConfig().getString(ZK_CONNECT_URL_PROP))\n+        .retryPolicy(new BoundedExponentialBackoffRetry(lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP),\n+            5000, lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP)))\n+        .sessionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_SESSION_TIMEOUT_MS_PROP, DEFAULT_ZK_SESSION_TIMEOUT_MS))\n+        .connectionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_CONNECTION_TIMEOUT_MS_PROP, DEFAULT_ZK_CONNECTION_TIMEOUT_MS))\n+        .build();\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  // Only used for testing\n+  public ZookeeperBasedLockProvider(\n+      final LockConfiguration lockConfiguration, final CuratorFramework curatorFrameworkClient) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+    this.curatorFrameworkClient = curatorFrameworkClient;\n+    synchronized (this.curatorFrameworkClient) {\n+      if (this.curatorFrameworkClient.getState() != CuratorFrameworkState.STARTED) {\n+        this.curatorFrameworkClient.start();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    LOG.info(generateLogStatement(LockState.ACQUIRING, generateLogSuffixString()));\n+    try {\n+      acquireLock(time, unit);\n+      LOG.info(generateLogStatement(LockState.ACQUIRED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_ACQUIRE, generateLogSuffixString()), e);\n+    }\n+    return lock != null && lock.isAcquiredInThisProcess();\n+  }\n+\n+  @Override\n+  public void unlock() {\n+    try {\n+      LOG.info(generateLogStatement(LockState.RELEASING, generateLogSuffixString()));\n+      if (lock == null || !lock.isAcquiredInThisProcess()) {\n+        return;\n+      }\n+      lock.release();\n+      lock = null;\n+      LOG.info(generateLogStatement(LockState.RELEASED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_RELEASE, generateLogSuffixString()), e);\n+    }\n+  }\n+\n+  @Override\n+  public void close() {\n+    try {\n+      if (lock != null) {\n+        lock.release();\n+        lock = null;\n+      }\n+      this.curatorFrameworkClient.close();\n+    } catch (Exception e) {\n+      LOG.error(generateLogStatement(LockState.FAILED_TO_RELEASE, generateLogSuffixString()));\n+    }\n+  }\n+\n+  @Override\n+  public InterProcessMutex getLock() {\n+    return this.lock;\n+  }\n+\n+  private void acquireLock(long time, TimeUnit unit) throws Exception {\n+    ValidationUtils.checkArgument(this.lock == null, LockState.ALREADY_ACQUIRED.name());\n+    InterProcessMutex newLock = new InterProcessMutex(\n+        this.curatorFrameworkClient, lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP) + \"/\"\n+        + this.lockConfiguration.getConfig().getString(ZK_LOCK_KEY_PROP));\n+    boolean acquired = newLock.acquire(time, unit);\n+    if (!acquired) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_ACQUIRE, generateLogSuffixString()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY4MTU1NA=="}, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 139}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxNTgxMjYwOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMzowNjozNFrOIxf--w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQwOToxMzoxOVrOIxlAlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODc3NTE2Mw==", "bodyText": "is this a recursive call? won't it recurse infinitely?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588775163", "createdAt": "2021-03-05T23:06:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockProvider.class);\n+\n+  protected LockConfiguration lockConfiguration;\n+\n+  @Override\n+  public final void lockInterruptibly() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final void lock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final boolean tryLock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    try {\n+      return tryLock(time, unit);\n+    } catch (Exception e) {\n+      throw new HoodieLockException(e);\n+    }\n+  }\n+\n+  public T getLock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final Condition newCondition() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void close() {\n+    try {\n+      close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODg1NzQ5Mw==", "bodyText": "Intended to super.close()", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588857493", "createdAt": "2021-03-06T09:13:19Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockProvider.class);\n+\n+  protected LockConfiguration lockConfiguration;\n+\n+  @Override\n+  public final void lockInterruptibly() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final void lock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final boolean tryLock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    try {\n+      return tryLock(time, unit);\n+    } catch (Exception e) {\n+      throw new HoodieLockException(e);\n+    }\n+  }\n+\n+  public T getLock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final Condition newCondition() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public void close() {\n+    try {\n+      close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODc3NTE2Mw=="}, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcxNTgxMzMyOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMzowNjo1MFrOIxf_Yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQwOToxMzoyN1rOIxlAmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODc3NTI2Nw==", "bodyText": "same here. infinite loop?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588775267", "createdAt": "2021-03-05T23:06:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockProvider.class);\n+\n+  protected LockConfiguration lockConfiguration;\n+\n+  @Override\n+  public final void lockInterruptibly() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final void lock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final boolean tryLock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    try {\n+      return tryLock(time, unit);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODg1NzQ5OQ==", "bodyText": "same as above", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588857499", "createdAt": "2021-03-06T09:13:27Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockProvider.class);\n+\n+  protected LockConfiguration lockConfiguration;\n+\n+  @Override\n+  public final void lockInterruptibly() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final void lock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final boolean tryLock() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    try {\n+      return tryLock(time, unit);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODc3NTI2Nw=="}, "originalCommit": {"oid": "4dd32416d378254600bd1a810ed6dc0c2dbec2b6"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgwNDk0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowMDoyMVrOIMF6SA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowMTowOFrOIMF6pA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MDY2NA==", "bodyText": "AtomicReference to handle multiple threads using the same client object? +1", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549550664", "createdAt": "2020-12-29T03:00:21Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieClient.java", "diffHunk": "@@ -48,6 +50,7 @@\n   protected final transient Configuration hadoopConf;\n   protected final HoodieWriteConfig config;\n   protected final String basePath;\n+  protected AtomicReference<Option<HoodieInstant>> latestWriteInstantCompletedBeforeWriter = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MDc1Ng==", "bodyText": "Since this is specific to writing, can we put this in the AbstractHoodieWriteClient subclass?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549550756", "createdAt": "2020-12-29T03:01:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieClient.java", "diffHunk": "@@ -48,6 +50,7 @@\n   protected final transient Configuration hadoopConf;\n   protected final HoodieWriteConfig config;\n   protected final String basePath;\n+  protected AtomicReference<Option<HoodieInstant>> latestWriteInstantCompletedBeforeWriter = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MDY2NA=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgwNzE1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowMjo0MFrOIMF7gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wMlQwNjo1MjoyOFrOINVVHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MDk3Ng==", "bodyText": "use the concurrency mode config introduced by the other PR and use that instead to fence this block?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549550976", "createdAt": "2020-12-29T03:02:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDg1MTg2OQ==", "bodyText": "That's the plan, once it's landed will change this, for now, this config is just a place holder.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550851869", "createdAt": "2021-01-02T06:52:28Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MDk3Ng=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgwNzYwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowMzowOFrOIMF7vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowMzowOFrOIMF7vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTAzNg==", "bodyText": "some more context on basePath, inner exception if any ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549551036", "createdAt": "2020-12-29T03:03:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {\n+      // get strategy and lock type\n+      LockConfiguration lockConfiguration = new LockConfiguration(config.getProps());\n+      LockProvider lockProvider = (LockProvider) ReflectionUtils.loadClass(config.getLockProviderClass(),\n+          lockConfiguration, fs.getConf());\n+      try {\n+        // TODO : Get timeout and set the timeout\n+        boolean acquired = lockProvider.tryLock();\n+        if (!acquired) {\n+          throw new HoodieException(\"Unable to acquire lock\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgwODU3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowMzo0N1rOIMF8Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODowNzo1M1rOINcqog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTE2Mg==", "bodyText": "is TODO still valid?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549551162", "createdAt": "2020-12-29T03:03:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {\n+      // get strategy and lock type\n+      LockConfiguration lockConfiguration = new LockConfiguration(config.getProps());\n+      LockProvider lockProvider = (LockProvider) ReflectionUtils.loadClass(config.getLockProviderClass(),\n+          lockConfiguration, fs.getConf());\n+      try {\n+        // TODO : Get timeout and set the timeout\n+        boolean acquired = lockProvider.tryLock();\n+        if (!acquired) {\n+          throw new HoodieException(\"Unable to acquire lock\");\n+        }\n+        LOG.info(\"Acquired lock for instant time \" + instantTime);\n+        // TODO : Move the following to a \"critical section\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3MjA2Ng==", "bodyText": "Not anymore", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550972066", "createdAt": "2021-01-03T08:07:53Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {\n+      // get strategy and lock type\n+      LockConfiguration lockConfiguration = new LockConfiguration(config.getProps());\n+      LockProvider lockProvider = (LockProvider) ReflectionUtils.loadClass(config.getLockProviderClass(),\n+          lockConfiguration, fs.getConf());\n+      try {\n+        // TODO : Get timeout and set the timeout\n+        boolean acquired = lockProvider.tryLock();\n+        if (!acquired) {\n+          throw new HoodieException(\"Unable to acquire lock\");\n+        }\n+        LOG.info(\"Acquired lock for instant time \" + instantTime);\n+        // TODO : Move the following to a \"critical section\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTE2Mg=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgwOTEwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNDoyM1rOIMF8hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNDoyM1rOIMF8hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTIzNw==", "bodyText": "We need a new Exception class here. HoodieWriteConflictException or sth", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549551237", "createdAt": "2020-12-29T03:04:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {\n+      // get strategy and lock type\n+      LockConfiguration lockConfiguration = new LockConfiguration(config.getProps());\n+      LockProvider lockProvider = (LockProvider) ReflectionUtils.loadClass(config.getLockProviderClass(),\n+          lockConfiguration, fs.getConf());\n+      try {\n+        // TODO : Get timeout and set the timeout\n+        boolean acquired = lockProvider.tryLock();\n+        if (!acquired) {\n+          throw new HoodieException(\"Unable to acquire lock\");\n+        }\n+        LOG.info(\"Acquired lock for instant time \" + instantTime);\n+        // TODO : Move the following to a \"critical section\"\n+        // Create a Hoodie table which encapsulated the commits and files visible.\n+        // Important to create this after the lock to ensure latest commits show up in the timeline without need for reload\n+        HoodieTable table = createTable(config, hadoopConf);\n+        try {\n+          metadata = resolveWriteConflictIfAny(table, instantTime, metadata);\n+          return commitStats(instantTime, stats, extraMetadata, commitActionType, Collections.emptyMap(), metadata);\n+        } catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgwOTU0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNDozNlrOIMF8uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNDozNlrOIMF8uQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTI4OQ==", "bodyText": "why unlock here again, when you do it in finally", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549551289", "createdAt": "2020-12-29T03:04:36Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {\n+      // get strategy and lock type\n+      LockConfiguration lockConfiguration = new LockConfiguration(config.getProps());\n+      LockProvider lockProvider = (LockProvider) ReflectionUtils.loadClass(config.getLockProviderClass(),\n+          lockConfiguration, fs.getConf());\n+      try {\n+        // TODO : Get timeout and set the timeout\n+        boolean acquired = lockProvider.tryLock();\n+        if (!acquired) {\n+          throw new HoodieException(\"Unable to acquire lock\");\n+        }\n+        LOG.info(\"Acquired lock for instant time \" + instantTime);\n+        // TODO : Move the following to a \"critical section\"\n+        // Create a Hoodie table which encapsulated the commits and files visible.\n+        // Important to create this after the lock to ensure latest commits show up in the timeline without need for reload\n+        HoodieTable table = createTable(config, hadoopConf);\n+        try {\n+          metadata = resolveWriteConflictIfAny(table, instantTime, metadata);\n+          return commitStats(instantTime, stats, extraMetadata, commitActionType, Collections.emptyMap(), metadata);\n+        } catch (Exception e) {\n+          // if strategy throws exception, first release lock\n+          lockProvider.unlock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgxMjExOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNjozNlrOIMF-BA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNjozNlrOIMF-BA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTYyMA==", "bodyText": "I was expecting to use the atomic reference above. and just pull all the writes that happened after the latestWriteInstantCompletedBeforeWriter ? Also lets not assume that time in seconds is what we will use here.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549551620", "createdAt": "2020-12-29T03:06:36Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -821,6 +865,38 @@ protected void finalizeWrite(HoodieTable<T, I, K, O> table, String instantTime,\n     }\n   }\n \n+  private HoodieCommitMetadata resolveWriteConflictIfAny(HoodieTable<T, I, K, O> table, final String instantTime,\n+                                            HoodieCommitMetadata thisCommitMetadata) {\n+    Long currentTimeInSecs = System.currentTimeMillis() / 1000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgxMzIwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNzoxNlrOIMF-ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wMlQwNzowNTo0N1rOINVZBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTc1NA==", "bodyText": "can we move the reflection part to the config object?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549551754", "createdAt": "2020-12-29T03:07:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {\n+      // get strategy and lock type\n+      LockConfiguration lockConfiguration = new LockConfiguration(config.getProps());\n+      LockProvider lockProvider = (LockProvider) ReflectionUtils.loadClass(config.getLockProviderClass(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDg1Mjg2OQ==", "bodyText": "Already have it that way for ConflictResolutionStrategy, but this reflection requires parameters hence kept it here. If we moved this to config object, we have to make the getLockProviderClass parameterized which will be a problem for clients to get just the class name. Other option is to have 2 overloaded getters. Let me know what you think.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550852869", "createdAt": "2021-01-02T07:05:47Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -203,6 +208,45 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+\n+    HoodieCommitMetadata metadata = CommitUtils.buildMetadata(stats, partitionToReplaceFileIds, extraMetadata,\n+        operationType, config.getSchema(), commitActionType);\n+    if (config.isMultiWriterEnabled()) {\n+      // get strategy and lock type\n+      LockConfiguration lockConfiguration = new LockConfiguration(config.getProps());\n+      LockProvider lockProvider = (LockProvider) ReflectionUtils.loadClass(config.getLockProviderClass(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTc1NA=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgxMzU4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNzozNFrOIMF-tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzowNzozNFrOIMF-tA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MTc5Ng==", "bodyText": "typos", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549551796", "createdAt": "2020-12-29T03:07:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -821,6 +865,38 @@ protected void finalizeWrite(HoodieTable<T, I, K, O> table, String instantTime,\n     }\n   }\n \n+  private HoodieCommitMetadata resolveWriteConflictIfAny(HoodieTable<T, I, K, O> table, final String instantTime,\n+                                            HoodieCommitMetadata thisCommitMetadata) {\n+    Long currentTimeInSecs = System.currentTimeMillis() / 1000;\n+    ConflictResolutionStrategy resolutionStrategy = config.getWriteConflictResolutionStrategy();\n+    Option<HoodieInstant> lastCompletedInstantBeforeWriterStarted = this.latestWriteInstantCompletedBeforeWriter.get();\n+    String lastInstantTimestamp = lastCompletedInstantBeforeWriterStarted.isPresent()\n+        ? lastCompletedInstantBeforeWriterStarted.get().getTimestamp() : \"0\";\n+    Stream<HoodieInstant> instantStream = table.getActiveTimeline()\n+        .getAllCommitsTimeline()\n+        .getCommitsAndCompactionTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsInRange(lastInstantTimestamp, String.valueOf(currentTimeInSecs))\n+        .getInstants();\n+\n+    LOG.info(\"Current eligible instants during write oHoodieWriteConfigf instant \" + instantTime + \" = \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgxNjg5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxMDoxMFrOIMGAcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODowODowNlrOINcqtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MjI0MQ==", "bodyText": "we should probably have an API to resolveConflict(instant1, instant2) ? i.e like our preCombine method? that will give resolutionStrategy implementors to just reason between two instants and whether they conflict or not, and if they do, then munge the commit metadata somehow.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549552241", "createdAt": "2020-12-29T03:10:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -821,6 +865,38 @@ protected void finalizeWrite(HoodieTable<T, I, K, O> table, String instantTime,\n     }\n   }\n \n+  private HoodieCommitMetadata resolveWriteConflictIfAny(HoodieTable<T, I, K, O> table, final String instantTime,\n+                                            HoodieCommitMetadata thisCommitMetadata) {\n+    Long currentTimeInSecs = System.currentTimeMillis() / 1000;\n+    ConflictResolutionStrategy resolutionStrategy = config.getWriteConflictResolutionStrategy();\n+    Option<HoodieInstant> lastCompletedInstantBeforeWriterStarted = this.latestWriteInstantCompletedBeforeWriter.get();\n+    String lastInstantTimestamp = lastCompletedInstantBeforeWriterStarted.isPresent()\n+        ? lastCompletedInstantBeforeWriterStarted.get().getTimestamp() : \"0\";\n+    Stream<HoodieInstant> instantStream = table.getActiveTimeline()\n+        .getAllCommitsTimeline()\n+        .getCommitsAndCompactionTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsInRange(lastInstantTimestamp, String.valueOf(currentTimeInSecs))\n+        .getInstants();\n+\n+    LOG.info(\"Current eligible instants during write oHoodieWriteConfigf instant \" + instantTime + \" = \"\n+        + instantStream.collect(Collectors.toList()));\n+\n+    boolean hasConflict = instantStream.anyMatch(instant -> {\n+      try {\n+        return resolutionStrategy.hasConflict(thisCommitMetadata, HoodieCommitMetadata.fromBytes(\n+            table.getActiveTimeline().getInstantDetails(instant).get(), HoodieCommitMetadata.class));\n+      } catch (IOException io) {\n+        throw new HoodieCommitException(\"Unable to determine if conflict exists\", io);\n+      }\n+    });\n+    HoodieCommitMetadata newMetadataAfterConflictResolution = thisCommitMetadata;\n+    if (hasConflict) {\n+      newMetadataAfterConflictResolution = resolutionStrategy.resolveConflict(config, hadoopConf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3MjA4NA==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550972084", "createdAt": "2021-01-03T08:08:06Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -821,6 +865,38 @@ protected void finalizeWrite(HoodieTable<T, I, K, O> table, String instantTime,\n     }\n   }\n \n+  private HoodieCommitMetadata resolveWriteConflictIfAny(HoodieTable<T, I, K, O> table, final String instantTime,\n+                                            HoodieCommitMetadata thisCommitMetadata) {\n+    Long currentTimeInSecs = System.currentTimeMillis() / 1000;\n+    ConflictResolutionStrategy resolutionStrategy = config.getWriteConflictResolutionStrategy();\n+    Option<HoodieInstant> lastCompletedInstantBeforeWriterStarted = this.latestWriteInstantCompletedBeforeWriter.get();\n+    String lastInstantTimestamp = lastCompletedInstantBeforeWriterStarted.isPresent()\n+        ? lastCompletedInstantBeforeWriterStarted.get().getTimestamp() : \"0\";\n+    Stream<HoodieInstant> instantStream = table.getActiveTimeline()\n+        .getAllCommitsTimeline()\n+        .getCommitsAndCompactionTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsInRange(lastInstantTimestamp, String.valueOf(currentTimeInSecs))\n+        .getInstants();\n+\n+    LOG.info(\"Current eligible instants during write oHoodieWriteConfigf instant \" + instantTime + \" = \"\n+        + instantStream.collect(Collectors.toList()));\n+\n+    boolean hasConflict = instantStream.anyMatch(instant -> {\n+      try {\n+        return resolutionStrategy.hasConflict(thisCommitMetadata, HoodieCommitMetadata.fromBytes(\n+            table.getActiveTimeline().getInstantDetails(instant).get(), HoodieCommitMetadata.class));\n+      } catch (IOException io) {\n+        throw new HoodieCommitException(\"Unable to determine if conflict exists\", io);\n+      }\n+    });\n+    HoodieCommitMetadata newMetadataAfterConflictResolution = thisCommitMetadata;\n+    if (hasConflict) {\n+      newMetadataAfterConflictResolution = resolutionStrategy.resolveConflict(config, hadoopConf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MjI0MQ=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgxNzM3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConcurrentFileWritesConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxMDo1MVrOIMGAug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxMDo1MVrOIMGAug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MjMxNA==", "bodyText": "throw a special purpose exception here please", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549552314", "createdAt": "2020-12-29T03:10:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+public class ConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(ConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitMetadata firstInstant, HoodieCommitMetadata secondInstant) {\n+    // TODO : Ensure file ids are only the UUID and not the full name, also cann UUID's clash then for insert/insert ?\n+    Set<String> fileIdsSetForFirstInstant = firstInstant.getFileIdWithoutSuffixAndRelativePaths().keySet();\n+    Set<String> fileIdsSetForSecondInstant = secondInstant.getFileIdWithoutSuffixAndRelativePaths().keySet();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public HoodieCommitMetadata resolveConflict(HoodieWriteConfig config, Configuration configuration) {\n+    throw new UnsupportedOperationException(\"Cannot resolve conflicts for overlapping writes\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgxNzQ2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConcurrentFileWritesConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxMTowNFrOIMGAzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxMTowNFrOIMGAzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MjMzMg==", "bodyText": "more information on these logs", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549552332", "createdAt": "2020-12-29T03:11:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+public class ConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(ConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitMetadata firstInstant, HoodieCommitMetadata secondInstant) {\n+    // TODO : Ensure file ids are only the UUID and not the full name, also cann UUID's clash then for insert/insert ?\n+    Set<String> fileIdsSetForFirstInstant = firstInstant.getFileIdWithoutSuffixAndRelativePaths().keySet();\n+    Set<String> fileIdsSetForSecondInstant = secondInstant.getFileIdWithoutSuffixAndRelativePaths().keySet();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes \" + intersection);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgxODIwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxMTo0OFrOIMGBMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxMTo0OFrOIMGBMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MjQzNQ==", "bodyText": "please add APIMaturity annotations and javadocs.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549552435", "createdAt": "2020-12-29T03:11:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,35 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+/**\n+ * Strategy for conflict resolution with multiple writers. Provide pluggable implementations for different\n+ * kinds of strategies to execute to resolve conflicts when multiple writers are mutating the hoodie table.\n+ */\n+public interface ConflictResolutionStrategy {\n+\n+  boolean hasConflict(HoodieCommitMetadata c1, HoodieCommitMetadata c2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgxOTA2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/FileSystemBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxMjo0MFrOIMGBpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODowODozMVrOINcq8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MjU0OQ==", "bodyText": "please use FSUtils.getFS() always ! to get the filesystem object", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549552549", "createdAt": "2020-12-29T03:12:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/FileSystemBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieLockException;\n+\n+import java.io.IOException;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.FILESYSTEM_LOCK_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+/**\n+ * This lock provider is used to testing purposes only. It provides a simple file system based lock using HDFS atomic\n+ * create operation. This lock does not support cleaning/expiring the lock after a failed write hence cannot be used\n+ * in production environments.\n+ */\n+public class FileSystemBasedLockProvider extends LockProvider {\n+\n+  private static final String LOCK_NAME = \"acquired\";\n+\n+  private String lockPath;\n+  private FileSystem fs;\n+\n+  public FileSystemBasedLockProvider(LockConfiguration lockConfiguration, final Configuration configuration) {\n+    try {\n+      this.lockConfiguration = lockConfiguration;\n+      this.lockPath = lockConfiguration.getConfig().getString(FILESYSTEM_LOCK_PATH_PROP);\n+      this.fs = FileSystem.get(configuration);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3MjE0NA==", "bodyText": "Moved this to test scope", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550972144", "createdAt": "2021-01-03T08:08:31Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/FileSystemBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieLockException;\n+\n+import java.io.IOException;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.FILESYSTEM_LOCK_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+/**\n+ * This lock provider is used to testing purposes only. It provides a simple file system based lock using HDFS atomic\n+ * create operation. This lock does not support cleaning/expiring the lock after a failed write hence cannot be used\n+ * in production environments.\n+ */\n+public class FileSystemBasedLockProvider extends LockProvider {\n+\n+  private static final String LOCK_NAME = \"acquired\";\n+\n+  private String lockPath;\n+  private FileSystem fs;\n+\n+  public FileSystemBasedLockProvider(LockConfiguration lockConfiguration, final Configuration configuration) {\n+    try {\n+      this.lockConfiguration = lockConfiguration;\n+      this.lockPath = lockConfiguration.getConfig().getString(FILESYSTEM_LOCK_PATH_PROP);\n+      this.fs = FileSystem.get(configuration);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MjU0OQ=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyMjE3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/FileSystemBasedLockProvider.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxNTowOVrOIMGDOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDowNToxNVrOINabZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1Mjk1NA==", "bodyText": "whats the guarantee that unlock() will fail for thread B, if thread A had actually created the lock file. Don't think this will work. We should remove this implementation, if we cannot get it right IMO", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549552954", "createdAt": "2020-12-29T03:15:09Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/FileSystemBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieLockException;\n+\n+import java.io.IOException;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.FILESYSTEM_LOCK_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+/**\n+ * This lock provider is used to testing purposes only. It provides a simple file system based lock using HDFS atomic\n+ * create operation. This lock does not support cleaning/expiring the lock after a failed write hence cannot be used\n+ * in production environments.\n+ */\n+public class FileSystemBasedLockProvider extends LockProvider {\n+\n+  private static final String LOCK_NAME = \"acquired\";\n+\n+  private String lockPath;\n+  private FileSystem fs;\n+\n+  public FileSystemBasedLockProvider(LockConfiguration lockConfiguration, final Configuration configuration) {\n+    try {\n+      this.lockConfiguration = lockConfiguration;\n+      this.lockPath = lockConfiguration.getConfig().getString(FILESYSTEM_LOCK_PATH_PROP);\n+      this.fs = FileSystem.get(configuration);\n+    } catch (IOException io) {\n+      throw new HoodieIOException(\"Unable to create file systems\", io);\n+    }\n+  }\n+\n+  @Override\n+  public void acquireLock() {\n+    try {\n+      fs.create(new Path(lockPath + \"/\" + LOCK_NAME)).close();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to acquire lock\", e);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    fs.close();\n+  }\n+\n+  @Override\n+  public boolean tryLock() {\n+    try {\n+      int numRetries = 0;\n+      while (fs.exists(new Path(lockPath + \"/\" + LOCK_NAME))\n+          && (numRetries <= lockConfiguration.getConfig().getInteger(HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP))) {\n+        Thread.sleep(lockConfiguration.getConfig().getInteger(HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP));\n+      }\n+      acquireLock();\n+      return true;\n+    } catch (IOException | InterruptedException e) {\n+      throw new HoodieLockException(\"Failed to acquire lock\", e);\n+    }\n+  }\n+\n+  @Override\n+  public void unlock() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzAyNA==", "bodyText": "which is indeed hard. I dont its possible on cloud stores.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553024", "createdAt": "2020-12-29T03:15:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/FileSystemBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieLockException;\n+\n+import java.io.IOException;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.FILESYSTEM_LOCK_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+/**\n+ * This lock provider is used to testing purposes only. It provides a simple file system based lock using HDFS atomic\n+ * create operation. This lock does not support cleaning/expiring the lock after a failed write hence cannot be used\n+ * in production environments.\n+ */\n+public class FileSystemBasedLockProvider extends LockProvider {\n+\n+  private static final String LOCK_NAME = \"acquired\";\n+\n+  private String lockPath;\n+  private FileSystem fs;\n+\n+  public FileSystemBasedLockProvider(LockConfiguration lockConfiguration, final Configuration configuration) {\n+    try {\n+      this.lockConfiguration = lockConfiguration;\n+      this.lockPath = lockConfiguration.getConfig().getString(FILESYSTEM_LOCK_PATH_PROP);\n+      this.fs = FileSystem.get(configuration);\n+    } catch (IOException io) {\n+      throw new HoodieIOException(\"Unable to create file systems\", io);\n+    }\n+  }\n+\n+  @Override\n+  public void acquireLock() {\n+    try {\n+      fs.create(new Path(lockPath + \"/\" + LOCK_NAME)).close();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to acquire lock\", e);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    fs.close();\n+  }\n+\n+  @Override\n+  public boolean tryLock() {\n+    try {\n+      int numRetries = 0;\n+      while (fs.exists(new Path(lockPath + \"/\" + LOCK_NAME))\n+          && (numRetries <= lockConfiguration.getConfig().getInteger(HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP))) {\n+        Thread.sleep(lockConfiguration.getConfig().getInteger(HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP));\n+      }\n+      acquireLock();\n+      return true;\n+    } catch (IOException | InterruptedException e) {\n+      throw new HoodieLockException(\"Failed to acquire lock\", e);\n+    }\n+  }\n+\n+  @Override\n+  public void unlock() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1Mjk1NA=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNTM5Ng==", "bodyText": "This was just implemented for my testing (mentioned in the comments), I've moved it to test scope.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550935396", "createdAt": "2021-01-03T00:05:15Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/FileSystemBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieLockException;\n+\n+import java.io.IOException;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.FILESYSTEM_LOCK_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+/**\n+ * This lock provider is used to testing purposes only. It provides a simple file system based lock using HDFS atomic\n+ * create operation. This lock does not support cleaning/expiring the lock after a failed write hence cannot be used\n+ * in production environments.\n+ */\n+public class FileSystemBasedLockProvider extends LockProvider {\n+\n+  private static final String LOCK_NAME = \"acquired\";\n+\n+  private String lockPath;\n+  private FileSystem fs;\n+\n+  public FileSystemBasedLockProvider(LockConfiguration lockConfiguration, final Configuration configuration) {\n+    try {\n+      this.lockConfiguration = lockConfiguration;\n+      this.lockPath = lockConfiguration.getConfig().getString(FILESYSTEM_LOCK_PATH_PROP);\n+      this.fs = FileSystem.get(configuration);\n+    } catch (IOException io) {\n+      throw new HoodieIOException(\"Unable to create file systems\", io);\n+    }\n+  }\n+\n+  @Override\n+  public void acquireLock() {\n+    try {\n+      fs.create(new Path(lockPath + \"/\" + LOCK_NAME)).close();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to acquire lock\", e);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    fs.close();\n+  }\n+\n+  @Override\n+  public boolean tryLock() {\n+    try {\n+      int numRetries = 0;\n+      while (fs.exists(new Path(lockPath + \"/\" + LOCK_NAME))\n+          && (numRetries <= lockConfiguration.getConfig().getInteger(HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP))) {\n+        Thread.sleep(lockConfiguration.getConfig().getInteger(HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP));\n+      }\n+      acquireLock();\n+      return true;\n+    } catch (IOException | InterruptedException e) {\n+      throw new HoodieLockException(\"Failed to acquire lock\", e);\n+    }\n+  }\n+\n+  @Override\n+  public void unlock() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1Mjk1NA=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyMzUzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxNjowN1rOIMGD4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxNjowN1rOIMGD4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzEyMg==", "bodyText": "more context?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553122", "createdAt": "2020-12-29T03:16:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private CuratorFramework curatorFrameworkClient;\n+  private final AtomicReference<InterProcessMutex> lock = new AtomicReference<>();\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final Configuration conf) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = CuratorFrameworkFactory.builder()\n+        .connectString(lockConfiguration.getConfig().getString(ZK_CONNECT_URL_PROP))\n+        .retryPolicy(new BoundedExponentialBackoffRetry(lockConfiguration.getConfig().getInteger(HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP),\n+            5000, lockConfiguration.getConfig().getInteger(HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP)))\n+        .namespace(lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP))\n+        .sessionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_SESSION_TIMEOUT_MS_PROP, DEFAULT_ZK_SESSION_TIMEOUT_MS))\n+        .connectionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_CONNECTION_TIMEOUT_MS_PROP, DEFAULT_ZK_CONNECTION_TIMEOUT_MS))\n+        .build();\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final CuratorFramework curatorFramework) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = curatorFramework;\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+  }\n+  \n+  @Override\n+  public void acquireLock() throws Exception {\n+    ValidationUtils.checkArgument(this.lock.get() == null, \"Lock is already acquired\");\n+    InterProcessMutex newLock = new InterProcessMutex(\n+        this.curatorFrameworkClient, lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP) + \"/\"\n+        + this.lockConfiguration.getConfig().getString(ZK_LOCK_KEY_PROP));\n+    newLock.acquire();\n+    lock.compareAndSet(null, newLock);\n+  }\n+\n+  @Override\n+  public boolean tryLock() {\n+    LOG.info(\"Trying to acquire lock for ZkBasePath \" + lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP)\n+        + \" and lock key \" + this.lockConfiguration.getConfig().getString(ZK_LOCK_KEY_PROP));\n+    try {\n+      acquireLock();\n+    } catch (Exception e) {\n+      throw new HoodieLockException(\"Unable to acquire lock\", e);\n+    }\n+    return lock.get() != null && lock.get().isAcquiredInThisProcess();\n+  }\n+\n+  @Override\n+  public void unlock() {\n+    try {\n+      LOG.info(\"Releasing lock for ZkBasePath \"\n+          + lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP) + \" and lock key \"\n+          + this.lockConfiguration.getConfig().getString(ZK_LOCK_KEY_PROP));\n+      if (lock.get() == null) {\n+        return;\n+      }\n+      lock.get().release();\n+      lock.set(null);\n+      LOG.info(\"Released lock\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyNDg1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxNjo1MFrOIMGEeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxNjo1MFrOIMGEeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzI3Mw==", "bodyText": "lets drop the HOODIE_ prefix from the properties, per how we have named the configs thus far", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553273", "createdAt": "2020-12-29T03:16:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.client.lock.ConcurrentFileWritesConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.FileSystemBasedLockProvider;\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.lock.LockProvider;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_HOODIE_LOCK_ACQUIRE_NUM_RETRIES;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS;\n+import static org.apache.hudi.common.config.LockConfiguration.HIVE_DATABASE_NAME_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HIVE_TABLE_NAME_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HOODIE_LOCK_PREFIX;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_PORT_PROP;\n+\n+/**\n+ * Write callback related config.\n+ */\n+public class HoodieLockConfig extends DefaultHoodieConfig {\n+\n+  // Pluggable type of lock provider\n+  public static final String HOODIE_LOCK_PROVIDER_CLASS_PROP = HOODIE_LOCK_PREFIX + \"provider\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyNTUwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxNzoyNlrOIMGE1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDowNTo0NFrOINabrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzM2Ng==", "bodyText": "repeating my earlier comment; lets just use the concurrency mode config from the other PR .", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553366", "createdAt": "2020-12-29T03:17:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -122,6 +123,10 @@\n   private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n   private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n \n+  // Enable multi writer support\n+  private static final String HOODIE_TABLE_MULTIWRITER_ENABLED_PROP = \"hoodie.table.multiwriter.enabled\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNTQ2OQ==", "bodyText": "Responded earlier", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550935469", "createdAt": "2021-01-03T00:05:44Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -122,6 +123,10 @@\n   private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n   private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n \n+  // Enable multi writer support\n+  private static final String HOODIE_TABLE_MULTIWRITER_ENABLED_PROP = \"hoodie.table.multiwriter.enabled\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzM2Ng=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyNzA1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxODo0MVrOIMGFmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwMDowNjowM1rOINab5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzU2MA==", "bodyText": "so, I was expecting we will do some kind of atomic swap, not just a set. if you just want to set, even a volatile variable is fine right?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553560", "createdAt": "2020-12-29T03:18:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -385,6 +386,10 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n     } else {\n       writeTimer = metrics.getDeltaCommitCtx();\n     }\n+    latestWriteInstantCompletedBeforeWriter.set(table.getMetaClient().getActiveTimeline().getAllCommitsTimeline()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDkzNTUyNA==", "bodyText": "That was the intention, missed this, fixed now", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550935524", "createdAt": "2021-01-03T00:06:03Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -385,6 +386,10 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n     } else {\n       writeTimer = metrics.getDeltaCommitCtx();\n     }\n+    latestWriteInstantCompletedBeforeWriter.set(table.getMetaClient().getActiveTimeline().getAllCommitsTimeline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzU2MA=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyNzUwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkExecuteClusteringCommitActionExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxOTowN1rOIMGF1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODowOTo0MVrOINcrXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzYyMg==", "bodyText": "why is this relevant here?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553622", "createdAt": "2020-12-29T03:19:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkExecuteClusteringCommitActionExecutor.java", "diffHunk": "@@ -118,7 +118,7 @@ public SparkExecuteClusteringCommitActionExecutor(HoodieEngineContext context,\n       Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n       return ((ClusteringExecutionStrategy<T, JavaRDD<HoodieRecord<? extends HoodieRecordPayload>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>>)\n           ReflectionUtils.loadClass(config.getClusteringExecutionStrategyClass(), table, context, config))\n-          .performClustering(inputRecords, clusteringGroup.getNumOutputFileGroups(), instantTime, strategyParams, readerSchema);\n+          .performClustering(inputRecords, 0, instantTime, strategyParams, readerSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3MjI1NQ==", "bodyText": "The build was failing and not able to find this because I probably hadn't built the avro classes, so these changes were just a workaround, they have been reverted now.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550972255", "createdAt": "2021-01-03T08:09:41Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkExecuteClusteringCommitActionExecutor.java", "diffHunk": "@@ -118,7 +118,7 @@ public SparkExecuteClusteringCommitActionExecutor(HoodieEngineContext context,\n       Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n       return ((ClusteringExecutionStrategy<T, JavaRDD<HoodieRecord<? extends HoodieRecordPayload>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>>)\n           ReflectionUtils.loadClass(config.getClusteringExecutionStrategyClass(), table, context, config))\n-          .performClustering(inputRecords, clusteringGroup.getNumOutputFileGroups(), instantTime, strategyParams, readerSchema);\n+          .performClustering(inputRecords, 0, instantTime, strategyParams, readerSchema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzYyMg=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyNzg2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/scala/org/apache/hudi/AvroConversionHelper.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoxOTozM1rOIMGGCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODoxMzowNVrOINcsYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzY3Mw==", "bodyText": "back these changes out?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553673", "createdAt": "2020-12-29T03:19:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/scala/org/apache/hudi/AvroConversionHelper.scala", "diffHunk": "@@ -314,7 +313,7 @@ object AvroConversionHelper {\n           } else {\n             val sourceArray = item.asInstanceOf[Seq[Any]]\n             val sourceArraySize = sourceArray.size\n-            val targetList = new util.ArrayList[Any](sourceArraySize)\n+            val targetList = new java.util.ArrayList[Any](sourceArraySize)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3MjUxNA==", "bodyText": "Reverted these changes, but for some reason my intellij complains and cannot find util.Arraylist etc", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550972514", "createdAt": "2021-01-03T08:13:05Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/scala/org/apache/hudi/AvroConversionHelper.scala", "diffHunk": "@@ -314,7 +313,7 @@ object AvroConversionHelper {\n           } else {\n             val sourceArray = item.asInstanceOf[Seq[Any]]\n             val sourceArraySize = sourceArray.size\n-            val targetList = new util.ArrayList[Any](sourceArraySize)\n+            val targetList = new java.util.ArrayList[Any](sourceArraySize)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzY3Mw=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyODQ1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMDowNlrOIMGGWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMDowNlrOIMGGWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1Mzc1Mw==", "bodyText": "lets add a new test around the multi writing please. it should ideally be agnostic of COW/MOR. and we can just test MOR.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553753", "createdAt": "2020-12-29T03:20:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -216,6 +222,71 @@ public void testDeduplicationOnUpsert() throws Exception {\n     testDeduplication(SparkRDDWriteClient::upsert);\n   }\n \n+  @Test\n+  public void testMultiWriter() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyODgyOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMDoyN1rOIMGGiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMDoyN1rOIMGGiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzgwMQ==", "bodyText": "Drop the HOODIE_ ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553801", "createdAt": "2020-12-29T03:20:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.config;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Configuration for managing locks. Since this configuration needs to be shared with HiveMetaStore based lock,\n+ * which is in a different package than other lock providers, we use this as a data transfer object in hoodie-common\n+ */\n+public class LockConfiguration {\n+\n+  public static final String HOODIE_LOCK_PREFIX = \"hoodie.writer.lock.\";\n+  public static final String HOODIE_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP = HOODIE_LOCK_PREFIX + \"wait_time_ms\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyOTA3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMDo1M1rOIMGGrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODoxNDozMFrOINcs-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzgzOQ==", "bodyText": "whats the reason to have this in hudi-common? locking is only used by writing correct.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553839", "createdAt": "2020-12-29T03:20:53Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.config;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Configuration for managing locks. Since this configuration needs to be shared with HiveMetaStore based lock,\n+ * which is in a different package than other lock providers, we use this as a data transfer object in hoodie-common\n+ */\n+public class LockConfiguration {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3MjY2NA==", "bodyText": "So we need to pass the same configs to HiveMetastoreBasedLockProvider which is in hudi-hive-sync as well as ZookeeperBasedLockProvider which is in hudi-client. This object helps to keep the config reading from the same class.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550972664", "createdAt": "2021-01-03T08:14:30Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.config;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Configuration for managing locks. Since this configuration needs to be shared with HiveMetaStore based lock,\n+ * which is in a different package than other lock providers, we use this as a data transfer object in hoodie-common\n+ */\n+public class LockConfiguration {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzgzOQ=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgyOTYyOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMToyNVrOIMGG8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODoxNDo1OVrOINctJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzkwNg==", "bodyText": "can this be an interface?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549553906", "createdAt": "2020-12-29T03:21:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider implements Lock, AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3MjcwOA==", "bodyText": "Any specific reason you want this to be an interface ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550972708", "createdAt": "2021-01-03T08:14:59Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider implements Lock, AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1MzkwNg=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgzMDg4OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMjo0MlrOIMGHmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODozMDowM1rOINczkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1NDA3Mw==", "bodyText": "we should probably support this right. even of the lock() , since this has a timeout param?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549554073", "createdAt": "2020-12-29T03:22:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider implements Lock, AutoCloseable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockProvider.class);\n+\n+  protected LockConfiguration lockConfiguration;\n+\n+  protected abstract void acquireLock() throws Exception;\n+\n+  @Override\n+  public final void lock() {\n+    LOG.info(\"Acquiring lock\");\n+    try {\n+      acquireLock();\n+    } catch (Exception e) {\n+      throw new RuntimeException(e);\n+    }\n+    LOG.info(\"Acquired lock\");\n+  }\n+\n+  @Override\n+  public final void lockInterruptibly() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final boolean tryLock(long time, TimeUnit unit) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3NDM1Mw==", "bodyText": "Had implemented this differently earlier, changed some of the contracts, take a look again please", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550974353", "createdAt": "2021-01-03T08:30:03Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider implements Lock, AutoCloseable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockProvider.class);\n+\n+  protected LockConfiguration lockConfiguration;\n+\n+  protected abstract void acquireLock() throws Exception;\n+\n+  @Override\n+  public final void lock() {\n+    LOG.info(\"Acquiring lock\");\n+    try {\n+      acquireLock();\n+    } catch (Exception e) {\n+      throw new RuntimeException(e);\n+    }\n+    LOG.info(\"Acquired lock\");\n+  }\n+\n+  @Override\n+  public final void lockInterruptibly() {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public final boolean tryLock(long time, TimeUnit unit) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1NDA3Mw=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgzMTQxOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMzoxM1rOIMGH3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODozNjowNFrOINc1tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1NDE0Mg==", "bodyText": "can this sit somewhere else? in a helper method?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549554142", "createdAt": "2020-12-29T03:23:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java", "diffHunk": "@@ -109,6 +108,17 @@ public void setCompacted(Boolean compacted) {\n     return filePaths;\n   }\n \n+  public HashMap<String, String> getFileIdWithoutSuffixAndRelativePaths() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3NDkwMQ==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550974901", "createdAt": "2021-01-03T08:36:04Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java", "diffHunk": "@@ -109,6 +108,17 @@ public void setCompacted(Boolean compacted) {\n     return filePaths;\n   }\n \n+  public HashMap<String, String> getFileIdWithoutSuffixAndRelativePaths() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1NDE0Mg=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjgzMTY4OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMzoyMzoyOVrOIMGIAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wM1QwODozNjoyMFrOINc12A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1NDE3OQ==", "bodyText": "why this change?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r549554179", "createdAt": "2020-12-29T03:23:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java", "diffHunk": "@@ -284,7 +284,7 @@ private Object typeConvert(Schema.Field field) {\n   private Object generateFixedType(Schema localSchema) {\n     // TODO: Need to implement valid data generation for fixed type\n     GenericFixed genericFixed = new GenericData.Fixed(localSchema);\n-    switch (localSchema.getLogicalType().getName()) {\n+    switch (localSchema.getType().getName()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDk3NDkzNg==", "bodyText": "Same, my intellij doesn't work, reverted", "url": "https://github.com/apache/hudi/pull/2374#discussion_r550974936", "createdAt": "2021-01-03T08:36:20Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java", "diffHunk": "@@ -284,7 +284,7 @@ private Object typeConvert(Schema.Field field) {\n   private Object generateFixedType(Schema localSchema) {\n     // TODO: Need to implement valid data generation for fixed type\n     GenericFixed genericFixed = new GenericData.Fixed(localSchema);\n-    switch (localSchema.getLogicalType().getName()) {\n+    switch (localSchema.getType().getName()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU1NDE3OQ=="}, "originalCommit": {"oid": "592e4bda9aed9af895b60db0d07393b2a6916d30"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDI0Mjc3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODozOTowOFrOIefT-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODozOTowOFrOIefT-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0MTIxMQ==", "bodyText": "how about empty instead of null. In general, like to avoid using null for any kind of sentinel", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568841211", "createdAt": "2021-02-02T18:39:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -188,6 +203,8 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n       postCommit(table, metadata, instantTime, extraMetadata);\n       emitCommitMetrics(instantTime, metadata, commitActionType);\n       LOG.info(\"Committed \" + instantTime);\n+      // Reset the last completed write instant\n+      latestCompletedWriteInstant.set(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDI1MjI4OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo0MToyNVrOIefZuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzozOToxOFrOIe0kEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0MjY4Mw==", "bodyText": "whats NO_WRITER ? its kind of difficult to understand . can we remove this", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568842683", "createdAt": "2021-02-02T18:41:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Different concurrency modes for write operations.\n+ */\n+public enum WriteConcurrencyMode {\n+  NO_WRITER(\"no_writer\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE4OTM5Mw==", "bodyText": "Old code, removed.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569189393", "createdAt": "2021-02-03T07:39:18Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Different concurrency modes for write operations.\n+ */\n+public enum WriteConcurrencyMode {\n+  NO_WRITER(\"no_writer\"),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0MjY4Mw=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDI1OTc2OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo0MzozNVrOIefexQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo0MzozNVrOIefexQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0Mzk3Mw==", "bodyText": "rename to : supportsOptimisticConcurrencyControl() to match mode name.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568843973", "createdAt": "2021-02-02T18:43:35Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Different concurrency modes for write operations.\n+ */\n+public enum WriteConcurrencyMode {\n+  NO_WRITER(\"no_writer\"),\n+  // Only a single writer can perform write ops\n+  SINGLE_WRITER(\"single_writer\"),\n+  // Multiple writer can perform write ops with lazy conflict resolution using locks\n+  OPTIMISTIC_CONCURRENCY_CONTROL_SHARED_LOCK(\"optimistic_concurrency_control_shared_lock\");\n+\n+  private final String value;\n+\n+  WriteConcurrencyMode(String value) {\n+    this.value = value;\n+  }\n+\n+  /**\n+   * Getter for write concurrency mode.\n+   * @return\n+   */\n+  public String value() {\n+    return value;\n+  }\n+\n+  /**\n+   * Convert string value to WriteConcurrencyMode.\n+   */\n+  public static WriteConcurrencyMode fromValue(String value) {\n+    switch (value.toLowerCase(Locale.ROOT)) {\n+      case \"no_writer\":\n+        return NO_WRITER;\n+      case \"single_writer\":\n+        return SINGLE_WRITER;\n+      case \"optimistic_concurrency_control_shared_lock\":\n+        return OPTIMISTIC_CONCURRENCY_CONTROL_SHARED_LOCK;\n+      default:\n+        throw new HoodieException(\"Invalid value of Type.\");\n+    }\n+  }\n+\n+  public boolean isMultiWriter() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDI2NjcxOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitMetadataUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo0NTozNVrOIefjTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo0NTozNVrOIefjTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0NTEzMw==", "bodyText": "lets just add this to an existing class like CommitUtils", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568845133", "createdAt": "2021-02-02T18:45:35Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitMetadataUtils.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Helper class to manipulate commit metadata.\n+ */\n+public class CommitMetadataUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDI2OTIzOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/TableService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo0NjoxM1rOIefk3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo0MzoxNVrOIe0rfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0NTUzMg==", "bodyText": "should archival be here too?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568845532", "createdAt": "2021-02-02T18:46:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/TableService.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Supported runtime table services.\n+ */\n+public enum TableService {\n+  COMPACT(\"compact\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5MTI5NQ==", "bodyText": "It should be, but for each of these table services, we now follow the steps of schedule -> inflight -> complete. Archival doesn't do that right now and we never run archival async (unlike clean), so I haven't added it here. I have refactored the clean actions to do this. Filed a ticket for async archival -> https://issues.apache.org/jira/browse/HUDI-1576", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569191295", "createdAt": "2021-02-03T07:43:15Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/TableService.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Supported runtime table services.\n+ */\n+public enum TableService {\n+  COMPACT(\"compact\"),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0NTUzMg=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDI3Mzk4OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/TableService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo0NzowOFrOIefniA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo0NDoxN1rOIe0thQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0NjIxNg==", "bodyText": "rename: Sting name. I actually don't see the need for this field atm now.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568846216", "createdAt": "2021-02-02T18:47:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/TableService.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Supported runtime table services.\n+ */\n+public enum TableService {\n+  COMPACT(\"compact\"),\n+  CLUSTER(\"cluster\"),\n+  CLEAN(\"clean\");\n+  private final String value;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5MTgxMw==", "bodyText": "removed", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569191813", "createdAt": "2021-02-03T07:44:17Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/TableService.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Supported runtime table services.\n+ */\n+public enum TableService {\n+  COMPACT(\"compact\"),\n+  CLUSTER(\"cluster\"),\n+  CLEAN(\"clean\");\n+  private final String value;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0NjIxNg=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDI5MDgyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo1MDo0OVrOIefxbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo0NjowOFrOIe0xTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0ODc1MA==", "bodyText": "lets call this  initIfNeeded. to avoid overloading bootstrap which has its own meaning. .", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568848750", "createdAt": "2021-02-02T18:50:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -220,7 +253,7 @@ void emitCommitMetrics(String instantTime, HoodieCommitMetadata metadata, String\n     }\n   }\n \n-  protected void syncTableMetadata() {\n+  protected void syncTableMetadata(boolean bootstrapIfNeeded) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5Mjc4MQ==", "bodyText": "init also has a different meaning inside the HoodieBackedTableMetadataWriter where init just means initializing the table metadata. How about just bootstrapMetadata ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569192781", "createdAt": "2021-02-03T07:46:08Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -220,7 +253,7 @@ void emitCommitMetrics(String instantTime, HoodieCommitMetadata metadata, String\n     }\n   }\n \n-  protected void syncTableMetadata() {\n+  protected void syncTableMetadata(boolean bootstrapIfNeeded) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg0ODc1MA=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDI5OTMwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo1Mjo1OVrOIef2pA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQwNjozNjo1MFrOIfkuag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MDA4NA==", "bodyText": "Do we need this check with OCC mode? in any case, we should ensure the bootstrap code downgrades to single writer, so users don't have to worry about this. Most people do bootstrap then followed by writing anyway.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568850084", "createdAt": "2021-02-02T18:52:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -239,6 +272,9 @@ public void bootstrap(Option<Map<String, String>> extraMetadata) {\n     if (rollbackPending) {\n       rollBackInflightBootstrap();\n     }\n+    if (config.getWriteConcurrencyMode().isMultiWriter()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTk3ODQ3NA==", "bodyText": "Right now, we don't have multi-writer so people might remain cautious. Once we support multiwriter, it's easy to set one config and run bootstrap/incremental, guarding against such behaviors.\nRight now, do we have a way to enforce users to downgrade to a single writer ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569978474", "createdAt": "2021-02-04T06:36:50Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -239,6 +272,9 @@ public void bootstrap(Option<Map<String, String>> extraMetadata) {\n     if (rollbackPending) {\n       rollBackInflightBootstrap();\n     }\n+    if (config.getWriteConcurrencyMode().isMultiWriter()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MDA4NA=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDMwMDgyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo1MzoyNlrOIef3rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo0ODowNVrOIe01Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MDM0OA==", "bodyText": "just double check if the indents are okay here?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568850348", "createdAt": "2021-02-02T18:53:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -403,30 +439,32 @@ protected void postCommit(HoodieTable<T, I, K, O> table, HoodieCommitMetadata me\n       // Delete the marker directory for the instant.\n       new MarkerFiles(table, instantTime).quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());\n \n-      // Do an inline compaction if enabled\n-      if (config.isInlineCompaction()) {\n-        runAnyPendingCompactions(table);\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n-        inlineCompact(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n-      }\n+      if (config.isInlineTableServiceEnabled()) {\n+        // Do an inline compaction if enabled\n+        if (config.isInlineCompaction()) {\n+          runAnyPendingCompactions(table);\n+          metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n+          inlineCompact(extraMetadata);\n+        } else {\n+          metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n+        }\n \n-      // Do an inline clustering if enabled\n-      if (config.isInlineClustering()) {\n-        runAnyPendingClustering(table);\n-        metadata.addMetadata(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"true\");\n-        inlineCluster(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"false\");\n-      }\n+        // Do an inline clustering if enabled", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5MzgxNQ==", "bodyText": "Intellij says ok, not sure why it's appearing this way here", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569193815", "createdAt": "2021-02-03T07:48:05Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -403,30 +439,32 @@ protected void postCommit(HoodieTable<T, I, K, O> table, HoodieCommitMetadata me\n       // Delete the marker directory for the instant.\n       new MarkerFiles(table, instantTime).quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());\n \n-      // Do an inline compaction if enabled\n-      if (config.isInlineCompaction()) {\n-        runAnyPendingCompactions(table);\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n-        inlineCompact(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n-      }\n+      if (config.isInlineTableServiceEnabled()) {\n+        // Do an inline compaction if enabled\n+        if (config.isInlineCompaction()) {\n+          runAnyPendingCompactions(table);\n+          metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n+          inlineCompact(extraMetadata);\n+        } else {\n+          metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n+        }\n \n-      // Do an inline clustering if enabled\n-      if (config.isInlineClustering()) {\n-        runAnyPendingClustering(table);\n-        metadata.addMetadata(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"true\");\n-        inlineCluster(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"false\");\n-      }\n+        // Do an inline clustering if enabled", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MDM0OA=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDMwNTEzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo1NDozMlrOIef6fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo0ODo0NlrOIe029Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MTA3MQ==", "bodyText": "why was this change needed", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568851071", "createdAt": "2021-02-02T18:54:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -599,6 +637,7 @@ public HoodieRestoreMetadata restoreToInstant(final String instantTime) throws H\n   public HoodieCleanMetadata clean(String cleanInstantTime) throws HoodieIOException {\n     LOG.info(\"Cleaner started\");\n     final Timer.Context timerContext = metrics.getCleanCtx();\n+    scheduleCleaningAtInstant(cleanInstantTime, Option.empty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5NDIyOQ==", "bodyText": "I refactored the code to ensure we follow schedule -> inflight -> complete for clean actions as well.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569194229", "createdAt": "2021-02-03T07:48:46Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -599,6 +637,7 @@ public HoodieRestoreMetadata restoreToInstant(final String instantTime) throws H\n   public HoodieCleanMetadata clean(String cleanInstantTime) throws HoodieIOException {\n     LOG.info(\"Cleaner started\");\n     final Timer.Context timerContext = metrics.getCleanCtx();\n+    scheduleCleaningAtInstant(cleanInstantTime, Option.empty());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MTA3MQ=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDMxMDY0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo1NTo0M1rOIef90w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo1MzowOVrOIe0_wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MTkyMw==", "bodyText": "so, this block only enables the inline execution of the table services? or does it also cover the scheduling of cleaning, compaction, clustering etc?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568851923", "createdAt": "2021-02-02T18:55:43Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -403,30 +439,32 @@ protected void postCommit(HoodieTable<T, I, K, O> table, HoodieCommitMetadata me\n       // Delete the marker directory for the instant.\n       new MarkerFiles(table, instantTime).quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());\n \n-      // Do an inline compaction if enabled\n-      if (config.isInlineCompaction()) {\n-        runAnyPendingCompactions(table);\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n-        inlineCompact(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n-      }\n+      if (config.isInlineTableServiceEnabled()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MjMyNw==", "bodyText": "We should also protect the scheduling, correct? (may be it comes down the line)", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568852327", "createdAt": "2021-02-02T18:56:24Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -403,30 +439,32 @@ protected void postCommit(HoodieTable<T, I, K, O> table, HoodieCommitMetadata me\n       // Delete the marker directory for the instant.\n       new MarkerFiles(table, instantTime).quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());\n \n-      // Do an inline compaction if enabled\n-      if (config.isInlineCompaction()) {\n-        runAnyPendingCompactions(table);\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n-        inlineCompact(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n-      }\n+      if (config.isInlineTableServiceEnabled()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MTkyMw=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5NjQ4MQ==", "bodyText": "I just wrapped all table services together as inline or async. The scheduling is protected via lock in inline as well as async.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569196481", "createdAt": "2021-02-03T07:53:09Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -403,30 +439,32 @@ protected void postCommit(HoodieTable<T, I, K, O> table, HoodieCommitMetadata me\n       // Delete the marker directory for the instant.\n       new MarkerFiles(table, instantTime).quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());\n \n-      // Do an inline compaction if enabled\n-      if (config.isInlineCompaction()) {\n-        runAnyPendingCompactions(table);\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n-        inlineCompact(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n-      }\n+      if (config.isInlineTableServiceEnabled()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MTkyMw=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDMxNzMyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo1NzozMFrOIegCGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo1NzowMlrOIe1ITw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MzAxOQ==", "bodyText": "we should clear annotate/document that this we do not support multiple writers on the same writeClient instance.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568853019", "createdAt": "2021-02-02T18:57:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -98,6 +112,7 @@\n   private transient HoodieWriteCommitCallback commitCallback;\n   private transient AsyncCleanerService asyncCleanerService;\n   protected final boolean rollbackPending;\n+  protected AtomicReference<Option<HoodieInstant>> latestCompletedWriteInstant = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5ODY3MQ==", "bodyText": "Created issue -> https://issues.apache.org/jira/browse/HUDI-1577", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569198671", "createdAt": "2021-02-03T07:57:02Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -98,6 +112,7 @@\n   private transient HoodieWriteCommitCallback commitCallback;\n   private transient AsyncCleanerService asyncCleanerService;\n   protected final boolean rollbackPending;\n+  protected AtomicReference<Option<HoodieInstant>> latestCompletedWriteInstant = new AtomicReference<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1MzAxOQ=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDMyNjc0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo1OTo0NVrOIegH-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxODo1OTo0NVrOIegH-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1NDUyMQ==", "bodyText": "can this code sit somewhere else?  we should try to keep the write client file pretty lean and do only the control flow", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568854521", "createdAt": "2021-02-02T18:59:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -859,6 +973,68 @@ protected void finalizeWrite(HoodieTable<T, I, K, O> table, String instantTime,\n     }\n   }\n \n+  private HoodieCommitMetadata resolveWriteConflictIfAny(HoodieTable<T, I, K, O> table, HoodieBackedTableMetadataWriter metadataWriter,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 350}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDMyODM4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowMDowOVrOIegI7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowMDowOVrOIegI7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1NDc2Ng==", "bodyText": "same with this.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568854766", "createdAt": "2021-02-02T19:00:09Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -859,6 +973,68 @@ protected void finalizeWrite(HoodieTable<T, I, K, O> table, String instantTime,\n     }\n   }\n \n+  private HoodieCommitMetadata resolveWriteConflictIfAny(HoodieTable<T, I, K, O> table, HoodieBackedTableMetadataWriter metadataWriter,\n+                                                         final String instantTime, HoodieCommitMetadata thisCommitMetadata)\n+      throws HoodieWriteConflictException {\n+    ConflictResolutionStrategy resolutionStrategy = config.getWriteConflictResolutionStrategy();\n+    Option<HoodieInstant> lastCompletedInstantBeforeWriterStarted = this.latestCompletedWriteInstant.get();\n+    String lastInstantTimestamp = lastCompletedInstantBeforeWriterStarted.isPresent()\n+        ? lastCompletedInstantBeforeWriterStarted.get().getTimestamp() : \"0\";\n+    // Get completed instants timeline\n+    Stream<HoodieInstant> completedInstantStream = table.getActiveTimeline()\n+        .getAllCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .getCommitsAndCompactionTimeline()\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastInstantTimestamp)\n+        .getInstants();\n+\n+    // Get pending replace and compaction instants timeline\n+    Stream<HoodieInstant> pendingReplaceAndRequestedInstantStream = table.getActiveTimeline()\n+        .getAllCommitsTimeline()\n+        .filterPendingCompactionAndReplaceTimeline()\n+        .findInstantsAfter(lastInstantTimestamp)\n+        .getInstants();\n+\n+    Stream<HoodieInstant> instantStream = Stream.concat(completedInstantStream, pendingReplaceAndRequestedInstantStream);\n+    final HoodieCommitOperation thisOperation = new HoodieCommitOperation(thisCommitMetadata, instantTime);\n+    instantStream.forEach(instant -> {\n+      try {\n+        HoodieCommitOperation otherOperation = new HoodieCommitOperation(HoodieCommitMetadata.fromBytes(\n+            table.getActiveTimeline().getInstantDetails(instant).get(), HoodieCommitMetadata.class), instant.getTimestamp());\n+        if (resolutionStrategy.hasConflict(thisOperation, otherOperation)) {\n+          LOG.info(\"Conflict encountered between instant = \" + thisOperation.getInstant() + \" and instant = \"\n+              + otherOperation.getInstant() + \", attempting to resolve it now\");\n+          resolutionStrategy.resolveConflict(metadataWriter, table, thisOperation, otherOperation);\n+        }\n+      } catch (IOException io) {\n+        throw new HoodieWriteConflictException(\"Unable to resolve conflict, if present\", io);\n+      }\n+    });\n+    return thisOperation.getCommitMetadata();\n+  }\n+\n+  private boolean executeCriticalSection(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 391}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDMzMTIzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowMDo1MVrOIegKww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowMDo1MVrOIegKww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1NTIzNQ==", "bodyText": "please move these comments into the actual implementation. and not in this file", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568855235", "createdAt": "2021-02-02T19:00:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -907,11 +1083,20 @@ public void close() {\n     // release AsyncCleanerService\n     AsyncCleanerService.forceShutdown(asyncCleanerService);\n     asyncCleanerService = null;\n-\n     // Stop timeline-server if running\n     super.close();\n     // Calling this here releases any resources used by your index, so make sure to finish any related operations\n     // before this point\n     this.index.close();\n+    // HiveMetastoreClient does not implement AutoCloseable. Additionally, we cannot call close() after unlock()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 425}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDMzOTc5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowMzoxN1rOIegQYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQwNjozNDoyNlrOIfkrGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1NjY3NQ==", "bodyText": "we should check somewhere that user cannot turn on both async and inline? , if we are adding an explicit config.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568856675", "createdAt": "2021-02-02T19:03:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -41,7 +41,8 @@\n   public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n   public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n   public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n+  // Turn on inline cleaning\n+  public static final String INLINE_CLEAN_PROP = \"hoodie.clean.inline\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTk3NzYyNQ==", "bodyText": "Right now, this config is harmless, it just assumes the role of autoClean.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569977625", "createdAt": "2021-02-04T06:34:26Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -41,7 +41,8 @@\n   public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n   public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n   public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n+  // Turn on inline cleaning\n+  public static final String INLINE_CLEAN_PROP = \"hoodie.clean.inline\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1NjY3NQ=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDM0NDkwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowNDo0MFrOIegThg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwODowMTo1N1rOIe1S7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1NzQ3OA==", "bodyText": "this does not read easily. rename inlineCleaningEnabled or shouldCleanInline() or something like tht? (same wherever applicable)", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568857478", "createdAt": "2021-02-02T19:04:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -389,6 +397,10 @@ public boolean isAsyncClean() {\n     return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.ASYNC_CLEAN_PROP));\n   }\n \n+  public boolean isInlineCleaning() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTIwMTM4OA==", "bodyText": "I kept the same naming convention as before for isInlineCompaction etc. I refactored all of them to inline<..>Enabled", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569201388", "createdAt": "2021-02-03T08:01:57Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -389,6 +397,10 @@ public boolean isAsyncClean() {\n     return Boolean.parseBoolean(props.getProperty(HoodieCompactionConfig.ASYNC_CLEAN_PROP));\n   }\n \n+  public boolean isInlineCleaning() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1NzQ3OA=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDM0ODgyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowNTo0NFrOIegV5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo1OToyNFrOIe1Ntg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1ODA4Ng==", "bodyText": "same. rename inlineTableServices()?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568858086", "createdAt": "2021-02-02T19:05:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -923,6 +935,39 @@ public int getMetadataCleanerCommitsRetained() {\n     return Integer.parseInt(props.getProperty(HoodieMetadataConfig.CLEANER_COMMITS_RETAINED_PROP));\n   }\n \n+  /**\n+   * Hoodie Client Lock Configs.\n+   * @return\n+   */\n+\n+  public String getLockProviderClass() {\n+    return props.getProperty(HoodieLockConfig.LOCK_PROVIDER_CLASS_PROP);\n+  }\n+\n+  public String getLockHiveDatabaseName() {\n+    return props.getProperty(HIVE_DATABASE_NAME_PROP);\n+  }\n+\n+  public String getLockHiveTableName() {\n+    return props.getProperty(HIVE_TABLE_NAME_PROP);\n+  }\n+\n+  public ConflictResolutionStrategy getWriteConflictResolutionStrategy() {\n+    return ReflectionUtils.loadClass(props.getProperty(HoodieLockConfig.WRITE_CONFLICT_RESOLUTION_STRATEGY_CLASS_PROP));\n+  }\n+\n+  public Long getLockAcquireWaitTimeoutInMs() {\n+    return Long.valueOf(props.getProperty(LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP));\n+  }\n+\n+  public WriteConcurrencyMode getWriteConcurrencyMode() {\n+    return WriteConcurrencyMode.fromValue(props.getProperty(WRITE_CONCURRENCY_MODE_PROP));\n+  }\n+\n+  public Boolean isInlineTableServiceEnabled() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTIwMDA1NA==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569200054", "createdAt": "2021-02-03T07:59:24Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -923,6 +935,39 @@ public int getMetadataCleanerCommitsRetained() {\n     return Integer.parseInt(props.getProperty(HoodieMetadataConfig.CLEANER_COMMITS_RETAINED_PROP));\n   }\n \n+  /**\n+   * Hoodie Client Lock Configs.\n+   * @return\n+   */\n+\n+  public String getLockProviderClass() {\n+    return props.getProperty(HoodieLockConfig.LOCK_PROVIDER_CLASS_PROP);\n+  }\n+\n+  public String getLockHiveDatabaseName() {\n+    return props.getProperty(HIVE_DATABASE_NAME_PROP);\n+  }\n+\n+  public String getLockHiveTableName() {\n+    return props.getProperty(HIVE_TABLE_NAME_PROP);\n+  }\n+\n+  public ConflictResolutionStrategy getWriteConflictResolutionStrategy() {\n+    return ReflectionUtils.loadClass(props.getProperty(HoodieLockConfig.WRITE_CONFLICT_RESOLUTION_STRATEGY_CLASS_PROP));\n+  }\n+\n+  public Long getLockAcquireWaitTimeoutInMs() {\n+    return Long.valueOf(props.getProperty(LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP));\n+  }\n+\n+  public WriteConcurrencyMode getWriteConcurrencyMode() {\n+    return WriteConcurrencyMode.fromValue(props.getProperty(WRITE_CONCURRENCY_MODE_PROP));\n+  }\n+\n+  public Boolean isInlineTableServiceEnabled() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1ODA4Ng=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDM1MDg1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowNjowOFrOIegXGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo1ODo0NlrOIe1MRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1ODM5NA==", "bodyText": "rename to initIfNeeded consistently", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568858394", "createdAt": "2021-02-02T19:06:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -88,7 +88,7 @@\n   protected SerializableConfiguration hadoopConf;\n   protected final transient HoodieEngineContext engineContext;\n \n-  protected HoodieBackedTableMetadataWriter(Configuration hadoopConf, HoodieWriteConfig writeConfig, HoodieEngineContext engineContext) {\n+  protected HoodieBackedTableMetadataWriter(Configuration hadoopConf, HoodieWriteConfig writeConfig, HoodieEngineContext engineContext, boolean bootstrapIfNeeded) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5OTY4NQ==", "bodyText": "refactored to bootstrapMetadata", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569199685", "createdAt": "2021-02-03T07:58:46Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -88,7 +88,7 @@\n   protected SerializableConfiguration hadoopConf;\n   protected final transient HoodieEngineContext engineContext;\n \n-  protected HoodieBackedTableMetadataWriter(Configuration hadoopConf, HoodieWriteConfig writeConfig, HoodieEngineContext engineContext) {\n+  protected HoodieBackedTableMetadataWriter(Configuration hadoopConf, HoodieWriteConfig writeConfig, HoodieEngineContext engineContext, boolean bootstrapIfNeeded) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1ODM5NA=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4NDM1MTc1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxOTowNjoyMlrOIegXow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QwNzo1ODozNFrOIe1L2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1ODUzMQ==", "bodyText": "we can also rename this method if needed", "url": "https://github.com/apache/hudi/pull/2374#discussion_r568858531", "createdAt": "2021-02-02T19:06:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -230,13 +235,18 @@ protected void initTableMetadata() {\n     }\n   }\n \n-  protected void bootstrapIfNeeded(HoodieEngineContext engineContext, HoodieTableMetaClient datasetMetaClient) throws IOException {\n-    HoodieTimer timer = new HoodieTimer().startTimer();\n-    boolean exists = datasetMetaClient.getFs().exists(new Path(metadataWriteConfig.getBasePath(), HoodieTableMetaClient.METAFOLDER_NAME));\n-    if (!exists) {\n-      // Initialize for the first time by listing partitions and files directly from the file system\n-      bootstrapFromFilesystem(engineContext, datasetMetaClient);\n-      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.INITIALIZE_STR, timer.endTimer()));\n+  protected void bootstrapIfNeeded(HoodieEngineContext engineContext, HoodieTableMetaClient datasetMetaClient) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTE5OTU3Nw==", "bodyText": "refactored to bootstrapMetadata", "url": "https://github.com/apache/hudi/pull/2374#discussion_r569199577", "createdAt": "2021-02-03T07:58:34Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -230,13 +235,18 @@ protected void initTableMetadata() {\n     }\n   }\n \n-  protected void bootstrapIfNeeded(HoodieEngineContext engineContext, HoodieTableMetaClient datasetMetaClient) throws IOException {\n-    HoodieTimer timer = new HoodieTimer().startTimer();\n-    boolean exists = datasetMetaClient.getFs().exists(new Path(metadataWriteConfig.getBasePath(), HoodieTableMetaClient.METAFOLDER_NAME));\n-    if (!exists) {\n-      // Initialize for the first time by listing partitions and files directly from the file system\n-      bootstrapFromFilesystem(engineContext, datasetMetaClient);\n-      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.INITIALIZE_STR, timer.endTimer()));\n+  protected void bootstrapIfNeeded(HoodieEngineContext engineContext, HoodieTableMetaClient datasetMetaClient) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODg1ODUzMQ=="}, "originalCommit": {"oid": "8b8d5945136066148f200d30c5d3bf6d5be70cbf"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNDQ4NTE0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzozNzo1NFrOIv2rtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzozNzo1NFrOIv2rtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA0OTkxMA==", "bodyText": "lock as a package name feels off to me. Can we have org.apache.hudi.client.transaction.TransactionManager?\nThen .lock can be a sub package under i.e .transaction.lock.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587049910", "createdAt": "2021-03-04T03:37:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -30,16 +31,19 @@\n import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n import org.apache.hudi.client.heartbeat.HeartbeatUtils;\n+import org.apache.hudi.client.lock.TransactionManager;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNDQ4OTMzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzozODo0NVrOIv2uTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDowMToyNVrOIxYLKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1MDU3NA==", "bodyText": "So, model package should just contain pojos i.e data structure objects. Lets move TableService elsewhere", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587050574", "createdAt": "2021-03-04T03:38:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -30,16 +31,19 @@\n import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n import org.apache.hudi.client.heartbeat.HeartbeatUtils;\n+import org.apache.hudi.client.lock.TransactionManager;\n import org.apache.hudi.common.engine.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;\n import org.apache.hudi.common.model.HoodieKey;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.TableService;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc3NzA4OQ==", "bodyText": "I kept it here because WriteOperationType is also here.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587777089", "createdAt": "2021-03-04T19:55:53Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -30,16 +31,19 @@\n import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n import org.apache.hudi.client.heartbeat.HeartbeatUtils;\n+import org.apache.hudi.client.lock.TransactionManager;\n import org.apache.hudi.common.engine.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;\n import org.apache.hudi.common.model.HoodieKey;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.TableService;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1MDU3NA=="}, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY0NzIwOA==", "bodyText": "If its an enum, its okay. I thought it had code. Still not a model per se, but we can do the moving in a different PR", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588647208", "createdAt": "2021-03-05T20:01:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -30,16 +31,19 @@\n import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n import org.apache.hudi.client.heartbeat.HeartbeatUtils;\n+import org.apache.hudi.client.lock.TransactionManager;\n import org.apache.hudi.common.engine.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieFailedWritesCleaningPolicy;\n import org.apache.hudi.common.model.HoodieKey;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.TableService;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1MDU3NA=="}, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNDQ5NzY4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0MDoyNlrOIv2zFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0MDoyNlrOIv2zFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1MTc5OA==", "bodyText": "lets move the preCommit() call out of here? commit() calling preCommit() is bit confusing to read", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587051798", "createdAt": "2021-03-04T03:40:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -193,8 +200,22 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n     return true;\n   }\n \n+  protected void commit(HoodieTable table, String commitActionType, String instantTime, HoodieCommitMetadata metadata,\n+                      List<HoodieWriteStat> stats) throws IOException {\n+    preCommit(instantTime, metadata);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNDUwMTA3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0MDo1OVrOIv20vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0MDo1OVrOIv20vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1MjIyMg==", "bodyText": "typo: not supported", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587052222", "createdAt": "2021-03-04T03:40:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -210,6 +231,11 @@ void emitCommitMetrics(String instantTime, HoodieCommitMetadata metadata, String\n     }\n   }\n \n+  protected void preCommit(String instantTime, HoodieCommitMetadata metadata) {\n+    // no-op\n+    // TODO : Conflict resolution is not support for Flink,Java engines", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNDUxNzExOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0MzozNFrOIv29Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQyMToyNzoyOFrOIxpoFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NDQxMA==", "bodyText": "can the above setters be passed to an overloaded beginTransaction(..) call? Whenever we have these contracts that some setters must be called ahead of a beginTransaction, makes for a harder maintenance/read", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587054410", "createdAt": "2021-03-04T03:43:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -359,10 +388,21 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan\n    * Common method containing steps to be performed before write (upsert/insert/...\n    * @param instantTime\n    * @param writeOperationType\n+   * @param metaClient\n    */\n-  protected void preWrite(String instantTime, WriteOperationType writeOperationType) {\n+  protected void preWrite(String instantTime, WriteOperationType writeOperationType,\n+      HoodieTableMetaClient metaClient) {\n     setOperationType(writeOperationType);\n-    syncTableMetadata();\n+    this.txnManager.setLastCompletedTransaction(metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .lastInstant());\n+    LOG.info(\"Last Instant Cached by writer with instant \" + instantTime + \" is \" + this.txnManager.getLastCompletedTransactionOwner());\n+    this.txnManager.setTransactionOwner(Option.of(new HoodieInstant(State.INFLIGHT, metaClient.getCommitActionType(), instantTime)));\n+    this.txnManager.beginTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc3OTUwOA==", "bodyText": "I also realized this during implementation but wanted to keep beginTransaction(..) API simple. I've added a overridden method now", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587779508", "createdAt": "2021-03-04T19:58:03Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -359,10 +388,21 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan\n    * Common method containing steps to be performed before write (upsert/insert/...\n    * @param instantTime\n    * @param writeOperationType\n+   * @param metaClient\n    */\n-  protected void preWrite(String instantTime, WriteOperationType writeOperationType) {\n+  protected void preWrite(String instantTime, WriteOperationType writeOperationType,\n+      HoodieTableMetaClient metaClient) {\n     setOperationType(writeOperationType);\n-    syncTableMetadata();\n+    this.txnManager.setLastCompletedTransaction(metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .lastInstant());\n+    LOG.info(\"Last Instant Cached by writer with instant \" + instantTime + \" is \" + this.txnManager.getLastCompletedTransactionOwner());\n+    this.txnManager.setTransactionOwner(Option.of(new HoodieInstant(State.INFLIGHT, metaClient.getCommitActionType(), instantTime)));\n+    this.txnManager.beginTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NDQxMA=="}, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY0ODg3MA==", "bodyText": "Understood. but if some calls are needed prior to this, then either the object constructor should take them or the method should take them as arguments. Or we have a transaction builder or sorts.\nSo I would say, if this is always the case, i.e the setters are needed, then we change the beginTransaction() signature for good, not just an overload.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588648870", "createdAt": "2021-03-05T20:03:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -359,10 +388,21 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan\n    * Common method containing steps to be performed before write (upsert/insert/...\n    * @param instantTime\n    * @param writeOperationType\n+   * @param metaClient\n    */\n-  protected void preWrite(String instantTime, WriteOperationType writeOperationType) {\n+  protected void preWrite(String instantTime, WriteOperationType writeOperationType,\n+      HoodieTableMetaClient metaClient) {\n     setOperationType(writeOperationType);\n-    syncTableMetadata();\n+    this.txnManager.setLastCompletedTransaction(metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .lastInstant());\n+    LOG.info(\"Last Instant Cached by writer with instant \" + instantTime + \" is \" + this.txnManager.getLastCompletedTransactionOwner());\n+    this.txnManager.setTransactionOwner(Option.of(new HoodieInstant(State.INFLIGHT, metaClient.getCommitActionType(), instantTime)));\n+    this.txnManager.beginTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NDQxMA=="}, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODkzMzE0MA==", "bodyText": "Made changes accordingly", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588933140", "createdAt": "2021-03-06T21:27:28Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -359,10 +388,21 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan\n    * Common method containing steps to be performed before write (upsert/insert/...\n    * @param instantTime\n    * @param writeOperationType\n+   * @param metaClient\n    */\n-  protected void preWrite(String instantTime, WriteOperationType writeOperationType) {\n+  protected void preWrite(String instantTime, WriteOperationType writeOperationType,\n+      HoodieTableMetaClient metaClient) {\n     setOperationType(writeOperationType);\n-    syncTableMetadata();\n+    this.txnManager.setLastCompletedTransaction(metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .lastInstant());\n+    LOG.info(\"Last Instant Cached by writer with instant \" + instantTime + \" is \" + this.txnManager.getLastCompletedTransactionOwner());\n+    this.txnManager.setTransactionOwner(Option.of(new HoodieInstant(State.INFLIGHT, metaClient.getCommitActionType(), instantTime)));\n+    this.txnManager.beginTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NDQxMA=="}, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNDUyNDYyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0NDozN1rOIv3BJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQwOToxNTo1M1rOIxlBjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NTM5Nw==", "bodyText": "do we need to distinguish between endTransaction() and an abort ? i.e any cleanups in the transaction manager to be done here upon exception?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587055397", "createdAt": "2021-03-04T03:44:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -359,10 +388,21 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan\n    * Common method containing steps to be performed before write (upsert/insert/...\n    * @param instantTime\n    * @param writeOperationType\n+   * @param metaClient\n    */\n-  protected void preWrite(String instantTime, WriteOperationType writeOperationType) {\n+  protected void preWrite(String instantTime, WriteOperationType writeOperationType,\n+      HoodieTableMetaClient metaClient) {\n     setOperationType(writeOperationType);\n-    syncTableMetadata();\n+    this.txnManager.setLastCompletedTransaction(metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .lastInstant());\n+    LOG.info(\"Last Instant Cached by writer with instant \" + instantTime + \" is \" + this.txnManager.getLastCompletedTransactionOwner());\n+    this.txnManager.setTransactionOwner(Option.of(new HoodieInstant(State.INFLIGHT, metaClient.getCommitActionType(), instantTime)));\n+    this.txnManager.beginTransaction();\n+    try {\n+      syncTableMetadata();\n+    } finally {\n+      this.txnManager.endTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODExOTc1OA==", "bodyText": "Right now, endTransaction() is doing the job or end & abort - no difference in behavior. Both ensure that if lock was acquired release and cleanup other state.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588119758", "createdAt": "2021-03-05T08:37:21Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -359,10 +388,21 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan\n    * Common method containing steps to be performed before write (upsert/insert/...\n    * @param instantTime\n    * @param writeOperationType\n+   * @param metaClient\n    */\n-  protected void preWrite(String instantTime, WriteOperationType writeOperationType) {\n+  protected void preWrite(String instantTime, WriteOperationType writeOperationType,\n+      HoodieTableMetaClient metaClient) {\n     setOperationType(writeOperationType);\n-    syncTableMetadata();\n+    this.txnManager.setLastCompletedTransaction(metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .lastInstant());\n+    LOG.info(\"Last Instant Cached by writer with instant \" + instantTime + \" is \" + this.txnManager.getLastCompletedTransactionOwner());\n+    this.txnManager.setTransactionOwner(Option.of(new HoodieInstant(State.INFLIGHT, metaClient.getCommitActionType(), instantTime)));\n+    this.txnManager.beginTransaction();\n+    try {\n+      syncTableMetadata();\n+    } finally {\n+      this.txnManager.endTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NTM5Nw=="}, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY1MDIwMA==", "bodyText": "I was wondering about the following scenario. if the cleanup fails, I guess it throws an error in both cases also? if so, this is okay. gtg", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588650200", "createdAt": "2021-03-05T20:05:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -359,10 +388,21 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan\n    * Common method containing steps to be performed before write (upsert/insert/...\n    * @param instantTime\n    * @param writeOperationType\n+   * @param metaClient\n    */\n-  protected void preWrite(String instantTime, WriteOperationType writeOperationType) {\n+  protected void preWrite(String instantTime, WriteOperationType writeOperationType,\n+      HoodieTableMetaClient metaClient) {\n     setOperationType(writeOperationType);\n-    syncTableMetadata();\n+    this.txnManager.setLastCompletedTransaction(metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .lastInstant());\n+    LOG.info(\"Last Instant Cached by writer with instant \" + instantTime + \" is \" + this.txnManager.getLastCompletedTransactionOwner());\n+    this.txnManager.setTransactionOwner(Option.of(new HoodieInstant(State.INFLIGHT, metaClient.getCommitActionType(), instantTime)));\n+    this.txnManager.beginTransaction();\n+    try {\n+      syncTableMetadata();\n+    } finally {\n+      this.txnManager.endTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NTM5Nw=="}, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODg1Nzc0Mw==", "bodyText": "Yes, underlying unlock implementation should throw error.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588857743", "createdAt": "2021-03-06T09:15:53Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -359,10 +388,21 @@ public abstract O bulkInsertPreppedRecords(I preppedRecords, final String instan\n    * Common method containing steps to be performed before write (upsert/insert/...\n    * @param instantTime\n    * @param writeOperationType\n+   * @param metaClient\n    */\n-  protected void preWrite(String instantTime, WriteOperationType writeOperationType) {\n+  protected void preWrite(String instantTime, WriteOperationType writeOperationType,\n+      HoodieTableMetaClient metaClient) {\n     setOperationType(writeOperationType);\n-    syncTableMetadata();\n+    this.txnManager.setLastCompletedTransaction(metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .lastInstant());\n+    LOG.info(\"Last Instant Cached by writer with instant \" + instantTime + \" is \" + this.txnManager.getLastCompletedTransactionOwner());\n+    this.txnManager.setTransactionOwner(Option.of(new HoodieInstant(State.INFLIGHT, metaClient.getCommitActionType(), instantTime)));\n+    this.txnManager.beginTransaction();\n+    try {\n+      syncTableMetadata();\n+    } finally {\n+      this.txnManager.endTransaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NTM5Nw=="}, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNDUzMDk0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0NTozNlrOIv3EZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0NTozNlrOIv3EZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NjIzMA==", "bodyText": "duplicate line?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587056230", "createdAt": "2021-03-04T03:45:36Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -389,29 +429,33 @@ protected void postCommit(HoodieTable<T, I, K, O> table, HoodieCommitMetadata me\n       // Delete the marker directory for the instant.\n       new MarkerFiles(table, instantTime).quietDeleteMarkerDir(context, config.getMarkersDeleteParallelism());\n \n-      // Do an inline compaction if enabled\n-      if (config.isInlineCompaction()) {\n-        runAnyPendingCompactions(table);\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n-        inlineCompact(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n-      }\n+      if (config.inlineTableServices()) {\n+        // Do an inline compaction if enabled\n+        if (config.inlineCompactionEnabled()) {\n+          runAnyPendingCompactions(table);\n+          metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"true\");\n+          inlineCompact(extraMetadata);\n+        } else {\n+          metadata.addMetadata(HoodieCompactionConfig.INLINE_COMPACT_PROP, \"false\");\n+        }\n \n-      // Do an inline clustering if enabled\n-      if (config.isInlineClustering()) {\n-        runAnyPendingClustering(table);\n-        metadata.addMetadata(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"true\");\n-        inlineCluster(extraMetadata);\n-      } else {\n-        metadata.addMetadata(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"false\");\n+        // Do an inline clustering if enabled\n+        if (config.inlineClusteringEnabled()) {\n+          runAnyPendingClustering(table);\n+          metadata.addMetadata(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"true\");\n+          inlineCluster(extraMetadata);\n+        } else {\n+          metadata.addMetadata(HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"false\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwNDU0MzQ1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0NzozOVrOIv3Kpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQwMzo0NzozOVrOIv3Kpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzA1NzgzMQ==", "bodyText": "rename: compactionInstantTime", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587057831", "createdAt": "2021-03-04T03:47:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -797,7 +853,9 @@ public Boolean rollbackFailedWrites() {\n    * Performs a compaction operation on a table, serially before or after an insert/upsert action.\n    */\n   protected Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n-    Option<String> compactionInstantTimeOpt = scheduleCompaction(extraMetadata);\n+    String schedulingCompactionInstant = HoodieActiveTimeline.createNewInstantTime();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61275eac2605bdb087762050588603bab4c4ee2d"}, "originalPosition": 250}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODcxNTUxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoxNDoyNFrOIweopg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoxNDoyNFrOIweopg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNDQ4Ng==", "bodyText": "rename: getCandidateInstants() to clarify intent.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587704486", "createdAt": "2021-03-04T18:14:24Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.stream.Stream;\n+\n+/**\n+ * Strategy interface for conflict resolution with multiple writers.\n+ * Users can provide pluggable implementations for different kinds of strategies to resolve conflicts when multiple\n+ * writers are mutating the hoodie table.\n+ */\n+public interface ConflictResolutionStrategy {\n+\n+  /**\n+   * Stream of instants to check conflicts against.\n+   * @return\n+   */\n+  Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant, Option<HoodieInstant> lastSuccessfulInstant);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODcxNjQwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoxNDo0MFrOIwepOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoxNDo0MFrOIwepOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNDYzMg==", "bodyText": "this reads nicely :)", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587704632", "createdAt": "2021-03-04T18:14:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.stream.Stream;\n+\n+/**\n+ * Strategy interface for conflict resolution with multiple writers.\n+ * Users can provide pluggable implementations for different kinds of strategies to resolve conflicts when multiple\n+ * writers are mutating the hoodie table.\n+ */\n+public interface ConflictResolutionStrategy {\n+\n+  /**\n+   * Stream of instants to check conflicts against.\n+   * @return\n+   */\n+  Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant, Option<HoodieInstant> lastSuccessfulInstant);\n+\n+  /**\n+   * Implementations of this method will determine whether a conflict exists between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODcyMjQ3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoxNTo0OVrOIwesog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQwOToxNjo0MlrOIxlBxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNTUwNg==", "bodyText": "having the metadata writer passed in, feels off. any way to avoid this?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587705506", "createdAt": "2021-03-04T18:15:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.stream.Stream;\n+\n+/**\n+ * Strategy interface for conflict resolution with multiple writers.\n+ * Users can provide pluggable implementations for different kinds of strategies to resolve conflicts when multiple\n+ * writers are mutating the hoodie table.\n+ */\n+public interface ConflictResolutionStrategy {\n+\n+  /**\n+   * Stream of instants to check conflicts against.\n+   * @return\n+   */\n+  Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant, Option<HoodieInstant> lastSuccessfulInstant);\n+\n+  /**\n+   * Implementations of this method will determine whether a conflict exists between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation);\n+\n+  /**\n+   * Implementations of this method will determine how to resolve a conflict between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  Option<HoodieCommitMetadata> resolveConflict(Option<HoodieBackedTableMetadataWriter> metadataWriter, HoodieTable table,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgxMzY4NA==", "bodyText": "So this is being passed to allow for the metadata to be manipulated to do some kind of conflict resolution. Right now it's not being used anywhere, I can remove it but will need to be added sometime soon when we need to do conflict resolution more than just throwing exception.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587813684", "createdAt": "2021-03-04T20:47:24Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.stream.Stream;\n+\n+/**\n+ * Strategy interface for conflict resolution with multiple writers.\n+ * Users can provide pluggable implementations for different kinds of strategies to resolve conflicts when multiple\n+ * writers are mutating the hoodie table.\n+ */\n+public interface ConflictResolutionStrategy {\n+\n+  /**\n+   * Stream of instants to check conflicts against.\n+   * @return\n+   */\n+  Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant, Option<HoodieInstant> lastSuccessfulInstant);\n+\n+  /**\n+   * Implementations of this method will determine whether a conflict exists between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation);\n+\n+  /**\n+   * Implementations of this method will determine how to resolve a conflict between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  Option<HoodieCommitMetadata> resolveConflict(Option<HoodieBackedTableMetadataWriter> metadataWriter, HoodieTable table,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNTUwNg=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY1MTQwNg==", "bodyText": "lets remove it. we can introduce it as needed. This will simplify the implementation, as it stands now.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588651406", "createdAt": "2021-03-05T20:06:35Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.stream.Stream;\n+\n+/**\n+ * Strategy interface for conflict resolution with multiple writers.\n+ * Users can provide pluggable implementations for different kinds of strategies to resolve conflicts when multiple\n+ * writers are mutating the hoodie table.\n+ */\n+public interface ConflictResolutionStrategy {\n+\n+  /**\n+   * Stream of instants to check conflicts against.\n+   * @return\n+   */\n+  Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant, Option<HoodieInstant> lastSuccessfulInstant);\n+\n+  /**\n+   * Implementations of this method will determine whether a conflict exists between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation);\n+\n+  /**\n+   * Implementations of this method will determine how to resolve a conflict between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  Option<HoodieCommitMetadata> resolveConflict(Option<HoodieBackedTableMetadataWriter> metadataWriter, HoodieTable table,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNTUwNg=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODg1Nzc5OQ==", "bodyText": "Removed", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588857799", "createdAt": "2021-03-06T09:16:42Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.stream.Stream;\n+\n+/**\n+ * Strategy interface for conflict resolution with multiple writers.\n+ * Users can provide pluggable implementations for different kinds of strategies to resolve conflicts when multiple\n+ * writers are mutating the hoodie table.\n+ */\n+public interface ConflictResolutionStrategy {\n+\n+  /**\n+   * Stream of instants to check conflicts against.\n+   * @return\n+   */\n+  Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant, Option<HoodieInstant> lastSuccessfulInstant);\n+\n+  /**\n+   * Implementations of this method will determine whether a conflict exists between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation);\n+\n+  /**\n+   * Implementations of this method will determine how to resolve a conflict between 2 commits.\n+   * @param thisOperation\n+   * @param otherOperation\n+   * @return\n+   */\n+  @PublicAPIMethod(maturity = ApiMaturityLevel.EVOLVING)\n+  Option<HoodieCommitMetadata> resolveConflict(Option<HoodieBackedTableMetadataWriter> metadataWriter, HoodieTable table,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNTUwNg=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODcyNjM1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/HoodieCommitOperation.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoxNjo0NlrOIwevEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDoxMDowMFrOIxYnAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNjEzMQ==", "bodyText": "rename: ConflictingOperation", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587706131", "createdAt": "2021-03-04T18:16:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/HoodieCommitOperation.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieCommonMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMMIT_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.DELTA_COMMIT_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is used to hold all information used to identify how to resolve conflicts between instants.\n+ * Since we interchange payload types between AVRO specific records and POJO's, this object serves as\n+ * a common payload to manage these conversions.\n+ */\n+public class HoodieCommitOperation {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgxNjA5NQ==", "bodyText": "So this is just wrap the CommitMetadata to a common payload. ConflictingOperation suggests this is already a conflicting operation which it is not yet. Open to other suggestions if you have", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587816095", "createdAt": "2021-03-04T20:51:22Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/HoodieCommitOperation.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieCommonMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMMIT_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.DELTA_COMMIT_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is used to hold all information used to identify how to resolve conflicts between instants.\n+ * Since we interchange payload types between AVRO specific records and POJO's, this object serves as\n+ * a common payload to manage these conversions.\n+ */\n+public class HoodieCommitOperation {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNjEzMQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY1NDMzNw==", "bodyText": "So the thing is , this has the common metadata, overloading \"commit\" is always confusing in our code base, since commit often refers to a specific action type. Given its used specifically, in the conflict resolution scenario, I think somethihg like ConcurrentOperation captures the intent. Its just concurrent to the current operation, the conflict resolution will determine if its actually a conflict like you mentioned.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588654337", "createdAt": "2021-03-05T20:10:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/HoodieCommitOperation.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieCommonMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMMIT_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.DELTA_COMMIT_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is used to hold all information used to identify how to resolve conflicts between instants.\n+ * Since we interchange payload types between AVRO specific records and POJO's, this object serves as\n+ * a common payload to manage these conversions.\n+ */\n+public class HoodieCommitOperation {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNjEzMQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODczNjIyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/LockManager.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoxOTowNFrOIwe1Hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQxOTo0MzozOFrOIxo_Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNzY3OQ==", "bodyText": "any special handling for InterruptedException? this is a common cause of bugs in such locking code paths", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587707679", "createdAt": "2021-03-04T18:19:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/LockManager.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * This class wraps implementations of {@link LockProvider} and provides an easy way to manage the lifecycle of a lock.\n+ */\n+public class LockManager implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockManager.class);\n+  private final HoodieWriteConfig writeConfig;\n+  private final LockConfiguration lockConfiguration;\n+  private final SerializableConfiguration hadoopConf;\n+  private volatile LockProvider lockProvider;\n+  // Holds the latest completed write instant to know which ones to check conflict against\n+  private final AtomicReference<Option<HoodieInstant>> latestCompletedWriteInstant;\n+\n+  public LockManager(HoodieWriteConfig writeConfig, FileSystem fs) {\n+    this.latestCompletedWriteInstant = new AtomicReference<>(Option.empty());\n+    this.writeConfig = writeConfig;\n+    this.hadoopConf = new SerializableConfiguration(fs.getConf());\n+    this.lockConfiguration = new LockConfiguration(writeConfig.getProps());\n+  }\n+\n+  public void lock() {\n+    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      LockProvider lockProvider = getLockProvider();\n+      boolean acquired = false;\n+      try {\n+        int retries = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP);\n+        long waitTimeInMs = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP);\n+        int retryCount = 0;\n+        while (retryCount <= retries) {\n+          acquired = lockProvider.tryLock(writeConfig.getLockAcquireWaitTimeoutInMs(), TimeUnit.MILLISECONDS);\n+          if (acquired) {\n+            break;\n+          }\n+          LOG.info(\"Retrying...\");\n+          Thread.sleep(waitTimeInMs);\n+          retryCount++;\n+        }\n+      } catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODA2NjAwNw==", "bodyText": "If the lock was accepted by the LockProvider server but an interrupted exception happens then we rely on the fact that the lock will timeout after X mins (settings in HiveMetastore & Zookeeper). I have tested in in my production runs.\nI have added some special checks for HiveMetastore in case of interruptedException but for Zookeeper it's not possible to do those checks.\n acquired = lockProvider.tryLock(writeConfig.getLockAcquireWaitTimeoutInMs(), TimeUnit.MILLISECONDS);\n          if (acquired) {\n            break;\n          }\n\nAnother extremely low probability is for the above code, lock is acquired but the running thread gets Interrupted before it can break. Again in this case, we just rely on the Lock Timeout on the server side of the LockProviders.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588066007", "createdAt": "2021-03-05T06:40:54Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/LockManager.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * This class wraps implementations of {@link LockProvider} and provides an easy way to manage the lifecycle of a lock.\n+ */\n+public class LockManager implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockManager.class);\n+  private final HoodieWriteConfig writeConfig;\n+  private final LockConfiguration lockConfiguration;\n+  private final SerializableConfiguration hadoopConf;\n+  private volatile LockProvider lockProvider;\n+  // Holds the latest completed write instant to know which ones to check conflict against\n+  private final AtomicReference<Option<HoodieInstant>> latestCompletedWriteInstant;\n+\n+  public LockManager(HoodieWriteConfig writeConfig, FileSystem fs) {\n+    this.latestCompletedWriteInstant = new AtomicReference<>(Option.empty());\n+    this.writeConfig = writeConfig;\n+    this.hadoopConf = new SerializableConfiguration(fs.getConf());\n+    this.lockConfiguration = new LockConfiguration(writeConfig.getProps());\n+  }\n+\n+  public void lock() {\n+    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      LockProvider lockProvider = getLockProvider();\n+      boolean acquired = false;\n+      try {\n+        int retries = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP);\n+        long waitTimeInMs = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP);\n+        int retryCount = 0;\n+        while (retryCount <= retries) {\n+          acquired = lockProvider.tryLock(writeConfig.getLockAcquireWaitTimeoutInMs(), TimeUnit.MILLISECONDS);\n+          if (acquired) {\n+            break;\n+          }\n+          LOG.info(\"Retrying...\");\n+          Thread.sleep(waitTimeInMs);\n+          retryCount++;\n+        }\n+      } catch (Exception e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNzY3OQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY1NTI3NQ==", "bodyText": "I think. for the retries, we should handle interrupted exception and continue retrying. Thats what I was getting at.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588655275", "createdAt": "2021-03-05T20:11:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/LockManager.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * This class wraps implementations of {@link LockProvider} and provides an easy way to manage the lifecycle of a lock.\n+ */\n+public class LockManager implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockManager.class);\n+  private final HoodieWriteConfig writeConfig;\n+  private final LockConfiguration lockConfiguration;\n+  private final SerializableConfiguration hadoopConf;\n+  private volatile LockProvider lockProvider;\n+  // Holds the latest completed write instant to know which ones to check conflict against\n+  private final AtomicReference<Option<HoodieInstant>> latestCompletedWriteInstant;\n+\n+  public LockManager(HoodieWriteConfig writeConfig, FileSystem fs) {\n+    this.latestCompletedWriteInstant = new AtomicReference<>(Option.empty());\n+    this.writeConfig = writeConfig;\n+    this.hadoopConf = new SerializableConfiguration(fs.getConf());\n+    this.lockConfiguration = new LockConfiguration(writeConfig.getProps());\n+  }\n+\n+  public void lock() {\n+    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      LockProvider lockProvider = getLockProvider();\n+      boolean acquired = false;\n+      try {\n+        int retries = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP);\n+        long waitTimeInMs = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP);\n+        int retryCount = 0;\n+        while (retryCount <= retries) {\n+          acquired = lockProvider.tryLock(writeConfig.getLockAcquireWaitTimeoutInMs(), TimeUnit.MILLISECONDS);\n+          if (acquired) {\n+            break;\n+          }\n+          LOG.info(\"Retrying...\");\n+          Thread.sleep(waitTimeInMs);\n+          retryCount++;\n+        }\n+      } catch (Exception e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNzY3OQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODkyMjYzMA==", "bodyText": "added", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588922630", "createdAt": "2021-03-06T19:43:38Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/LockManager.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * This class wraps implementations of {@link LockProvider} and provides an easy way to manage the lifecycle of a lock.\n+ */\n+public class LockManager implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockManager.class);\n+  private final HoodieWriteConfig writeConfig;\n+  private final LockConfiguration lockConfiguration;\n+  private final SerializableConfiguration hadoopConf;\n+  private volatile LockProvider lockProvider;\n+  // Holds the latest completed write instant to know which ones to check conflict against\n+  private final AtomicReference<Option<HoodieInstant>> latestCompletedWriteInstant;\n+\n+  public LockManager(HoodieWriteConfig writeConfig, FileSystem fs) {\n+    this.latestCompletedWriteInstant = new AtomicReference<>(Option.empty());\n+    this.writeConfig = writeConfig;\n+    this.hadoopConf = new SerializableConfiguration(fs.getConf());\n+    this.lockConfiguration = new LockConfiguration(writeConfig.getProps());\n+  }\n+\n+  public void lock() {\n+    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      LockProvider lockProvider = getLockProvider();\n+      boolean acquired = false;\n+      try {\n+        int retries = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP);\n+        long waitTimeInMs = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP);\n+        int retryCount = 0;\n+        while (retryCount <= retries) {\n+          acquired = lockProvider.tryLock(writeConfig.getLockAcquireWaitTimeoutInMs(), TimeUnit.MILLISECONDS);\n+          if (acquired) {\n+            break;\n+          }\n+          LOG.info(\"Retrying...\");\n+          Thread.sleep(waitTimeInMs);\n+          retryCount++;\n+        }\n+      } catch (Exception e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwNzY3OQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODczOTcxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/LockManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoxOTo1N1rOIwe3PQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQwNjozMjo0NVrOIw0g5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwODIyMQ==", "bodyText": "I assume the providers unlock() will throw more exceptions.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587708221", "createdAt": "2021-03-04T18:19:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/LockManager.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * This class wraps implementations of {@link LockProvider} and provides an easy way to manage the lifecycle of a lock.\n+ */\n+public class LockManager implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockManager.class);\n+  private final HoodieWriteConfig writeConfig;\n+  private final LockConfiguration lockConfiguration;\n+  private final SerializableConfiguration hadoopConf;\n+  private volatile LockProvider lockProvider;\n+  // Holds the latest completed write instant to know which ones to check conflict against\n+  private final AtomicReference<Option<HoodieInstant>> latestCompletedWriteInstant;\n+\n+  public LockManager(HoodieWriteConfig writeConfig, FileSystem fs) {\n+    this.latestCompletedWriteInstant = new AtomicReference<>(Option.empty());\n+    this.writeConfig = writeConfig;\n+    this.hadoopConf = new SerializableConfiguration(fs.getConf());\n+    this.lockConfiguration = new LockConfiguration(writeConfig.getProps());\n+  }\n+\n+  public void lock() {\n+    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      LockProvider lockProvider = getLockProvider();\n+      boolean acquired = false;\n+      try {\n+        int retries = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP);\n+        long waitTimeInMs = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP);\n+        int retryCount = 0;\n+        while (retryCount <= retries) {\n+          acquired = lockProvider.tryLock(writeConfig.getLockAcquireWaitTimeoutInMs(), TimeUnit.MILLISECONDS);\n+          if (acquired) {\n+            break;\n+          }\n+          LOG.info(\"Retrying...\");\n+          Thread.sleep(waitTimeInMs);\n+          retryCount++;\n+        }\n+      } catch (Exception e) {\n+        throw new HoodieLockException(\"Unable to acquire lock \", e);\n+      }\n+      if (!acquired) {\n+        throw new HoodieLockException(\"Unable to acquire lock, lock object \" + lockProvider.getLock());\n+      }\n+    }\n+  }\n+\n+  public void unlock() {\n+    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      getLockProvider().unlock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODA2Mjk0OQ==", "bodyText": "Yes, both providers underlying implementation return void but throw exceptions", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588062949", "createdAt": "2021-03-05T06:32:45Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/LockManager.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * This class wraps implementations of {@link LockProvider} and provides an easy way to manage the lifecycle of a lock.\n+ */\n+public class LockManager implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(LockManager.class);\n+  private final HoodieWriteConfig writeConfig;\n+  private final LockConfiguration lockConfiguration;\n+  private final SerializableConfiguration hadoopConf;\n+  private volatile LockProvider lockProvider;\n+  // Holds the latest completed write instant to know which ones to check conflict against\n+  private final AtomicReference<Option<HoodieInstant>> latestCompletedWriteInstant;\n+\n+  public LockManager(HoodieWriteConfig writeConfig, FileSystem fs) {\n+    this.latestCompletedWriteInstant = new AtomicReference<>(Option.empty());\n+    this.writeConfig = writeConfig;\n+    this.hadoopConf = new SerializableConfiguration(fs.getConf());\n+    this.lockConfiguration = new LockConfiguration(writeConfig.getProps());\n+  }\n+\n+  public void lock() {\n+    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      LockProvider lockProvider = getLockProvider();\n+      boolean acquired = false;\n+      try {\n+        int retries = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP);\n+        long waitTimeInMs = lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP);\n+        int retryCount = 0;\n+        while (retryCount <= retries) {\n+          acquired = lockProvider.tryLock(writeConfig.getLockAcquireWaitTimeoutInMs(), TimeUnit.MILLISECONDS);\n+          if (acquired) {\n+            break;\n+          }\n+          LOG.info(\"Retrying...\");\n+          Thread.sleep(waitTimeInMs);\n+          retryCount++;\n+        }\n+      } catch (Exception e) {\n+        throw new HoodieLockException(\"Unable to acquire lock \", e);\n+      }\n+      if (!acquired) {\n+        throw new HoodieLockException(\"Unable to acquire lock, lock object \" + lockProvider.getLock());\n+      }\n+    }\n+  }\n+\n+  public void unlock() {\n+    if (writeConfig.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      getLockProvider().unlock();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcwODIyMQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODc1NTc3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoyNDowNFrOIwfBxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMDo1MjoyOVrOIwlfAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMDkxOA==", "bodyText": "can we avoid passing the \"0\" or use an existing constant for init instant time etc", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587710918", "createdAt": "2021-03-04T18:24:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgxNjcwNQ==", "bodyText": "Used HoodieTimeline.INIT_INSTANT_TS now", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587816705", "createdAt": "2021-03-04T20:52:29Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMDkxOA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODc2MTA5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoyNToyNFrOIwfFKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQxOTowNjoxM1rOIxou9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMTc4NQ==", "bodyText": "is this an error though. can we move to INFO", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587711785", "createdAt": "2021-03-04T18:25:24Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes between first operation = \" + thisOperation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgxNzA5OQ==", "bodyText": "I can make it WARN. This is useful to debug issues and INFO might get ignored.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587817099", "createdAt": "2021-03-04T20:53:11Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes between first operation = \" + thisOperation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMTc4NQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY1NjI0OA==", "bodyText": "WARN should indicate abnormal execution. So may be or may be not. I still think INFO is the cleanest, since this code is supposed to handle the conflcting case. For debugging, users can always turn it on.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588656248", "createdAt": "2021-03-05T20:12:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes between first operation = \" + thisOperation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMTc4NQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODkxODUxNw==", "bodyText": "Okay, made it INFO", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588918517", "createdAt": "2021-03-06T19:06:13Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes between first operation = \" + thisOperation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMTc4NQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODc3MjcxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoyNzo1N1rOIwfMMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDozMDoxM1rOIxZvmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMzU4Ng==", "bodyText": "but replace commit can also result from INSERT_OVERWRITE correct? how do we distinguish this? I feel we need a more nuanced check here. So ensure only writes fail each other.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587713586", "createdAt": "2021-03-04T18:27:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes between first operation = \" + thisOperation\n+          + \", second operation = \" + otherOperation + \" , intersecting file ids \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Option<HoodieCommitMetadata> resolveConflict(Option<HoodieBackedTableMetadataWriter> metadataWriter, HoodieTable table,\n+                                              HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // NOTE that any commits from table services such as compaction, clustering or cleaning since the\n+    // overlapping of files is handled using MVCC. Since compaction is eventually written as commit, we need to ensure\n+    // we handle this during conflict resolution and not treat the commit from compaction operation as a regular commit.\n+    if (otherOperation.getOperationType() == WriteOperationType.UNKNOWN", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODA4Njg1MQ==", "bodyText": "Re-did the comments let me know if it's more clear now.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588086851", "createdAt": "2021-03-05T07:34:02Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes between first operation = \" + thisOperation\n+          + \", second operation = \" + otherOperation + \" , intersecting file ids \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Option<HoodieCommitMetadata> resolveConflict(Option<HoodieBackedTableMetadataWriter> metadataWriter, HoodieTable table,\n+                                              HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // NOTE that any commits from table services such as compaction, clustering or cleaning since the\n+    // overlapping of files is handled using MVCC. Since compaction is eventually written as commit, we need to ensure\n+    // we handle this during conflict resolution and not treat the commit from compaction operation as a regular commit.\n+    if (otherOperation.getOperationType() == WriteOperationType.UNKNOWN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMzU4Ng=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY3MjkyMQ==", "bodyText": "See newer review, for followup", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588672921", "createdAt": "2021-03-05T20:30:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/SimpleConcurrentFileWritesConflictResolutionStrategy.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.ConcurrentModificationException;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.COMPACTION_ACTION;\n+import static org.apache.hudi.common.table.timeline.HoodieTimeline.REPLACE_COMMIT_ACTION;\n+\n+/**\n+ * This class is a basic implementation of a conflict resolution strategy for concurrent writes {@link ConflictResolutionStrategy}.\n+ */\n+public class SimpleConcurrentFileWritesConflictResolutionStrategy\n+    implements ConflictResolutionStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(SimpleConcurrentFileWritesConflictResolutionStrategy.class);\n+\n+  @Override\n+  public Stream<HoodieInstant> getInstantsStream(HoodieActiveTimeline activeTimeline, HoodieInstant currentInstant,\n+                                                 Option<HoodieInstant> lastSuccessfulInstant) {\n+\n+    // To find which instants are conflicting, we apply the following logic\n+    // 1. Get completed instants timeline only for commits that have happened since the last successful write.\n+    // 2. Get any scheduled or completed compaction or clustering operations that have started and/or finished\n+    // after the current instant. We need to check for write conflicts since they may have mutated the same files\n+    // that are being newly created by the current write.\n+    Stream<HoodieInstant> completedCommitsInstantStream = activeTimeline\n+        .getCommitsTimeline()\n+        // TODO : getWriteTimeline to ensure we include replace commits as well\n+        .filterCompletedInstants()\n+        .findInstantsAfter(lastSuccessfulInstant.isPresent() ? lastSuccessfulInstant.get().getTimestamp() : \"0\")\n+        .getInstants();\n+\n+    Stream<HoodieInstant> compactionAndClusteringTimeline = activeTimeline\n+        .getTimelineOfActions(CollectionUtils.createSet(REPLACE_COMMIT_ACTION, COMPACTION_ACTION))\n+        .findInstantsAfter(currentInstant.getTimestamp())\n+        .getInstants();\n+    return Stream.concat(completedCommitsInstantStream, compactionAndClusteringTimeline);\n+  }\n+\n+  @Override\n+  public boolean hasConflict(HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // TODO : UUID's can clash even for insert/insert, handle that case.\n+    Set<String> fileIdsSetForFirstInstant = thisOperation.getMutatedFileIds();\n+    Set<String> fileIdsSetForSecondInstant = otherOperation.getMutatedFileIds();\n+    Set<String> intersection = new HashSet<>(fileIdsSetForFirstInstant);\n+    intersection.retainAll(fileIdsSetForSecondInstant);\n+    if (!intersection.isEmpty()) {\n+      LOG.error(\"Found conflicting writes between first operation = \" + thisOperation\n+          + \", second operation = \" + otherOperation + \" , intersecting file ids \" + intersection);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Option<HoodieCommitMetadata> resolveConflict(Option<HoodieBackedTableMetadataWriter> metadataWriter, HoodieTable table,\n+                                              HoodieCommitOperation thisOperation, HoodieCommitOperation otherOperation) {\n+    // NOTE that any commits from table services such as compaction, clustering or cleaning since the\n+    // overlapping of files is handled using MVCC. Since compaction is eventually written as commit, we need to ensure\n+    // we handle this during conflict resolution and not treat the commit from compaction operation as a regular commit.\n+    if (otherOperation.getOperationType() == WriteOperationType.UNKNOWN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxMzU4Ng=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODc3NzQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/TransactionManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoyOTowNVrOIwfPMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoyOTowNVrOIwfPMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxNDM1Mg==", "bodyText": "combine into a single log line?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587714352", "createdAt": "2021-03-04T18:29:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/TransactionManager.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * This class allows clients to start and end transactions. Anything done between a start and end transaction is\n+ * guaranteed to be atomic.\n+ */\n+public class TransactionManager implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(TransactionManager.class);\n+\n+  private final LockManager lockManager;\n+  private Option<HoodieInstant> currentTxnOwnerInstant;\n+  private Option<HoodieInstant> lastCompletedTxnOwnerInstant;\n+\n+  public TransactionManager(HoodieWriteConfig config, FileSystem fs) {\n+    this.lockManager = new LockManager(config, fs);\n+  }\n+\n+  public synchronized void setLastCompletedTransaction(Option<HoodieInstant> instant) {\n+    this.lastCompletedTxnOwnerInstant = instant;\n+    lockManager.compareAndSetLatestCompletedWriteInstant(lockManager.getLatestCompletedWriteInstant().get(), instant);\n+    LOG.info(\"Latest completed transaction instant \" + instant);\n+  }\n+\n+  public synchronized void setTransactionOwner(Option<HoodieInstant> instant) {\n+    this.currentTxnOwnerInstant = instant;\n+    LOG.info(\"Current transaction instant \" + instant);\n+  }\n+\n+  public synchronized void beginTransaction() {\n+    LOG.info(\"Transaction starting\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODc3NzgxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/TransactionManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoyOToxMVrOIwfPZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODoyOToxMVrOIwfPZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxNDQwNQ==", "bodyText": "same . combine?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587714405", "createdAt": "2021-03-04T18:29:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/TransactionManager.java", "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * This class allows clients to start and end transactions. Anything done between a start and end transaction is\n+ * guaranteed to be atomic.\n+ */\n+public class TransactionManager implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(TransactionManager.class);\n+\n+  private final LockManager lockManager;\n+  private Option<HoodieInstant> currentTxnOwnerInstant;\n+  private Option<HoodieInstant> lastCompletedTxnOwnerInstant;\n+\n+  public TransactionManager(HoodieWriteConfig config, FileSystem fs) {\n+    this.lockManager = new LockManager(config, fs);\n+  }\n+\n+  public synchronized void setLastCompletedTransaction(Option<HoodieInstant> instant) {\n+    this.lastCompletedTxnOwnerInstant = instant;\n+    lockManager.compareAndSetLatestCompletedWriteInstant(lockManager.getLatestCompletedWriteInstant().get(), instant);\n+    LOG.info(\"Latest completed transaction instant \" + instant);\n+  }\n+\n+  public synchronized void setTransactionOwner(Option<HoodieInstant> instant) {\n+    this.currentTxnOwnerInstant = instant;\n+    LOG.info(\"Current transaction instant \" + instant);\n+  }\n+\n+  public synchronized void beginTransaction() {\n+    LOG.info(\"Transaction starting\");\n+    LOG.info(\"Transaction Owner \" + currentTxnOwnerInstant);\n+    lockManager.lock();\n+    LOG.info(\"Transaction started\");\n+  }\n+\n+  public synchronized void endTransaction() {\n+    LOG.info(\"Transaction ending\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODc4NDc3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozMDo0OFrOIwfT2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozMDo0OFrOIwfT2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxNTU0NQ==", "bodyText": "make it final. In general, can you make a pass to ensure what can be final is made final.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587715545", "createdAt": "2021-03-04T18:30:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private CuratorFramework curatorFrameworkClient;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODc5MTIyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozMjozMVrOIwfYAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDoyMTowN1rOIxZLXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxNjYxMA==", "bodyText": "how are you testing this? I don't see curator added to any of the bundles? Same for the zookeeper dependencies. This is a really really important part", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587716610", "createdAt": "2021-03-04T18:32:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODExODMyMw==", "bodyText": "ZK & Curator comes with Hbase-Server. Do you want me to add it explicitly in the bundles ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588118323", "createdAt": "2021-03-05T08:34:56Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxNjYxMA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY2MDcwMg==", "bodyText": "In the bundles, we explicitly white list dependencies. So not sure how transitive dependencies would have been picked up.  How are you testing all this - not using the utilities/spark bundles?\nStandard practice for dependencies that are actually used in the projects code, is to explicitly deal with the dependency. We should not depend on hbase-server transitiively bringing it in.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588660702", "createdAt": "2021-03-05T20:17:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxNjYxMA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY2MzY0Nw==", "bodyText": "vmacs:target vs$ jar tf hudi-spark-bundle_2.11-0.8.0-SNAPSHOT.jar | grep curator | wc -l\n       0\nvmacs:target vs$ jar tf hudi-spark-bundle_2.11-0.8.0-SNAPSHOT.jar | grep hbase | wc -l\n    4734\nvmacs:target vs$ \n\nDon't see curator, in fact.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588663647", "createdAt": "2021-03-05T20:21:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxNjYxMA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODgwMTc5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozNTowNlrOIwfenA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMDo1NzozM1rOIwlrVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxODMwMA==", "bodyText": "but we seem to synchronize down below?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587718300", "createdAt": "2021-03-04T18:35:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgxOTg2MA==", "bodyText": "The synchronized method is just for testing to start the ZK else parallel tests end up triggering start multiple times and that causes issues.. Since we don't use @VisibleTesting, I've put a comment", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587819860", "createdAt": "2021-03-04T20:57:33Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxODMwMA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODgwNzUzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozNTo0OFrOIwfiKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozNTo0OFrOIwfiKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcxOTIxMQ==", "bodyText": "why public", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587719211", "createdAt": "2021-03-04T18:35:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private CuratorFramework curatorFrameworkClient;\n+  private volatile InterProcessMutex lock = null;\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final Configuration conf) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = CuratorFrameworkFactory.builder()\n+        .connectString(lockConfiguration.getConfig().getString(ZK_CONNECT_URL_PROP))\n+        .retryPolicy(new BoundedExponentialBackoffRetry(lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP),\n+            5000, lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP)))\n+        .sessionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_SESSION_TIMEOUT_MS_PROP, DEFAULT_ZK_SESSION_TIMEOUT_MS))\n+        .connectionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_CONNECTION_TIMEOUT_MS_PROP, DEFAULT_ZK_CONNECTION_TIMEOUT_MS))\n+        .build();\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final CuratorFramework curatorFrameworkClient) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = curatorFrameworkClient;\n+    synchronized (this.curatorFrameworkClient) {\n+      if (this.curatorFrameworkClient.getState() != CuratorFrameworkState.STARTED) {\n+        this.curatorFrameworkClient.start();\n+      }\n+    }\n+  }\n+\n+  ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+  }\n+  \n+  public void acquireLock(long time, TimeUnit unit) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODgxNzI0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozNzoyM1rOIwfoXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDo0MTowNVrOIxaXZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMDc5Nw==", "bodyText": "so, the lock can be acquired by another process and we don't raise this exception?  is that ok? I see lines 94-96 above, where we simply do the assignment and not raise any exceptions from an else block", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587720797", "createdAt": "2021-03-04T18:37:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private CuratorFramework curatorFrameworkClient;\n+  private volatile InterProcessMutex lock = null;\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final Configuration conf) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = CuratorFrameworkFactory.builder()\n+        .connectString(lockConfiguration.getConfig().getString(ZK_CONNECT_URL_PROP))\n+        .retryPolicy(new BoundedExponentialBackoffRetry(lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP),\n+            5000, lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP)))\n+        .sessionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_SESSION_TIMEOUT_MS_PROP, DEFAULT_ZK_SESSION_TIMEOUT_MS))\n+        .connectionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_CONNECTION_TIMEOUT_MS_PROP, DEFAULT_ZK_CONNECTION_TIMEOUT_MS))\n+        .build();\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final CuratorFramework curatorFrameworkClient) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = curatorFrameworkClient;\n+    synchronized (this.curatorFrameworkClient) {\n+      if (this.curatorFrameworkClient.getState() != CuratorFrameworkState.STARTED) {\n+        this.curatorFrameworkClient.start();\n+      }\n+    }\n+  }\n+\n+  ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+  }\n+  \n+  public void acquireLock(long time, TimeUnit unit) throws Exception {\n+    ValidationUtils.checkArgument(this.lock == null, LockState.ALREADY_ACQUIRED.name());\n+    InterProcessMutex newLock = new InterProcessMutex(\n+        this.curatorFrameworkClient, lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP) + \"/\"\n+        + this.lockConfiguration.getConfig().getString(ZK_LOCK_KEY_PROP));\n+    newLock.acquire(time, unit);\n+    if (newLock.isAcquiredInThisProcess()) {\n+      lock = newLock;\n+    }\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    LOG.info(generateLogStatement(LockState.ACQUIRING, generateLogSuffixString()));\n+    try {\n+      acquireLock(time, unit);\n+      LOG.info(generateLogStatement(LockState.ACQUIRED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_ACQUIRE, generateLogSuffixString()), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODEyNDgyNQ==", "bodyText": "This is not the code on my local. Something amiss happened during rebase and squash commits on my local. I have re-done the merge to ensure nothing got lost.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588124825", "createdAt": "2021-03-05T08:45:58Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private CuratorFramework curatorFrameworkClient;\n+  private volatile InterProcessMutex lock = null;\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final Configuration conf) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = CuratorFrameworkFactory.builder()\n+        .connectString(lockConfiguration.getConfig().getString(ZK_CONNECT_URL_PROP))\n+        .retryPolicy(new BoundedExponentialBackoffRetry(lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP),\n+            5000, lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP)))\n+        .sessionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_SESSION_TIMEOUT_MS_PROP, DEFAULT_ZK_SESSION_TIMEOUT_MS))\n+        .connectionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_CONNECTION_TIMEOUT_MS_PROP, DEFAULT_ZK_CONNECTION_TIMEOUT_MS))\n+        .build();\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final CuratorFramework curatorFrameworkClient) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = curatorFrameworkClient;\n+    synchronized (this.curatorFrameworkClient) {\n+      if (this.curatorFrameworkClient.getState() != CuratorFrameworkState.STARTED) {\n+        this.curatorFrameworkClient.start();\n+      }\n+    }\n+  }\n+\n+  ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+  }\n+  \n+  public void acquireLock(long time, TimeUnit unit) throws Exception {\n+    ValidationUtils.checkArgument(this.lock == null, LockState.ALREADY_ACQUIRED.name());\n+    InterProcessMutex newLock = new InterProcessMutex(\n+        this.curatorFrameworkClient, lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP) + \"/\"\n+        + this.lockConfiguration.getConfig().getString(ZK_LOCK_KEY_PROP));\n+    newLock.acquire(time, unit);\n+    if (newLock.isAcquiredInThisProcess()) {\n+      lock = newLock;\n+    }\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    LOG.info(generateLogStatement(LockState.ACQUIRING, generateLogSuffixString()));\n+    try {\n+      acquireLock(time, unit);\n+      LOG.info(generateLogStatement(LockState.ACQUIRED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_ACQUIRE, generateLogSuffixString()), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMDc5Nw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY4MzEwOA==", "bodyText": "I think the actual code you have know handles this.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588683108", "createdAt": "2021-03-05T20:41:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/lock/ZookeeperBasedLockProvider.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.lock;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.framework.imps.CuratorFrameworkState;\n+import org.apache.curator.framework.recipes.locks.InterProcessMutex;\n+import org.apache.curator.retry.BoundedExponentialBackoffRetry;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.lock.LockProvider;\n+import org.apache.hudi.common.lock.LockState;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+/**\n+ * A zookeeper based lock. This {@link LockProvider} implementation allows to lock table operations\n+ * using zookeeper. Users need to have a Zookeeper cluster deployed to be able to use this lock.\n+ */\n+@NotThreadSafe\n+public class ZookeeperBasedLockProvider extends LockProvider {\n+\n+  private static final Logger LOG = LogManager.getLogger(ZookeeperBasedLockProvider.class);\n+\n+  private CuratorFramework curatorFrameworkClient;\n+  private volatile InterProcessMutex lock = null;\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final Configuration conf) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = CuratorFrameworkFactory.builder()\n+        .connectString(lockConfiguration.getConfig().getString(ZK_CONNECT_URL_PROP))\n+        .retryPolicy(new BoundedExponentialBackoffRetry(lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP),\n+            5000, lockConfiguration.getConfig().getInteger(LOCK_ACQUIRE_NUM_RETRIES_PROP)))\n+        .sessionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_SESSION_TIMEOUT_MS_PROP, DEFAULT_ZK_SESSION_TIMEOUT_MS))\n+        .connectionTimeoutMs(lockConfiguration.getConfig().getInteger(ZK_CONNECTION_TIMEOUT_MS_PROP, DEFAULT_ZK_CONNECTION_TIMEOUT_MS))\n+        .build();\n+    this.curatorFrameworkClient.start();\n+  }\n+\n+  public ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration, final CuratorFramework curatorFrameworkClient) {\n+    this(lockConfiguration);\n+    this.curatorFrameworkClient = curatorFrameworkClient;\n+    synchronized (this.curatorFrameworkClient) {\n+      if (this.curatorFrameworkClient.getState() != CuratorFrameworkState.STARTED) {\n+        this.curatorFrameworkClient.start();\n+      }\n+    }\n+  }\n+\n+  ZookeeperBasedLockProvider(final LockConfiguration lockConfiguration) {\n+    checkRequiredProps(lockConfiguration);\n+    this.lockConfiguration = lockConfiguration;\n+  }\n+  \n+  public void acquireLock(long time, TimeUnit unit) throws Exception {\n+    ValidationUtils.checkArgument(this.lock == null, LockState.ALREADY_ACQUIRED.name());\n+    InterProcessMutex newLock = new InterProcessMutex(\n+        this.curatorFrameworkClient, lockConfiguration.getConfig().getString(ZK_BASE_PATH_PROP) + \"/\"\n+        + this.lockConfiguration.getConfig().getString(ZK_LOCK_KEY_PROP));\n+    newLock.acquire(time, unit);\n+    if (newLock.isAcquiredInThisProcess()) {\n+      lock = newLock;\n+    }\n+  }\n+\n+  @Override\n+  public boolean tryLock(long time, TimeUnit unit) {\n+    LOG.info(generateLogStatement(LockState.ACQUIRING, generateLogSuffixString()));\n+    try {\n+      acquireLock(time, unit);\n+      LOG.info(generateLogStatement(LockState.ACQUIRED, generateLogSuffixString()));\n+    } catch (Exception e) {\n+      throw new HoodieLockException(generateLogStatement(LockState.FAILED_TO_ACQUIRE, generateLogSuffixString()), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMDc5Nw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODgyMTE4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/HoodieMetadataConversionUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozODoxNFrOIwfqzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQwODo0NjoxNVrOIw4TMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMTQyMA==", "bodyText": "does this have an unit test on its own?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587721420", "createdAt": "2021-03-04T18:38:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/HoodieMetadataConversionUtils.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.avro.model.HoodieCompactionPlan;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.avro.model.HoodieSavepointMetadata;\n+import org.apache.hudi.client.ReplaceArchivalHelper;\n+import org.apache.hudi.common.model.ActionType;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRollingStatMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.CompactionUtils;\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to convert between different action related payloads and {@link HoodieArchivedMetaEntry}.\n+ */\n+public class HoodieMetadataConversionUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODEyNDk3Ng==", "bodyText": "Added one now", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588124976", "createdAt": "2021-03-05T08:46:15Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/HoodieMetadataConversionUtils.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.avro.model.HoodieCompactionPlan;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.avro.model.HoodieSavepointMetadata;\n+import org.apache.hudi.client.ReplaceArchivalHelper;\n+import org.apache.hudi.common.model.ActionType;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRollingStatMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.CompactionUtils;\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to convert between different action related payloads and {@link HoodieArchivedMetaEntry}.\n+ */\n+public class HoodieMetadataConversionUtils {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMTQyMA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODgyMzIwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/HoodieMetadataConversionUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozODozOFrOIwfr8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMTowNjo1MlrOIwmAVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMTcxMg==", "bodyText": "would this come back to haunt us ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587721712", "createdAt": "2021-03-04T18:38:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/HoodieMetadataConversionUtils.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.avro.model.HoodieCompactionPlan;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.avro.model.HoodieSavepointMetadata;\n+import org.apache.hudi.client.ReplaceArchivalHelper;\n+import org.apache.hudi.common.model.ActionType;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRollingStatMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.CompactionUtils;\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to convert between different action related payloads and {@link HoodieArchivedMetaEntry}.\n+ */\n+public class HoodieMetadataConversionUtils {\n+\n+  public static HoodieArchivedMetaEntry createMetaWrapper(HoodieInstant hoodieInstant, HoodieTableMetaClient metaClient) throws IOException {\n+    HoodieArchivedMetaEntry archivedMetaWrapper = new HoodieArchivedMetaEntry();\n+    archivedMetaWrapper.setCommitTime(hoodieInstant.getTimestamp());\n+    archivedMetaWrapper.setActionState(hoodieInstant.getState().name());\n+    switch (hoodieInstant.getAction()) {\n+      case HoodieTimeline.CLEAN_ACTION: {\n+        if (hoodieInstant.isCompleted()) {\n+          archivedMetaWrapper.setHoodieCleanMetadata(CleanerUtils.getCleanerMetadata(metaClient, hoodieInstant));\n+        } else {\n+          archivedMetaWrapper.setHoodieCleanerPlan(CleanerUtils.getCleanerPlan(metaClient, hoodieInstant));\n+        }\n+        archivedMetaWrapper.setActionType(ActionType.clean.name());\n+        break;\n+      }\n+      case HoodieTimeline.COMMIT_ACTION:\n+      case HoodieTimeline.DELTA_COMMIT_ACTION: {\n+        HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n+                .fromBytes(metaClient.getCommitTimeline().getInstantDetails(hoodieInstant).get(), HoodieCommitMetadata.class);\n+        archivedMetaWrapper.setHoodieCommitMetadata(convertCommitMetadata(commitMetadata));\n+        archivedMetaWrapper.setActionType(ActionType.commit.name());\n+        break;\n+      }\n+      case HoodieTimeline.REPLACE_COMMIT_ACTION: {\n+        HoodieReplaceCommitMetadata replaceCommitMetadata = HoodieReplaceCommitMetadata\n+                .fromBytes(metaClient.getCommitTimeline().getInstantDetails(hoodieInstant).get(), HoodieReplaceCommitMetadata.class);\n+        archivedMetaWrapper.setHoodieReplaceCommitMetadata(ReplaceArchivalHelper.convertReplaceCommitMetadata(replaceCommitMetadata));\n+        archivedMetaWrapper.setActionType(ActionType.replacecommit.name());\n+        break;\n+      }\n+      case HoodieTimeline.ROLLBACK_ACTION: {\n+        archivedMetaWrapper.setHoodieRollbackMetadata(TimelineMetadataUtils.deserializeAvroMetadata(\n+                metaClient.getCommitTimeline().getInstantDetails(hoodieInstant).get(), HoodieRollbackMetadata.class));\n+        archivedMetaWrapper.setActionType(ActionType.rollback.name());\n+        break;\n+      }\n+      case HoodieTimeline.SAVEPOINT_ACTION: {\n+        archivedMetaWrapper.setHoodieSavePointMetadata(TimelineMetadataUtils.deserializeAvroMetadata(\n+                metaClient.getCommitTimeline().getInstantDetails(hoodieInstant).get(), HoodieSavepointMetadata.class));\n+        archivedMetaWrapper.setActionType(ActionType.savepoint.name());\n+        break;\n+      }\n+      case HoodieTimeline.COMPACTION_ACTION: {\n+        HoodieCompactionPlan plan = CompactionUtils.getCompactionPlan(metaClient, hoodieInstant.getTimestamp());\n+        archivedMetaWrapper.setHoodieCompactionPlan(plan);\n+        archivedMetaWrapper.setActionType(ActionType.compaction.name());\n+        break;\n+      }\n+      default: {\n+        throw new UnsupportedOperationException(\"Action not fully supported yet\");\n+      }\n+    }\n+    return archivedMetaWrapper;\n+  }\n+\n+  public static HoodieArchivedMetaEntry createMetaWrapper(HoodieInstant hoodieInstant,\n+                                                          HoodieCommitMetadata hoodieCommitMetadata) {\n+    HoodieArchivedMetaEntry archivedMetaWrapper = new HoodieArchivedMetaEntry();\n+    archivedMetaWrapper.setCommitTime(hoodieInstant.getTimestamp());\n+    archivedMetaWrapper.setActionState(hoodieInstant.getState().name());\n+    archivedMetaWrapper.setHoodieCommitMetadata(convertCommitMetadata(hoodieCommitMetadata));\n+    archivedMetaWrapper.setActionType(ActionType.commit.name());\n+    return archivedMetaWrapper;\n+  }\n+\n+  public static org.apache.hudi.avro.model.HoodieCommitMetadata convertCommitMetadata(\n+          HoodieCommitMetadata hoodieCommitMetadata) {\n+    ObjectMapper mapper = new ObjectMapper();\n+    // Need this to ignore other public get() methods\n+    mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n+    org.apache.hudi.avro.model.HoodieCommitMetadata avroMetaData =\n+            mapper.convertValue(hoodieCommitMetadata, org.apache.hudi.avro.model.HoodieCommitMetadata.class);\n+    // Do not archive Rolling Stats, cannot set to null since AVRO will throw null pointer\n+    avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, \"\");\n+    return avroMetaData;\n+  }\n+\n+  // TODO : Fix converting from SpecificRecord to POJO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgyNTIzOQ==", "bodyText": "I've removed the method for now to be sure no one uses it.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587825239", "createdAt": "2021-03-04T21:06:52Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/HoodieMetadataConversionUtils.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.avro.model.HoodieCompactionPlan;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.avro.model.HoodieSavepointMetadata;\n+import org.apache.hudi.client.ReplaceArchivalHelper;\n+import org.apache.hudi.common.model.ActionType;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRollingStatMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.CompactionUtils;\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to convert between different action related payloads and {@link HoodieArchivedMetaEntry}.\n+ */\n+public class HoodieMetadataConversionUtils {\n+\n+  public static HoodieArchivedMetaEntry createMetaWrapper(HoodieInstant hoodieInstant, HoodieTableMetaClient metaClient) throws IOException {\n+    HoodieArchivedMetaEntry archivedMetaWrapper = new HoodieArchivedMetaEntry();\n+    archivedMetaWrapper.setCommitTime(hoodieInstant.getTimestamp());\n+    archivedMetaWrapper.setActionState(hoodieInstant.getState().name());\n+    switch (hoodieInstant.getAction()) {\n+      case HoodieTimeline.CLEAN_ACTION: {\n+        if (hoodieInstant.isCompleted()) {\n+          archivedMetaWrapper.setHoodieCleanMetadata(CleanerUtils.getCleanerMetadata(metaClient, hoodieInstant));\n+        } else {\n+          archivedMetaWrapper.setHoodieCleanerPlan(CleanerUtils.getCleanerPlan(metaClient, hoodieInstant));\n+        }\n+        archivedMetaWrapper.setActionType(ActionType.clean.name());\n+        break;\n+      }\n+      case HoodieTimeline.COMMIT_ACTION:\n+      case HoodieTimeline.DELTA_COMMIT_ACTION: {\n+        HoodieCommitMetadata commitMetadata = HoodieCommitMetadata\n+                .fromBytes(metaClient.getCommitTimeline().getInstantDetails(hoodieInstant).get(), HoodieCommitMetadata.class);\n+        archivedMetaWrapper.setHoodieCommitMetadata(convertCommitMetadata(commitMetadata));\n+        archivedMetaWrapper.setActionType(ActionType.commit.name());\n+        break;\n+      }\n+      case HoodieTimeline.REPLACE_COMMIT_ACTION: {\n+        HoodieReplaceCommitMetadata replaceCommitMetadata = HoodieReplaceCommitMetadata\n+                .fromBytes(metaClient.getCommitTimeline().getInstantDetails(hoodieInstant).get(), HoodieReplaceCommitMetadata.class);\n+        archivedMetaWrapper.setHoodieReplaceCommitMetadata(ReplaceArchivalHelper.convertReplaceCommitMetadata(replaceCommitMetadata));\n+        archivedMetaWrapper.setActionType(ActionType.replacecommit.name());\n+        break;\n+      }\n+      case HoodieTimeline.ROLLBACK_ACTION: {\n+        archivedMetaWrapper.setHoodieRollbackMetadata(TimelineMetadataUtils.deserializeAvroMetadata(\n+                metaClient.getCommitTimeline().getInstantDetails(hoodieInstant).get(), HoodieRollbackMetadata.class));\n+        archivedMetaWrapper.setActionType(ActionType.rollback.name());\n+        break;\n+      }\n+      case HoodieTimeline.SAVEPOINT_ACTION: {\n+        archivedMetaWrapper.setHoodieSavePointMetadata(TimelineMetadataUtils.deserializeAvroMetadata(\n+                metaClient.getCommitTimeline().getInstantDetails(hoodieInstant).get(), HoodieSavepointMetadata.class));\n+        archivedMetaWrapper.setActionType(ActionType.savepoint.name());\n+        break;\n+      }\n+      case HoodieTimeline.COMPACTION_ACTION: {\n+        HoodieCompactionPlan plan = CompactionUtils.getCompactionPlan(metaClient, hoodieInstant.getTimestamp());\n+        archivedMetaWrapper.setHoodieCompactionPlan(plan);\n+        archivedMetaWrapper.setActionType(ActionType.compaction.name());\n+        break;\n+      }\n+      default: {\n+        throw new UnsupportedOperationException(\"Action not fully supported yet\");\n+      }\n+    }\n+    return archivedMetaWrapper;\n+  }\n+\n+  public static HoodieArchivedMetaEntry createMetaWrapper(HoodieInstant hoodieInstant,\n+                                                          HoodieCommitMetadata hoodieCommitMetadata) {\n+    HoodieArchivedMetaEntry archivedMetaWrapper = new HoodieArchivedMetaEntry();\n+    archivedMetaWrapper.setCommitTime(hoodieInstant.getTimestamp());\n+    archivedMetaWrapper.setActionState(hoodieInstant.getState().name());\n+    archivedMetaWrapper.setHoodieCommitMetadata(convertCommitMetadata(hoodieCommitMetadata));\n+    archivedMetaWrapper.setActionType(ActionType.commit.name());\n+    return archivedMetaWrapper;\n+  }\n+\n+  public static org.apache.hudi.avro.model.HoodieCommitMetadata convertCommitMetadata(\n+          HoodieCommitMetadata hoodieCommitMetadata) {\n+    ObjectMapper mapper = new ObjectMapper();\n+    // Need this to ignore other public get() methods\n+    mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\n+    org.apache.hudi.avro.model.HoodieCommitMetadata avroMetaData =\n+            mapper.convertValue(hoodieCommitMetadata, org.apache.hudi.avro.model.HoodieCommitMetadata.class);\n+    // Do not archive Rolling Stats, cannot set to null since AVRO will throw null pointer\n+    avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, \"\");\n+    return avroMetaData;\n+  }\n+\n+  // TODO : Fix converting from SpecificRecord to POJO", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMTcxMg=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODgyNTIxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozOToxMVrOIwftSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMTowODoxNlrOIwmGcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMjA1Ng==", "bodyText": "Do we need this? is this TODO still relevant", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587722056", "createdAt": "2021-03-04T18:39:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.HoodieCommitOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieCommonMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import java.io.IOException;\n+import java.util.stream.Stream;\n+\n+public class TransactionUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(TransactionUtils.class);\n+\n+  /**\n+   * Resolve any write conflicts when committing data.\n+   * @param table\n+   * @param metadataWriter\n+   * @param currentTxnOwnerInstant\n+   * @param thisCommitMetadata\n+   * @param config\n+   * @param lastCompletedTxnOwnerInstant\n+   * @return\n+   * @throws HoodieWriteConflictException\n+   */\n+  public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(final HoodieTable table, final Option<HoodieBackedTableMetadataWriter> metadataWriter,\n+                                                                       final Option<HoodieInstant> currentTxnOwnerInstant, final Option<HoodieCommitMetadata> thisCommitMetadata,\n+                                                                       final HoodieWriteConfig config, Option<HoodieInstant> lastCompletedTxnOwnerInstant)\n+      throws HoodieWriteConflictException {\n+    if (config.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      ConflictResolutionStrategy resolutionStrategy = config.getWriteConflictResolutionStrategy();\n+      Stream<HoodieInstant> instantStream = resolutionStrategy.getInstantsStream(table.getActiveTimeline(), currentTxnOwnerInstant.get(), lastCompletedTxnOwnerInstant);\n+      // TODO : metadataWriter.reload() inside resolve write conflict ??", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgyNjgwMA==", "bodyText": "I put this TODO since we are going to need a way to use the MetadataWriter to manipulate any concurrent actions performed, we will address this use-case in a follow up PR. I have removed metadata writer for now.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587826800", "createdAt": "2021-03-04T21:08:16Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.HoodieCommitOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieCommonMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import java.io.IOException;\n+import java.util.stream.Stream;\n+\n+public class TransactionUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(TransactionUtils.class);\n+\n+  /**\n+   * Resolve any write conflicts when committing data.\n+   * @param table\n+   * @param metadataWriter\n+   * @param currentTxnOwnerInstant\n+   * @param thisCommitMetadata\n+   * @param config\n+   * @param lastCompletedTxnOwnerInstant\n+   * @return\n+   * @throws HoodieWriteConflictException\n+   */\n+  public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(final HoodieTable table, final Option<HoodieBackedTableMetadataWriter> metadataWriter,\n+                                                                       final Option<HoodieInstant> currentTxnOwnerInstant, final Option<HoodieCommitMetadata> thisCommitMetadata,\n+                                                                       final HoodieWriteConfig config, Option<HoodieInstant> lastCompletedTxnOwnerInstant)\n+      throws HoodieWriteConflictException {\n+    if (config.getWriteConcurrencyMode().supportsOptimisticConcurrencyControl()) {\n+      ConflictResolutionStrategy resolutionStrategy = config.getWriteConflictResolutionStrategy();\n+      Stream<HoodieInstant> instantStream = resolutionStrategy.getInstantsStream(table.getActiveTimeline(), currentTxnOwnerInstant.get(), lastCompletedTxnOwnerInstant);\n+      // TODO : metadataWriter.reload() inside resolve write conflict ??", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMjA1Ng=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODgyNzE4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/HoodieMetadataConversionUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozOTozOFrOIwfueQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODozOTozOFrOIwfueQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMjM2MQ==", "bodyText": "rename to MetadataConversionUtils ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587722361", "createdAt": "2021-03-04T18:39:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/HoodieMetadataConversionUtils.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.avro.model.HoodieCompactionPlan;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.avro.model.HoodieSavepointMetadata;\n+import org.apache.hudi.client.ReplaceArchivalHelper;\n+import org.apache.hudi.common.model.ActionType;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRollingStatMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.CompactionUtils;\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to convert between different action related payloads and {@link HoodieArchivedMetaEntry}.\n+ */\n+public class HoodieMetadataConversionUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODgzNDg5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo0MToyMFrOIwfzGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMTowNzoxMFrOIwmB6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMzU0NA==", "bodyText": "unit test this method?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587723544", "createdAt": "2021-03-04T18:41:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.HoodieCommitOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieCommonMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import java.io.IOException;\n+import java.util.stream.Stream;\n+\n+public class TransactionUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(TransactionUtils.class);\n+\n+  /**\n+   * Resolve any write conflicts when committing data.\n+   * @param table\n+   * @param metadataWriter\n+   * @param currentTxnOwnerInstant\n+   * @param thisCommitMetadata\n+   * @param config\n+   * @param lastCompletedTxnOwnerInstant\n+   * @return\n+   * @throws HoodieWriteConflictException\n+   */\n+  public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(final HoodieTable table, final Option<HoodieBackedTableMetadataWriter> metadataWriter,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczNTI3OA==", "bodyText": "is the metadataWriter really used inside this method?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587735278", "createdAt": "2021-03-04T18:59:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.HoodieCommitOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieCommonMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import java.io.IOException;\n+import java.util.stream.Stream;\n+\n+public class TransactionUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(TransactionUtils.class);\n+\n+  /**\n+   * Resolve any write conflicts when committing data.\n+   * @param table\n+   * @param metadataWriter\n+   * @param currentTxnOwnerInstant\n+   * @param thisCommitMetadata\n+   * @param config\n+   * @param lastCompletedTxnOwnerInstant\n+   * @return\n+   * @throws HoodieWriteConflictException\n+   */\n+  public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(final HoodieTable table, final Option<HoodieBackedTableMetadataWriter> metadataWriter,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMzU0NA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgyNTY0MQ==", "bodyText": "Replied above.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587825641", "createdAt": "2021-03-04T21:07:10Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/utils/TransactionUtils.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.HoodieCommitOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieCommonMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieWriteConflictException;\n+import org.apache.hudi.metadata.HoodieBackedTableMetadataWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import java.io.IOException;\n+import java.util.stream.Stream;\n+\n+public class TransactionUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(TransactionUtils.class);\n+\n+  /**\n+   * Resolve any write conflicts when committing data.\n+   * @param table\n+   * @param metadataWriter\n+   * @param currentTxnOwnerInstant\n+   * @param thisCommitMetadata\n+   * @param config\n+   * @param lastCompletedTxnOwnerInstant\n+   * @return\n+   * @throws HoodieWriteConflictException\n+   */\n+  public static Option<HoodieCommitMetadata> resolveWriteConflictIfAny(final HoodieTable table, final Option<HoodieBackedTableMetadataWriter> metadataWriter,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyMzU0NA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODg0ODc1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo0NTowNlrOIwf7-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMDo0MjoyNVrOIxacfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNTgxNw==", "bodyText": "should there be a single property? i.e hoodie.clean.async= false does imply hoodie.clean.inline=true right?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587725817", "createdAt": "2021-03-04T18:45:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -43,7 +43,8 @@\n   public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n   public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n   public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n+  // Turn on inline cleaning\n+  public static final String INLINE_CLEAN_PROP = \"hoodie.clean.inline\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODE0MDYzMg==", "bodyText": "I was trying to keep the same concepts of inline for clustering, compact. The problem is there are autoClean & autoCommit but no autoCompact or autoCluster etc. Additionally, we used inline as the flag to toggle between inline & async for compact & cluster while we have chosen async as the flag for clean.\nI have removed hoodie.clean.inline. We can address these in another PR after we decide what convention to follow.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588140632", "createdAt": "2021-03-05T09:10:54Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -43,7 +43,8 @@\n   public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n   public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n   public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n+  // Turn on inline cleaning\n+  public static final String INLINE_CLEAN_PROP = \"hoodie.clean.inline\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNTgxNw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY4NDQxMw==", "bodyText": "sure. the idea is to not proliferate, since it then becomes one more config to take care of.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588684413", "createdAt": "2021-03-05T20:42:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -43,7 +43,8 @@\n   public static final String CLEANER_POLICY_PROP = \"hoodie.cleaner.policy\";\n   public static final String AUTO_CLEAN_PROP = \"hoodie.clean.automatic\";\n   public static final String ASYNC_CLEAN_PROP = \"hoodie.clean.async\";\n-\n+  // Turn on inline cleaning\n+  public static final String INLINE_CLEAN_PROP = \"hoodie.clean.inline\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNTgxNw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODg1MzIyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo0NjowMVrOIwf-qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQwNTozODoyNFrOIwzbIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNjUwNw==", "bodyText": "auto clean is different from the cleaning mode itself. lets just have an assignment to the hardcoded string \"true\"?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587726507", "createdAt": "2021-03-04T18:46:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -114,6 +115,7 @@\n       HoodieFailedWritesCleaningPolicy.EAGER.name();\n   private static final String DEFAULT_AUTO_CLEAN = \"true\";\n   private static final String DEFAULT_ASYNC_CLEAN = \"false\";\n+  private static final String DEFAULT_INLINE_CLEAN = DEFAULT_AUTO_CLEAN;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODA0NTA4OQ==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588045089", "createdAt": "2021-03-05T05:38:24Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -114,6 +115,7 @@\n       HoodieFailedWritesCleaningPolicy.EAGER.name();\n   private static final String DEFAULT_AUTO_CLEAN = \"true\";\n   private static final String DEFAULT_ASYNC_CLEAN = \"false\";\n+  private static final String DEFAULT_INLINE_CLEAN = DEFAULT_AUTO_CLEAN;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNjUwNw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODg2MjczOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo0ODowN1rOIwgESg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQxOTowNDo1N1rOIxouUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNzk0Ng==", "bodyText": "are these ever used in hudi-common? if we don't anticipate readers using this. we should just keep all this in hudi-client-common under a transaction package", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587727946", "createdAt": "2021-03-04T18:48:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.client.lock.SimpleConcurrentFileWritesConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ZookeeperBasedLockProvider;\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.lock.LockProvider;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgzMDk5OA==", "bodyText": "These configs are cross used for HiveMetastoreLockProvider which does not depend on hudi-client-common. Hence these are in hudi-common", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587830998", "createdAt": "2021-03-04T21:15:47Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.client.lock.SimpleConcurrentFileWritesConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ZookeeperBasedLockProvider;\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.lock.LockProvider;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNzk0Ng=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODY4NTg2Mw==", "bodyText": "But, could you explain why even HiveMetastoreLockProvider that has to be in hudi-common? That was my main point. We will be needlessly increasing the weight of hudi-common, which is picked up by hudi-hadoop-mr for e.g.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588685863", "createdAt": "2021-03-05T20:43:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.client.lock.SimpleConcurrentFileWritesConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ZookeeperBasedLockProvider;\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.lock.LockProvider;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNzk0Ng=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODkxODM1Mw==", "bodyText": "HiveMetastoreLockProvider has always been in hudi-hive-sync package. Only the LockConfiguration is in hudi-common since that is shared across hudi-hive-sync and hudi-client-common", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588918353", "createdAt": "2021-03-06T19:04:57Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.client.lock.SimpleConcurrentFileWritesConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ZookeeperBasedLockProvider;\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.lock.LockProvider;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyNzk0Ng=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODg3MDg4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo1MDowOFrOIwgJZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQwNTozNzoxNFrOIwzZqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyOTI1Mg==", "bodyText": "given we have had some typo related issues recently, please check each line once for correctness", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587729252", "createdAt": "2021-03-04T18:50:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.client.lock.SimpleConcurrentFileWritesConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ZookeeperBasedLockProvider;\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.lock.LockProvider;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_CLIENT_NUM_RETRIES;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_NUM_RETRIES;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.HIVE_DATABASE_NAME_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HIVE_TABLE_NAME_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_PREFIX;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_PORT_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+\n+/**\n+ * Hoodie Configs for Locks.\n+ */\n+public class HoodieLockConfig extends DefaultHoodieConfig {\n+\n+  // Pluggable type of lock provider\n+  public static final String LOCK_PROVIDER_CLASS_PROP = LOCK_PREFIX + \"provider\";\n+  public static final String DEFAULT_LOCK_PROVIDER_CLASS = ZookeeperBasedLockProvider.class.getName();\n+  // Pluggable strategies to use when resolving conflicts\n+  public static final String WRITE_CONFLICT_RESOLUTION_STRATEGY_CLASS_PROP =\n+      LOCK_PREFIX + \"conflict.resolution.strategy\";\n+  public static final String DEFAULT_WRITE_CONFLICT_RESOLUTION_STRATEGY_CLASS =\n+      SimpleConcurrentFileWritesConflictResolutionStrategy.class.getName();\n+\n+  private HoodieLockConfig(Properties props) {\n+    super(props);\n+  }\n+\n+  public static HoodieLockConfig.Builder newBuilder() {\n+    return new HoodieLockConfig.Builder();\n+  }\n+\n+  public static class Builder {\n+\n+    private final Properties props = new Properties();\n+\n+    public HoodieLockConfig.Builder fromFile(File propertiesFile) throws IOException {\n+      try (FileReader reader = new FileReader(propertiesFile)) {\n+        this.props.load(reader);\n+        return this;\n+      }\n+    }\n+\n+    public HoodieLockConfig.Builder fromProperties(Properties props) {\n+      this.props.putAll(props);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withLockProvider(Class<? extends LockProvider> lockProvider) {\n+      props.setProperty(LOCK_PROVIDER_CLASS_PROP, lockProvider.getName());\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withHiveDatabaseName(String databaseName) {\n+      props.setProperty(HIVE_DATABASE_NAME_PROP, databaseName);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withHiveTableName(String tableName) {\n+      props.setProperty(HIVE_TABLE_NAME_PROP, tableName);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkQuorum(String zkQuorum) {\n+      props.setProperty(ZK_CONNECT_URL_PROP, zkQuorum);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkBasePath(String zkBasePath) {\n+      props.setProperty(ZK_BASE_PATH_PROP, zkBasePath);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkPort(String zkPort) {\n+      props.setProperty(ZK_PORT_PROP, zkPort);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkLockKey(String zkLockKey) {\n+      props.setProperty(ZK_LOCK_KEY_PROP, zkLockKey);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkConnectionTimeoutInMs(Long connectionTimeoutInMs) {\n+      props.setProperty(ZK_CONNECTION_TIMEOUT_MS_PROP, String.valueOf(connectionTimeoutInMs));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkSessionTimeoutInMs(Long sessionTimeoutInMs) {\n+      props.setProperty(ZK_SESSION_TIMEOUT_MS_PROP, String.valueOf(sessionTimeoutInMs));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withNumRetries(int numRetries) {\n+      props.setProperty(LOCK_ACQUIRE_NUM_RETRIES_PROP, String.valueOf(numRetries));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withRetryWaitTimeInMillis(Long retryWaitTimeInMillis) {\n+      props.setProperty(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP, String.valueOf(retryWaitTimeInMillis));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withClientNumRetries(int clientNumRetries) {\n+      props.setProperty(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP, String.valueOf(clientNumRetries));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withClientRetryWaitTimeInMillis(Long clientRetryWaitTimeInMillis) {\n+      props.setProperty(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP, String.valueOf(clientRetryWaitTimeInMillis));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withLockWaitTimeInMillis(Long waitTimeInMillis) {\n+      props.setProperty(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP, String.valueOf(waitTimeInMillis));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withConflictResolutionStrategy(ConflictResolutionStrategy conflictResolutionStrategy) {\n+      props.setProperty(WRITE_CONFLICT_RESOLUTION_STRATEGY_CLASS_PROP, conflictResolutionStrategy.getClass().getName());\n+      return this;\n+    }\n+\n+    public HoodieLockConfig build() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODA0NDcxMg==", "bodyText": "checked", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588044712", "createdAt": "2021-03-05T05:37:14Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieLockConfig.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.client.lock.SimpleConcurrentFileWritesConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ConflictResolutionStrategy;\n+import org.apache.hudi.client.lock.ZookeeperBasedLockProvider;\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.lock.LockProvider;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_CLIENT_NUM_RETRIES;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_NUM_RETRIES;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_CONNECTION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.DEFAULT_ZK_SESSION_TIMEOUT_MS;\n+import static org.apache.hudi.common.config.LockConfiguration.HIVE_DATABASE_NAME_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.HIVE_TABLE_NAME_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_NUM_RETRIES_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.LOCK_PREFIX;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_BASE_PATH_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECTION_TIMEOUT_MS_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_CONNECT_URL_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_LOCK_KEY_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_PORT_PROP;\n+import static org.apache.hudi.common.config.LockConfiguration.ZK_SESSION_TIMEOUT_MS_PROP;\n+\n+\n+/**\n+ * Hoodie Configs for Locks.\n+ */\n+public class HoodieLockConfig extends DefaultHoodieConfig {\n+\n+  // Pluggable type of lock provider\n+  public static final String LOCK_PROVIDER_CLASS_PROP = LOCK_PREFIX + \"provider\";\n+  public static final String DEFAULT_LOCK_PROVIDER_CLASS = ZookeeperBasedLockProvider.class.getName();\n+  // Pluggable strategies to use when resolving conflicts\n+  public static final String WRITE_CONFLICT_RESOLUTION_STRATEGY_CLASS_PROP =\n+      LOCK_PREFIX + \"conflict.resolution.strategy\";\n+  public static final String DEFAULT_WRITE_CONFLICT_RESOLUTION_STRATEGY_CLASS =\n+      SimpleConcurrentFileWritesConflictResolutionStrategy.class.getName();\n+\n+  private HoodieLockConfig(Properties props) {\n+    super(props);\n+  }\n+\n+  public static HoodieLockConfig.Builder newBuilder() {\n+    return new HoodieLockConfig.Builder();\n+  }\n+\n+  public static class Builder {\n+\n+    private final Properties props = new Properties();\n+\n+    public HoodieLockConfig.Builder fromFile(File propertiesFile) throws IOException {\n+      try (FileReader reader = new FileReader(propertiesFile)) {\n+        this.props.load(reader);\n+        return this;\n+      }\n+    }\n+\n+    public HoodieLockConfig.Builder fromProperties(Properties props) {\n+      this.props.putAll(props);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withLockProvider(Class<? extends LockProvider> lockProvider) {\n+      props.setProperty(LOCK_PROVIDER_CLASS_PROP, lockProvider.getName());\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withHiveDatabaseName(String databaseName) {\n+      props.setProperty(HIVE_DATABASE_NAME_PROP, databaseName);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withHiveTableName(String tableName) {\n+      props.setProperty(HIVE_TABLE_NAME_PROP, tableName);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkQuorum(String zkQuorum) {\n+      props.setProperty(ZK_CONNECT_URL_PROP, zkQuorum);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkBasePath(String zkBasePath) {\n+      props.setProperty(ZK_BASE_PATH_PROP, zkBasePath);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkPort(String zkPort) {\n+      props.setProperty(ZK_PORT_PROP, zkPort);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkLockKey(String zkLockKey) {\n+      props.setProperty(ZK_LOCK_KEY_PROP, zkLockKey);\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkConnectionTimeoutInMs(Long connectionTimeoutInMs) {\n+      props.setProperty(ZK_CONNECTION_TIMEOUT_MS_PROP, String.valueOf(connectionTimeoutInMs));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withZkSessionTimeoutInMs(Long sessionTimeoutInMs) {\n+      props.setProperty(ZK_SESSION_TIMEOUT_MS_PROP, String.valueOf(sessionTimeoutInMs));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withNumRetries(int numRetries) {\n+      props.setProperty(LOCK_ACQUIRE_NUM_RETRIES_PROP, String.valueOf(numRetries));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withRetryWaitTimeInMillis(Long retryWaitTimeInMillis) {\n+      props.setProperty(LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP, String.valueOf(retryWaitTimeInMillis));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withClientNumRetries(int clientNumRetries) {\n+      props.setProperty(LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP, String.valueOf(clientNumRetries));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withClientRetryWaitTimeInMillis(Long clientRetryWaitTimeInMillis) {\n+      props.setProperty(LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP, String.valueOf(clientRetryWaitTimeInMillis));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withLockWaitTimeInMillis(Long waitTimeInMillis) {\n+      props.setProperty(LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP, String.valueOf(waitTimeInMillis));\n+      return this;\n+    }\n+\n+    public HoodieLockConfig.Builder withConflictResolutionStrategy(ConflictResolutionStrategy conflictResolutionStrategy) {\n+      props.setProperty(WRITE_CONFLICT_RESOLUTION_STRATEGY_CLASS_PROP, conflictResolutionStrategy.getClass().getName());\n+      return this;\n+    }\n+\n+    public HoodieLockConfig build() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzcyOTI1Mg=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODg4NjM4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/BaseCleanActionExecutor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo1NDowMFrOIwgTKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQwMzoyNjo0OFrOIwxAfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczMTc1Mw==", "bodyText": "I don't understand why we just pick the last cleaned metadata. Lets do the generically right thing. If you want to handle more than more cleaning operation, lets return a list?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587731753", "createdAt": "2021-03-04T18:54:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/BaseCleanActionExecutor.java", "diffHunk": "@@ -195,30 +126,24 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan\n \n   @Override\n   public HoodieCleanMetadata execute() {\n+    List<HoodieCleanMetadata> cleanMetadataList = new ArrayList<>();\n     // If there are inflight(failed) or previously requested clean operation, first perform them\n     List<HoodieInstant> pendingCleanInstants = table.getCleanTimeline()\n         .filterInflightsAndRequested().getInstants().collect(Collectors.toList());\n     if (pendingCleanInstants.size() > 0) {\n       pendingCleanInstants.forEach(hoodieInstant -> {\n         LOG.info(\"Finishing previously unfinished cleaner instant=\" + hoodieInstant);\n         try {\n-          runPendingClean(table, hoodieInstant);\n+          cleanMetadataList.add(runPendingClean(table, hoodieInstant));\n         } catch (Exception e) {\n           LOG.warn(\"Failed to perform previous clean operation, instant: \" + hoodieInstant, e);\n         }\n       });\n       table.getMetaClient().reloadActiveTimeline();\n     }\n-\n-    // Plan and execute a new clean action\n-    Option<HoodieCleanerPlan> cleanerPlanOpt = requestClean(instantTime);\n-    if (cleanerPlanOpt.isPresent()) {\n-      table.getMetaClient().reloadActiveTimeline();\n-      HoodieCleanerPlan cleanerPlan = cleanerPlanOpt.get();\n-      if ((cleanerPlan.getFilePathsToBeDeletedPerPartition() != null) && !cleanerPlan.getFilePathsToBeDeletedPerPartition().isEmpty()) {\n-        return runClean(table, HoodieTimeline.getCleanRequestedInstant(instantTime), cleanerPlan);\n-      }\n-    }\n-    return null;\n+    // return the last clean metadata for now\n+    // TODO (NA) : Clean only the earliest pending clean just like how we do for other table services", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczMTk2OA==", "bodyText": "Won't this for e.g mess up the metadata table? by missing some deletes?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587731968", "createdAt": "2021-03-04T18:54:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/BaseCleanActionExecutor.java", "diffHunk": "@@ -195,30 +126,24 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan\n \n   @Override\n   public HoodieCleanMetadata execute() {\n+    List<HoodieCleanMetadata> cleanMetadataList = new ArrayList<>();\n     // If there are inflight(failed) or previously requested clean operation, first perform them\n     List<HoodieInstant> pendingCleanInstants = table.getCleanTimeline()\n         .filterInflightsAndRequested().getInstants().collect(Collectors.toList());\n     if (pendingCleanInstants.size() > 0) {\n       pendingCleanInstants.forEach(hoodieInstant -> {\n         LOG.info(\"Finishing previously unfinished cleaner instant=\" + hoodieInstant);\n         try {\n-          runPendingClean(table, hoodieInstant);\n+          cleanMetadataList.add(runPendingClean(table, hoodieInstant));\n         } catch (Exception e) {\n           LOG.warn(\"Failed to perform previous clean operation, instant: \" + hoodieInstant, e);\n         }\n       });\n       table.getMetaClient().reloadActiveTimeline();\n     }\n-\n-    // Plan and execute a new clean action\n-    Option<HoodieCleanerPlan> cleanerPlanOpt = requestClean(instantTime);\n-    if (cleanerPlanOpt.isPresent()) {\n-      table.getMetaClient().reloadActiveTimeline();\n-      HoodieCleanerPlan cleanerPlan = cleanerPlanOpt.get();\n-      if ((cleanerPlan.getFilePathsToBeDeletedPerPartition() != null) && !cleanerPlan.getFilePathsToBeDeletedPerPartition().isEmpty()) {\n-        return runClean(table, HoodieTimeline.getCleanRequestedInstant(instantTime), cleanerPlan);\n-      }\n-    }\n-    return null;\n+    // return the last clean metadata for now\n+    // TODO (NA) : Clean only the earliest pending clean just like how we do for other table services", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczMTc1Mw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODAwNTUwMQ==", "bodyText": "Currently, the clean metadata from runPendingClean is never returned if you see above in line 205. The current logic is as follows\n\nFor all pending clean operations, we just return null.\nIf there is new clean to be done, we do the new clean and return the metadata.\n\nClean metadata is always persisted before this method inside the runClean method and the return value of this method is NOT used by the client.\nI made above changes to reuse the same methods and  to keep the same behavior, except with one change :\n\nFor all pending clean operations, we return the latest pending clean from previous runs.\nIf there is new clean to be done, we do the new clean and return the metadata.\n\nOther logic remains the same. The returned metadata is ONLY used for a) Logging b) Metrics.\nLet's refactor all these issues in the ActionExecutor in follow up PR. Filed issue here -> https://issues.apache.org/jira/browse/HUDI-1666", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588005501", "createdAt": "2021-03-05T03:26:48Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/BaseCleanActionExecutor.java", "diffHunk": "@@ -195,30 +126,24 @@ private HoodieCleanMetadata runClean(HoodieTable<T, I, K, O> table, HoodieInstan\n \n   @Override\n   public HoodieCleanMetadata execute() {\n+    List<HoodieCleanMetadata> cleanMetadataList = new ArrayList<>();\n     // If there are inflight(failed) or previously requested clean operation, first perform them\n     List<HoodieInstant> pendingCleanInstants = table.getCleanTimeline()\n         .filterInflightsAndRequested().getInstants().collect(Collectors.toList());\n     if (pendingCleanInstants.size() > 0) {\n       pendingCleanInstants.forEach(hoodieInstant -> {\n         LOG.info(\"Finishing previously unfinished cleaner instant=\" + hoodieInstant);\n         try {\n-          runPendingClean(table, hoodieInstant);\n+          cleanMetadataList.add(runPendingClean(table, hoodieInstant));\n         } catch (Exception e) {\n           LOG.warn(\"Failed to perform previous clean operation, instant: \" + hoodieInstant, e);\n         }\n       });\n       table.getMetaClient().reloadActiveTimeline();\n     }\n-\n-    // Plan and execute a new clean action\n-    Option<HoodieCleanerPlan> cleanerPlanOpt = requestClean(instantTime);\n-    if (cleanerPlanOpt.isPresent()) {\n-      table.getMetaClient().reloadActiveTimeline();\n-      HoodieCleanerPlan cleanerPlan = cleanerPlanOpt.get();\n-      if ((cleanerPlan.getFilePathsToBeDeletedPerPartition() != null) && !cleanerPlan.getFilePathsToBeDeletedPerPartition().isEmpty()) {\n-        return runClean(table, HoodieTimeline.getCleanRequestedInstant(instantTime), cleanerPlan);\n-      }\n-    }\n-    return null;\n+    // return the last clean metadata for now\n+    // TODO (NA) : Clean only the earliest pending clean just like how we do for other table services", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczMTc1Mw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODg5NTcwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/BaseCleanPlanActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo1NjoxMVrOIwgZEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMToxNjozNlrOIwmYig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczMzI2NQ==", "bodyText": "I assume, this is all just code moved from the other class. if not , please point out what has changed", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587733265", "createdAt": "2021-03-04T18:56:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/BaseCleanPlanActionExecutor.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clean;\n+\n+import org.apache.hudi.avro.model.HoodieActionInstant;\n+import org.apache.hudi.avro.model.HoodieCleanFileInfo;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.BaseActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public abstract class BaseCleanPlanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCleanerPlan>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(CleanPlanner.class);\n+\n+  private final Option<Map<String, String>> extraMetadata;\n+\n+  public BaseCleanPlanActionExecutor(HoodieEngineContext context,\n+                                     HoodieWriteConfig config,\n+                                     HoodieTable<T, I, K, O> table,\n+                                     String instantTime,\n+                                     Option<Map<String, String>> extraMetadata) {\n+    super(context, config, table, instantTime);\n+    this.extraMetadata = extraMetadata;\n+  }\n+\n+  protected abstract Option<HoodieCleanerPlan> createCleanerPlan();\n+\n+  /**\n+   * Generates List of files to be cleaned.\n+   *\n+   * @param context HoodieEngineContext\n+   * @return Cleaner Plan\n+   */\n+  HoodieCleanerPlan requestClean(HoodieEngineContext context) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgzMTQzNA==", "bodyText": "Yes, no change.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587831434", "createdAt": "2021-03-04T21:16:36Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clean/BaseCleanPlanActionExecutor.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clean;\n+\n+import org.apache.hudi.avro.model.HoodieActionInstant;\n+import org.apache.hudi.avro.model.HoodieCleanFileInfo;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.BaseActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public abstract class BaseCleanPlanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieCleanerPlan>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(CleanPlanner.class);\n+\n+  private final Option<Map<String, String>> extraMetadata;\n+\n+  public BaseCleanPlanActionExecutor(HoodieEngineContext context,\n+                                     HoodieWriteConfig config,\n+                                     HoodieTable<T, I, K, O> table,\n+                                     String instantTime,\n+                                     Option<Map<String, String>> extraMetadata) {\n+    super(context, config, table, instantTime);\n+    this.extraMetadata = extraMetadata;\n+  }\n+\n+  protected abstract Option<HoodieCleanerPlan> createCleanerPlan();\n+\n+  /**\n+   * Generates List of files to be cleaned.\n+   *\n+   * @param context HoodieEngineContext\n+   * @return Cleaner Plan\n+   */\n+  HoodieCleanerPlan requestClean(HoodieEngineContext context) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczMzI2NQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODg5ODA2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo1Njo1MFrOIwgaiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQwMzowOTo0OVrOIwwtTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczMzY0Mw==", "bodyText": "lets create a code cleanup JIRA for this. else we may not get to this.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587733643", "createdAt": "2021-03-04T18:56:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -66,6 +70,11 @@ public BaseCommitActionExecutor(HoodieEngineContext context, HoodieWriteConfig c\n     this.operationType = operationType;\n     this.extraMetadata = extraMetadata;\n     this.taskContextSupplier = context.getTaskContextSupplier();\n+    this.txnManager = new TransactionManager(config, table.getMetaClient().getFs());\n+    // TODO : Remove this once we refactor and move out autoCommit method from here, since the TxnManager is held in {@link AbstractHoodieWriteClient}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODAwMDU5MA==", "bodyText": "https://issues.apache.org/jira/browse/HUDI-1665", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588000590", "createdAt": "2021-03-05T03:09:49Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -66,6 +70,11 @@ public BaseCommitActionExecutor(HoodieEngineContext context, HoodieWriteConfig c\n     this.operationType = operationType;\n     this.extraMetadata = extraMetadata;\n     this.taskContextSupplier = context.getTaskContextSupplier();\n+    this.txnManager = new TransactionManager(config, table.getMetaClient().getFs());\n+    // TODO : Remove this once we refactor and move out autoCommit method from here, since the TxnManager is held in {@link AbstractHoodieWriteClient}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczMzY0Mw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODkxMDc1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxODo1OTozOVrOIwgiWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMToxNzoyMVrOIwmaTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczNTY0MQ==", "bodyText": "revisit the todo?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587735641", "createdAt": "2021-03-04T18:59:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -117,12 +126,24 @@ protected String getCommitActionType() {\n   protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n     if (config.shouldAutoCommit()) {\n       LOG.info(\"Auto commit enabled: Committing \" + instantTime);\n-      commit(extraMetadata, result);\n+      autoCommit(extraMetadata, result);\n     } else {\n       LOG.info(\"Auto commit disabled for \" + instantTime);\n     }\n   }\n \n+  protected void autoCommit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata<O> result) {\n+    this.txnManager.beginTransaction();\n+    try {\n+      // TODO : Refactor this method so we can pass a valid metadata table writer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgzMTg4Nw==", "bodyText": "Removed metadata writer for now.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587831887", "createdAt": "2021-03-04T21:17:21Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -117,12 +126,24 @@ protected String getCommitActionType() {\n   protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n     if (config.shouldAutoCommit()) {\n       LOG.info(\"Auto commit enabled: Committing \" + instantTime);\n-      commit(extraMetadata, result);\n+      autoCommit(extraMetadata, result);\n     } else {\n       LOG.info(\"Auto commit disabled for \" + instantTime);\n     }\n   }\n \n+  protected void autoCommit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata<O> result) {\n+    this.txnManager.beginTransaction();\n+    try {\n+      // TODO : Refactor this method so we can pass a valid metadata table writer", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczNTY0MQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODkyNTMyOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowMjowMFrOIwgr3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMjoyMTowM1rOIwoqCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczODA3OA==", "bodyText": "I think the common term is . zookeeper chroot?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587738078", "createdAt": "2021-03-04T19:02:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.config;\n+\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration for managing locks. Since this configuration needs to be shared with HiveMetaStore based lock,\n+ * which is in a different package than other lock providers, we use this as a data transfer object in hoodie-common\n+ */\n+public class LockConfiguration implements Serializable {\n+\n+  public static final String LOCK_PREFIX = \"hoodie.writer.lock.\";\n+  public static final String LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP = LOCK_PREFIX + \"wait_time_ms_between_retry\";\n+  public static final String DEFAULT_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS = String.valueOf(5000L);\n+  public static final String LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP = LOCK_PREFIX + \"client.wait_time_ms_between_retry\";\n+  public static final String DEFAULT_LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS = String.valueOf(10000L);\n+  public static final String LOCK_ACQUIRE_NUM_RETRIES_PROP = LOCK_PREFIX + \"num_retries\";\n+  public static final String DEFAULT_LOCK_ACQUIRE_NUM_RETRIES = String.valueOf(3);\n+  public static final String LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP = LOCK_PREFIX + \"client.num_retries\";\n+  public static final String DEFAULT_LOCK_ACQUIRE_CLIENT_NUM_RETRIES = String.valueOf(0);\n+  public static final String LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP = LOCK_PREFIX + \"wait_time_ms\";\n+  public static final int DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS = 60 * 1000;\n+  // configs for file system based locks. NOTE: This only works for DFS with atomic create/delete operation\n+  public static final String FILESYSTEM_BASED_LOCK_PROPERTY_PREFIX = LOCK_PREFIX + \"filesystem.\";\n+  public static final String FILESYSTEM_LOCK_PATH_PROP = FILESYSTEM_BASED_LOCK_PROPERTY_PREFIX + \"path\";\n+  // configs for metastore based locks\n+  public static final String HIVE_METASTORE_LOCK_PROPERTY_PREFIX = LOCK_PREFIX + \"hivemetastore.\";\n+  public static final String HIVE_DATABASE_NAME_PROP = HIVE_METASTORE_LOCK_PROPERTY_PREFIX + \"database\";\n+  public static final String HIVE_TABLE_NAME_PROP = HIVE_METASTORE_LOCK_PROPERTY_PREFIX + \"table\";\n+  // Zookeeper configs for zk based locks\n+  public static final String ZOOKEEPER_BASED_LOCK_PROPERTY_PREFIX = LOCK_PREFIX + \"zookeeper.\";\n+  public static final String ZK_BASE_PATH_PROP = ZOOKEEPER_BASED_LOCK_PROPERTY_PREFIX + \"zk_base_path\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzg2ODY4Mw==", "bodyText": "This is the base path for the zk lock which users can select. I have not exposed a config to change the chroot. This will be the default.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587868683", "createdAt": "2021-03-04T22:21:03Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/config/LockConfiguration.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.config;\n+\n+import java.io.Serializable;\n+import java.util.Properties;\n+\n+/**\n+ * Configuration for managing locks. Since this configuration needs to be shared with HiveMetaStore based lock,\n+ * which is in a different package than other lock providers, we use this as a data transfer object in hoodie-common\n+ */\n+public class LockConfiguration implements Serializable {\n+\n+  public static final String LOCK_PREFIX = \"hoodie.writer.lock.\";\n+  public static final String LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS_PROP = LOCK_PREFIX + \"wait_time_ms_between_retry\";\n+  public static final String DEFAULT_LOCK_ACQUIRE_RETRY_WAIT_TIME_IN_MILLIS = String.valueOf(5000L);\n+  public static final String LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS_PROP = LOCK_PREFIX + \"client.wait_time_ms_between_retry\";\n+  public static final String DEFAULT_LOCK_ACQUIRE_CLIENT_RETRY_WAIT_TIME_IN_MILLIS = String.valueOf(10000L);\n+  public static final String LOCK_ACQUIRE_NUM_RETRIES_PROP = LOCK_PREFIX + \"num_retries\";\n+  public static final String DEFAULT_LOCK_ACQUIRE_NUM_RETRIES = String.valueOf(3);\n+  public static final String LOCK_ACQUIRE_CLIENT_NUM_RETRIES_PROP = LOCK_PREFIX + \"client.num_retries\";\n+  public static final String DEFAULT_LOCK_ACQUIRE_CLIENT_NUM_RETRIES = String.valueOf(0);\n+  public static final String LOCK_ACQUIRE_WAIT_TIMEOUT_MS_PROP = LOCK_PREFIX + \"wait_time_ms\";\n+  public static final int DEFAULT_ACQUIRE_LOCK_WAIT_TIMEOUT_MS = 60 * 1000;\n+  // configs for file system based locks. NOTE: This only works for DFS with atomic create/delete operation\n+  public static final String FILESYSTEM_BASED_LOCK_PROPERTY_PREFIX = LOCK_PREFIX + \"filesystem.\";\n+  public static final String FILESYSTEM_LOCK_PATH_PROP = FILESYSTEM_BASED_LOCK_PROPERTY_PREFIX + \"path\";\n+  // configs for metastore based locks\n+  public static final String HIVE_METASTORE_LOCK_PROPERTY_PREFIX = LOCK_PREFIX + \"hivemetastore.\";\n+  public static final String HIVE_DATABASE_NAME_PROP = HIVE_METASTORE_LOCK_PROPERTY_PREFIX + \"database\";\n+  public static final String HIVE_TABLE_NAME_PROP = HIVE_METASTORE_LOCK_PROPERTY_PREFIX + \"table\";\n+  // Zookeeper configs for zk based locks\n+  public static final String ZOOKEEPER_BASED_LOCK_PROPERTY_PREFIX = LOCK_PREFIX + \"zookeeper.\";\n+  public static final String ZK_BASE_PATH_PROP = ZOOKEEPER_BASED_LOCK_PROPERTY_PREFIX + \"zk_base_path\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczODA3OA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODkzMjAxOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowMjo0NFrOIwgwdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNlQxOTowNDowNlrOIxouCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczOTI1NQ==", "bodyText": "can this be an interface?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587739255", "createdAt": "2021-03-04T19:02:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MDAyMA==", "bodyText": "if there is no shared code here, we should go for an interface vs an abstract class", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587740020", "createdAt": "2021-03-04T19:03:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczOTI1NQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzg2NzE5MA==", "bodyText": "I want to mark some methods as not implementable which is why abstract class is chosen", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587867190", "createdAt": "2021-03-04T22:18:18Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczOTI1NQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODc3NTM5Mg==", "bodyText": "We can always to interface and default methods, right?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588775392", "createdAt": "2021-03-05T23:07:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczOTI1NQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODkxODI4MQ==", "bodyText": "I want to make some methods final which we cannot do in interfaces. But it's fine, I have changed it to interface, I don't have a very strong preference.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588918281", "createdAt": "2021-03-06T19:04:06Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/lock/LockProvider.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.lock;\n+\n+import org.apache.hudi.common.config.LockConfiguration;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.exception.HoodieLockException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.Lock;\n+\n+/**\n+ * Pluggable lock implementations using this provider class.\n+ */\n+public abstract class LockProvider<T> implements Lock, AutoCloseable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzczOTI1NQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk0MjEwOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommonMetadata.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowNDoxNlrOIwg3ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowNDoxNlrOIwg3ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MTAzNA==", "bodyText": "rename: HoodieMetadataWrapper", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587741034", "createdAt": "2021-03-04T19:04:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommonMetadata.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+\n+public class HoodieCommonMetadata {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk0MzE3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/TableService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowNDozNlrOIwg4LA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowNDozNlrOIwg4LA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MTIyOA==", "bodyText": "rename: TableServiceType", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587741228", "createdAt": "2021-03-04T19:04:36Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/TableService.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+\n+/**\n+ * Supported runtime table services.\n+ */\n+public enum TableService {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk0NDczOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowNDo1OFrOIwg5Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQyMzowNDoyMFrOIxf8QA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MTQ3MA==", "bodyText": "rename: isOptimistic... ?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587741470", "createdAt": "2021-03-04T19:04:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Different concurrency modes for write operations.\n+ */\n+public enum WriteConcurrencyMode {\n+  // Only a single writer can perform write ops\n+  SINGLE_WRITER(\"single_writer\"),\n+  // Multiple writer can perform write ops with lazy conflict resolution using locks\n+  OPTIMISTIC_CONCURRENCY_CONTROL(\"optimistic_concurrency_control\");\n+\n+  private final String value;\n+\n+  WriteConcurrencyMode(String value) {\n+    this.value = value;\n+  }\n+\n+  /**\n+   * Getter for write concurrency mode.\n+   * @return\n+   */\n+  public String value() {\n+    return value;\n+  }\n+\n+  /**\n+   * Convert string value to WriteConcurrencyMode.\n+   */\n+  public static WriteConcurrencyMode fromValue(String value) {\n+    switch (value.toLowerCase(Locale.ROOT)) {\n+      case \"single_writer\":\n+        return SINGLE_WRITER;\n+      case \"optimistic_concurrency_control\":\n+        return OPTIMISTIC_CONCURRENCY_CONTROL;\n+      default:\n+        throw new HoodieException(\"Invalid value of Type.\");\n+    }\n+  }\n+\n+  public boolean supportsOptimisticConcurrencyControl() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgzNjk4Mw==", "bodyText": "I feel supportsOptimisticConcurrencyControl as it is more direct vs isOptimisticConcurrencyControl which sounds a little weird. Let me know if you have a strong preference.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587836983", "createdAt": "2021-03-04T21:26:16Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Different concurrency modes for write operations.\n+ */\n+public enum WriteConcurrencyMode {\n+  // Only a single writer can perform write ops\n+  SINGLE_WRITER(\"single_writer\"),\n+  // Multiple writer can perform write ops with lazy conflict resolution using locks\n+  OPTIMISTIC_CONCURRENCY_CONTROL(\"optimistic_concurrency_control\");\n+\n+  private final String value;\n+\n+  WriteConcurrencyMode(String value) {\n+    this.value = value;\n+  }\n+\n+  /**\n+   * Getter for write concurrency mode.\n+   * @return\n+   */\n+  public String value() {\n+    return value;\n+  }\n+\n+  /**\n+   * Convert string value to WriteConcurrencyMode.\n+   */\n+  public static WriteConcurrencyMode fromValue(String value) {\n+    switch (value.toLowerCase(Locale.ROOT)) {\n+      case \"single_writer\":\n+        return SINGLE_WRITER;\n+      case \"optimistic_concurrency_control\":\n+        return OPTIMISTIC_CONCURRENCY_CONTROL;\n+      default:\n+        throw new HoodieException(\"Invalid value of Type.\");\n+    }\n+  }\n+\n+  public boolean supportsOptimisticConcurrencyControl() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MTQ3MA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODc3NDQ2NA==", "bodyText": "I was thinking from perspective of, you are just doing a direct comparison. thus hte isXXX naming. but I see your view. lets keep it as is", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588774464", "createdAt": "2021-03-05T23:04:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteConcurrencyMode.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import org.apache.hudi.exception.HoodieException;\n+\n+import java.util.Locale;\n+\n+/**\n+ * Different concurrency modes for write operations.\n+ */\n+public enum WriteConcurrencyMode {\n+  // Only a single writer can perform write ops\n+  SINGLE_WRITER(\"single_writer\"),\n+  // Multiple writer can perform write ops with lazy conflict resolution using locks\n+  OPTIMISTIC_CONCURRENCY_CONTROL(\"optimistic_concurrency_control\");\n+\n+  private final String value;\n+\n+  WriteConcurrencyMode(String value) {\n+    this.value = value;\n+  }\n+\n+  /**\n+   * Getter for write concurrency mode.\n+   * @return\n+   */\n+  public String value() {\n+    return value;\n+  }\n+\n+  /**\n+   * Convert string value to WriteConcurrencyMode.\n+   */\n+  public static WriteConcurrencyMode fromValue(String value) {\n+    switch (value.toLowerCase(Locale.ROOT)) {\n+      case \"single_writer\":\n+        return SINGLE_WRITER;\n+      case \"optimistic_concurrency_control\":\n+        return OPTIMISTIC_CONCURRENCY_CONTROL;\n+      default:\n+        throw new HoodieException(\"Invalid value of Type.\");\n+    }\n+  }\n+\n+  public boolean supportsOptimisticConcurrencyControl() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MTQ3MA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk0ODE5OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowNTozOVrOIwg7HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMToyNzoxNFrOIwmwVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MTk4MQ==", "bodyText": "what is this really?  how can there be an write that is unknown?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587741981", "createdAt": "2021-03-04T19:05:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -82,6 +84,10 @@ public static WriteOperationType fromValue(String value) {\n         return INSERT_OVERWRITE_TABLE;\n       case \"cluster\":\n         return CLUSTER;\n+      case \"compact\":\n+        return COMPACT;\n+      case \"unknown\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgzNzUyNg==", "bodyText": "This was introduced to address older metadata when we don't store the WriteOperationType in the metadata..", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587837526", "createdAt": "2021-03-04T21:27:14Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -82,6 +84,10 @@ public static WriteOperationType fromValue(String value) {\n         return INSERT_OVERWRITE_TABLE;\n       case \"cluster\":\n         return CLUSTER;\n+      case \"compact\":\n+        return COMPACT;\n+      case \"unknown\":", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MTk4MQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk1MTcyOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowNjozMFrOIwg9aQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowNjozMFrOIwg9aQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0MjU2OQ==", "bodyText": "lets unit tests these?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587742569", "createdAt": "2021-03-04T19:06:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java", "diffHunk": "@@ -93,4 +95,28 @@ private static HoodieCommitMetadata buildMetadataFromStats(List<HoodieWriteStat>\n         + \"numReplaceFileIds:\" + partitionToReplaceFileIds.values().stream().mapToInt(e -> e.size()).sum());\n     return commitMetadata;\n   }\n+\n+  public static HashMap<String, String> getFileIdWithoutSuffixAndRelativePaths(Map<String, List<org.apache.hudi.avro.model.HoodieWriteStat>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk2MDY2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowODo0M1rOIwhDCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowODo0M1rOIwhDCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0NDAwOQ==", "bodyText": "revisit todo?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587744009", "createdAt": "2021-03-04T19:08:43Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -313,6 +320,16 @@ public void cleanHandles() {\n     return writeHandle;\n   }\n \n+  @Override\n+  protected void preCommit(String instantTime, HoodieCommitMetadata metadata) {\n+    // Create a Hoodie table after startTxn which encapsulated the commits and files visible.\n+    // Important to create this after the lock to ensure latest commits show up in the timeline without need for reload\n+    HoodieTable table = createTable(config, hadoopConf);\n+    // TODO : Metadata Writer is not supported for Flink", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk2MjQzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOTowOToxNlrOIwhEMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQwOToyMDoyNVrOIw5otA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0NDMwNg==", "bodyText": "this does not seem ok to do?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587744306", "createdAt": "2021-03-04T19:09:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java", "diffHunk": "@@ -265,6 +266,20 @@ public void rollbackBootstrap(HoodieEngineContext context, String instantTime) {\n     throw new HoodieNotSupportedException(\"Bootstrap is not supported yet\");\n   }\n \n+  /**\n+   * TODO :\n+   * Refactor {@link FlinkCleanActionExecutor} to support scheduling of cleaning.\n+   * @param context HoodieEngineContext\n+   * @param instantTime Instant Time for scheduling cleaning\n+   * @param extraMetadata additional metadata to write into plan\n+   * @return\n+   */\n+  @Override\n+  public Option<HoodieCleanerPlan> scheduleCleaning(HoodieEngineContext context, String instantTime, Option<Map<String, String>> extraMetadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODE0Njg2OA==", "bodyText": "Yeah, had a TODO on this, addressed it now.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588146868", "createdAt": "2021-03-05T09:20:25Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/HoodieFlinkCopyOnWriteTable.java", "diffHunk": "@@ -265,6 +266,20 @@ public void rollbackBootstrap(HoodieEngineContext context, String instantTime) {\n     throw new HoodieNotSupportedException(\"Bootstrap is not supported yet\");\n   }\n \n+  /**\n+   * TODO :\n+   * Refactor {@link FlinkCleanActionExecutor} to support scheduling of cleaning.\n+   * @param context HoodieEngineContext\n+   * @param instantTime Instant Time for scheduling cleaning\n+   * @param extraMetadata additional metadata to write into plan\n+   * @return\n+   */\n+  @Override\n+  public Option<HoodieCleanerPlan> scheduleCleaning(HoodieEngineContext context, String instantTime, Option<Map<String, String>> extraMetadata) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0NDMwNg=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk2ODQyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxMDo0NlrOIwhH7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxMDo0NlrOIwhH7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0NTI2Mw==", "bodyText": "revisit comment.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587745263", "createdAt": "2021-03-04T19:10:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/client/HoodieJavaWriteClient.java", "diffHunk": "@@ -224,6 +231,16 @@ protected void completeCompaction(HoodieCommitMetadata metadata,\n     return getTableAndInitCtx(metaClient, operationType);\n   }\n \n+  @Override\n+  protected void preCommit(String instantTime, HoodieCommitMetadata metadata) {\n+    // Create a Hoodie table after startTxn which encapsulated the commits and files visible.\n+    // Important to create this after the lock to ensure latest commits show up in the timeline without need for reload\n+    HoodieTable table = createTable(config, hadoopConf);\n+    // TODO : Metadata Writer is not supported for Java", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk3NTk0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxMjo0NlrOIwhMgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxMjo0NlrOIwhMgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0NjQzMg==", "bodyText": "lets clean this up?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587746432", "createdAt": "2021-03-04T19:12:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -380,11 +391,40 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n-    new SparkUpgradeDowngrade(metaClient, config, context).run(metaClient, HoodieTableVersion.current(), config, context, instantTime);\n-    return getTableAndInitCtx(metaClient, operationType);\n+    if (HoodieTableVersion.current() != metaClient.getTableConfig().getTableVersion()) {\n+      // TODO : Force clean up of all inflights, do this once pending rollback removal PR is landed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk4MzA2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxNDozOVrOIwhQ_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNVQwOToxMTo1OVrOIw5SxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0NzU4MA==", "bodyText": "lets add a method needsUpgradeOrDowngrade()  to the upgradedowngrade class?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587747580", "createdAt": "2021-03-04T19:14:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -380,11 +391,40 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n-    new SparkUpgradeDowngrade(metaClient, config, context).run(metaClient, HoodieTableVersion.current(), config, context, instantTime);\n-    return getTableAndInitCtx(metaClient, operationType);\n+    if (HoodieTableVersion.current() != metaClient.getTableConfig().getTableVersion()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4ODE0MTI1Mw==", "bodyText": "Added", "url": "https://github.com/apache/hudi/pull/2374#discussion_r588141253", "createdAt": "2021-03-05T09:11:59Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -380,11 +391,40 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n-    new SparkUpgradeDowngrade(metaClient, config, context).run(metaClient, HoodieTableVersion.current(), config, context, instantTime);\n-    return getTableAndInitCtx(metaClient, operationType);\n+    if (HoodieTableVersion.current() != metaClient.getTableConfig().getTableVersion()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0NzU4MA=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk4NTAxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxNToxMlrOIwhSNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxNToxMlrOIwhSNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0Nzg5NA==", "bodyText": "should we check concurrency mode before taking these locks?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587747894", "createdAt": "2021-03-04T19:15:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -380,11 +391,40 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n-    new SparkUpgradeDowngrade(metaClient, config, context).run(metaClient, HoodieTableVersion.current(), config, context, instantTime);\n-    return getTableAndInitCtx(metaClient, operationType);\n+    if (HoodieTableVersion.current() != metaClient.getTableConfig().getTableVersion()) {\n+      // TODO : Force clean up of all inflights, do this once pending rollback removal PR is landed\n+      // this.rollbackFailedWrites();\n+      this.txnManager.beginTransaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk4ODIyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxNTo1NFrOIwhUIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMjoyOTo1OVrOIwo8Dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0ODM4Nw==", "bodyText": "comment valid? What should we do about this issue? Can you elabortate?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587748387", "createdAt": "2021-03-04T19:15:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -380,11 +391,40 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n-    new SparkUpgradeDowngrade(metaClient, config, context).run(metaClient, HoodieTableVersion.current(), config, context, instantTime);\n-    return getTableAndInitCtx(metaClient, operationType);\n+    if (HoodieTableVersion.current() != metaClient.getTableConfig().getTableVersion()) {\n+      // TODO : Force clean up of all inflights, do this once pending rollback removal PR is landed\n+      // this.rollbackFailedWrites();\n+      this.txnManager.beginTransaction();\n+      try {\n+        // Ensure no inflight commits\n+        TransactionUtils.resolveConflictIfAnyForUpgradeDowngrade(metaClient);\n+        new SparkUpgradeDowngrade(metaClient, config, context).run(metaClient, HoodieTableVersion.current(), config, context, instantTime);\n+      } finally {\n+        this.txnManager.endTransaction();\n+      }\n+    }\n+    return getTableAndInitCtx(metaClient, operationType, instantTime);\n   }\n \n-  private HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> getTableAndInitCtx(HoodieTableMetaClient metaClient, WriteOperationType operationType) {\n+  // TODO : To enforce priority between table service and ingestion writer, use transactions here and invoke strategy", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzg3MzI5NQ==", "bodyText": "So basically the following is a use-case for us in production :\n\nWriter starts to write fresh data to files f1,f2, c1 is inflight\nSchedule clustering, c2.cluster for files f1,f2\nc1 and c2 in progress\nc2.cluster finishes\nc1 attempts to finish and notices that c2 has overlapping file ids and aborts\n\nWe want to override the priority of c1 over c2 to avoid violating freshness SLA. A design and PR for this is going to follow after this PR is landed.", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587873295", "createdAt": "2021-03-04T22:29:59Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -380,11 +391,40 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n-    new SparkUpgradeDowngrade(metaClient, config, context).run(metaClient, HoodieTableVersion.current(), config, context, instantTime);\n-    return getTableAndInitCtx(metaClient, operationType);\n+    if (HoodieTableVersion.current() != metaClient.getTableConfig().getTableVersion()) {\n+      // TODO : Force clean up of all inflights, do this once pending rollback removal PR is landed\n+      // this.rollbackFailedWrites();\n+      this.txnManager.beginTransaction();\n+      try {\n+        // Ensure no inflight commits\n+        TransactionUtils.resolveConflictIfAnyForUpgradeDowngrade(metaClient);\n+        new SparkUpgradeDowngrade(metaClient, config, context).run(metaClient, HoodieTableVersion.current(), config, context, instantTime);\n+      } finally {\n+        this.txnManager.endTransaction();\n+      }\n+    }\n+    return getTableAndInitCtx(metaClient, operationType, instantTime);\n   }\n \n-  private HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> getTableAndInitCtx(HoodieTableMetaClient metaClient, WriteOperationType operationType) {\n+  // TODO : To enforce priority between table service and ingestion writer, use transactions here and invoke strategy", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0ODM4Nw=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcwODk5NTc2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQxOToxNzozOFrOIwhYww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wNFQyMToyMDozMFrOIwmhfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0OTU3MQ==", "bodyText": "we lose auto closing by moving to  a regular try-catch. Why is this change needed?", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587749571", "createdAt": "2021-03-04T19:17:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -401,13 +441,25 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   public void syncTableMetadata() {\n     // Open up the metadata table again, for syncing\n-    try (HoodieTableMetadataWriter writer = SparkHoodieBackedTableMetadataWriter.create(hadoopConf, config, context)) {\n+    try {\n+      HoodieTableMetadataWriter writer =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NzgzMzcyNg==", "bodyText": "Reverted", "url": "https://github.com/apache/hudi/pull/2374#discussion_r587833726", "createdAt": "2021-03-04T21:20:30Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -401,13 +441,25 @@ protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<\n   @Override\n   public void syncTableMetadata() {\n     // Open up the metadata table again, for syncing\n-    try (HoodieTableMetadataWriter writer = SparkHoodieBackedTableMetadataWriter.create(hadoopConf, config, context)) {\n+    try {\n+      HoodieTableMetadataWriter writer =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4Nzc0OTU3MQ=="}, "originalCommit": {"oid": "27bae91298c897984335b1c6c3b39e758d962e6b"}, "originalPosition": 217}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4054, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}