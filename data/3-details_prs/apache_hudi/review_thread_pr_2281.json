{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI3NTQ1MDAx", "number": 2281, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzoxOTowNVrOFGj2fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDo0MTozMVrOFKGUqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDIzMTY1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzoxOTowNVrOIHnyUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzoxOTowNVrOIHnyUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2MjgwMg==", "bodyText": "Hi @garyli1019 , thanks for adding flink unit test infra.\nhow about enhancing HoodieWriteConfig instead of adding HoodieDataSourceConfig, it seems they have lots in common", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544862802", "createdAt": "2020-12-17T07:19:05Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDI1NTU5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzoyNjo0N1rOIHn_Yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxMzozNjo0OFrOIJ7aQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng==", "bodyText": "Most of these options have been defined in DataSourceOptions, I have been planning to move them to hudi-client-common (https://issues.apache.org/jira/browse/HUDI-1438), till then we can reuse these options.\nDo you mind waiting for a while, we can continue this PR when HUDI-1438 is finished\nWDYT \uff1f", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544866146", "createdAt": "2020-12-17T07:26:47Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+\n+  public static final String TABLE_NAME_PROP = HoodieWriteConfig.TABLE_NAME;\n+  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n+  public static final String RECORDKEY_FIELD_PROP = KeyGeneratorOptions.RECORDKEY_FIELD_OPT_KEY;\n+  public static final String PARTITIONPATH_FIELD_PROP = KeyGeneratorOptions.PARTITIONPATH_FIELD_OPT_KEY;\n+\n+  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n+  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n+  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n+  public static final String DEFAULT_KEYGENERATOR_CLASS = SimpleAvroKeyGenerator.class.getName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg4NTM0Nw==", "bodyText": "I agree that we should move DataSourceOptions to hudi-client-common. Are we looking for moving them intoHoodieWriteConfig? If so I can do it on this PR.\nIMO we need to write tests for every single Flink-related feature actually, otherwise, the features are not trustworthy, so I think we should land this asap and force unit tests for the upcoming Flink features.", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544885347", "createdAt": "2020-12-17T08:04:29Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+\n+  public static final String TABLE_NAME_PROP = HoodieWriteConfig.TABLE_NAME;\n+  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n+  public static final String RECORDKEY_FIELD_PROP = KeyGeneratorOptions.RECORDKEY_FIELD_OPT_KEY;\n+  public static final String PARTITIONPATH_FIELD_PROP = KeyGeneratorOptions.PARTITIONPATH_FIELD_OPT_KEY;\n+\n+  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n+  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n+  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n+  public static final String DEFAULT_KEYGENERATOR_CLASS = SimpleAvroKeyGenerator.class.getName();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg4ODUyNQ==", "bodyText": "I agree that we should move DataSourceOptions to hudi-client-common. Are we looking for moving them intoHoodieWriteConfig? If so I can do it on this PR.\nIMO we need to write tests for every single Flink-related feature actually, otherwise, the features are not trustworthy, so I think we should land this asap and force unit tests for the upcoming Flink features.\n\nI think DataSourceOptions should be independent of HoodieWriteConfig just as it is before.\n@liujinhui1994 do you have time for HUDI-1438 recently? if not, do you mind resign it to @garyli1019", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544888525", "createdAt": "2020-12-17T08:10:21Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+\n+  public static final String TABLE_NAME_PROP = HoodieWriteConfig.TABLE_NAME;\n+  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n+  public static final String RECORDKEY_FIELD_PROP = KeyGeneratorOptions.RECORDKEY_FIELD_OPT_KEY;\n+  public static final String PARTITIONPATH_FIELD_PROP = KeyGeneratorOptions.PARTITIONPATH_FIELD_OPT_KEY;\n+\n+  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n+  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n+  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n+  public static final String DEFAULT_KEYGENERATOR_CLASS = SimpleAvroKeyGenerator.class.getName();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk0ODI3MA==", "bodyText": "@garyli1019 @wangxianghu\nI'm almost done, I will post it as soon as possible", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544948270", "createdAt": "2020-12-17T09:43:19Z", "author": {"login": "liujinhui1994"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+\n+  public static final String TABLE_NAME_PROP = HoodieWriteConfig.TABLE_NAME;\n+  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n+  public static final String RECORDKEY_FIELD_PROP = KeyGeneratorOptions.RECORDKEY_FIELD_OPT_KEY;\n+  public static final String PARTITIONPATH_FIELD_PROP = KeyGeneratorOptions.PARTITIONPATH_FIELD_OPT_KEY;\n+\n+  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n+  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n+  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n+  public static final String DEFAULT_KEYGENERATOR_CLASS = SimpleAvroKeyGenerator.class.getName();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk1MjE3OA==", "bodyText": "@liujinhui1994 awesome!", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544952178", "createdAt": "2020-12-17T09:49:01Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+\n+  public static final String TABLE_NAME_PROP = HoodieWriteConfig.TABLE_NAME;\n+  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n+  public static final String RECORDKEY_FIELD_PROP = KeyGeneratorOptions.RECORDKEY_FIELD_OPT_KEY;\n+  public static final String PARTITIONPATH_FIELD_PROP = KeyGeneratorOptions.PARTITIONPATH_FIELD_OPT_KEY;\n+\n+  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n+  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n+  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n+  public static final String DEFAULT_KEYGENERATOR_CLASS = SimpleAvroKeyGenerator.class.getName();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI4MTQ3NQ==", "bodyText": "hi @liujinhui1994 any update on refactoring DataSourceOptions ticket?", "url": "https://github.com/apache/hudi/pull/2281#discussion_r547281475", "createdAt": "2020-12-22T13:36:48Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieDataSourceConfig.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.keygen.SimpleAvroKeyGenerator;\n+import org.apache.hudi.keygen.constant.KeyGeneratorOptions;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieDataSourceConfig extends DefaultHoodieConfig {\n+\n+  public static final String TABLE_NAME_PROP = HoodieWriteConfig.TABLE_NAME;\n+  public static final String PRECOMBINE_FIELD_PROP = \"hoodie.datasource.write.precombine.field\";\n+  public static final String RECORDKEY_FIELD_PROP = KeyGeneratorOptions.RECORDKEY_FIELD_OPT_KEY;\n+  public static final String PARTITIONPATH_FIELD_PROP = KeyGeneratorOptions.PARTITIONPATH_FIELD_OPT_KEY;\n+\n+  public static final String WRITE_PAYLOAD_CLASS = \"hoodie.datasource.write.payload.class\";\n+  public static final String DEFAULT_WRITE_PAYLOAD_CLASS = OverwriteWithLatestAvroPayload.class.getName();\n+  public static final String KEYGENERATOR_CLASS_PROP = \"hoodie.datasource.write.keygenerator.class\";\n+  public static final String DEFAULT_KEYGENERATOR_CLASS = SimpleAvroKeyGenerator.class.getName();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2NjE0Ng=="}, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTA2NjQ4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMDozODowM1rOIHvQBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxMzozNTo0MFrOIJ7YBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk4NTA5NA==", "bodyText": "There are many fields that have not been used. Can we add them when we would use them?", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544985094", "createdAt": "2020-12-17T10:38:03Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.testutils.minicluster.HdfsTestService;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);\n+  private String testMethodName;\n+  protected transient Configuration hadoopConf = null;\n+  protected transient FileSystem fs;\n+  protected transient HoodieFlinkWriteClient writeClient;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI4MDkwMA==", "bodyText": "sure, will do", "url": "https://github.com/apache/hudi/pull/2281#discussion_r547280900", "createdAt": "2020-12-22T13:35:40Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.testutils.minicluster.HdfsTestService;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);\n+  private String testMethodName;\n+  protected transient Configuration hadoopConf = null;\n+  protected transient FileSystem fs;\n+  protected transient HoodieFlinkWriteClient writeClient;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk4NTA5NA=="}, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTA3MDgwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMDozOTowN1rOIHvSkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMDozOTowN1rOIHvSkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk4NTc0Nw==", "bodyText": "Since this method has been deprecated. Can we use the normal one?", "url": "https://github.com/apache/hudi/pull/2281#discussion_r544985747", "createdAt": "2020-12-17T10:39:07Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.testutils.minicluster.HdfsTestService;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);\n+  private String testMethodName;\n+  protected transient Configuration hadoopConf = null;\n+  protected transient FileSystem fs;\n+  protected transient HoodieFlinkWriteClient writeClient;\n+  protected transient HoodieTableFileSystemView tableView;\n+  protected transient MiniClusterWithClientResource flinkCluster = null;\n+\n+  // dfs\n+  protected String dfsBasePath;\n+  protected transient HdfsTestService hdfsTestService;\n+  protected transient MiniDFSCluster dfsCluster;\n+  protected transient DistributedFileSystem dfs;\n+\n+  @BeforeEach\n+  public void setTestMethodName(TestInfo testInfo) {\n+    if (testInfo.getTestMethod().isPresent()) {\n+      testMethodName = testInfo.getTestMethod().get().getName();\n+    } else {\n+      testMethodName = \"Unknown\";\n+    }\n+  }\n+\n+  protected void initFlinkMiniCluster() {\n+    flinkCluster = new MiniClusterWithClientResource(\n+        new MiniClusterResourceConfiguration.Builder()\n+            .setNumberSlotsPerTaskManager(2)\n+            .setNumberTaskManagers(1)\n+            .build());\n+  }\n+\n+  protected void initFileSystem() {\n+    hadoopConf = new Configuration();\n+    initFileSystemWithConfiguration(hadoopConf);\n+  }\n+\n+  private void initFileSystemWithConfiguration(Configuration configuration) {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    fs = FSUtils.getFs(basePath, configuration);\n+    if (fs instanceof LocalFileSystem) {\n+      LocalFileSystem lfs = (LocalFileSystem) fs;\n+      // With LocalFileSystem, with checksum disabled, fs.open() returns an inputStream which is FSInputStream\n+      // This causes ClassCastExceptions in LogRecordScanner (and potentially other places) calling fs.open\n+      // So, for the tests, we enforce checksum verification to circumvent the problem\n+      lfs.setVerifyChecksum(true);\n+    }\n+  }\n+\n+  /**\n+   * Initializes an instance of {@link HoodieTableMetaClient} with a special table type specified by\n+   * {@code getTableType()}.\n+   *\n+   * @throws IOException\n+   */\n+  protected void initMetaClient() throws IOException {\n+    initMetaClient(getTableType());\n+  }\n+\n+  protected void initMetaClient(HoodieTableType tableType) throws IOException {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    metaClient = HoodieTestUtils.init(hadoopConf, basePath, tableType);\n+  }\n+\n+\n+  /**\n+   * Cleanups file system.\n+   *\n+   * @throws IOException\n+   */\n+  protected void cleanupFileSystem() throws IOException {\n+    if (fs != null) {\n+      LOG.warn(\"Closing file-system instance used in previous test-run\");\n+      fs.close();\n+      fs = null;\n+    }\n+  }\n+\n+  protected void cleanupFlinkMiniCluster() {\n+    if (flinkCluster != null) {\n+      flinkCluster.after();\n+      flinkCluster = null;\n+    }\n+  }\n+\n+  public static class SimpleSink implements SinkFunction<HoodieRecord> {\n+\n+    // must be static\n+    public static List<HoodieRecord> valuesList = new ArrayList<>();\n+\n+    @Override\n+    public synchronized void invoke(HoodieRecord value) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f58a7b737e9d7f3ca6119950c665b77823dc94f9"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2MDk1ODE3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxMTozNDo1MFrOIMrUGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxMzoxNToyN1rOIMtATg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2MzQ4MQ==", "bodyText": "Why do we need to move this code snippet from HoodieClientTestHarness in this PR?", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550163481", "createdAt": "2020-12-30T11:34:50Z", "author": {"login": "yanghua"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java", "diffHunk": "@@ -52,6 +53,24 @@ protected void initPath() {\n     }\n   }\n \n+  /**\n+   * Initializes a test data generator which used to generate test datas.\n+   *\n+   */\n+  protected void initTestDataGenerator() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47e7bff5653047351ea61e4572141254eb9ea568"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE5MTE4Mg==", "bodyText": "This can be reuse by both Spark and Flink client test harness and only depend on hudi's code.", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550191182", "createdAt": "2020-12-30T13:15:27Z", "author": {"login": "garyli1019"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java", "diffHunk": "@@ -52,6 +53,24 @@ protected void initPath() {\n     }\n   }\n \n+  /**\n+   * Initializes a test data generator which used to generate test datas.\n+   *\n+   */\n+  protected void initTestDataGenerator() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2MzQ4MQ=="}, "originalCommit": {"oid": "47e7bff5653047351ea61e4572141254eb9ea568"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2MDk2MjgwOnYy", "diffSide": "RIGHT", "path": "hudi-flink/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxMTozNjo0N1rOIMrWfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxMzoyNzo0N1rOIMtOvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NDA5NA==", "bodyText": "Test suite or Test frameworks sounds better?", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550164094", "createdAt": "2020-12-30T11:36:47Z", "author": {"login": "yanghua"}, "path": "hudi-flink/pom.xml", "diffHunk": "@@ -173,5 +173,102 @@\n       <artifactId>bijection-avro_${scala.binary.version}</artifactId>\n       <version>0.9.7</version>\n     </dependency>\n+\n+    <!-- Test -->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47e7bff5653047351ea61e4572141254eb9ea568"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE5NDg3OA==", "bodyText": "changed to Junit Test Suite", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550194878", "createdAt": "2020-12-30T13:27:47Z", "author": {"login": "garyli1019"}, "path": "hudi-flink/pom.xml", "diffHunk": "@@ -173,5 +173,102 @@\n       <artifactId>bijection-avro_${scala.binary.version}</artifactId>\n       <version>0.9.7</version>\n     </dependency>\n+\n+    <!-- Test -->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NDA5NA=="}, "originalCommit": {"oid": "47e7bff5653047351ea61e4572141254eb9ea568"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2MDk3NDAxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxMTo0Mjo1MlrOIMrctw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxMzoxMzoxMlrOIMs91A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NTY4Nw==", "bodyText": "SimpleTestSinkFunction  sounds better?", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550165687", "createdAt": "2020-12-30T11:42:52Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);\n+  private String testMethodName;\n+  protected transient Configuration hadoopConf = null;\n+  protected transient FileSystem fs;\n+  protected transient MiniClusterWithClientResource flinkCluster = null;\n+\n+  @BeforeEach\n+  public void setTestMethodName(TestInfo testInfo) {\n+    if (testInfo.getTestMethod().isPresent()) {\n+      testMethodName = testInfo.getTestMethod().get().getName();\n+    } else {\n+      testMethodName = \"Unknown\";\n+    }\n+  }\n+\n+  protected void initFlinkMiniCluster() {\n+    flinkCluster = new MiniClusterWithClientResource(\n+        new MiniClusterResourceConfiguration.Builder()\n+            .setNumberSlotsPerTaskManager(2)\n+            .setNumberTaskManagers(1)\n+            .build());\n+  }\n+\n+  protected void initFileSystem() {\n+    hadoopConf = new Configuration();\n+    initFileSystemWithConfiguration(hadoopConf);\n+  }\n+\n+  private void initFileSystemWithConfiguration(Configuration configuration) {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    fs = FSUtils.getFs(basePath, configuration);\n+    if (fs instanceof LocalFileSystem) {\n+      LocalFileSystem lfs = (LocalFileSystem) fs;\n+      // With LocalFileSystem, with checksum disabled, fs.open() returns an inputStream which is FSInputStream\n+      // This causes ClassCastExceptions in LogRecordScanner (and potentially other places) calling fs.open\n+      // So, for the tests, we enforce checksum verification to circumvent the problem\n+      lfs.setVerifyChecksum(true);\n+    }\n+  }\n+\n+  /**\n+   * Initializes an instance of {@link HoodieTableMetaClient} with a special table type specified by\n+   * {@code getTableType()}.\n+   *\n+   * @throws IOException\n+   */\n+  protected void initMetaClient() throws IOException {\n+    initMetaClient(getTableType());\n+  }\n+\n+  protected void initMetaClient(HoodieTableType tableType) throws IOException {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    metaClient = HoodieTestUtils.init(hadoopConf, basePath, tableType);\n+  }\n+\n+\n+  /**\n+   * Cleanups file system.\n+   *\n+   * @throws IOException\n+   */\n+  protected void cleanupFileSystem() throws IOException {\n+    if (fs != null) {\n+      LOG.warn(\"Closing file-system instance used in previous test-run\");\n+      fs.close();\n+      fs = null;\n+    }\n+  }\n+\n+  protected void cleanupFlinkMiniCluster() {\n+    if (flinkCluster != null) {\n+      flinkCluster.after();\n+      flinkCluster = null;\n+    }\n+  }\n+\n+  public static class SimpleSink implements SinkFunction<HoodieRecord> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47e7bff5653047351ea61e4572141254eb9ea568"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE5MDU0OA==", "bodyText": "yes, agree", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550190548", "createdAt": "2020-12-30T13:13:12Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);\n+  private String testMethodName;\n+  protected transient Configuration hadoopConf = null;\n+  protected transient FileSystem fs;\n+  protected transient MiniClusterWithClientResource flinkCluster = null;\n+\n+  @BeforeEach\n+  public void setTestMethodName(TestInfo testInfo) {\n+    if (testInfo.getTestMethod().isPresent()) {\n+      testMethodName = testInfo.getTestMethod().get().getName();\n+    } else {\n+      testMethodName = \"Unknown\";\n+    }\n+  }\n+\n+  protected void initFlinkMiniCluster() {\n+    flinkCluster = new MiniClusterWithClientResource(\n+        new MiniClusterResourceConfiguration.Builder()\n+            .setNumberSlotsPerTaskManager(2)\n+            .setNumberTaskManagers(1)\n+            .build());\n+  }\n+\n+  protected void initFileSystem() {\n+    hadoopConf = new Configuration();\n+    initFileSystemWithConfiguration(hadoopConf);\n+  }\n+\n+  private void initFileSystemWithConfiguration(Configuration configuration) {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    fs = FSUtils.getFs(basePath, configuration);\n+    if (fs instanceof LocalFileSystem) {\n+      LocalFileSystem lfs = (LocalFileSystem) fs;\n+      // With LocalFileSystem, with checksum disabled, fs.open() returns an inputStream which is FSInputStream\n+      // This causes ClassCastExceptions in LogRecordScanner (and potentially other places) calling fs.open\n+      // So, for the tests, we enforce checksum verification to circumvent the problem\n+      lfs.setVerifyChecksum(true);\n+    }\n+  }\n+\n+  /**\n+   * Initializes an instance of {@link HoodieTableMetaClient} with a special table type specified by\n+   * {@code getTableType()}.\n+   *\n+   * @throws IOException\n+   */\n+  protected void initMetaClient() throws IOException {\n+    initMetaClient(getTableType());\n+  }\n+\n+  protected void initMetaClient(HoodieTableType tableType) throws IOException {\n+    if (basePath == null) {\n+      throw new IllegalStateException(\"The base path has not been initialized.\");\n+    }\n+    metaClient = HoodieTestUtils.init(hadoopConf, basePath, tableType);\n+  }\n+\n+\n+  /**\n+   * Cleanups file system.\n+   *\n+   * @throws IOException\n+   */\n+  protected void cleanupFileSystem() throws IOException {\n+    if (fs != null) {\n+      LOG.warn(\"Closing file-system instance used in previous test-run\");\n+      fs.close();\n+      fs = null;\n+    }\n+  }\n+\n+  protected void cleanupFlinkMiniCluster() {\n+    if (flinkCluster != null) {\n+      flinkCluster.after();\n+      flinkCluster = null;\n+    }\n+  }\n+\n+  public static class SimpleSink implements SinkFunction<HoodieRecord> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NTY4Nw=="}, "originalCommit": {"oid": "47e7bff5653047351ea61e4572141254eb9ea568"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2MDk3ODIxOnYy", "diffSide": "RIGHT", "path": "hudi-flink/src/main/java/org/apache/hudi/source/JsonStringToHoodieRecordMapFunction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxMTo0NDo1OFrOIMrfEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxMzoyODoxM1rOIMtPNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NjI4OA==", "bodyText": "Can we reuse the first constructor?", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550166288", "createdAt": "2020-12-30T11:44:58Z", "author": {"login": "yanghua"}, "path": "hudi-flink/src/main/java/org/apache/hudi/source/JsonStringToHoodieRecordMapFunction.java", "diffHunk": "@@ -40,32 +43,47 @@\n  */\n public class JsonStringToHoodieRecordMapFunction implements MapFunction<String, HoodieRecord> {\n \n-  private final HoodieFlinkStreamer.Config cfg;\n+  private TypedProperties props;\n   private KeyGenerator keyGenerator;\n   private AvroConvertor avroConvertor;\n+  private Option<String> schemaStr = Option.empty();\n+  private String payloadClassName;\n+  private String orderingField;\n \n-  public JsonStringToHoodieRecordMapFunction(HoodieFlinkStreamer.Config cfg) {\n-    this.cfg = cfg;\n+  public JsonStringToHoodieRecordMapFunction(TypedProperties props) {\n+    this.props = props;\n+    init();\n+  }\n+\n+  public JsonStringToHoodieRecordMapFunction(TypedProperties props, String schemaStr) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47e7bff5653047351ea61e4572141254eb9ea568"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE5NDk5OQ==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550194999", "createdAt": "2020-12-30T13:28:13Z", "author": {"login": "garyli1019"}, "path": "hudi-flink/src/main/java/org/apache/hudi/source/JsonStringToHoodieRecordMapFunction.java", "diffHunk": "@@ -40,32 +43,47 @@\n  */\n public class JsonStringToHoodieRecordMapFunction implements MapFunction<String, HoodieRecord> {\n \n-  private final HoodieFlinkStreamer.Config cfg;\n+  private TypedProperties props;\n   private KeyGenerator keyGenerator;\n   private AvroConvertor avroConvertor;\n+  private Option<String> schemaStr = Option.empty();\n+  private String payloadClassName;\n+  private String orderingField;\n \n-  public JsonStringToHoodieRecordMapFunction(HoodieFlinkStreamer.Config cfg) {\n-    this.cfg = cfg;\n+  public JsonStringToHoodieRecordMapFunction(TypedProperties props) {\n+    this.props = props;\n+    init();\n+  }\n+\n+  public JsonStringToHoodieRecordMapFunction(TypedProperties props, String schemaStr) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2NjI4OA=="}, "originalCommit": {"oid": "47e7bff5653047351ea61e4572141254eb9ea568"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2MTMzMDU2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDozODozN1rOIMuiYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDozODozN1rOIMuiYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxNjI4OQ==", "bodyText": "This line is too long, it would be better if we could break it. But it does not matter, let's refactor it next time.", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550216289", "createdAt": "2020-12-30T14:38:37Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "diffHunk": "@@ -104,29 +80,7 @@ public HoodieWriteableTestTable forCommit(String instantTime) {\n     return (HoodieWriteableTestTable) super.forCommit(instantTime);\n   }\n \n-  public String getFileIdWithInserts(String partition) throws Exception {\n-    return getFileIdWithInserts(partition, new HoodieRecord[0]);\n-  }\n-\n-  public String getFileIdWithInserts(String partition, HoodieRecord... records) throws Exception {\n-    return getFileIdWithInserts(partition, Arrays.asList(records));\n-  }\n-\n-  public String getFileIdWithInserts(String partition, List<HoodieRecord> records) throws Exception {\n-    String fileId = UUID.randomUUID().toString();\n-    withInserts(partition, fileId, records);\n-    return fileId;\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId) throws Exception {\n-    return withInserts(partition, fileId, new HoodieRecord[0]);\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId, HoodieRecord... records) throws Exception {\n-    return withInserts(partition, fileId, Arrays.asList(records));\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records) throws Exception {\n+  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records, TaskContextSupplier contextSupplier) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f75fa543b1d3b255963fe3f9e15a1ede0f80c66"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2MTMzMDY4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDozODozOVrOIMuicA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDozODozOVrOIMuicA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxNjMwNA==", "bodyText": "This line is too long, it would be better if we could break it. But it does not matter, let's refactor it next time.", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550216304", "createdAt": "2020-12-30T14:38:39Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/test/java/org/apache/hudi/testutils/HoodieWriteableTestTable.java", "diffHunk": "@@ -104,29 +80,7 @@ public HoodieWriteableTestTable forCommit(String instantTime) {\n     return (HoodieWriteableTestTable) super.forCommit(instantTime);\n   }\n \n-  public String getFileIdWithInserts(String partition) throws Exception {\n-    return getFileIdWithInserts(partition, new HoodieRecord[0]);\n-  }\n-\n-  public String getFileIdWithInserts(String partition, HoodieRecord... records) throws Exception {\n-    return getFileIdWithInserts(partition, Arrays.asList(records));\n-  }\n-\n-  public String getFileIdWithInserts(String partition, List<HoodieRecord> records) throws Exception {\n-    String fileId = UUID.randomUUID().toString();\n-    withInserts(partition, fileId, records);\n-    return fileId;\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId) throws Exception {\n-    return withInserts(partition, fileId, new HoodieRecord[0]);\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId, HoodieRecord... records) throws Exception {\n-    return withInserts(partition, fileId, Arrays.asList(records));\n-  }\n-\n-  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records) throws Exception {\n+  public HoodieWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records, TaskContextSupplier contextSupplier) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f75fa543b1d3b255963fe3f9e15a1ede0f80c66"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2MTMzNjc1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDo0MTozMVrOIMumEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDo0MTozMVrOIMumEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxNzIzMg==", "bodyText": "It would be better to split the non-static and static fields, moreover, it is a logger.", "url": "https://github.com/apache/hudi/pull/2281#discussion_r550217232", "createdAt": "2020-12-30T14:41:31Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -0,0 +1,136 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+\n+import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestInfo;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieFlinkClientTestHarness extends HoodieCommonTestHarness implements Serializable {\n+\n+  protected static final Logger LOG = LogManager.getLogger(HoodieFlinkClientTestHarness.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f75fa543b1d3b255963fe3f9e15a1ede0f80c66"}, "originalPosition": 46}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4205, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}