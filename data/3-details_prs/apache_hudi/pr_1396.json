{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg2NDU5MzE2", "number": 1396, "title": "[HUDI-687] Stop incremental reader on RO table before a pending compaction", "bodyText": "What is the purpose of the pull request\nexample timeline:\nt0 -> create bucket1.parquet\nt1 -> create and append updates bucket1.log\nt2 -> request compaction\nt3 -> create bucket2.parquet\nif compaction at t2 takes a long time, incremental reads using HoodieParquetInputFormat may make progress to read commits at t3 and skip data ingested at t1 leading to 'data loss' .(Data will still be on disk, but incremental readers wont see it because its in log file and readers move to t3)\nTo workaround this problem, we want to stop returning data belonging to commits > compaction_requested/inprogress_instant. After compaction is complete, incremental reader would see updates in t2, t3, so on. Disadvantage is that long running compactions can make it look like reader is 'stuck'. But that is better than skipping updates.\nBrief change log\n\nChange HoodieParquetInputFormat to read commits prior to compaction instant\nAdded unit tests to validate behavior\nFix broken test utils for reading records\n\nVerify this pull request\nThis change added tests and can be verified as follows:\nmvn test (TestMergeOnReadTable and TestHoodieActiveTimeline)\nSome discussion is on #1389, sorry I messed up rebase, so resending as a new PR to avoid confusion\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-03-11T03:08:50Z", "url": "https://github.com/apache/hudi/pull/1396", "merged": true, "mergeCommit": {"oid": "c0f96e072650d39433929c6efe3bc0b2cd882a39"}, "closed": true, "closedAt": "2020-04-10T17:45:42Z", "author": {"login": "satishkotha"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcNCpb5AFqTM3MzkxNzQxMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcWUx8QAFqTM5MTU3MTgyNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczOTE3NDEx", "url": "https://github.com/apache/hudi/pull/1396#pullrequestreview-373917411", "createdAt": "2020-03-12T21:32:10Z", "commit": {"oid": "a023c310341f954ef3a9ee91e12853ca9376fe19"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0MDY0NDk0", "url": "https://github.com/apache/hudi/pull/1396#pullrequestreview-374064494", "createdAt": "2020-03-13T05:56:20Z", "commit": {"oid": "a023c310341f954ef3a9ee91e12853ca9376fe19"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwNTo1NjoyMFrOF14Yrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwNTo1OTo0MlrOF14bjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MjY3MQ==", "bodyText": "rename: instantTime", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392042671", "createdAt": "2020-03-13T05:56:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTimeline.java", "diffHunk": "@@ -144,6 +162,11 @@\n    */\n   HoodieTimeline findInstantsAfter(String commitTime, int numCommits);\n \n+  /**\n+   * Create a new Timeline with all instants before specified time.\n+   */\n+  HoodieTimeline findInstantsBefore(String time);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a023c310341f954ef3a9ee91e12853ca9376fe19"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzA1NA==", "bodyText": "Should this belong in the HoodieTimeline class itself? It seems like you can easily build this using other methods, outside of HoodieTimeline?\nGiven this is an interim fix, I would like this to be as isolated as possible from the core classes..", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392043054", "createdAt": "2020-03-13T05:57:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTimeline.java", "diffHunk": "@@ -127,6 +127,24 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+\n+  /**\n+   * Get all instants (commits, delta commits) that produce new data, in the active timeline.\n+   */\n+  HoodieTimeline getCommitsTimeline();\n+\n+  /**\n+   * Timeline to include all instants before the first 'requested compaction'.\n+   *\n+   * For example, say timeline: commit at t0, deltacommit at t1, compaction requested at t2, deltacommit at t3,\n+   * this method would return t0, t1\n+   *\n+   * If there is no pending compaction this is equivalent to getting all instants in the timeline.\n+   *\n+   * @return\n+   */\n+  HoodieTimeline filterInstantsBeforePendingCompaction();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a023c310341f954ef3a9ee91e12853ca9376fe19"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzE1MA==", "bodyText": "In any case, rename to filterInstantsBeforeFirstPendingCompaction?", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392043150", "createdAt": "2020-03-13T05:58:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTimeline.java", "diffHunk": "@@ -127,6 +127,24 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+\n+  /**\n+   * Get all instants (commits, delta commits) that produce new data, in the active timeline.\n+   */\n+  HoodieTimeline getCommitsTimeline();\n+\n+  /**\n+   * Timeline to include all instants before the first 'requested compaction'.\n+   *\n+   * For example, say timeline: commit at t0, deltacommit at t1, compaction requested at t2, deltacommit at t3,\n+   * this method would return t0, t1\n+   *\n+   * If there is no pending compaction this is equivalent to getting all instants in the timeline.\n+   *\n+   * @return\n+   */\n+  HoodieTimeline filterInstantsBeforePendingCompaction();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzA1NA=="}, "originalCommit": {"oid": "a023c310341f954ef3a9ee91e12853ca9376fe19"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzI0MA==", "bodyText": "getCommitsAndCompactionTimeline wont do?", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392043240", "createdAt": "2020-03-13T05:58:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTimeline.java", "diffHunk": "@@ -127,6 +127,24 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+\n+  /**\n+   * Get all instants (commits, delta commits) that produce new data, in the active timeline.\n+   */\n+  HoodieTimeline getCommitsTimeline();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a023c310341f954ef3a9ee91e12853ca9376fe19"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzQwNA==", "bodyText": "lets move the code from HoodieDefaultTimeline here?", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392043404", "createdAt": "2020-03-13T05:59:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java", "diffHunk": "@@ -116,6 +117,24 @@\n     return returns.toArray(new FileStatus[returns.size()]);\n   }\n \n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   */\n+  protected HoodieTimeline filterInstantsTimeline(HoodieTimeline timeline) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a023c310341f954ef3a9ee91e12853ca9376fe19"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3OTE5NzYx", "url": "https://github.com/apache/hudi/pull/1396#pullrequestreview-377919761", "createdAt": "2020-03-19T17:03:46Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNzowMzo0NlrOF43_dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNzoxMTo1OFrOF44Uww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4MTk0Mg==", "bodyText": "Was this working because these tests only dealt with 1 file-groups ? I can see count based assertions using records collected from this method", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395181942", "createdAt": "2020-03-19T17:03:46Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/test/java/org/apache/hudi/common/HoodieMergeOnReadTestUtils.java", "diffHunk": "@@ -47,27 +48,35 @@\n \n   public static List<GenericRecord> getRecordsUsingInputFormat(List<String> inputPaths, String basePath) {\n     JobConf jobConf = new JobConf();\n+    return getRecordsUsingInputFormat(inputPaths, basePath, jobConf, new HoodieParquetRealtimeInputFormat());\n+  }\n+\n+  public static List<GenericRecord> getRecordsUsingInputFormat(List<String> inputPaths,\n+                                                               String basePath,\n+                                                               JobConf jobConf,\n+                                                               HoodieParquetInputFormat inputFormat) {\n     Schema schema = HoodieAvroUtils.addMetadataFields(\n         new Schema.Parser().parse(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA));\n-    HoodieParquetRealtimeInputFormat inputFormat = new HoodieParquetRealtimeInputFormat();\n     setPropsForInputFormat(inputFormat, jobConf, schema, basePath);\n     return inputPaths.stream().map(path -> {\n       setInputPath(jobConf, path);\n       List<GenericRecord> records = new ArrayList<>();\n       try {\n         List<InputSplit> splits = Arrays.asList(inputFormat.getSplits(jobConf, 1));\n-        RecordReader recordReader = inputFormat.getRecordReader(splits.get(0), jobConf, null);\n-        Void key = (Void) recordReader.createKey();\n-        ArrayWritable writable = (ArrayWritable) recordReader.createValue();\n-        while (recordReader.next(key, writable)) {\n-          GenericRecordBuilder newRecord = new GenericRecordBuilder(schema);\n-          // writable returns an array with [field1, field2, _hoodie_commit_time,\n-          // _hoodie_commit_seqno]\n-          Writable[] values = writable.get();\n-          schema.getFields().forEach(field -> {\n-            newRecord.set(field, values[2]);\n-          });\n-          records.add(newRecord.build());\n+        for (InputSplit split : splits) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4NTgxNQ==", "bodyText": "nit: Add non-empty message for the assertion.", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395185815", "createdAt": "2020-03-19T17:09:29Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/table/string/TestHoodieActiveTimeline.java", "diffHunk": "@@ -160,6 +159,8 @@ public void testTimelineOperations() {\n         .filterCompletedInstants().findInstantsInRange(\"04\", \"11\").getInstants().map(HoodieInstant::getTimestamp));\n     HoodieTestUtils.assertStreamEquals(\"\", Stream.of(\"09\", \"11\"), timeline.getCommitTimeline().filterCompletedInstants()\n         .findInstantsAfter(\"07\", 2).getInstants().map(HoodieInstant::getTimestamp));\n+    HoodieTestUtils.assertStreamEquals(\"\", Stream.of(\"01\", \"03\", \"05\"), timeline.getCommitTimeline().filterCompletedInstants()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4NzM5NQ==", "bodyText": "We can add similar test-cases for Copy-On-Write table with RO inputformat alone to make it uniform. @satishkotha : Can you look at it as part of this PR or a follow-up ?", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395187395", "createdAt": "2020-03-19T17:11:58Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/TestMergeOnReadTable.java", "diffHunk": "@@ -80,6 +85,12 @@\n \n public class TestMergeOnReadTable extends HoodieClientTestHarness {\n \n+  private HoodieParquetInputFormat roInputFormat;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgzNTY3MTMw", "url": "https://github.com/apache/hudi/pull/1396#pullrequestreview-383567130", "createdAt": "2020-03-30T06:43:56Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNjo0Mzo1NlrOF9bjcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNjo0Mzo1NlrOF9bjcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk1ODg5OQ==", "bodyText": "Didn't realize you can have an overridden method with a return type which is a sub-type of original return type. Learnt something new today :)", "url": "https://github.com/apache/hudi/pull/1396#discussion_r399958899", "createdAt": "2020-03-30T06:43:56Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java", "diffHunk": "@@ -136,6 +136,13 @@ public HoodieDefaultTimeline findInstantsAfter(String instantTime, int numCommit\n         details);\n   }\n \n+  @Override\n+  public HoodieDefaultTimeline findInstantsBefore(String instantTime) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NjQ3MDA2", "url": "https://github.com/apache/hudi/pull/1396#pullrequestreview-387647006", "createdAt": "2020-04-04T00:42:01Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNFQwMDo0MjowMVrOGAtcAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNFQwMDo0MjowMVrOGAtcAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzYzNQ==", "bodyText": "This seems like the crux of the change? and most of the other code is improving tests etc. If so, this seems like a  reasonable interim solution to me... Although we should encourage users to do incremental pull out of the RTInputFormat really ...\nThe core problem of \"data loss\" being brought in this issue, feels like a mis-expectation really :)", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403397635", "createdAt": "2020-04-04T00:42:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java", "diffHunk": "@@ -118,6 +119,34 @@\n     return returns.toArray(new FileStatus[returns.size()]);\n   }\n \n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   */\n+  protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    Option<HoodieInstant> pendingCompactionInstant = timeline.filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NjQ3MjUy", "url": "https://github.com/apache/hudi/pull/1396#pullrequestreview-387647252", "createdAt": "2020-04-04T00:43:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNFQwMDo0Mzo0NVrOGAtdIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNFQwMDo0NToxM1rOGAteFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzkyMw==", "bodyText": "Let's make it clear that this an incremental on RO.. rename to testIncrementalROReadsWithCOmpaction()?", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403397923", "createdAt": "2020-04-04T00:43:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/TestMergeOnReadTable.java", "diffHunk": "@@ -186,6 +168,96 @@ public void testSimpleInsertAndUpdate() throws Exception {\n     }\n   }\n \n+  // test incremental read does not go past compaction instant for RO views\n+  // For RT views, incremental read can go past compaction\n+  @Test\n+  public void testIncrementalReadsWithCompaction() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODE2Ng==", "bodyText": "I am surprised by the amount of test helper code you needed to write yourself.. Should we be using code from some existing helpers.. or should these helpers move somewhere else, so they are beneficial over all?", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403398166", "createdAt": "2020-04-04T00:45:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/TestMergeOnReadTable.java", "diffHunk": "@@ -1311,4 +1383,111 @@ private void assertNoWriteErrors(List<WriteStatus> statuses) {\n       assertFalse(\"Errors found in write of \" + status.getFileId(), status.hasErrors());\n     }\n   }\n+  \n+  private FileStatus[] insertAndGetFilePaths(List<HoodieRecord> records, HoodieWriteClient client,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 237}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "692604937ad658192c2518e0be1e6a1cd4bae993", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/692604937ad658192c2518e0be1e6a1cd4bae993", "committedDate": "2020-04-09T21:54:15Z", "message": "[HUDI-687] Stop incremental reader on RO table when there is a pending compaction"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "692604937ad658192c2518e0be1e6a1cd4bae993", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/692604937ad658192c2518e0be1e6a1cd4bae993", "committedDate": "2020-04-09T21:54:15Z", "message": "[HUDI-687] Stop incremental reader on RO table when there is a pending compaction"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxNTcxODI1", "url": "https://github.com/apache/hudi/pull/1396#pullrequestreview-391571825", "createdAt": "2020-04-10T17:45:04Z", "commit": {"oid": "692604937ad658192c2518e0be1e6a1cd4bae993"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3828, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}