{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDExMDY3OTM2", "number": 1574, "title": "[HUDI-701]Add unit test for HDFSParquetImportCommand", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nAdd unit test for HDFSParquetImportCommand\nBrief change log\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\nThis pull request is a trivial rework / code cleanup without any test coverage.\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-04-30T01:49:50Z", "url": "https://github.com/apache/hudi/pull/1574", "merged": true, "mergeCommit": {"oid": "3a2fe13fcb7c168f8ff023e3bdb6ae482b400316"}, "closed": true, "closedAt": "2020-05-14T11:15:50Z", "author": {"login": "hddong"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcckcOBABqjMyODczNjMzMjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABchK1_EAH2gAyNDExMDY3OTM2OmQ4MjVhMzczZjFlMDdmMzMxZTA4NTUxM2NhNmY2ZjkxOGY0MmYzZGM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA2NTg5Njc1", "url": "https://github.com/apache/hudi/pull/1574#pullrequestreview-406589675", "createdAt": "2020-05-06T13:03:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzowMzozOVrOGRR6vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxODowOVrOGRSgYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3MjU0MQ==", "bodyText": "use Files.exists(xx)?", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420772541", "createdAt": "2020-05-06T13:03:39Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestHDFSParquetImportCommand.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.utilities.HDFSParquetImporter;\n+import org.apache.hudi.utilities.TestHDFSParquetImporter;\n+import org.apache.hudi.utilities.TestHDFSParquetImporter.HoodieTripModel;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.jupiter.api.Assertions.assertAll;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link org.apache.hudi.cli.commands.HDFSParquetImportCommand}.\n+ */\n+public class ITTestHDFSParquetImportCommand extends AbstractShellIntegrationTest {\n+\n+  private Path sourcePath;\n+  private Path targetPath;\n+  private String tableName;\n+  private String schemaFile;\n+  private String tablePath;\n+\n+  private List<GenericRecord> insertData;\n+  private TestHDFSParquetImporter importer;\n+\n+  @BeforeEach\n+  public void init() throws IOException, ParseException {\n+    tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    sourcePath = new Path(basePath, \"source\");\n+    targetPath = new Path(tablePath);\n+    schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+    // create schema file\n+    try (FSDataOutputStream schemaFileOS = fs.create(new Path(schemaFile))) {\n+      schemaFileOS.write(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA.getBytes());\n+    }\n+\n+    importer = new TestHDFSParquetImporter();\n+    insertData = importer.createInsertRecords(sourcePath);\n+  }\n+\n+  /**\n+   * Test case for 'hdfsparquetimport' with insert.\n+   */\n+  @Test\n+  public void testConvertWithInsert() throws IOException {\n+    String command = String.format(\"hdfsparquetimport --srcPath %s --targetPath %s --tableName %s \"\n+        + \"--tableType %s --rowKeyField %s\" + \" --partitionPathField %s --parallelism %s \"\n+        + \"--schemaFilePath %s --format %s --sparkMemory %s --retry %s --sparkMaster %s\",\n+        sourcePath.toString(), targetPath.toString(), tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"_row_key\", \"timestamp\", \"1\", schemaFile, \"parquet\", \"2G\", \"1\", \"local\");\n+    CommandResult cr = getShell().executeCommand(command);\n+\n+    assertAll(\"Command run success\",\n+        () -> assertTrue(cr.isSuccess()),\n+        () -> assertEquals(\"Table imported to hoodie format\", cr.getResult().toString()));\n+\n+    // Check hudi table exist\n+    String metaPath = targetPath + File.separator + HoodieTableMetaClient.METAFOLDER_NAME;\n+    assertTrue(new File(metaPath).exists(), \"Hoodie table not exist.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ==", "bodyText": "Please note that I am not talking about you here. But these array indexes containing numbers are very ugly and unreadable. We should think of a way to improve it.\nWe should parse all parameters and it is best to do:\n\nDefine appropriate variables to store each parameter to improve the readability of the code;\nRefactor it, yes, the parse of the parameters should be order-independent;\n\nWDYT? @hddong @vinothchandar", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420779385", "createdAt": "2020-05-06T13:13:58Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -82,17 +82,17 @@ public static void main(String[] args) throws Exception {\n         break;\n       case IMPORT:\n       case UPSERT:\n-        assert (args.length >= 12);\n+        assert (args.length >= 13);\n         String propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[11])) {\n-          propsFilePath = args[11];\n+        if (!StringUtils.isNullOrEmpty(args[12])) {\n+          propsFilePath = args[12];\n         }\n         List<String> configs = new ArrayList<>();\n-        if (args.length > 12) {\n-          configs.addAll(Arrays.asList(args).subList(12, args.length));\n+        if (args.length > 13) {\n+          configs.addAll(Arrays.asList(args).subList(13, args.length));\n         }\n-        returnCode = dataLoad(jsc, command, args[1], args[2], args[3], args[4], args[5], args[6],\n-            Integer.parseInt(args[7]), args[8], args[9], Integer.parseInt(args[10]), propsFilePath, configs);\n+        returnCode = dataLoad(jsc, command, args[3], args[4], args[5], args[6], args[7], args[8],", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4MDczMA==", "bodyText": "\"Spark Master \" -> \"Spark Master\" (remove the right empty backspace)", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420780730", "createdAt": "2020-05-06T13:15:57Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HDFSParquetImportCommand.java", "diffHunk": "@@ -57,6 +56,7 @@ public String convert(\n       @CliOption(key = \"schemaFilePath\", mandatory = true,\n           help = \"Path for Avro schema file\") final String schemaFilePath,\n       @CliOption(key = \"format\", mandatory = true, help = \"Format for the input data\") final String format,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4MjE3OA==", "bodyText": "As a refactor suggestion, it would be better to define a data structure to store the cli args to avoid change the signature of the method frequently.", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420782178", "createdAt": "2020-05-06T13:18:09Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HDFSParquetImportCommand.java", "diffHunk": "@@ -78,8 +76,8 @@ public String convert(\n       cmd = SparkCommand.UPSERT.toString();\n     }\n \n-    sparkLauncher.addAppArgs(cmd, srcPath, targetPath, tableName, tableType, rowKeyField, partitionPathField,\n-        parallelism, schemaFilePath, sparkMemory, retry, propsFilePath);\n+    sparkLauncher.addAppArgs(cmd, master, sparkMemory, srcPath, targetPath, tableName, tableType, rowKeyField,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExNjQzOTMx", "url": "https://github.com/apache/hudi/pull/1574#pullrequestreview-411643931", "createdAt": "2020-05-14T09:51:19Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "526cbc6e88cf2f4e9645a3d71828662355bb3a6b", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/526cbc6e88cf2f4e9645a3d71828662355bb3a6b", "committedDate": "2020-05-14T09:55:46Z", "message": "Add test for HDFSParquetImportCommand"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "526cbc6e88cf2f4e9645a3d71828662355bb3a6b", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/526cbc6e88cf2f4e9645a3d71828662355bb3a6b", "committedDate": "2020-05-14T09:55:46Z", "message": "Add test for HDFSParquetImportCommand"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d825a373f1e07f331e085513ca6f6f918f42f3dc", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/d825a373f1e07f331e085513ca6f6f918f42f3dc", "committedDate": "2020-05-14T10:23:36Z", "message": "fix"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3045, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}