{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk0NTI5MjM4", "number": 1452, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxMjozNjoxMFrODsC6ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNTo1NDoxM1rODvaBkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NTExNzEwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "isResolved": false, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxMjozNjoxMFrOF8vPhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwOToyNDo1NlrOGCmrzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA==", "bodyText": "Why do you want to implement it specifically for CLEAN command? Any specific reasons? I could not understand the purpose of this PR basically. :)", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399232900", "createdAt": "2020-03-27T12:36:10Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTYwOTQyMA==", "bodyText": "@pratyakshsharma only CLEAN command can  specify the sparkMaster now. Otherwise, sparkMaster not contained in args of other command.", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399609420", "createdAt": "2020-03-28T02:43:55Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTYzNDQ5Nw==", "bodyText": "I have the same concern with @pratyakshsharma , can we make other commands also support specifying the spark master?", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399634497", "createdAt": "2020-03-28T08:04:57Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY3MDY4OQ==", "bodyText": "@yanghua yep, it is better to add sparkMaster  for other commands. Is it need in this PR? I think I can create a new jira to do this, and then we can init jsc by sparkMaster for all commands.", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399670689", "createdAt": "2020-03-28T14:51:58Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY3OTAzNg==", "bodyText": "@hddong I can see in SparkMain.java class that sparkMaster and sparkMemory are both present in args of other command as well. For example -\n\nhttps://github.com/apache/incubator-hudi/blob/04449f33feb300b99750c52ec37f2561aa644456/hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java#L216\nhttps://github.com/apache/incubator-hudi/blob/04449f33feb300b99750c52ec37f2561aa644456/hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java#L234\n\nAm I missing something here?\nAlso all these positional arguments will be changed to proper config objects as per this PR (#1174). You might want to take a look at this one.", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399679036", "createdAt": "2020-03-28T16:10:00Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTczMzQzNQ==", "bodyText": "@prashantwason my mistake, thanks for your point out.", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399733435", "createdAt": "2020-03-29T02:05:16Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTc3ODk2OQ==", "bodyText": "Let us address other commands also in this PR then :) or maybe you can wait for the other PR I mentioned before doing these changes. I would leave this decision on you :)", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399778969", "createdAt": "2020-03-29T10:43:57Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4ODc1NA==", "bodyText": "@pratyakshsharma yes, it's need, i'm doing. Your PR was Left behind for too long :)", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399888754", "createdAt": "2020-03-30T01:42:54Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM4NDE0Mg==", "bodyText": "sounds good.", "url": "https://github.com/apache/hudi/pull/1452#discussion_r405384142", "createdAt": "2020-04-08T09:24:56Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, "originalCommit": {"oid": "d0be469c7325f1772181eb835af1bff52c3b9393"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMDM0ODE5OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNTo0ODoyN1rOGBzxGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNTo0ODoyN1rOGBzxGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU0OTkxNA==", "bodyText": "Let's use List<SparkCommand>?", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404549914", "createdAt": "2020-04-07T05:48:27Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -118,53 +121,55 @@ public static void main(String[] args) throws Exception {\n         break;\n       case COMPACT_VALIDATE:\n         assert (args.length == 7);\n-        doCompactValidate(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6]);\n+        doCompactValidate(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]));\n         returnCode = 0;\n         break;\n       case COMPACT_REPAIR:\n         assert (args.length == 8);\n-        doCompactRepair(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactRepair(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]));\n         returnCode = 0;\n         break;\n       case COMPACT_UNSCHEDULE_FILE:\n         assert (args.length == 9);\n-        doCompactUnscheduleFile(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactUnscheduleFile(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));\n         returnCode = 0;\n         break;\n       case COMPACT_UNSCHEDULE_PLAN:\n         assert (args.length == 9);\n-        doCompactUnschedule(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactUnschedule(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));\n         returnCode = 0;\n         break;\n       case CLEAN:\n         assert (args.length >= 5);\n         propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[3])) {\n-          propsFilePath = args[3];\n+        if (!StringUtils.isNullOrEmpty(args[4])) {\n+          propsFilePath = args[4];\n         }\n         configs = new ArrayList<>();\n         if (args.length > 5) {\n           configs.addAll(Arrays.asList(args).subList(5, args.length));\n         }\n-        clean(jsc, args[1], args[2], propsFilePath, args[4], configs);\n+        clean(jsc, args[3], propsFilePath, configs);\n         break;\n       default:\n         break;\n     }\n     System.exit(returnCode);\n   }\n \n-  private static void clean(JavaSparkContext jsc, String basePath, String sparkMaster, String propsFilePath,\n-                            String sparkMemory, List<String> configs) throws Exception {\n+  private static boolean sparkMasterContained(SparkCommand command) {\n+    List masterContained = Arrays.asList(SparkCommand.COMPACT_VALIDATE, SparkCommand.COMPACT_REPAIR,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0647035d3b9b7efbac5155b71603074e54587a76"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMDM0OTg5OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNTo0OToxMFrOGBzyEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNTo0OToxMFrOGBzyEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDE2MQ==", "bodyText": "The indent of arg is wrong?", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404550161", "createdAt": "2020-04-07T05:49:10Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -118,53 +121,55 @@ public static void main(String[] args) throws Exception {\n         break;\n       case COMPACT_VALIDATE:\n         assert (args.length == 7);\n-        doCompactValidate(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6]);\n+        doCompactValidate(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]));\n         returnCode = 0;\n         break;\n       case COMPACT_REPAIR:\n         assert (args.length == 8);\n-        doCompactRepair(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactRepair(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]));\n         returnCode = 0;\n         break;\n       case COMPACT_UNSCHEDULE_FILE:\n         assert (args.length == 9);\n-        doCompactUnscheduleFile(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactUnscheduleFile(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));\n         returnCode = 0;\n         break;\n       case COMPACT_UNSCHEDULE_PLAN:\n         assert (args.length == 9);\n-        doCompactUnschedule(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactUnschedule(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));\n         returnCode = 0;\n         break;\n       case CLEAN:\n         assert (args.length >= 5);\n         propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[3])) {\n-          propsFilePath = args[3];\n+        if (!StringUtils.isNullOrEmpty(args[4])) {\n+          propsFilePath = args[4];\n         }\n         configs = new ArrayList<>();\n         if (args.length > 5) {\n           configs.addAll(Arrays.asList(args).subList(5, args.length));\n         }\n-        clean(jsc, args[1], args[2], propsFilePath, args[4], configs);\n+        clean(jsc, args[3], propsFilePath, configs);\n         break;\n       default:\n         break;\n     }\n     System.exit(returnCode);\n   }\n \n-  private static void clean(JavaSparkContext jsc, String basePath, String sparkMaster, String propsFilePath,\n-                            String sparkMemory, List<String> configs) throws Exception {\n+  private static boolean sparkMasterContained(SparkCommand command) {\n+    List masterContained = Arrays.asList(SparkCommand.COMPACT_VALIDATE, SparkCommand.COMPACT_REPAIR,\n+        SparkCommand.COMPACT_UNSCHEDULE_PLAN, SparkCommand.COMPACT_UNSCHEDULE_FILE, SparkCommand.CLEAN);\n+    return masterContained.contains(command);\n+  }\n+\n+  private static void clean(JavaSparkContext jsc, String basePath, String propsFilePath,\n+                            List<String> configs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0647035d3b9b7efbac5155b71603074e54587a76"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMDM1NDM0OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNTo1MToyOVrOGBz08g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwNTo0OToxM1rOGCf2wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA==", "bodyText": "Can we extract the string literals in this method to be constant fields?", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404550898", "createdAt": "2020-04-07T05:51:29Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "diffHunk": "@@ -61,9 +62,14 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax\n   }\n \n   public static JavaSparkContext initJavaSparkConf(String name) {\n+    return initJavaSparkConf(name, Option.empty(), Option.empty());\n+  }\n+\n+  public static JavaSparkContext initJavaSparkConf(String name, Option<String> master,\n+                                                   Option<String> executorMemory) {\n     SparkConf sparkConf = new SparkConf().setAppName(name);\n \n-    String defMasterFromEnv = sparkConf.getenv(\"SPARK_MASTER\");\n+    String defMasterFromEnv = master.orElse(sparkConf.getenv(\"SPARK_MASTER\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0647035d3b9b7efbac5155b71603074e54587a76"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY4NTg5Mg==", "bodyText": "Can we extract the string literals in this method to be constant fields?\n\n@yanghua There is constant fields DEFAULT_SPARK_MASTER for master.", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404685892", "createdAt": "2020-04-07T09:57:46Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "diffHunk": "@@ -61,9 +62,14 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax\n   }\n \n   public static JavaSparkContext initJavaSparkConf(String name) {\n+    return initJavaSparkConf(name, Option.empty(), Option.empty());\n+  }\n+\n+  public static JavaSparkContext initJavaSparkConf(String name, Option<String> master,\n+                                                   Option<String> executorMemory) {\n     SparkConf sparkConf = new SparkConf().setAppName(name);\n \n-    String defMasterFromEnv = sparkConf.getenv(\"SPARK_MASTER\");\n+    String defMasterFromEnv = master.orElse(sparkConf.getenv(\"SPARK_MASTER\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA=="}, "originalCommit": {"oid": "0647035d3b9b7efbac5155b71603074e54587a76"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5MDYyMA==", "bodyText": "Actually, I mean, we should avoid using string literals e.g. SPARK_MASTER , spark.hadoop.mapred.output.compress and so on. WDYT?", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404690620", "createdAt": "2020-04-07T10:05:27Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "diffHunk": "@@ -61,9 +62,14 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax\n   }\n \n   public static JavaSparkContext initJavaSparkConf(String name) {\n+    return initJavaSparkConf(name, Option.empty(), Option.empty());\n+  }\n+\n+  public static JavaSparkContext initJavaSparkConf(String name, Option<String> master,\n+                                                   Option<String> executorMemory) {\n     SparkConf sparkConf = new SparkConf().setAppName(name);\n \n-    String defMasterFromEnv = sparkConf.getenv(\"SPARK_MASTER\");\n+    String defMasterFromEnv = master.orElse(sparkConf.getenv(\"SPARK_MASTER\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA=="}, "originalCommit": {"oid": "0647035d3b9b7efbac5155b71603074e54587a76"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDkwMDY3MA==", "bodyText": "@yanghua yes, necessary, how about add a file HoodieCliSparkConfig.", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404900670", "createdAt": "2020-04-07T15:27:58Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "diffHunk": "@@ -61,9 +62,14 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax\n   }\n \n   public static JavaSparkContext initJavaSparkConf(String name) {\n+    return initJavaSparkConf(name, Option.empty(), Option.empty());\n+  }\n+\n+  public static JavaSparkContext initJavaSparkConf(String name, Option<String> master,\n+                                                   Option<String> executorMemory) {\n     SparkConf sparkConf = new SparkConf().setAppName(name);\n \n-    String defMasterFromEnv = sparkConf.getenv(\"SPARK_MASTER\");\n+    String defMasterFromEnv = master.orElse(sparkConf.getenv(\"SPARK_MASTER\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA=="}, "originalCommit": {"oid": "0647035d3b9b7efbac5155b71603074e54587a76"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTI3MjI1Nw==", "bodyText": "+1 to introduce HoodieCliSparkConfig", "url": "https://github.com/apache/hudi/pull/1452#discussion_r405272257", "createdAt": "2020-04-08T05:49:13Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "diffHunk": "@@ -61,9 +62,14 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax\n   }\n \n   public static JavaSparkContext initJavaSparkConf(String name) {\n+    return initJavaSparkConf(name, Option.empty(), Option.empty());\n+  }\n+\n+  public static JavaSparkContext initJavaSparkConf(String name, Option<String> master,\n+                                                   Option<String> executorMemory) {\n     SparkConf sparkConf = new SparkConf().setAppName(name);\n \n-    String defMasterFromEnv = sparkConf.getenv(\"SPARK_MASTER\");\n+    String defMasterFromEnv = master.orElse(sparkConf.getenv(\"SPARK_MASTER\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA=="}, "originalCommit": {"oid": "0647035d3b9b7efbac5155b71603074e54587a76"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMDM2MDUxOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNTo1NDoxM1rOGBz4fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwNTo1NDoxM1rOGBz4fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MTgwNQ==", "bodyText": "indent issue.", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404551805", "createdAt": "2020-04-07T05:54:13Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "diffHunk": "@@ -61,9 +62,14 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax\n   }\n \n   public static JavaSparkContext initJavaSparkConf(String name) {\n+    return initJavaSparkConf(name, Option.empty(), Option.empty());\n+  }\n+\n+  public static JavaSparkContext initJavaSparkConf(String name, Option<String> master,\n+                                                   Option<String> executorMemory) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0647035d3b9b7efbac5155b71603074e54587a76"}, "originalPosition": 16}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4713, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}