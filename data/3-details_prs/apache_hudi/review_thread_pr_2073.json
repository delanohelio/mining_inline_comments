{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgwODAzNDc3", "number": 2073, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QyMzoxNzo1OFrOEhCW1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMjo1MzoxN1rOEky4cQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDc3MDc3OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QyMzoxNzo1OFrOHOIqXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMToxNzowOFrOHTdLdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU4NDAzMA==", "bodyText": "It would be better to add a link to the address of the blog. WDYT?", "url": "https://github.com/apache/hudi/pull/2073#discussion_r484584030", "createdAt": "2020-09-07T23:17:58Z", "author": {"login": "yanghua"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -210,6 +210,8 @@ Sample config files for table wise overridden properties can be found under `hud\n   --op BULK_INSERT\n ```\n \n+For detailed information on how to configure and use `HoodieMultiTableDeltaStreamer`, please refer blog section.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5064f9cba7452d420a1bebc563bf85e38e53489d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDE2MzA2Mg==", "bodyText": "Looks like I need to generate index.html file to be able to add the link. @nsivabalan Can you please guide me here? Do I need to write the html manually or is there some alternate way of generating the html for this?", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490163062", "createdAt": "2020-09-17T11:17:08Z", "author": {"login": "pratyakshsharma"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -210,6 +210,8 @@ Sample config files for table wise overridden properties can be found under `hud\n   --op BULK_INSERT\n ```\n \n+For detailed information on how to configure and use `HoodieMultiTableDeltaStreamer`, please refer blog section.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU4NDAzMA=="}, "originalCommit": {"oid": "5064f9cba7452d420a1bebc563bf85e38e53489d"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDc3Njg3OnYy", "diffSide": "RIGHT", "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QyMzoyNDo0MlrOHOItgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMToyMDowOVrOHTdTEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU4NDgzNQ==", "bodyText": "Currently HoodieMultiTableDeltaStreamer supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.  Can we use some bold keywords?", "url": "https://github.com/apache/hudi/pull/2073#discussion_r484584835", "createdAt": "2020-09-07T23:24:42Z", "author": {"login": "yanghua"}, "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "diffHunk": "@@ -0,0 +1,101 @@\n+---\n+title: \"Ingest multiple tables using Hudi\"\n+excerpt: \"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`\"\n+author: pratyakshsharma\n+category: blog\n+---\n+\n+When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using `HoodieMultiTableDeltaStreamer` class which is a wrapper on top of the more popular `HoodieDeltaStreamer` class. Currently `HoodieMultiTableDeltaStreamer` supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5064f9cba7452d420a1bebc563bf85e38e53489d"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDE2NTAwOA==", "bodyText": "Sure. Thank you for pointing this out.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490165008", "createdAt": "2020-09-17T11:20:09Z", "author": {"login": "pratyakshsharma"}, "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "diffHunk": "@@ -0,0 +1,101 @@\n+---\n+title: \"Ingest multiple tables using Hudi\"\n+excerpt: \"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`\"\n+author: pratyakshsharma\n+category: blog\n+---\n+\n+When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using `HoodieMultiTableDeltaStreamer` class which is a wrapper on top of the more popular `HoodieDeltaStreamer` class. Currently `HoodieMultiTableDeltaStreamer` supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU4NDgzNQ=="}, "originalCommit": {"oid": "5064f9cba7452d420a1bebc563bf85e38e53489d"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDc4MTk3OnYy", "diffSide": "RIGHT", "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QyMzoyOTo0OVrOHOIwVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMToyMToxNlrOHTdV5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU4NTU1Ng==", "bodyText": "IMO, this item should not be a single one, right? It's an explanation, not about a configuration. It would be better to merge it with the third one. WDYT?", "url": "https://github.com/apache/hudi/pull/2073#discussion_r484585556", "createdAt": "2020-09-07T23:29:49Z", "author": {"login": "yanghua"}, "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "diffHunk": "@@ -0,0 +1,101 @@\n+---\n+title: \"Ingest multiple tables using Hudi\"\n+excerpt: \"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`\"\n+author: pratyakshsharma\n+category: blog\n+---\n+\n+When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using `HoodieMultiTableDeltaStreamer` class which is a wrapper on top of the more popular `HoodieDeltaStreamer` class. Currently `HoodieMultiTableDeltaStreamer` supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.\n+\n+This blog will guide you through configuring and running `HoodieMultiTableDeltaStreamer`.\n+\n+### Configuration\n+\n+ - `HoodieMultiTableDeltaStreamer` expects users to maintain table wise overridden properties in separate files in a dedicated config folder. Common properties can be configured via common properties file also.\n+ - By default, hudi datasets are created under the path `<base-path-prefix>/<database_name>/<name_of_table_to_be_ingested>`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5064f9cba7452d420a1bebc563bf85e38e53489d"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDE2NTczMg==", "bodyText": "Doing that.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490165732", "createdAt": "2020-09-17T11:21:16Z", "author": {"login": "pratyakshsharma"}, "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "diffHunk": "@@ -0,0 +1,101 @@\n+---\n+title: \"Ingest multiple tables using Hudi\"\n+excerpt: \"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`\"\n+author: pratyakshsharma\n+category: blog\n+---\n+\n+When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using `HoodieMultiTableDeltaStreamer` class which is a wrapper on top of the more popular `HoodieDeltaStreamer` class. Currently `HoodieMultiTableDeltaStreamer` supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.\n+\n+This blog will guide you through configuring and running `HoodieMultiTableDeltaStreamer`.\n+\n+### Configuration\n+\n+ - `HoodieMultiTableDeltaStreamer` expects users to maintain table wise overridden properties in separate files in a dedicated config folder. Common properties can be configured via common properties file also.\n+ - By default, hudi datasets are created under the path `<base-path-prefix>/<database_name>/<name_of_table_to_be_ingested>`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU4NTU1Ng=="}, "originalCommit": {"oid": "5064f9cba7452d420a1bebc563bf85e38e53489d"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDc4NTYxOnYy", "diffSide": "RIGHT", "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QyMzozMzo1M1rOHOIyVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QxMToyMzozNVrOHTdaQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU4NjA3MA==", "bodyText": "Maybe add more information looks better? e.g. the development of this feature in the future? And not end with code.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r484586070", "createdAt": "2020-09-07T23:33:53Z", "author": {"login": "yanghua"}, "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "diffHunk": "@@ -0,0 +1,101 @@\n+---\n+title: \"Ingest multiple tables using Hudi\"\n+excerpt: \"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`\"\n+author: pratyakshsharma\n+category: blog\n+---\n+\n+When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using `HoodieMultiTableDeltaStreamer` class which is a wrapper on top of the more popular `HoodieDeltaStreamer` class. Currently `HoodieMultiTableDeltaStreamer` supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.\n+\n+This blog will guide you through configuring and running `HoodieMultiTableDeltaStreamer`.\n+\n+### Configuration\n+\n+ - `HoodieMultiTableDeltaStreamer` expects users to maintain table wise overridden properties in separate files in a dedicated config folder. Common properties can be configured via common properties file also.\n+ - By default, hudi datasets are created under the path `<base-path-prefix>/<database_name>/<name_of_table_to_be_ingested>`.\n+ - You need to provide the names of tables to be ingested via the property `hoodie.deltastreamer.ingestion.tablesToBeIngested` in the format `<database>.<table>`, for example \n+ \n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\n+``` \n+ \n+ - If you do not provide database name, then it is assumed the table belongs to default database and the hudi dataset for the concerned table is created under the path `<base-path-prefix>/default/<name_of_table_to_be_ingested>`. Also there is a provision to override the default path for hudi datasets. You can create hudi dataset for a particular table by setting the property `hoodie.deltastreamer.ingestion.targetBasePath` in table level config file\n+ - There are a lot of properties that one might like to override per table, for example\n+ \n+```java\n+hoodie.datasource.write.recordkey.field=_row_key\n+hoodie.datasource.write.partitionpath.field=created_at\n+hoodie.deltastreamer.source.kafka.topic=topic2\n+hoodie.deltastreamer.keygen.timebased.timestamp.type=UNIX_TIMESTAMP\n+hoodie.deltastreamer.keygen.timebased.input.dateformat=yyyy-MM-dd HH:mm:ss.S\n+hoodie.datasource.hive_sync.table=short_trip_uber_hive_dummy_table\n+hoodie.deltastreamer.ingestion.targetBasePath=s3:///temp/hudi/table1\n+```  \n+ \n+ - Properties like above need to be set for every table to be ingested. As already suggested at the beginning, users are expected to maintain separate config files for every table by setting the below property\n+ \n+```java\n+hoodie.deltastreamer.ingestion.<db>.<table>.configFile=s3:///tmp/config/config1.properties\n+``` \n+\n+If you do not want to set the above property for every table, you can simply create config files for every table to be ingested under the config folder with the name - `<database>_<table>_config.properties`. For example if you want to ingest table1 and table2 from dummy database, where config folder is set to `s3:///tmp/config`, then you need to create 2 config files on the given paths - `s3:///tmp/config/dummy_table1_config.properties` and `s3:///tmp/config/dummy_table2_config.properties`.\n+\n+ - Finally you can specify all the common properties in a common properties file. Common properties file does not necessarily have to lie under config folder but it is advised to keep it along with other config files. This file will contain the below properties\n+ \n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\n+hoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///tmp/config_table1.properties\n+hoodie.deltastreamer.ingestion.db2.table2.configFile=s3:///tmp/config_table2.properties\n+``` \n+\n+### Run Command\n+\n+`HoodieMultiTableDeltaStreamer` can be run similar to how one runs `HoodieDeltaStreamer`. Please refer to the example given below for the command. \n+\n+\n+### Example\n+\n+Suppose you want to ingest table1 and table2 from db1 and want to ingest the 2 tables under the path `s3:///temp/hudi`. You can ingest them using the below command\n+\n+```java\n+[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n+  --props s3:///temp/hudi-ingestion-config/kafka-source.properties \\\n+  --config-folder s3:///temp/hudi-ingestion-config \\\n+  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n+  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n+  --source-ordering-field impresssiontime \\\n+  --base-path-prefix s3:///temp/hudi \\ \n+  --target-table dummy_table \\\n+  --op UPSERT\n+```\n+\n+s3:///temp/config/kafka-source.properties\n+\n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db1.table2\n+hoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///temp/hudi-ingestion-config/config_table1.properties\n+hoodie.deltastreamer.ingestion.db21.table2.configFile=s3:///temp/hudi-ingestion-config/config_table2.properties\n+\n+#Kafka props\n+bootstrap.servers=localhost:9092\n+auto.offset.reset=earliest\n+schema.registry.url=http://localhost:8081\n+\n+hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.CustomKeyGenerator\n+```\n+\n+s3:///temp/hudi-ingestion-config/config_table1.properties\n+\n+```java\n+hoodie.datasource.write.recordkey.field=_row_key1\n+hoodie.datasource.write.partitionpath.field=created_at\n+hoodie.deltastreamer.source.kafka.topic=topic1\n+```\n+\n+s3:///temp/hudi-ingestion-config/config_table2.properties\n+\n+```java\n+hoodie.datasource.write.recordkey.field=_row_key2\n+hoodie.datasource.write.partitionpath.field=created_at\n+hoodie.deltastreamer.source.kafka.topic=topic2\n+```", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5064f9cba7452d420a1bebc563bf85e38e53489d"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDE2Njg1MA==", "bodyText": "Ok, makes sense.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490166850", "createdAt": "2020-09-17T11:23:35Z", "author": {"login": "pratyakshsharma"}, "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "diffHunk": "@@ -0,0 +1,101 @@\n+---\n+title: \"Ingest multiple tables using Hudi\"\n+excerpt: \"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`\"\n+author: pratyakshsharma\n+category: blog\n+---\n+\n+When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using `HoodieMultiTableDeltaStreamer` class which is a wrapper on top of the more popular `HoodieDeltaStreamer` class. Currently `HoodieMultiTableDeltaStreamer` supports COPY_ON_WRITE storage type only and the ingestion is done in a sequential way.\n+\n+This blog will guide you through configuring and running `HoodieMultiTableDeltaStreamer`.\n+\n+### Configuration\n+\n+ - `HoodieMultiTableDeltaStreamer` expects users to maintain table wise overridden properties in separate files in a dedicated config folder. Common properties can be configured via common properties file also.\n+ - By default, hudi datasets are created under the path `<base-path-prefix>/<database_name>/<name_of_table_to_be_ingested>`.\n+ - You need to provide the names of tables to be ingested via the property `hoodie.deltastreamer.ingestion.tablesToBeIngested` in the format `<database>.<table>`, for example \n+ \n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\n+``` \n+ \n+ - If you do not provide database name, then it is assumed the table belongs to default database and the hudi dataset for the concerned table is created under the path `<base-path-prefix>/default/<name_of_table_to_be_ingested>`. Also there is a provision to override the default path for hudi datasets. You can create hudi dataset for a particular table by setting the property `hoodie.deltastreamer.ingestion.targetBasePath` in table level config file\n+ - There are a lot of properties that one might like to override per table, for example\n+ \n+```java\n+hoodie.datasource.write.recordkey.field=_row_key\n+hoodie.datasource.write.partitionpath.field=created_at\n+hoodie.deltastreamer.source.kafka.topic=topic2\n+hoodie.deltastreamer.keygen.timebased.timestamp.type=UNIX_TIMESTAMP\n+hoodie.deltastreamer.keygen.timebased.input.dateformat=yyyy-MM-dd HH:mm:ss.S\n+hoodie.datasource.hive_sync.table=short_trip_uber_hive_dummy_table\n+hoodie.deltastreamer.ingestion.targetBasePath=s3:///temp/hudi/table1\n+```  \n+ \n+ - Properties like above need to be set for every table to be ingested. As already suggested at the beginning, users are expected to maintain separate config files for every table by setting the below property\n+ \n+```java\n+hoodie.deltastreamer.ingestion.<db>.<table>.configFile=s3:///tmp/config/config1.properties\n+``` \n+\n+If you do not want to set the above property for every table, you can simply create config files for every table to be ingested under the config folder with the name - `<database>_<table>_config.properties`. For example if you want to ingest table1 and table2 from dummy database, where config folder is set to `s3:///tmp/config`, then you need to create 2 config files on the given paths - `s3:///tmp/config/dummy_table1_config.properties` and `s3:///tmp/config/dummy_table2_config.properties`.\n+\n+ - Finally you can specify all the common properties in a common properties file. Common properties file does not necessarily have to lie under config folder but it is advised to keep it along with other config files. This file will contain the below properties\n+ \n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db2.table2\n+hoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///tmp/config_table1.properties\n+hoodie.deltastreamer.ingestion.db2.table2.configFile=s3:///tmp/config_table2.properties\n+``` \n+\n+### Run Command\n+\n+`HoodieMultiTableDeltaStreamer` can be run similar to how one runs `HoodieDeltaStreamer`. Please refer to the example given below for the command. \n+\n+\n+### Example\n+\n+Suppose you want to ingest table1 and table2 from db1 and want to ingest the 2 tables under the path `s3:///temp/hudi`. You can ingest them using the below command\n+\n+```java\n+[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n+  --props s3:///temp/hudi-ingestion-config/kafka-source.properties \\\n+  --config-folder s3:///temp/hudi-ingestion-config \\\n+  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n+  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n+  --source-ordering-field impresssiontime \\\n+  --base-path-prefix s3:///temp/hudi \\ \n+  --target-table dummy_table \\\n+  --op UPSERT\n+```\n+\n+s3:///temp/config/kafka-source.properties\n+\n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested=db1.table1,db1.table2\n+hoodie.deltastreamer.ingestion.db1.table1.configFile=s3:///temp/hudi-ingestion-config/config_table1.properties\n+hoodie.deltastreamer.ingestion.db21.table2.configFile=s3:///temp/hudi-ingestion-config/config_table2.properties\n+\n+#Kafka props\n+bootstrap.servers=localhost:9092\n+auto.offset.reset=earliest\n+schema.registry.url=http://localhost:8081\n+\n+hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.CustomKeyGenerator\n+```\n+\n+s3:///temp/hudi-ingestion-config/config_table1.properties\n+\n+```java\n+hoodie.datasource.write.recordkey.field=_row_key1\n+hoodie.datasource.write.partitionpath.field=created_at\n+hoodie.deltastreamer.source.kafka.topic=topic1\n+```\n+\n+s3:///temp/hudi-ingestion-config/config_table2.properties\n+\n+```java\n+hoodie.datasource.write.recordkey.field=_row_key2\n+hoodie.datasource.write.partitionpath.field=created_at\n+hoodie.deltastreamer.source.kafka.topic=topic2\n+```", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU4NjA3MA=="}, "originalCommit": {"oid": "5064f9cba7452d420a1bebc563bf85e38e53489d"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDExMjI1OnYy", "diffSide": "RIGHT", "path": "content/blog/ingest-multiple-tables-using-hudi/index.html", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMjoxNDozOVrOHT7kgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxMjoxNToyN1rOHUKprA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY2MDk5NQ==", "bodyText": "Hi, It seems we do not need the *.html file. You can refer to this PR #1992", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490660995", "createdAt": "2020-09-18T02:14:39Z", "author": {"login": "yanghua"}, "path": "content/blog/ingest-multiple-tables-using-hudi/index.html", "diffHunk": "@@ -0,0 +1,233 @@\n+<!doctype html>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b419d605fc4dc5698be29103cd7f19e561c29f4"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY2NzgzNw==", "bodyText": "yeah, I don't think you need this file.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490667837", "createdAt": "2020-09-18T02:41:54Z", "author": {"login": "nsivabalan"}, "path": "content/blog/ingest-multiple-tables-using-hudi/index.html", "diffHunk": "@@ -0,0 +1,233 @@\n+<!doctype html>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY2MDk5NQ=="}, "originalCommit": {"oid": "1b419d605fc4dc5698be29103cd7f19e561c29f4"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDkwODA3Ng==", "bodyText": "Thank you for pointing the PR. Removed the file now.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490908076", "createdAt": "2020-09-18T12:15:27Z", "author": {"login": "pratyakshsharma"}, "path": "content/blog/ingest-multiple-tables-using-hudi/index.html", "diffHunk": "@@ -0,0 +1,233 @@\n+<!doctype html>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY2MDk5NQ=="}, "originalCommit": {"oid": "1b419d605fc4dc5698be29103cd7f19e561c29f4"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDE3NTA0OnYy", "diffSide": "RIGHT", "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMjo1MToyN1rOHT8IuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxMjoxNzoxMFrOHUKs2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY3MDI2NQ==", "bodyText": "my 2 cents. take your call though. Instead of \"one of the most common problems that one faces is simplifying the onboarding process for multiple tables\", how about \"one of the major challenges is the complexity in onboarding multiple tables\".", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490670265", "createdAt": "2020-09-18T02:51:27Z", "author": {"login": "nsivabalan"}, "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "diffHunk": "@@ -0,0 +1,104 @@\n+---\n+title: \"Ingest multiple tables using Hudi\"\n+excerpt: \"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`\"\n+author: pratyaksh.sharma.hudi\n+category: blog\n+---\n+\n+When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using `HoodieMultiTableDeltaStreamer` class which is a wrapper on top of the more popular `HoodieDeltaStreamer` class. Currently `HoodieMultiTableDeltaStreamer` supports **COPY_ON_WRITE** storage type only and the ingestion is done in a **sequential** way.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b419d605fc4dc5698be29103cd7f19e561c29f4"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDkwODg5MQ==", "bodyText": "Guess will keep it in its original form. :) Thank you for your kind suggestion though.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490908891", "createdAt": "2020-09-18T12:17:10Z", "author": {"login": "pratyakshsharma"}, "path": "docs/_posts/2020-08-22-ingest-multiple-tables-using-hudi.md", "diffHunk": "@@ -0,0 +1,104 @@\n+---\n+title: \"Ingest multiple tables using Hudi\"\n+excerpt: \"Ingesting multiple tables using Hudi at a single go is now possible. This blog gives a detailed explanation of how to achieve the same using `HoodieMultiTableDeltaStreamer.java`\"\n+author: pratyaksh.sharma.hudi\n+category: blog\n+---\n+\n+When building a change data capture pipeline for already existing or newly created relational databases, one of the most common problems that one faces is simplifying the onboarding process for multiple tables. Ingesting multiple tables to Hudi dataset at a single go is now possible using `HoodieMultiTableDeltaStreamer` class which is a wrapper on top of the more popular `HoodieDeltaStreamer` class. Currently `HoodieMultiTableDeltaStreamer` supports **COPY_ON_WRITE** storage type only and the ingestion is done in a **sequential** way.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY3MDI2NQ=="}, "originalCommit": {"oid": "1b419d605fc4dc5698be29103cd7f19e561c29f4"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MDE3ODQxOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQwMjo1MzoxN1rOHT8Kiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxMjoxNTo1OFrOHUKqjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY3MDczMQ==", "bodyText": "I don't know, just confirming. do you know this link actually works.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490670731", "createdAt": "2020-09-18T02:53:17Z", "author": {"login": "nsivabalan"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -210,6 +210,8 @@ Sample config files for table wise overridden properties can be found under `hud\n   --op BULK_INSERT\n ```\n \n+For detailed information on how to configure and use `HoodieMultiTableDeltaStreamer`, please refer [blog section](/blog/ingest-multiple-tables-using-hudi).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b419d605fc4dc5698be29103cd7f19e561c29f4"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDkwODMwMg==", "bodyText": "yeah, it works :) I have tested.", "url": "https://github.com/apache/hudi/pull/2073#discussion_r490908302", "createdAt": "2020-09-18T12:15:58Z", "author": {"login": "pratyakshsharma"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -210,6 +210,8 @@ Sample config files for table wise overridden properties can be found under `hud\n   --op BULK_INSERT\n ```\n \n+For detailed information on how to configure and use `HoodieMultiTableDeltaStreamer`, please refer [blog section](/blog/ingest-multiple-tables-using-hudi).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDY3MDczMQ=="}, "originalCommit": {"oid": "1b419d605fc4dc5698be29103cd7f19e561c29f4"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4268, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}