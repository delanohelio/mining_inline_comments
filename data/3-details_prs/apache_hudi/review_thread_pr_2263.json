{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI0MjQzODY4", "number": 2263, "reviewThreads": {"totalCount": 60, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMTowNzoxNlrOE7wnow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjo0MjowN1rOFHXj7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDk4MDE5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMTowNzoxNlrOH3lTrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozMjo0OVrOH8IR6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NDk3NQ==", "bodyText": "Should this be a public method ? Rolling back inflight could probably remain private ? Or is this for the CLI ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528044975", "createdAt": "2020-11-21T01:07:16Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -726,6 +751,54 @@ private void rollbackPendingCommits() {\n     return compactionInstantTimeOpt;\n   }\n \n+  /**\n+   * Schedules a new clustering instant.\n+   *\n+   * @param extraMetadata Extra Metadata to be stored\n+   */\n+  public Option<String> scheduleClustering(Option<Map<String, String>> extraMetadata) throws HoodieIOException {\n+    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+    return scheduleClusteringAtInstant(instantTime, extraMetadata) ? Option.of(instantTime) : Option.empty();\n+  }\n+\n+  /**\n+   * Schedules a new clustering instant with passed-in instant time.\n+   *\n+   * @param instantTime clustering Instant Time\n+   * @param extraMetadata Extra Metadata to be stored\n+   */\n+  public boolean scheduleClusteringAtInstant(String instantTime, Option<Map<String, String>> extraMetadata) throws HoodieIOException {\n+    LOG.info(\"Scheduling clustering at instant time :\" + instantTime);\n+    Option<HoodieClusteringPlan> plan = createTable(config, hadoopConf)\n+        .scheduleClustering(context, instantTime, extraMetadata);\n+    return plan.isPresent();\n+  }\n+\n+  /**\n+   * Ensures clustering instant is in expected state and performs clustering for the plan stored in metadata.\n+   *\n+   * @param clusteringInstant Clustering Instant Time\n+   * @return Collection of Write Status\n+   */\n+  protected abstract HoodieWriteMetadata<O> cluster(String clusteringInstant, boolean shouldComplete);\n+\n+  /**\n+   * Executes a clustering plan on a table, serially before or after an insert/upsert action.\n+   */\n+  protected Option<String> inlineCluster(Option<Map<String, String>> extraMetadata) {\n+    Option<String> clusteringInstantOpt = scheduleClustering(extraMetadata);\n+    clusteringInstantOpt.ifPresent(clusteringInstant -> {\n+      // inline cluster should auto commit as the user is never given control\n+      cluster(clusteringInstant, true);\n+    });\n+    return clusteringInstantOpt;\n+  }\n+\n+  public void rollbackInflightClustering(HoodieInstant inflightInstant, HoodieTable<T, I, K, O> table) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxMjI2Ng==", "bodyText": "Changed to protected. I dont think this is needed for CLI.  We can change it back to public, when needed.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532812266", "createdAt": "2020-11-30T18:32:49Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -726,6 +751,54 @@ private void rollbackPendingCommits() {\n     return compactionInstantTimeOpt;\n   }\n \n+  /**\n+   * Schedules a new clustering instant.\n+   *\n+   * @param extraMetadata Extra Metadata to be stored\n+   */\n+  public Option<String> scheduleClustering(Option<Map<String, String>> extraMetadata) throws HoodieIOException {\n+    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+    return scheduleClusteringAtInstant(instantTime, extraMetadata) ? Option.of(instantTime) : Option.empty();\n+  }\n+\n+  /**\n+   * Schedules a new clustering instant with passed-in instant time.\n+   *\n+   * @param instantTime clustering Instant Time\n+   * @param extraMetadata Extra Metadata to be stored\n+   */\n+  public boolean scheduleClusteringAtInstant(String instantTime, Option<Map<String, String>> extraMetadata) throws HoodieIOException {\n+    LOG.info(\"Scheduling clustering at instant time :\" + instantTime);\n+    Option<HoodieClusteringPlan> plan = createTable(config, hadoopConf)\n+        .scheduleClustering(context, instantTime, extraMetadata);\n+    return plan.isPresent();\n+  }\n+\n+  /**\n+   * Ensures clustering instant is in expected state and performs clustering for the plan stored in metadata.\n+   *\n+   * @param clusteringInstant Clustering Instant Time\n+   * @return Collection of Write Status\n+   */\n+  protected abstract HoodieWriteMetadata<O> cluster(String clusteringInstant, boolean shouldComplete);\n+\n+  /**\n+   * Executes a clustering plan on a table, serially before or after an insert/upsert action.\n+   */\n+  protected Option<String> inlineCluster(Option<Map<String, String>> extraMetadata) {\n+    Option<String> clusteringInstantOpt = scheduleClustering(extraMetadata);\n+    clusteringInstantOpt.ifPresent(clusteringInstant -> {\n+      // inline cluster should auto commit as the user is never given control\n+      cluster(clusteringInstant, true);\n+    });\n+    return clusteringInstantOpt;\n+  }\n+\n+  public void rollbackInflightClustering(HoodieInstant inflightInstant, HoodieTable<T, I, K, O> table) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NDk3NQ=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDk5NDM1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToxODoyMVrOH3lbMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozNDowMFrOH8IUjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0Njg5OA==", "bodyText": "Looks like the execution framework details are in the configs. Would be nice to have a wrapper around reflection utils that can pick the right execution engine based class but keep the strategy class name generic...not sure if it's too late for that..", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528046898", "createdAt": "2020-11-21T01:18:21Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxMjk0MQ==", "bodyText": "This is just the class name. It is possible to support both flink and spark clients with same class. Initial implementation is primarily targeted for Spark. So i explicitly called the class Spark.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532812941", "createdAt": "2020-11-30T18:34:00Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0Njg5OA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDk5NDc2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToxODo0N1rOH3lbcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToxODo0N1rOH3lbcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0Njk2MQ==", "bodyText": "extreme nit : s/a/an :)", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528046961", "createdAt": "2020-11-21T01:18:47Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDk5NzIyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyMDo0NVrOH3lctg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozNDoxN1rOH8IVOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NzI4Ng==", "bodyText": "Is this in MB ? Can we rename this to withClusteringMaxGroupSizeInMB ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528047286", "createdAt": "2020-11-21T01:20:45Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_GROUP_SIZE * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_GROUP_SIZE = \"hoodie.clustering.max.group.size\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = \"hoodie.clustering.max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"1\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.\n+  public static final String CLUSTERING_TARGET_FILE_SIZE = \"hoodie.clustering.target.file.size\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_FILE_SIZE = String.valueOf(1 * 1024 * 1024 * 1024L);\n+\n+  // Any strategy specific params can be saved with this prefix\n+  public static final String CLUSTERING_STRATEGY_PARAM_PREFIX = \"hoodie.clustering.strategy.param.\";\n+\n+  // constants related to clustering that may be used by more than 1 strategy.\n+  public static final String SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n+\n+  public HoodieClusteringConfig(Properties props) {\n+    super(props);\n+  }\n+\n+  public static Builder newBuilder() {\n+    return new Builder();\n+  }\n+\n+  public static class Builder {\n+\n+    private final Properties props = new Properties();\n+\n+    public Builder fromFile(File propertiesFile) throws IOException {\n+      try (FileReader reader = new FileReader(propertiesFile)) {\n+        this.props.load(reader);\n+        return this;\n+      }\n+    }\n+\n+    public Builder withScheduleClusteringStrategyClass(String clusteringStrategyClass) {\n+      props.setProperty(SCHEDULE_CLUSTERING_STRATEGY_CLASS, clusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withRunClusteringStrategyClass(String runClusteringStrategyClass) {\n+      props.setProperty(RUN_CLUSTERING_STRATEGY_CLASS, runClusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetPartitions(String clusteringTargetPartitions) {\n+      props.setProperty(CLUSTERING_TARGET_PARTITIONS, clusteringTargetPartitions);\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxGroupSize(long clusteringMaxGroupSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxMzExNA==", "bodyText": "Fixed", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532813114", "createdAt": "2020-11-30T18:34:17Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_GROUP_SIZE * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_GROUP_SIZE = \"hoodie.clustering.max.group.size\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = \"hoodie.clustering.max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"1\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.\n+  public static final String CLUSTERING_TARGET_FILE_SIZE = \"hoodie.clustering.target.file.size\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_FILE_SIZE = String.valueOf(1 * 1024 * 1024 * 1024L);\n+\n+  // Any strategy specific params can be saved with this prefix\n+  public static final String CLUSTERING_STRATEGY_PARAM_PREFIX = \"hoodie.clustering.strategy.param.\";\n+\n+  // constants related to clustering that may be used by more than 1 strategy.\n+  public static final String SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n+\n+  public HoodieClusteringConfig(Properties props) {\n+    super(props);\n+  }\n+\n+  public static Builder newBuilder() {\n+    return new Builder();\n+  }\n+\n+  public static class Builder {\n+\n+    private final Properties props = new Properties();\n+\n+    public Builder fromFile(File propertiesFile) throws IOException {\n+      try (FileReader reader = new FileReader(propertiesFile)) {\n+        this.props.load(reader);\n+        return this;\n+      }\n+    }\n+\n+    public Builder withScheduleClusteringStrategyClass(String clusteringStrategyClass) {\n+      props.setProperty(SCHEDULE_CLUSTERING_STRATEGY_CLASS, clusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withRunClusteringStrategyClass(String runClusteringStrategyClass) {\n+      props.setProperty(RUN_CLUSTERING_STRATEGY_CLASS, runClusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetPartitions(String clusteringTargetPartitions) {\n+      props.setProperty(CLUSTERING_TARGET_PARTITIONS, clusteringTargetPartitions);\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxGroupSize(long clusteringMaxGroupSize) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NzI4Ng=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDk5NzY3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyMTowNlrOH3lc8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozNDoyNFrOH8IVgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NzM0NA==", "bodyText": "Same this, if in MB, add it to the name or java docs ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528047344", "createdAt": "2020-11-21T01:21:06Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_GROUP_SIZE * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_GROUP_SIZE = \"hoodie.clustering.max.group.size\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = \"hoodie.clustering.max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"1\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.\n+  public static final String CLUSTERING_TARGET_FILE_SIZE = \"hoodie.clustering.target.file.size\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_FILE_SIZE = String.valueOf(1 * 1024 * 1024 * 1024L);\n+\n+  // Any strategy specific params can be saved with this prefix\n+  public static final String CLUSTERING_STRATEGY_PARAM_PREFIX = \"hoodie.clustering.strategy.param.\";\n+\n+  // constants related to clustering that may be used by more than 1 strategy.\n+  public static final String SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n+\n+  public HoodieClusteringConfig(Properties props) {\n+    super(props);\n+  }\n+\n+  public static Builder newBuilder() {\n+    return new Builder();\n+  }\n+\n+  public static class Builder {\n+\n+    private final Properties props = new Properties();\n+\n+    public Builder fromFile(File propertiesFile) throws IOException {\n+      try (FileReader reader = new FileReader(propertiesFile)) {\n+        this.props.load(reader);\n+        return this;\n+      }\n+    }\n+\n+    public Builder withScheduleClusteringStrategyClass(String clusteringStrategyClass) {\n+      props.setProperty(SCHEDULE_CLUSTERING_STRATEGY_CLASS, clusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withRunClusteringStrategyClass(String runClusteringStrategyClass) {\n+      props.setProperty(RUN_CLUSTERING_STRATEGY_CLASS, runClusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetPartitions(String clusteringTargetPartitions) {\n+      props.setProperty(CLUSTERING_TARGET_PARTITIONS, clusteringTargetPartitions);\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxGroupSize(long clusteringMaxGroupSize) {\n+      props.setProperty(CLUSTERING_MAX_GROUP_SIZE, String.valueOf(clusteringMaxGroupSize));\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxNumGroups(int maxNumGroups) {\n+      props.setProperty(CLUSTERING_MAX_NUM_GROUPS, String.valueOf(maxNumGroups));\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetFileSize(long targetFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxMzE4NQ==", "bodyText": "Fixed", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532813185", "createdAt": "2020-11-30T18:34:24Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_GROUP_SIZE * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_GROUP_SIZE = \"hoodie.clustering.max.group.size\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = \"hoodie.clustering.max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"1\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.\n+  public static final String CLUSTERING_TARGET_FILE_SIZE = \"hoodie.clustering.target.file.size\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_FILE_SIZE = String.valueOf(1 * 1024 * 1024 * 1024L);\n+\n+  // Any strategy specific params can be saved with this prefix\n+  public static final String CLUSTERING_STRATEGY_PARAM_PREFIX = \"hoodie.clustering.strategy.param.\";\n+\n+  // constants related to clustering that may be used by more than 1 strategy.\n+  public static final String SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n+\n+  public HoodieClusteringConfig(Properties props) {\n+    super(props);\n+  }\n+\n+  public static Builder newBuilder() {\n+    return new Builder();\n+  }\n+\n+  public static class Builder {\n+\n+    private final Properties props = new Properties();\n+\n+    public Builder fromFile(File propertiesFile) throws IOException {\n+      try (FileReader reader = new FileReader(propertiesFile)) {\n+        this.props.load(reader);\n+        return this;\n+      }\n+    }\n+\n+    public Builder withScheduleClusteringStrategyClass(String clusteringStrategyClass) {\n+      props.setProperty(SCHEDULE_CLUSTERING_STRATEGY_CLASS, clusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withRunClusteringStrategyClass(String runClusteringStrategyClass) {\n+      props.setProperty(RUN_CLUSTERING_STRATEGY_CLASS, runClusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetPartitions(String clusteringTargetPartitions) {\n+      props.setProperty(CLUSTERING_TARGET_PARTITIONS, clusteringTargetPartitions);\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxGroupSize(long clusteringMaxGroupSize) {\n+      props.setProperty(CLUSTERING_MAX_GROUP_SIZE, String.valueOf(clusteringMaxGroupSize));\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxNumGroups(int maxNumGroups) {\n+      props.setProperty(CLUSTERING_MAX_NUM_GROUPS, String.valueOf(maxNumGroups));\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetFileSize(long targetFileSize) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NzM0NA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTAwNDYxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyNzoyNFrOH3lgqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozNDo0N1rOH8IWUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODI5Nw==", "bodyText": "Should this be named \"cluster\" given the write client also has \"cluster\" ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528048297", "createdAt": "2020-11-21T01:27:24Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -326,6 +327,28 @@ public HoodieActiveTimeline getActiveTimeline() {\n   public abstract HoodieWriteMetadata<O> compact(HoodieEngineContext context,\n                                               String compactionInstantTime);\n \n+\n+  /**\n+   * Schedule clustering for the instant time.\n+   *\n+   * @param context HoodieEngineContext\n+   * @param instantTime Instant Time for scheduling clustering\n+   * @param extraMetadata additional metadata to write into plan\n+   * @return HoodieClusteringPlan, if there is enough data for clustering.\n+   */\n+  public abstract Option<HoodieClusteringPlan> scheduleClustering(HoodieEngineContext context,\n+                                                                  String instantTime,\n+                                                                  Option<Map<String, String>> extraMetadata);\n+\n+  /**\n+   * Run Clustering on the table. Clustering re-arranges the data so that it is optimized for data access.\n+   *\n+   * @param context HoodieEngineContext\n+   * @param clusteringInstantTime Instant Time\n+   */\n+  public abstract HoodieWriteMetadata<O> clustering(HoodieEngineContext context,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxMzM5NQ==", "bodyText": "Fixed", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532813395", "createdAt": "2020-11-30T18:34:47Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -326,6 +327,28 @@ public HoodieActiveTimeline getActiveTimeline() {\n   public abstract HoodieWriteMetadata<O> compact(HoodieEngineContext context,\n                                               String compactionInstantTime);\n \n+\n+  /**\n+   * Schedule clustering for the instant time.\n+   *\n+   * @param context HoodieEngineContext\n+   * @param instantTime Instant Time for scheduling clustering\n+   * @param extraMetadata additional metadata to write into plan\n+   * @return HoodieClusteringPlan, if there is enough data for clustering.\n+   */\n+  public abstract Option<HoodieClusteringPlan> scheduleClustering(HoodieEngineContext context,\n+                                                                  String instantTime,\n+                                                                  Option<Map<String, String>> extraMetadata);\n+\n+  /**\n+   * Run Clustering on the table. Clustering re-arranges the data so that it is optimized for data access.\n+   *\n+   * @param context HoodieEngineContext\n+   * @param clusteringInstantTime Instant Time\n+   */\n+  public abstract HoodieWriteMetadata<O> clustering(HoodieEngineContext context,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODI5Nw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTI2NzY0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoxMzoyOVrOH3navQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozNToxNVrOH8IXSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA3OTU0OQ==", "bodyText": "Better name for \"maxDataInGroup\" ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528079549", "createdAt": "2020-11-21T06:13:29Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices,\n+                                                                                     long maxDataInGroup);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxMzY0MA==", "bodyText": "Removed this argument per Li Wei suggestion.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532813640", "createdAt": "2020-11-30T18:35:15Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices,\n+                                                                                     long maxDataInGroup);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA3OTU0OQ=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTI3NDYwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/RunClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoyMToyM1rOH3ndxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjozNDoyOVrOIHzkqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDMyNA==", "bodyText": "Can you expand on why we need the \"schema\" parameter ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528080324", "createdAt": "2020-11-21T06:21:23Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/RunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+\n+/**\n+ * Pluggable implementation for writing data into new file groups based on ClusteringPlan.\n+ */\n+public abstract class RunClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(RunClusteringStrategy.class);\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public RunClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Run clustering to write inputRecords into new files as defined by rules in strategy parameters. The number of new\n+   * file groups created is bounded by numOutputGroups.\n+   * Note that commit is not done as part of strategy. commit is callers responsibility.\n+   */\n+  public abstract O performClustering(final I inputRecords, final int numOutputGroups, final String instantTime,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNTUwNg==", "bodyText": "This is \"readerSchema\" for all inputRecords being clustered. This is useful to read values for specified sort columns  from inputRecords.  Let me know if theres a better way to do this.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532815506", "createdAt": "2020-11-30T18:38:31Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/RunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+\n+/**\n+ * Pluggable implementation for writing data into new file groups based on ClusteringPlan.\n+ */\n+public abstract class RunClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(RunClusteringStrategy.class);\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public RunClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Run clustering to write inputRecords into new files as defined by rules in strategy parameters. The number of new\n+   * file groups created is bounded by numOutputGroups.\n+   * Note that commit is not done as part of strategy. commit is callers responsibility.\n+   */\n+  public abstract O performClustering(final I inputRecords, final int numOutputGroups, final String instantTime,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDMyNA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTkxNQ==", "bodyText": "lets rename the parameter to be more descriptive?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545055915", "createdAt": "2020-12-17T12:34:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/RunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+\n+/**\n+ * Pluggable implementation for writing data into new file groups based on ClusteringPlan.\n+ */\n+public abstract class RunClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(RunClusteringStrategy.class);\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public RunClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Run clustering to write inputRecords into new files as defined by rules in strategy parameters. The number of new\n+   * file groups created is bounded by numOutputGroups.\n+   * Note that commit is not done as part of strategy. commit is callers responsibility.\n+   */\n+  public abstract O performClustering(final I inputRecords, final int numOutputGroups, final String instantTime,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDMyNA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTI3NjY3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoyNDozOFrOH3neuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjo0MDo0MlrOIHzy0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDU2OA==", "bodyText": "Could we use StringUtils.EMPTY or create a constant ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528080568", "createdAt": "2020-11-21T06:24:38Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering\n+   */\n+  protected List<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = ((SyncableFileSystemView) getHoodieTable().getSliceView()).getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(ClusteringUtils.getAllFileGroupsInPendingClusteringPlans(getHoodieTable().getMetaClient()).keySet());\n+\n+    return hoodieTable.getSliceView().getLatestFileSlices(partition)\n+        // file ids already in clustering are not eligible\n+        .filter(slice -> !fgIdsInPendingCompactionAndClustering.contains(slice.getFileGroupId()))\n+        // files that have basefile size larger than clustering target file size are not eligible (Note that compaction can merge any updates)\n+        .filter(slice -> slice.getBaseFile().map(HoodieBaseFile::getFileSize).orElse(0L) < writeConfig.getClusteringTargetFileSize())\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Get parameters specific to strategy. These parameters are passed from 'schedule clustering' step to\n+   * 'run clustering' step. 'run clustering' step is typically async. So these params help with passing any required\n+   * context from schedule to run step.\n+   */\n+  protected abstract Map<String, String> getStrategyParams();\n+\n+  /**\n+   * Returns any specific parameters to be stored as part of clustering metadata.\n+   */\n+  protected Map<String, String> getExtraMetadata() {\n+    return Collections.emptyMap();\n+  }\n+\n+  /**\n+   * Version to support future changes for plan.\n+   */\n+  protected int getPlanVersion() {\n+    return CLUSTERING_PLAN_VERSION_1;\n+  }\n+\n+  /**\n+   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.\n+   */\n+  protected List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1OTUzNg==", "bodyText": "use the constant in L123 as well?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545059536", "createdAt": "2020-12-17T12:40:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering\n+   */\n+  protected List<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = ((SyncableFileSystemView) getHoodieTable().getSliceView()).getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(ClusteringUtils.getAllFileGroupsInPendingClusteringPlans(getHoodieTable().getMetaClient()).keySet());\n+\n+    return hoodieTable.getSliceView().getLatestFileSlices(partition)\n+        // file ids already in clustering are not eligible\n+        .filter(slice -> !fgIdsInPendingCompactionAndClustering.contains(slice.getFileGroupId()))\n+        // files that have basefile size larger than clustering target file size are not eligible (Note that compaction can merge any updates)\n+        .filter(slice -> slice.getBaseFile().map(HoodieBaseFile::getFileSize).orElse(0L) < writeConfig.getClusteringTargetFileSize())\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Get parameters specific to strategy. These parameters are passed from 'schedule clustering' step to\n+   * 'run clustering' step. 'run clustering' step is typically async. So these params help with passing any required\n+   * context from schedule to run step.\n+   */\n+  protected abstract Map<String, String> getStrategyParams();\n+\n+  /**\n+   * Returns any specific parameters to be stored as part of clustering metadata.\n+   */\n+  protected Map<String, String> getExtraMetadata() {\n+    return Collections.emptyMap();\n+  }\n+\n+  /**\n+   * Version to support future changes for plan.\n+   */\n+  protected int getPlanVersion() {\n+    return CLUSTERING_PLAN_VERSION_1;\n+  }\n+\n+  /**\n+   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.\n+   */\n+  protected List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDU2OA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTMyMDAzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo0MzoxM1rOH3n72w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMTozODoyN1rOH-1gOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4ODAyNw==", "bodyText": "Could we split this -> https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/CompactionStrategy.java into IStrategy that should have common methods expected from all strategies such as buildMetrics, getPlanVersion..and then ScheduleClusteringStrategy can extend it ?\nAfter that, let's move the buildMetrics into a utils class given it's the same across both compaction and clustering right now ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528088027", "createdAt": "2020-11-21T06:43:13Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNzU4Mw==", "bodyText": "I feel like inheritance is wrong pattern here. These look similar now, but i think metrics will change differently over time. So having tight integration may make it difficult for these two evolve differently. Let me know if you have strong opinion. I can change it.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532817583", "createdAt": "2020-11-30T18:42:13Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4ODAyNw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxNzYyNQ==", "bodyText": "My general idea was to consolidate different kinds of strategies for operations such as compaction, clustering right now (tiering etc in the future). If you take a look, lots of code around TOTAL IO etc is coped over, any other way we can consolidate this without taking away some flexibility ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533617625", "createdAt": "2020-12-01T18:07:33Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4ODAyNw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MDM2Mg==", "bodyText": "I moved buildMetrics to utils class. If you have strong opinion on common interface I can add it.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535650362", "createdAt": "2020-12-03T21:38:27Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4ODAyNw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTMzNzI4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo0NzoyMVrOH3oIOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo0MjoyMFrOH8Im-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5MTE5Mw==", "bodyText": "nit : updateIndex -> update index, lets follow one pattern", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528091193", "createdAt": "2020-11-21T06:47:21Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "diffHunk": "@@ -27,8 +27,20 @@\n \n public abstract class AbstractBulkInsertHelper<T extends HoodieRecordPayload, I, K, O, R> {\n \n+  /**\n+   * Mark instant as inflight, write input records, updateIndex and return result.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNzY1OQ==", "bodyText": "Fixed", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532817659", "createdAt": "2020-11-30T18:42:20Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "diffHunk": "@@ -27,8 +27,20 @@\n \n public abstract class AbstractBulkInsertHelper<T extends HoodieRecordPayload, I, K, O, R> {\n \n+  /**\n+   * Mark instant as inflight, write input records, updateIndex and return result.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5MTE5Mw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTM2NDcyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1Mzo1NVrOH3oa5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo0MjozMVrOH8InYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5NTk3Mg==", "bodyText": "Should this be a REPLACE_ACTION instead of COMPACTION_ACTION ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528095972", "createdAt": "2020-11-21T06:53:55Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -296,6 +298,57 @@ protected void completeCompaction(HoodieCommitMetadata metadata, JavaRDD<WriteSt\n     return statuses;\n   }\n \n+  @Override\n+  protected HoodieWriteMetadata<JavaRDD<WriteStatus>> cluster(String clusteringInstant, boolean shouldComplete) {\n+    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n+    HoodieTimeline pendingClusteringTimeline = table.getActiveTimeline().filterPendingReplaceTimeline();\n+    HoodieInstant inflightInstant = HoodieTimeline.getReplaceCommitInflightInstant(clusteringInstant);\n+    if (pendingClusteringTimeline.containsInstant(inflightInstant)) {\n+      rollbackInflightClustering(inflightInstant, table);\n+      table.getMetaClient().reloadActiveTimeline();\n+    }\n+    clusteringTimer = metrics.getClusteringCtx();\n+    LOG.info(\"Starting clustering at \" + clusteringInstant);\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> clusteringMetadata = table.clustering(context, clusteringInstant);\n+    JavaRDD<WriteStatus> statuses = clusteringMetadata.getWriteStatuses();\n+    if (shouldComplete && clusteringMetadata.getCommitMetadata().isPresent()) {\n+      completeClustering((HoodieReplaceCommitMetadata) clusteringMetadata.getCommitMetadata().get(), statuses, table, clusteringInstant);\n+    }\n+    return clusteringMetadata;\n+  }\n+\n+  protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<WriteStatus> writeStatuses,\n+                                    HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n+                                    String clusteringCommitTime) {\n+\n+    List<HoodieWriteStat> writeStats = writeStatuses.map(WriteStatus::getStat).collect();\n+    if (!writeStatuses.filter(WriteStatus::hasErrors).isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering failed to write to files:\"\n+          + writeStatuses.filter(WriteStatus::hasErrors).map(WriteStatus::getFileId).collect());\n+    }\n+    finalizeWrite(table, clusteringCommitTime, writeStats);\n+    try {\n+      LOG.info(\"Committing Clustering \" + clusteringCommitTime + \". Finished with result \" + metadata);\n+      table.getActiveTimeline().transitionReplaceInflightToComplete(\n+          HoodieTimeline.getReplaceCommitInflightInstant(clusteringCommitTime),\n+          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n+    } catch (IOException e) {\n+      throw new HoodieClusteringException(\"unable to transition clustering inflight to complete: \" + clusteringCommitTime,  e);\n+    }\n+\n+    if (clusteringTimer != null) {\n+      long durationInMs = metrics.getDurationInMs(clusteringTimer.stop());\n+      try {\n+        metrics.updateCommitMetrics(HoodieActiveTimeline.COMMIT_FORMATTER.parse(clusteringCommitTime).getTime(),\n+            durationInMs, metadata, HoodieActiveTimeline.COMPACTION_ACTION);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNzc2MQ==", "bodyText": "Good catch, fixed.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532817761", "createdAt": "2020-11-30T18:42:31Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -296,6 +298,57 @@ protected void completeCompaction(HoodieCommitMetadata metadata, JavaRDD<WriteSt\n     return statuses;\n   }\n \n+  @Override\n+  protected HoodieWriteMetadata<JavaRDD<WriteStatus>> cluster(String clusteringInstant, boolean shouldComplete) {\n+    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n+    HoodieTimeline pendingClusteringTimeline = table.getActiveTimeline().filterPendingReplaceTimeline();\n+    HoodieInstant inflightInstant = HoodieTimeline.getReplaceCommitInflightInstant(clusteringInstant);\n+    if (pendingClusteringTimeline.containsInstant(inflightInstant)) {\n+      rollbackInflightClustering(inflightInstant, table);\n+      table.getMetaClient().reloadActiveTimeline();\n+    }\n+    clusteringTimer = metrics.getClusteringCtx();\n+    LOG.info(\"Starting clustering at \" + clusteringInstant);\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> clusteringMetadata = table.clustering(context, clusteringInstant);\n+    JavaRDD<WriteStatus> statuses = clusteringMetadata.getWriteStatuses();\n+    if (shouldComplete && clusteringMetadata.getCommitMetadata().isPresent()) {\n+      completeClustering((HoodieReplaceCommitMetadata) clusteringMetadata.getCommitMetadata().get(), statuses, table, clusteringInstant);\n+    }\n+    return clusteringMetadata;\n+  }\n+\n+  protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<WriteStatus> writeStatuses,\n+                                    HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n+                                    String clusteringCommitTime) {\n+\n+    List<HoodieWriteStat> writeStats = writeStatuses.map(WriteStatus::getStat).collect();\n+    if (!writeStatuses.filter(WriteStatus::hasErrors).isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering failed to write to files:\"\n+          + writeStatuses.filter(WriteStatus::hasErrors).map(WriteStatus::getFileId).collect());\n+    }\n+    finalizeWrite(table, clusteringCommitTime, writeStats);\n+    try {\n+      LOG.info(\"Committing Clustering \" + clusteringCommitTime + \". Finished with result \" + metadata);\n+      table.getActiveTimeline().transitionReplaceInflightToComplete(\n+          HoodieTimeline.getReplaceCommitInflightInstant(clusteringCommitTime),\n+          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n+    } catch (IOException e) {\n+      throw new HoodieClusteringException(\"unable to transition clustering inflight to complete: \" + clusteringCommitTime,  e);\n+    }\n+\n+    if (clusteringTimer != null) {\n+      long durationInMs = metrics.getDurationInMs(clusteringTimer.stop());\n+      try {\n+        metrics.updateCommitMetrics(HoodieActiveTimeline.COMMIT_FORMATTER.parse(clusteringCommitTime).getTime(),\n+            durationInMs, metadata, HoodieActiveTimeline.COMPACTION_ACTION);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5NTk3Mg=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTM2NjIwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/SparkBulkInsertBasedRunClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NDoyMFrOH3ocEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo0MjozOVrOH8InsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5NjI3NA==", "bodyText": "Why is this not under the strategy package ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528096274", "createdAt": "2020-11-21T06:54:20Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/SparkBulkInsertBasedRunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.run;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.execution.bulkinsert.RDDCustomColumnsSortPartitioner;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.SparkBulkInsertHelper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Uses bulk_insert to write data into new files.\n+ */\n+public class SparkBulkInsertBasedRunClusteringStrategy<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNzg0MA==", "bodyText": "Moved.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532817840", "createdAt": "2020-11-30T18:42:39Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/SparkBulkInsertBasedRunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.run;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.execution.bulkinsert.RDDCustomColumnsSortPartitioner;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.SparkBulkInsertHelper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Uses bulk_insert to write data into new files.\n+ */\n+public class SparkBulkInsertBasedRunClusteringStrategy<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5NjI3NA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTM3NTE1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/SparkBoundedDayBasedScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NjozMFrOH3oiNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo0Mjo0NlrOH8In8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5Nzg0Ng==", "bodyText": "same question, should this go under strategy package ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528097846", "createdAt": "2020-11-21T06:56:30Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNzkwNQ==", "bodyText": "Moved", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532817905", "createdAt": "2020-11-30T18:42:46Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5Nzg0Ng=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTM3OTAzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NzozMFrOH3olBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo0NDowMlrOH8Iqxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5ODU2NA==", "bodyText": "Could you explain this change please ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528098564", "createdAt": "2020-11-21T06:57:30Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java", "diffHunk": "@@ -34,14 +35,18 @@\n \n public class SparkLazyInsertIterable<T extends HoodieRecordPayload> extends HoodieLazyInsertIterable<T> {\n \n+  private boolean addMetadataFields;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxODYzMQ==", "bodyText": "With regular bulk insert, RDD only has user specified columns. With bulk insert based clustering, RDD also has hoodie internal columns. So, I am adding this flag to make it work for both cases. Let me know if you want me to reorganize this  differently.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532818631", "createdAt": "2020-11-30T18:44:02Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java", "diffHunk": "@@ -34,14 +35,18 @@\n \n public class SparkLazyInsertIterable<T extends HoodieRecordPayload> extends HoodieLazyInsertIterable<T> {\n \n+  private boolean addMetadataFields;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5ODU2NA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTM3OTQ2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NzozOVrOH3olUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMDoxNDoxM1rOH-vwiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5ODY0MQ==", "bodyText": "same", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528098641", "createdAt": "2020-11-21T06:57:39Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java", "diffHunk": "@@ -41,20 +41,22 @@\n   private HoodieWriteConfig config;\n   private HoodieTable hoodieTable;\n   private List<String> fileIDPrefixes;\n+  private boolean addMetadataFields;\n \n   public BulkInsertMapFunction(String instantTime, boolean areRecordsSorted,\n                                HoodieWriteConfig config, HoodieTable hoodieTable,\n-                               List<String> fileIDPrefixes) {\n+                               List<String> fileIDPrefixes, boolean addMetadataFields) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxODY3OQ==", "bodyText": "With regular bulk insert, RDD only has user specified columns. With bulk insert based clustering, RDD also has hoodie internal columns. So, I am adding this flag to make it work for both cases. Let me know if you want me to reorganize this  differently.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532818679", "createdAt": "2020-11-30T18:44:07Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java", "diffHunk": "@@ -41,20 +41,22 @@\n   private HoodieWriteConfig config;\n   private HoodieTable hoodieTable;\n   private List<String> fileIDPrefixes;\n+  private boolean addMetadataFields;\n \n   public BulkInsertMapFunction(String instantTime, boolean areRecordsSorted,\n                                HoodieWriteConfig config, HoodieTable hoodieTable,\n-                               List<String> fileIDPrefixes) {\n+                               List<String> fileIDPrefixes, boolean addMetadataFields) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5ODY0MQ=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1ODgwNA==", "bodyText": "This problem exists today with the MOR tables when inserts go into log files. For this reason, we have a different constructor for the create handle that is called from the compaction code path -> \n  \n    \n      hudi/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieCreateHandle.java\n    \n    \n         Line 93\n      in\n      36ce5bc\n    \n    \n    \n    \n\n        \n          \n           public HoodieCreateHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T, I, K, O> hoodieTable, \n        \n    \n  \n\n. Can you try to do the same for clustering and avoid passing this flag here ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533658804", "createdAt": "2020-12-01T19:15:10Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java", "diffHunk": "@@ -41,20 +41,22 @@\n   private HoodieWriteConfig config;\n   private HoodieTable hoodieTable;\n   private List<String> fileIDPrefixes;\n+  private boolean addMetadataFields;\n \n   public BulkInsertMapFunction(String instantTime, boolean areRecordsSorted,\n                                HoodieWriteConfig config, HoodieTable hoodieTable,\n-                               List<String> fileIDPrefixes) {\n+                               List<String> fileIDPrefixes, boolean addMetadataFields) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5ODY0MQ=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU1NjIzMw==", "bodyText": "We need this in transform function below (We dont use HoodieCreateHandle here). I changed the name to follow same pattern as HoodieCreateHandle to make it more clear. Let me know if this is clear.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535556233", "createdAt": "2020-12-03T20:14:13Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java", "diffHunk": "@@ -41,20 +41,22 @@\n   private HoodieWriteConfig config;\n   private HoodieTable hoodieTable;\n   private List<String> fileIDPrefixes;\n+  private boolean addMetadataFields;\n \n   public BulkInsertMapFunction(String instantTime, boolean areRecordsSorted,\n                                HoodieWriteConfig config, HoodieTable hoodieTable,\n-                               List<String> fileIDPrefixes) {\n+                               List<String> fileIDPrefixes, boolean addMetadataFields) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5ODY0MQ=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTQyNjM1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzowOTo0MFrOH3pGug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo0NDo0NlrOH8IsVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODEwNzE5NA==", "bodyText": "This runs the clustering groups in sequence, is that what is expected ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528107194", "createdAt": "2020-11-21T07:09:40Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxOTAyOQ==", "bodyText": "I changed it to parallelize using java thread pool. PTAL.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532819029", "createdAt": "2020-11-30T18:44:46Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODEwNzE5NA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTQzNDUzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxMTo0OFrOH3pMqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxODowOTo0OVrOH85hCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODEwODcxMw==", "bodyText": "Why do we need this reload ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528108713", "createdAt": "2020-11-21T07:11:48Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxOTg5Mw==", "bodyText": "After this step, we convert inflight to commit. Without this reload, 'inflight' doesnt exist in table.getActiveTimeline(). So commit doesnt succeed. We seem to be doing similar step for compaction too.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532819893", "createdAt": "2020-11-30T18:46:08Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODEwODcxMw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYxODk1NQ==", "bodyText": "Okay", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533618955", "createdAt": "2020-12-01T18:09:49Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODEwODcxMw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTQ0MTI3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxMzo0OFrOH3pRsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo0NjoxNVrOH8Iv7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExMDAwMA==", "bodyText": "Java doc please", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528110000", "createdAt": "2020-11-21T07:13:48Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxOTk1MA==", "bodyText": "Added", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532819950", "createdAt": "2020-11-30T18:46:15Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExMDAwMA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTQ1MjgwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkScheduleClusteringActionExecutor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxNjo0MVrOH3pZ8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxODoxNzoxMVrOH85ycQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExMjExMw==", "bodyText": "At a high level, what is the use-case when we would need to cluster every N commits ? For compaction it makes sense..", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528112113", "createdAt": "2020-11-21T07:16:41Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkScheduleClusteringActionExecutor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.cluster.strategy.ScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class SparkScheduleClusteringActionExecutor<T extends HoodieRecordPayload> extends\n+    BaseScheduleClusteringActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkScheduleClusteringActionExecutor.class);\n+\n+  public SparkScheduleClusteringActionExecutor(HoodieEngineContext context,\n+                                               HoodieWriteConfig config,\n+                                               HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n+                                               String instantTime,\n+                                               Option<Map<String, String>> extraMetadata) {\n+    super(context, config, table, instantTime, extraMetadata);\n+  }\n+\n+  @Override\n+  protected Option<HoodieClusteringPlan> scheduleClustering() {\n+    LOG.info(\"Checking if clustering needs to be run on \" + config.getBasePath());\n+    Option<HoodieInstant> lastClusteringInstant = table.getActiveTimeline().getCompletedReplaceTimeline().lastInstant();\n+\n+    int commitsSinceLastClustering = table.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .findInstantsAfter(lastClusteringInstant.map(HoodieInstant::getTimestamp).orElse(\"0\"), Integer.MAX_VALUE)\n+        .countInstants();\n+    if (config.getInlineClusterMaxCommits() > commitsSinceLastClustering) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyMTM0OQ==", "bodyText": "I think this still makes sense for clustering too. Imagine, we are writing new data every 5minutes. It doesnt make sense to do file listing and figure out if theres enough data for clustering.  User knows there is enough data after N minutes, so they can select N/5 commits here.\nWe can also potentially move this to strategy specific implementation. But I do think most strategies will use this. Let me know if you want me to move this into strategy.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532821349", "createdAt": "2020-11-30T18:48:35Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkScheduleClusteringActionExecutor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.cluster.strategy.ScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class SparkScheduleClusteringActionExecutor<T extends HoodieRecordPayload> extends\n+    BaseScheduleClusteringActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkScheduleClusteringActionExecutor.class);\n+\n+  public SparkScheduleClusteringActionExecutor(HoodieEngineContext context,\n+                                               HoodieWriteConfig config,\n+                                               HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n+                                               String instantTime,\n+                                               Option<Map<String, String>> extraMetadata) {\n+    super(context, config, table, instantTime, extraMetadata);\n+  }\n+\n+  @Override\n+  protected Option<HoodieClusteringPlan> scheduleClustering() {\n+    LOG.info(\"Checking if clustering needs to be run on \" + config.getBasePath());\n+    Option<HoodieInstant> lastClusteringInstant = table.getActiveTimeline().getCompletedReplaceTimeline().lastInstant();\n+\n+    int commitsSinceLastClustering = table.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .findInstantsAfter(lastClusteringInstant.map(HoodieInstant::getTimestamp).orElse(\"0\"), Integer.MAX_VALUE)\n+        .countInstants();\n+    if (config.getInlineClusterMaxCommits() > commitsSinceLastClustering) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExMjExMw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyMzQwOQ==", "bodyText": "I think this is one more config for people to manage. Clustering use-case spans across many different angles, compacting small files into larger being just one of them. For compaction, since there aren't other use-cases, every N commit makes total sense. But the other side is to keep the same behaviors consistent across compaction and clustering so keeping this would provide users the same knobs.\nI'll let @vinothchandar take a call on this.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533623409", "createdAt": "2020-12-01T18:17:11Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkScheduleClusteringActionExecutor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.cluster.strategy.ScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class SparkScheduleClusteringActionExecutor<T extends HoodieRecordPayload> extends\n+    BaseScheduleClusteringActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkScheduleClusteringActionExecutor.class);\n+\n+  public SparkScheduleClusteringActionExecutor(HoodieEngineContext context,\n+                                               HoodieWriteConfig config,\n+                                               HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n+                                               String instantTime,\n+                                               Option<Map<String, String>> extraMetadata) {\n+    super(context, config, table, instantTime, extraMetadata);\n+  }\n+\n+  @Override\n+  protected Option<HoodieClusteringPlan> scheduleClustering() {\n+    LOG.info(\"Checking if clustering needs to be run on \" + config.getBasePath());\n+    Option<HoodieInstant> lastClusteringInstant = table.getActiveTimeline().getCompletedReplaceTimeline().lastInstant();\n+\n+    int commitsSinceLastClustering = table.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .findInstantsAfter(lastClusteringInstant.map(HoodieInstant::getTimestamp).orElse(\"0\"), Integer.MAX_VALUE)\n+        .countInstants();\n+    if (config.getInlineClusterMaxCommits() > commitsSinceLastClustering) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExMjExMw=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTQ3MTQxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoyMTozMlrOH3pnkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMDoxNToyM1rOH-v1YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExNTYwMA==", "bodyText": "So if there are N clustering groups consisting of M file groups that are being converted into O file groups, we will have N different RDD's. There are some performance limitation of unioning multiple different RDDs from what I remember. Can you run a simple test a) take 10000 records, parallelize them with jsc to create 1 RDD and then write them out to a file b) take 10000 records, create 1 RDD for each record and then union them and write them out to a single file ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528115600", "createdAt": "2020-11-21T07:21:32Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroup(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .reduce((rdd1, rdd2) -> engineContext.union(rdd1, rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatuses.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+    // merge all write status\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatuses);\n+    updateIndexAndCommitIfNeeded(writeStatuses, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatuses.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private JavaRDD<WriteStatus> runClusteringForGroup(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyNDIzNQ==", "bodyText": "Is there an alternative to union? Should we collect individual RDD and merge lists? I did testing only with 2 groups so far. So, I didn't see big performance degradation. I'll look into testing large number of groups as separate task. (We may also don't want number of groups to be really high as we want clustering to be atomic operation. Doing large amount of data/groups can increase chance of failures and slow down entire process.)", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532824235", "createdAt": "2020-11-30T18:53:32Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroup(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .reduce((rdd1, rdd2) -> engineContext.union(rdd1, rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatuses.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+    // merge all write status\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatuses);\n+    updateIndexAndCommitIfNeeded(writeStatuses, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatuses.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private JavaRDD<WriteStatus> runClusteringForGroup(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExNTYwMA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyNjQ3OQ==", "bodyText": "Yes, like we discussed offline, we should have a config to limit the number of clustering groups to avoid someone setting wrong configs for things like maxDataPerGroup and then result in a large number of RDDs to be unioned. I think we should still do a quick test to see if there is any degradation due to union before we land this diff so we are aware. It should be as simple as writing a test case that loops over code\nTestUnionPerformance\ntest for 10000 records\nlong startTime = System.currentMillis();\nRDD rdd = jsc.parallelize(Arrays.asList(record))\nfor (record in records)\n{\nrdd.union(jsc.parallelize(Arrays.asList(record)))\n}\nrdd.collect();\nwriteClient.bulkInsert(rdd);\nsout(\"time taken with union\" + System.currentMillis - starttime)\nlong startTime = System.currentMillis();\nRDD rdd = jsc.parallelize(records)\nrdd.collect();\nwriteClient.bulkInsert(rdd);\nsout(\"time taken without union\" + System.currentMillis - starttime)", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533626479", "createdAt": "2020-12-01T18:22:00Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroup(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .reduce((rdd1, rdd2) -> engineContext.union(rdd1, rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatuses.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+    // merge all write status\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatuses);\n+    updateIndexAndCommitIfNeeded(writeStatuses, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatuses.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private JavaRDD<WriteStatus> runClusteringForGroup(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExNTYwMA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU1NzQ3Mg==", "bodyText": "Ok, Changed to limit max groups to 50 (thats where we see significant degradation). I only applied for this strategy (because reduce is inside strategy code).", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535557472", "createdAt": "2020-12-03T20:15:23Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroup(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .reduce((rdd1, rdd2) -> engineContext.union(rdd1, rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatuses.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+    // merge all write status\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatuses);\n+    updateIndexAndCommitIfNeeded(writeStatuses, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatuses.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private JavaRDD<WriteStatus> runClusteringForGroup(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExNTYwMA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTQ3NjE4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoyMjo0OVrOH3prJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOTowMTo1NVrOH8JUoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExNjUxNg==", "bodyText": "Do we need this level of file readers in this part of the code ? Can we just call compactor.compact(context, compactionPlan, table, config, instantTime) ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528116516", "createdAt": "2020-11-21T07:22:49Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroup(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .reduce((rdd1, rdd2) -> engineContext.union(rdd1, rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatuses.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+    // merge all write status\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatuses);\n+    updateIndexAndCommitIfNeeded(writeStatuses, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatuses.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private JavaRDD<WriteStatus> runClusteringForGroup(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    JavaRDD<HoodieRecord<? extends HoodieRecordPayload>> inputRecords = jsc.parallelize(clusteringGroup.getSlices(), clusteringGroup.getSlices().size()).map(sliceInfo -> {\n+      long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n+      LOG.info(\"MaxMemoryPerCompaction run as part of clustering => \" + maxMemoryPerCompaction);\n+      try {\n+        Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n+        HoodieFileReader<? extends IndexedRecord> baseFileReader = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), new Path(sliceInfo.getDataFilePath()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyOTM0NQ==", "bodyText": "compactor.compact seems to write data into new files and returns WriteStatus. We then have to read data and use it for clustering. I think this additional is unnecessary.  Also, creating compactionplan for clustering looks like tight coupling to me. This is only few lines of code and abstractions look reasonable to me. Please take a look and let me know if you have any suggestions.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532829345", "createdAt": "2020-11-30T19:01:55Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroup(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .reduce((rdd1, rdd2) -> engineContext.union(rdd1, rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatuses.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+    // merge all write status\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatuses);\n+    updateIndexAndCommitIfNeeded(writeStatuses, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatuses.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private JavaRDD<WriteStatus> runClusteringForGroup(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    JavaRDD<HoodieRecord<? extends HoodieRecordPayload>> inputRecords = jsc.parallelize(clusteringGroup.getSlices(), clusteringGroup.getSlices().size()).map(sliceInfo -> {\n+      long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n+      LOG.info(\"MaxMemoryPerCompaction run as part of clustering => \" + maxMemoryPerCompaction);\n+      try {\n+        Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n+        HoodieFileReader<? extends IndexedRecord> baseFileReader = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), new Path(sliceInfo.getDataFilePath()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExNjUxNg=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjA5MTg5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNToxNjowNVrOH3vMvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMDowMDo0NlrOIIGkOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzAzOA==", "bodyText": "hello , i am interesting  about the scenario to limit the partition num of clustering. does cluster all partitions have some problem?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528207038", "createdAt": "2020-11-21T15:16:05Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzOTUyMQ==", "bodyText": "I think we do file listing per partition in the first strategy implementation. So if you have lot of partitions, that can cause some degradation. I reached out on slack to help with testing. I sent you commands to run inline clustering. Will you be able to test this PR by setting this config to a large value?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532839521", "createdAt": "2020-11-30T19:19:24Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzAzOA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM1NTEwMw==", "bodyText": "\"So if you have lot of partitions, that can cause some degradation\" will resolved in file listing  https://issues.apache.org/jira/browse/HUDI-1292?\ni am happy to test this scenario", "url": "https://github.com/apache/hudi/pull/2263#discussion_r539355103", "createdAt": "2020-12-09T14:32:55Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzAzOA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTkwODg1OQ==", "bodyText": "Yes, that ticket can resolve issues to some extent. But, in some cases, there are large day based tables with 1000s of partitions. we probably dont want to scan all partitions in those cases. So i think its useful to have this config.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r539908859", "createdAt": "2020-12-10T06:52:14Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzAzOA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM2NzA5Ng==", "bodyText": "@lw309637554 let me know if you got a chance to test this scenario (or clustering in general)", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545367096", "createdAt": "2020-12-17T20:00:46Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzAzOA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjA5ODc1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNToyNDozNlrOH3vP8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOToxOTo0N1rOH8J9fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzg1OA==", "bodyText": "The implement of buildClusteringGroupsForPartition method in SparkBoundedDayBasedScheduleClusteringStrategy. Get params about clustering from maxDataInGroup\u3001getWriteConfig().getClusteringTargetFileSize()\u3001getWriteConfig().getClusteringTargetFileSize(). Maybe other implement will need more params . Can we just abstract buildClusteringGroupsForPartition with the HoodieWriteConfig for param ?  maybe like :\nprotected abstract Stream buildClusteringGroupsForPartition(String partitionPath,\nList fileSlices,\nHoodieWriteConfig writeConfig);", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528207858", "createdAt": "2020-11-21T15:24:36Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices,\n+                                                                                     long maxDataInGroup);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgzOTgwNA==", "bodyText": "Sure. Changed it to keep it more flexible.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532839804", "createdAt": "2020-11-30T19:19:47Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices,\n+                                                                                     long maxDataInGroup);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzg1OA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjEwNDQ2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/SparkBulkInsertBasedRunClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNTozMDo0OFrOH3vShA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOToyMDo0NVrOH8J_3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwODUxNg==", "bodyText": "if use HoodieClusteringConfig.SORT_COLUMNS_PROPERTY in code  instead of import static better?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528208516", "createdAt": "2020-11-21T15:30:48Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/SparkBulkInsertBasedRunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.run;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.execution.bulkinsert.RDDCustomColumnsSortPartitioner;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.SparkBulkInsertHelper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg0MDQxNA==", "bodyText": "for constants, we seem to be using static imports in many places. So I keep as is. Let me know if you have strong reason to avoid static imports.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532840414", "createdAt": "2020-11-30T19:20:45Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/SparkBulkInsertBasedRunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.run;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.execution.bulkinsert.RDDCustomColumnsSortPartitioner;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.SparkBulkInsertHelper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwODUxNg=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjcwNzMyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMlQwNDowNzoyMlrOH3zlhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOToyMjowOVrOH8KDGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODI3ODkxNg==", "bodyText": "if we should  constraint the columns start of partitionpath and row_key. Or just auto fill the two columns? Because the records of same partition and row_key need in one file slice.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528278916", "createdAt": "2020-11-22T04:07:22Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg0MTI0MQ==", "bodyText": "I dont think we want to add this constraint. There are use cases where we want to sort data by other columns.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532841241", "createdAt": "2020-11-30T19:22:09Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODI3ODkxNg=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMzA5MjkzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMlQxMTo1OTowM1rOH32ZUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOToyMjoyNVrOH8KDvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODMyNDk0NA==", "bodyText": "can we get pendingClusterFileGroupid  from getHoodieTable().getSliceView() just like getPendingCompactionOperations?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528324944", "createdAt": "2020-11-22T11:59:03Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering\n+   */\n+  protected List<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = ((SyncableFileSystemView) getHoodieTable().getSliceView()).getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(ClusteringUtils.getAllFileGroupsInPendingClusteringPlans(getHoodieTable().getMetaClient()).keySet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg0MTQwNA==", "bodyText": "Good catch. I changed it to get this data from view.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r532841404", "createdAt": "2020-11-30T19:22:25Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering\n+   */\n+  protected List<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = ((SyncableFileSystemView) getHoodieTable().getSliceView()).getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(ClusteringUtils.getAllFileGroupsInPendingClusteringPlans(getHoodieTable().getMetaClient()).keySet());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODMyNDk0NA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0ODA4NjU1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOTowNjoyNVrOH87pqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMToyODoxNlrOH-07CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MzkzMA==", "bodyText": "I have a suggestion to change the way we are generating the sort cols. Instead of performing this operation during write where the following code will spend cycles to convert data to generic record again, can we do this during read time when the HoodieRecord is being constructed ? I think you are using HoodieFileSliceReader and HoodieMergedRecordScanner to do this, does it make sense to add a sortKey to HoodieRecord and when you are constructing the hoodie record where you anyways have a handle to the generic record, you can just set that value in the hoodie record ? This will also help avoid passing schemas around for partitioner..\nThe downside of this approach is that the RDD bloats up so the shuffle will be larger so the question is whether that's worse or converting to generic record is worse from a CPU perspective", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533653930", "createdAt": "2020-12-01T19:06:25Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTYwMjczMA==", "bodyText": "I like this suggestion. This 'sortColumn' is probably needed only for this 'BulkInsert' based clustering strategy.  So, just want to make sure its ok to add new fields in HoodieRecord even though this is needed only for one usecase.  If you think thats acceptable, I'll make the change. Let me know.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535602730", "createdAt": "2020-12-03T20:55:57Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MzkzMA=="}, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTYzNDY0Mw==", "bodyText": "@satishkotha Do we know the overhead of converting schemaString to schema and generic record ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535634643", "createdAt": "2020-12-03T21:22:04Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MzkzMA=="}, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY0MDg0MQ==", "bodyText": "@n3nash I dont have numbers for this. I'll add metrics and calculate this. If you have any details from past examples, let me know", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535640841", "createdAt": "2020-12-03T21:28:16Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MzkzMA=="}, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0ODA5OTAwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOToxMDowMVrOH87xdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMTo0MDo1NVrOH-1p_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1NTkyNw==", "bodyText": "The merged record scanner will try to keep an in-memory map, can we optimize for cases when there are no log files ? Or am I missing where this is done for COW / MOR where we are only reading parquet files ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533655927", "createdAt": "2020-12-01T19:10:01Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // run clustering for each group async and collect WriteStatus\n+    JavaRDD<WriteStatus> writeStatusRDD = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroupAsync(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .map(CompletableFuture::join)\n+        .reduce((rdd1, rdd2) -> rdd1.union(rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatusRDD.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatusRDD);\n+    updateIndexAndCommitIfNeeded(writeStatusRDD, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatusRDD.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private CompletableFuture<JavaRDD<WriteStatus>> runClusteringForGroupAsync(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {\n+    CompletableFuture<JavaRDD<WriteStatus>> writeStatusesFuture = CompletableFuture.supplyAsync(() -> {\n+      JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+      JavaRDD<HoodieRecord<? extends HoodieRecordPayload>> inputRecords = jsc.parallelize(clusteringGroup.getSlices(), clusteringGroup.getSlices().size()).map(sliceInfo -> {\n+        long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n+        LOG.info(\"MaxMemoryPerCompaction run as part of clustering => \" + maxMemoryPerCompaction);\n+        try {\n+          Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n+          HoodieFileReader<? extends IndexedRecord> baseFileReader = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), new Path(sliceInfo.getDataFilePath()));\n+          HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(table.getMetaClient().getFs(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTU2MTE5OA==", "bodyText": "I initially had implemented separate for COW/MOR. But it did not result in significant difference in performance in terms of run time. Memory utilization is definitely more with log files. But since we have memory limit configuration, to keep it simple, I just added one path that works for both COW/MOR.\nI brought back earlier implementation. Take a look. If you think thats not complex, we can keep both.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535561198", "createdAt": "2020-12-03T20:18:47Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // run clustering for each group async and collect WriteStatus\n+    JavaRDD<WriteStatus> writeStatusRDD = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroupAsync(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .map(CompletableFuture::join)\n+        .reduce((rdd1, rdd2) -> rdd1.union(rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatusRDD.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatusRDD);\n+    updateIndexAndCommitIfNeeded(writeStatusRDD, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatusRDD.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private CompletableFuture<JavaRDD<WriteStatus>> runClusteringForGroupAsync(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {\n+    CompletableFuture<JavaRDD<WriteStatus>> writeStatusesFuture = CompletableFuture.supplyAsync(() -> {\n+      JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+      JavaRDD<HoodieRecord<? extends HoodieRecordPayload>> inputRecords = jsc.parallelize(clusteringGroup.getSlices(), clusteringGroup.getSlices().size()).map(sliceInfo -> {\n+        long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n+        LOG.info(\"MaxMemoryPerCompaction run as part of clustering => \" + maxMemoryPerCompaction);\n+        try {\n+          Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n+          HoodieFileReader<? extends IndexedRecord> baseFileReader = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), new Path(sliceInfo.getDataFilePath()));\n+          HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(table.getMetaClient().getFs(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1NTkyNw=="}, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1Mjg2Mg==", "bodyText": "@satishkotha I see the new one looks messy, if the runtime/difference in performance is the same for MergedScanner and no memory overhead of FileSlices without any log files, let's keep your other implementation with MergedScanner for both. Added 1 comment on the FileSliceReader", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535652862", "createdAt": "2020-12-03T21:40:55Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // run clustering for each group async and collect WriteStatus\n+    JavaRDD<WriteStatus> writeStatusRDD = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroupAsync(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .map(CompletableFuture::join)\n+        .reduce((rdd1, rdd2) -> rdd1.union(rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatusRDD.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatusRDD);\n+    updateIndexAndCommitIfNeeded(writeStatusRDD, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatusRDD.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private CompletableFuture<JavaRDD<WriteStatus>> runClusteringForGroupAsync(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {\n+    CompletableFuture<JavaRDD<WriteStatus>> writeStatusesFuture = CompletableFuture.supplyAsync(() -> {\n+      JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+      JavaRDD<HoodieRecord<? extends HoodieRecordPayload>> inputRecords = jsc.parallelize(clusteringGroup.getSlices(), clusteringGroup.getSlices().size()).map(sliceInfo -> {\n+        long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n+        LOG.info(\"MaxMemoryPerCompaction run as part of clustering => \" + maxMemoryPerCompaction);\n+        try {\n+          Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n+          HoodieFileReader<? extends IndexedRecord> baseFileReader = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), new Path(sliceInfo.getDataFilePath()));\n+          HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(table.getMetaClient().getFs(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1NTkyNw=="}, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM2MDcxNDY3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMTozOTozOFrOH-1k1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjo0NTozOVrOII04Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ==", "bodyText": "Should we make this AbstractLogRecordScanner to allow for UnMergedScanner as well ? @satishkotha", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535651541", "createdAt": "2020-12-03T21:39:38Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY2ODU1NQ==", "bodyText": "UnmergedScanner doesnt seem to have a way to get record iterator. This requires lot more refactoring if we want to make it work for both.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535668555", "createdAt": "2020-12-03T21:55:52Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ=="}, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTcyMzc0Mg==", "bodyText": "Hmm, okay, in that case, can we keep the method signature of FileSliceReader as iterator rather than scanner so that way the FileSliceReader is generic and then we can figure out how to make those record scanner changes later ?\n@satishkotha", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535723742", "createdAt": "2020-12-03T23:32:33Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ=="}, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkzNDg2Mg==", "bodyText": "We also need process method in LogRecordScanner. So I kept this 'factory' method as is. But changed HoodieFileSliceReader constructor to take records. We can implement another factory method for UnmergedScanner. PTAL.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r538934862", "createdAt": "2020-12-09T01:28:20Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ=="}, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTAxNjI0Mw==", "bodyText": "@satishkotha We cannot use the scanner.getRecords() API, that will bring all the records into memory and cause the job to OOM. Let's just continue to return the iterator and then lazily fetch records as you iterate.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r539016243", "createdAt": "2020-12-09T05:28:26Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ=="}, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA0MTM5Ng==", "bodyText": "@n3nash getRecords just retruns 'ExternalSpillableMap'. So i dont think it can cause OOM. Anyway, changed it to iterator to keep it similar to reading records from base files.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r539041396", "createdAt": "2020-12-09T06:25:03Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ=="}, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyNTg2Nw==", "bodyText": "Nonetheless we should file a code cleanup JIRA to provide these iterators as core building blocks under a nice abstractions.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546125867", "createdAt": "2020-12-18T22:45:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ=="}, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDIwNzEzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzoxMDo1NlrOIHnkxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0MDoyOFrOIIR7mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1OTMzNA==", "bodyText": "lets please add java docs for all these configs?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r544859334", "createdAt": "2020-12-17T07:10:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String ASYNC_CLUSTERING_ENABLED = \"hoodie.clustering.enabled\";\n+  public static final String DEFAULT_ASYNC_CLUSTERING_ENABLED = \"false\";\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.strategy.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits an inline clustering will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1MzMwNQ==", "bodyText": "Added", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553305", "createdAt": "2020-12-18T03:40:28Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String ASYNC_CLUSTERING_ENABLED = \"hoodie.clustering.enabled\";\n+  public static final String DEFAULT_ASYNC_CLUSTERING_ENABLED = \"false\";\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.strategy.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits an inline clustering will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1OTMzNA=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDIyODY0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzoxODowMlrOIHnwnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0MDozNlrOIIR71A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2MjM2Nw==", "bodyText": "make this more readable, by putting each param on a line like before?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r544862367", "createdAt": "2020-12-17T07:18:02Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "diffHunk": "@@ -59,25 +58,39 @@ public static SparkBulkInsertHelper newInstance() {\n   }\n \n   @Override\n-  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,\n-                                                              String instantTime,\n-                                                              HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                                              HoodieWriteConfig config,\n-                                                              BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,\n-                                                              boolean performDedupe,\n-                                                              Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<HoodieRecord<T>> inputRecords, final String instantTime, final HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table, final HoodieWriteConfig config, final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor, final boolean performDedupe, final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1MzM2NA==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553364", "createdAt": "2020-12-18T03:40:36Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "diffHunk": "@@ -59,25 +58,39 @@ public static SparkBulkInsertHelper newInstance() {\n   }\n \n   @Override\n-  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,\n-                                                              String instantTime,\n-                                                              HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                                              HoodieWriteConfig config,\n-                                                              BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,\n-                                                              boolean performDedupe,\n-                                                              Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<HoodieRecord<T>> inputRecords, final String instantTime, final HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table, final HoodieWriteConfig config, final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor, final boolean performDedupe, final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2MjM2Nw=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQyMDA2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjowMjoxM1rOIHybWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0MjoxOFrOIIR9dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNzE0NQ==", "bodyText": "should this be called replaceTimer as well?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545037145", "createdAt": "2020-12-17T12:02:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java", "diffHunk": "@@ -48,6 +49,7 @@\n   private Timer deltaCommitTimer = null;\n   private Timer finalizeTimer = null;\n   private Timer compactionTimer = null;\n+  private Timer clusteringTimer = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1Mzc4Mg==", "bodyText": "This is specific to clustering operation and only used in WriteClient#cluster.\nWe could have a different timer for insertOverwrite and replace commands if needed. I like this approach because replace can mean multiple things. Let me know if you think common timer makes more sense.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553782", "createdAt": "2020-12-18T03:42:18Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java", "diffHunk": "@@ -48,6 +49,7 @@\n   private Timer deltaCommitTimer = null;\n   private Timer finalizeTimer = null;\n   private Timer compactionTimer = null;\n+  private Timer clusteringTimer = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNzE0NQ=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ3NTE0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNTo1MVrOIHy5_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0MjoyNlrOIIR9jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDk5MA==", "bodyText": "I think we should have 50 as the default value for this config and allow any value to passed in, as opposed to having this limit hard-coded.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545044990", "createdAt": "2020-12-17T12:15:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+  // With more than 50 groups, we see performance degradation with this Strategy implementation.\n+  private static final int MAX_CLUSTERING_GROUPS_STRATEGY = 50;\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices);\n+\n+  /**\n+   * Return list of partition paths to be considered for clustering.\n+   */\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    return partitionPaths;\n+  }\n+\n+  @Override\n+  public Option<HoodieClusteringPlan> generateClusteringPlan() {\n+    try {\n+      HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();\n+      LOG.info(\"Scheduling clustering for \" + metaClient.getBasePath());\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),\n+          getWriteConfig().shouldAssumeDatePartitioning());\n+\n+      // filter the partition paths if needed to reduce list status\n+      partitionPaths = filterPartitionPaths(partitionPaths);\n+\n+      if (partitionPaths.isEmpty()) {\n+        // In case no partitions could be picked, return no clustering plan\n+        return Option.empty();\n+      }\n+\n+      long maxClusteringGroups = getWriteConfig().getClusteringMaxNumGroups();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1MzgwNw==", "bodyText": "Fixed.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553807", "createdAt": "2020-12-18T03:42:26Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+  // With more than 50 groups, we see performance degradation with this Strategy implementation.\n+  private static final int MAX_CLUSTERING_GROUPS_STRATEGY = 50;\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices);\n+\n+  /**\n+   * Return list of partition paths to be considered for clustering.\n+   */\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    return partitionPaths;\n+  }\n+\n+  @Override\n+  public Option<HoodieClusteringPlan> generateClusteringPlan() {\n+    try {\n+      HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();\n+      LOG.info(\"Scheduling clustering for \" + metaClient.getBasePath());\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),\n+          getWriteConfig().shouldAssumeDatePartitioning());\n+\n+      // filter the partition paths if needed to reduce list status\n+      partitionPaths = filterPartitionPaths(partitionPaths);\n+\n+      if (partitionPaths.isEmpty()) {\n+        // In case no partitions could be picked, return no clustering plan\n+        return Option.empty();\n+      }\n+\n+      long maxClusteringGroups = getWriteConfig().getClusteringMaxNumGroups();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDk5MA=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTQ4MjA1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjoxNzozNFrOIHy92A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0MjozNFrOIIR9ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NTk3Ng==", "bodyText": "why not try to do this in parallel using context.map() etc? it should improve performance as well", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545045976", "createdAt": "2020-12-17T12:17:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+  // With more than 50 groups, we see performance degradation with this Strategy implementation.\n+  private static final int MAX_CLUSTERING_GROUPS_STRATEGY = 50;\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices);\n+\n+  /**\n+   * Return list of partition paths to be considered for clustering.\n+   */\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    return partitionPaths;\n+  }\n+\n+  @Override\n+  public Option<HoodieClusteringPlan> generateClusteringPlan() {\n+    try {\n+      HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();\n+      LOG.info(\"Scheduling clustering for \" + metaClient.getBasePath());\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),\n+          getWriteConfig().shouldAssumeDatePartitioning());\n+\n+      // filter the partition paths if needed to reduce list status\n+      partitionPaths = filterPartitionPaths(partitionPaths);\n+\n+      if (partitionPaths.isEmpty()) {\n+        // In case no partitions could be picked, return no clustering plan\n+        return Option.empty();\n+      }\n+\n+      long maxClusteringGroups = getWriteConfig().getClusteringMaxNumGroups();\n+      if (maxClusteringGroups > MAX_CLUSTERING_GROUPS_STRATEGY) {\n+        LOG.warn(\"Reducing max clustering groups to \" + MAX_CLUSTERING_GROUPS_STRATEGY + \" for performance reasons\");\n+        maxClusteringGroups = MAX_CLUSTERING_GROUPS_STRATEGY;\n+      }\n+\n+      List<HoodieClusteringGroup> clusteringGroups = partitionPaths.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1Mzg1OQ==", "bodyText": "Updated.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545553859", "createdAt": "2020-12-18T03:42:34Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+  // With more than 50 groups, we see performance degradation with this Strategy implementation.\n+  private static final int MAX_CLUSTERING_GROUPS_STRATEGY = 50;\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices);\n+\n+  /**\n+   * Return list of partition paths to be considered for clustering.\n+   */\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    return partitionPaths;\n+  }\n+\n+  @Override\n+  public Option<HoodieClusteringPlan> generateClusteringPlan() {\n+    try {\n+      HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();\n+      LOG.info(\"Scheduling clustering for \" + metaClient.getBasePath());\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),\n+          getWriteConfig().shouldAssumeDatePartitioning());\n+\n+      // filter the partition paths if needed to reduce list status\n+      partitionPaths = filterPartitionPaths(partitionPaths);\n+\n+      if (partitionPaths.isEmpty()) {\n+        // In case no partitions could be picked, return no clustering plan\n+        return Option.empty();\n+      }\n+\n+      long maxClusteringGroups = getWriteConfig().getClusteringMaxNumGroups();\n+      if (maxClusteringGroups > MAX_CLUSTERING_GROUPS_STRATEGY) {\n+        LOG.warn(\"Reducing max clustering groups to \" + MAX_CLUSTERING_GROUPS_STRATEGY + \" for performance reasons\");\n+        maxClusteringGroups = MAX_CLUSTERING_GROUPS_STRATEGY;\n+      }\n+\n+      List<HoodieClusteringGroup> clusteringGroups = partitionPaths.stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NTk3Ng=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTU0Mjg3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjozMjoyNlrOIHzgJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0MzoxOVrOIIR-ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NDc1OA==", "bodyText": "is this like a group number? Seems more like the number of output files produced by that group", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545054758", "createdAt": "2020-12-17T12:32:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareScheduleClusteringStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedScheduleClusteringStrategy.class);\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream().map(partition -> partition.replace(\"/\", \"-\"))\n+        .sorted(Comparator.reverseOrder()).map(partitionPath -> partitionPath.replace(\"-\", \"/\"))\n+        .limit(targetPartitionsForClustering > 0 ? targetPartitionsForClustering : partitionPaths.size())\n+        .collect(Collectors.toList());\n+  }\n+\n+  private int getNumberOfGroups(long groupSize, long targetFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDA0Mw==", "bodyText": "yes, this is calculating number of FileGroups. I updated the name to be explicit", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554043", "createdAt": "2020-12-18T03:43:19Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareScheduleClusteringStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedScheduleClusteringStrategy.class);\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream().map(partition -> partition.replace(\"/\", \"-\"))\n+        .sorted(Comparator.reverseOrder()).map(partitionPath -> partitionPath.replace(\"-\", \"/\"))\n+        .limit(targetPartitionsForClustering > 0 ? targetPartitionsForClustering : partitionPaths.size())\n+        .collect(Collectors.toList());\n+  }\n+\n+  private int getNumberOfGroups(long groupSize, long targetFileSize) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NDc1OA=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTU0NTA4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjozMzowM1rOIHzhcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0NDozMFrOIIR_yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTA4OA==", "bodyText": "why do we need the /,- replace logic. It should correctly even without that?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545055088", "createdAt": "2020-12-17T12:33:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareScheduleClusteringStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedScheduleClusteringStrategy.class);\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream().map(partition -> partition.replace(\"/\", \"-\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDM3OA==", "bodyText": "Seems to work fine without replacing '/'.  So I removed it. Btw, compaction has this code, so I'm not sure if there are special cases where this replace is needed. It should be easy to add if we run into any issue. So I removed it.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554378", "createdAt": "2020-12-18T03:44:30Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareScheduleClusteringStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedScheduleClusteringStrategy.class);\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream().map(partition -> partition.replace(\"/\", \"-\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTA4OA=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTU3Mjg2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjozOTo1OFrOIHzxKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0NDo1N1rOIISAPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1OTExMg==", "bodyText": "should the filtering for 2 happen at this level?  that seems like something a specific plan would do.\n1 makes sense to do at this level.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545059112", "createdAt": "2020-12-17T12:39:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDQ5NA==", "bodyText": "Moved 2 to strategy specific implementation.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554494", "createdAt": "2020-12-18T03:44:57Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1OTExMg=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTU3OTk5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjo0MTo0NFrOIHz1Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0NjoxMVrOIISBmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDE0Ng==", "bodyText": "this is actually just a ClusteringPlanStrategy right? i.e it generates clustering plans. It has less to do with scheduling of clustering itself?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545060146", "createdAt": "2020-12-17T12:41:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDM1Mg==", "bodyText": "Should we rename this class and the configs?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545060352", "createdAt": "2020-12-17T12:42:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDE0Ng=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDg0MQ==", "bodyText": "I followed compaction naming pattern. But agree schedule has different meaning. I changed it to ClusteringPlanStrategy. WriteClient methods are still scheduleClustering(String instant), let me know if you think this needs to change also.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554841", "createdAt": "2020-12-18T03:46:11Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDE0Ng=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTU4ODE3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjo0Mzo0MFrOIHz50w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0Njo0MVrOIISCLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MTMzMQ==", "bodyText": "can we rename numOutputGroups to something that can be easily understood in the bulk insert context. If we leak some clustering terminology here, it becomes to harder to read this.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545061331", "createdAt": "2020-12-17T12:43:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "diffHunk": "@@ -27,8 +27,21 @@\n \n public abstract class AbstractBulkInsertHelper<T extends HoodieRecordPayload, I, K, O, R> {\n \n+  /**\n+   * Mark instant as inflight, write input records, update index and return result.\n+   */\n   public abstract HoodieWriteMetadata<O> bulkInsert(I inputRecords, String instantTime,\n                                                     HoodieTable<T, I, K, O> table, HoodieWriteConfig config,\n                                                     BaseCommitActionExecutor<T, I, K, O, R> executor, boolean performDedupe,\n                                                     Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner);\n+\n+  /**\n+   * Only write input records. Does not change timeline/index. Return information about new files created.\n+   */\n+  public abstract O bulkInsert(I inputRecords, String instantTime,\n+                               HoodieTable<T, I, K, O> table, HoodieWriteConfig config,\n+                               boolean performDedupe,\n+                               Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner,\n+                               boolean addMetadataFields,\n+                               int numOutputGroups);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NDk4OQ==", "bodyText": "Looks like I already changed subclasses, but overlooked base class. Thanks for catching this.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545554989", "createdAt": "2020-12-18T03:46:41Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "diffHunk": "@@ -27,8 +27,21 @@\n \n public abstract class AbstractBulkInsertHelper<T extends HoodieRecordPayload, I, K, O, R> {\n \n+  /**\n+   * Mark instant as inflight, write input records, update index and return result.\n+   */\n   public abstract HoodieWriteMetadata<O> bulkInsert(I inputRecords, String instantTime,\n                                                     HoodieTable<T, I, K, O> table, HoodieWriteConfig config,\n                                                     BaseCommitActionExecutor<T, I, K, O, R> executor, boolean performDedupe,\n                                                     Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner);\n+\n+  /**\n+   * Only write input records. Does not change timeline/index. Return information about new files created.\n+   */\n+  public abstract O bulkInsert(I inputRecords, String instantTime,\n+                               HoodieTable<T, I, K, O> table, HoodieWriteConfig config,\n+                               boolean performDedupe,\n+                               Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner,\n+                               boolean addMetadataFields,\n+                               int numOutputGroups);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MTMzMQ=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNTU5NTUzOnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/LogFileSizeBasedCompactionStrategy.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxMjo0NToyMFrOIHz9yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0ODowMVrOIISDqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MjM0NA==", "bodyText": "are these cleanups strictly needed for this PR? if you have tested them already, its okay. but generally, separating these in a different refactor PR is preferrable.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545062344", "createdAt": "2020-12-17T12:45:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/LogFileSizeBasedCompactionStrategy.java", "diffHunk": "@@ -40,21 +36,6 @@\n public class LogFileSizeBasedCompactionStrategy extends BoundedIOCompactionStrategy\n     implements Comparator<HoodieCompactionOperation> {\n \n-  private static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILE_SIZE\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NTM3MA==", "bodyText": "I refactored it based on other review feedback. There are unit tests for this. So I am reasonably confident theres no errors. I'll keep such changes for another PR next time. Thanks", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545555370", "createdAt": "2020-12-18T03:48:01Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/LogFileSizeBasedCompactionStrategy.java", "diffHunk": "@@ -40,21 +36,6 @@\n public class LogFileSizeBasedCompactionStrategy extends BoundedIOCompactionStrategy\n     implements Comparator<HoodieCompactionOperation> {\n \n-  private static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILE_SIZE\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MjM0NA=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjA2MzU5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDoyODozM1rOIH4MnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0ODowOFrOIISDxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzMTY3Ng==", "bodyText": "place each parameter on its own line?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545131676", "createdAt": "2020-12-17T14:28:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "diffHunk": "@@ -59,25 +58,39 @@ public static SparkBulkInsertHelper newInstance() {\n   }\n \n   @Override\n-  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,\n-                                                              String instantTime,\n-                                                              HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                                              HoodieWriteConfig config,\n-                                                              BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,\n-                                                              boolean performDedupe,\n-                                                              Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<HoodieRecord<T>> inputRecords, final String instantTime, final HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table, final HoodieWriteConfig config, final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor, final boolean performDedupe, final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NTM5Nw==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545555397", "createdAt": "2020-12-18T03:48:08Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "diffHunk": "@@ -59,25 +58,39 @@ public static SparkBulkInsertHelper newInstance() {\n   }\n \n   @Override\n-  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,\n-                                                              String instantTime,\n-                                                              HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                                              HoodieWriteConfig config,\n-                                                              BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,\n-                                                              boolean performDedupe,\n-                                                              Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<HoodieRecord<T>> inputRecords, final String instantTime, final HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table, final HoodieWriteConfig config, final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor, final boolean performDedupe, final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzMTY3Ng=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjA5NDYzOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDozNDo1NFrOIH4fEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0ODoxOVrOIISECg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzNjQwMw==", "bodyText": "should it be called numOutputSlices?  the whole thing  is called a group right?  or may be you mean numOutputFileGroups? in which case, lets rename this to clarify", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545136403", "createdAt": "2020-12-17T14:34:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -40,6 +40,11 @@\n          }],\n          \"default\": null\n       },\n+      {\n+         \"name\":\"numOutputGroups\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NTQ2Ng==", "bodyText": "Changed it to numOutputFileGroups", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545555466", "createdAt": "2020-12-18T03:48:19Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -40,6 +40,11 @@\n          }],\n          \"default\": null\n       },\n+      {\n+         \"name\":\"numOutputGroups\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzNjQwMw=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNjExMTk0OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/FileSliceUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDozODozMlrOIH4pag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMzo0ODozMVrOIISEPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzOTA1MA==", "bodyText": "this is very metric specific. could we stick it somewhere else closer to actual usage in hudi-client-common? may be rename the class to something like FileSliceMetricUtils ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545139050", "createdAt": "2020-12-17T14:38:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/FileSliceUtils.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * A utility class for numeric.\n+ */\n+public class FileSliceUtils {\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static void addFileSliceCommonMetrics(List<FileSlice> fileSlices, Map<String, Double> metrics, long defaultBaseFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NTUxNw==", "bodyText": "Moved it as suggested.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545555517", "createdAt": "2020-12-18T03:48:31Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/FileSliceUtils.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * A utility class for numeric.\n+ */\n+public class FileSliceUtils {\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static void addFileSliceCommonMetrics(List<FileSlice> fileSlices, Map<String, Double> metrics, long defaultBaseFileSize) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzOTA1MA=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjYzNDU2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxMDoxOVrOII0MKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxMDoxOVrOII0MKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNDYwMw==", "bodyText": "may be call it execution.  hoodie.clustering.execution.strategy.class ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546114603", "createdAt": "2020-12-18T22:10:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjYzNjU2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxMToyNVrOII0NhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxMToyNVrOII0NhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNDk0OA==", "bodyText": "max.commits?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546114948", "createdAt": "2020-12-18T22:11:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjYzOTY5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxMjo1NlrOII0PaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxMjo1NlrOII0PaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNTQzMw==", "bodyText": "replace scan with list?\nshould we also do it based on last N commits? It helps for tables that receive data across partitions? This is probably some follow on work we can do to add a new plan strategy for this. Lets add a JIRA?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546115433", "createdAt": "2020-12-18T22:12:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY0NDUzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxNDo0NlrOII0SFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxNDo0NlrOII0SFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNjExOQ==", "bodyText": "hoodie.clustering.plan.strategy.daybased.lookback.partitions or something ties this to a specific strategy and also captures that this look at N partitions from now.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546116119", "createdAt": "2020-12-18T22:14:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY1MDQ1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxNjo1OFrOII0VTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxNjo1OFrOII0VTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNjk0MA==", "bodyText": "something to think about. should this be strategy specific too?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546116940", "createdAt": "2020-12-18T22:16:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY1NjQ1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxOTo0MFrOII0YsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxOTo0MFrOII0YsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNzgwOA==", "bodyText": "may be hoodie.clustering.max.bytes.per.group ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546117808", "createdAt": "2020-12-18T22:19:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_BYTES_IN_GROUP * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_BYTES_IN_GROUP = \"hoodie.clustering.max.bytes.group\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY1NzIwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMDowMVrOII0ZGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMDowMVrOII0ZGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNzkxNQ==", "bodyText": "strategy specific?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546117915", "createdAt": "2020-12-18T22:20:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_BYTES_IN_GROUP * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_BYTES_IN_GROUP = \"hoodie.clustering.max.bytes.group\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  // Maximum number of groups to create as part of ClusteringPlan. Increasing groups will increase parallelism.\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = \"hoodie.clustering.max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"30\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY1ODQ3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/BaseCreateClusteringPlanActionExecutor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMDo0NFrOII0Z4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMDo0NFrOII0Z4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExODExNA==", "bodyText": "drop the Create?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546118114", "createdAt": "2020-12-18T22:20:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/BaseCreateClusteringPlanActionExecutor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.BaseActionExecutor;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Map;\n+\n+public abstract class BaseCreateClusteringPlanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieClusteringPlan>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY2MDgwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMTo1M1rOII0bNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMTo1M1rOII0bNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExODQ1NQ==", "bodyText": "remove this?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546118455", "createdAt": "2020-12-18T22:21:53Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.utils.FileSliceMetricUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ClusteringPlanStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final transient HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY2MzQwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMjo0OFrOII0cpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMjo0OFrOII0cpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExODgyMQ==", "bodyText": "EMPTY_STRING?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546118821", "createdAt": "2020-12-18T22:22:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.utils.FileSliceMetricUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ClusteringPlanStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final transient HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering.\n+   */\n+  protected Stream<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    SyncableFileSystemView fileSystemView = (SyncableFileSystemView) getHoodieTable().getSliceView();\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getKey).collect(Collectors.toSet()));\n+\n+    return hoodieTable.getSliceView().getLatestFileSlices(partition)\n+        // file ids already in clustering are not eligible\n+        .filter(slice -> !fgIdsInPendingCompactionAndClustering.contains(slice.getFileGroupId()));\n+  }\n+\n+  /**\n+   * Get parameters specific to strategy. These parameters are passed from 'schedule clustering' step to\n+   * 'run clustering' step. 'run clustering' step is typically async. So these params help with passing any required\n+   * context from schedule to run step.\n+   */\n+  protected abstract Map<String, String> getStrategyParams();\n+\n+  /**\n+   * Returns any specific parameters to be stored as part of clustering metadata.\n+   */\n+  protected Map<String, String> getExtraMetadata() {\n+    return Collections.emptyMap();\n+  }\n+\n+  /**\n+   * Version to support future changes for plan.\n+   */\n+  protected int getPlanVersion() {\n+    return CLUSTERING_PLAN_VERSION_1;\n+  }\n+\n+  /**\n+   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.\n+   */\n+  protected List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY2NTA5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMzoyMlrOII0djA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyMzoyMlrOII0djA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExOTA1Mg==", "bodyText": "worth thinking about making this a static helper?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546119052", "createdAt": "2020-12-18T22:23:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.utils.FileSliceMetricUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ClusteringPlanStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final transient HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering.\n+   */\n+  protected Stream<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    SyncableFileSystemView fileSystemView = (SyncableFileSystemView) getHoodieTable().getSliceView();\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getKey).collect(Collectors.toSet()));\n+\n+    return hoodieTable.getSliceView().getLatestFileSlices(partition)\n+        // file ids already in clustering are not eligible\n+        .filter(slice -> !fgIdsInPendingCompactionAndClustering.contains(slice.getFileGroupId()));\n+  }\n+\n+  /**\n+   * Get parameters specific to strategy. These parameters are passed from 'schedule clustering' step to\n+   * 'run clustering' step. 'run clustering' step is typically async. So these params help with passing any required\n+   * context from schedule to run step.\n+   */\n+  protected abstract Map<String, String> getStrategyParams();\n+\n+  /**\n+   * Returns any specific parameters to be stored as part of clustering metadata.\n+   */\n+  protected Map<String, String> getExtraMetadata() {\n+    return Collections.emptyMap();\n+  }\n+\n+  /**\n+   * Version to support future changes for plan.\n+   */\n+  protected int getPlanVersion() {\n+    return CLUSTERING_PLAN_VERSION_1;\n+  }\n+\n+  /**\n+   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.\n+   */\n+  protected List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY2OTUwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoyNToxM1rOII0f9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjozMDoyNVrOII0mmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExOTY2OQ==", "bodyText": "same something like DayBased? to indicate date partitioning?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546119669", "createdAt": "2020-12-18T22:25:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> extends ClusteringPlanStrategy<T,I,K,O> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMTM2OQ==", "bodyText": "also rename the subclass to indicate that it prefers the recent partitions?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546121369", "createdAt": "2020-12-18T22:30:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> extends ClusteringPlanStrategy<T,I,K,O> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExOTY2OQ=="}, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY4NjYwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkBoundedDayBasedClusteringPlanStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjozMzozMlrOII0qGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjozMzozMlrOII0qGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMjI2NA==", "bodyText": "Something like hoodie.clustering.small.file.limit", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546122264", "createdAt": "2020-12-18T22:33:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkBoundedDayBasedClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.plan.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareClusteringPlanStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedClusteringPlanStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareClusteringPlanStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedClusteringPlanStrategy.class);\n+\n+  public SparkBoundedDayBasedClusteringPlanStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                    HoodieSparkEngineContext engineContext,\n+                                                    HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedClusteringPlanStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                    HoodieSparkEngineContext engineContext,\n+                                                    HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfOutputFileGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfOutputFileGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputFileGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream()\n+        .sorted(Comparator.reverseOrder())\n+        .limit(targetPartitionsForClustering > 0 ? targetPartitionsForClustering : partitionPaths.size())\n+        .collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  protected Stream<FileSlice> getFileSlicesEligibleForClustering(final String partition) {\n+    return super.getFileSlicesEligibleForClustering(partition)\n+        // files that have basefile size larger than clustering target file size are not eligible (Note that compaction can merge any updates)\n+        .filter(slice -> slice.getBaseFile().map(HoodieBaseFile::getFileSize).orElse(0L) < getWriteConfig().getClusteringTargetFileMaxBytes());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY5MjAxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjozNjoxMVrOII0tPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjozODowM1rOII0vdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMzA3MQ==", "bodyText": "we can add SerializableSchema (we do this for Hadoop Config object) ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546123071", "createdAt": "2020-12-18T22:36:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {\n+      Schema schema = new Schema.Parser().parse(schemaStr);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMzYzNw==", "bodyText": "See SerialiableConfiguration for reference", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546123637", "createdAt": "2020-12-18T22:38:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {\n+      Schema schema = new Schema.Parser().parse(schemaStr);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMzA3MQ=="}, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjY5OTQ4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjozOTo0OFrOII0xlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjozOTo0OFrOII0xlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyNDE4Mg==", "bodyText": "SparkClusteringCommitActionExecutor?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546124182", "createdAt": "2020-12-18T22:39:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.client.utils.ConcatenatingIterator;\n+import org.apache.hudi.common.model.ClusteringOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMjcwMzgyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjo0MjowN1rOII00JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjo0MjowN1rOII00JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyNDgzNg==", "bodyText": "this is very cool!", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546124836", "createdAt": "2020-12-18T22:42:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.client.utils.ConcatenatingIterator;\n+import org.apache.hudi.common.model.ClusteringOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // run clustering for each group async and collect WriteStatus\n+    JavaRDD<WriteStatus> writeStatusRDD = clusteringPlan.getInputGroups().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 93}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4186, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}