{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU5NDIyNjk0", "number": 1191, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wN1QwNjowMjoyNFrODWKH9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxOToyNlrODXCU9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI0NTYxMTQyOnYy", "diffSide": "RIGHT", "path": "hudi-test-suite/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wN1QwNjowMjoyNFrOFawf7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wN1QxMDo0NTowMlrOFa15mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzYwMTkwMA==", "bodyText": "Generating a custom Workload Pattern\nThere are 2 ways to generate a workload pattern:\n\nProgramatically\nChoose to write up the entire DAG of operations programatically, take a look at WorkflowDagGenerator class. Once you're ready with the DAG you want to execute, simply pass the class name as follows:\n\nspark-submit\n...\n...\n--class org.apache.hudi.bench.job.HudiTestSuiteJob \n--workload-generator-classname org.apache.hudi.bench.dag.scheduler.<your_workflowdaggenerator>\n...\n\nYAML file\nChoose to write up the entire DAG of operations in YAML, take a look at complex-workload-dag-cow.yaml or complex-workload-dag-mor.yaml. Once you're ready with the DAG you want to execute, simply pass the yaml file path as follows:\n\nspark-submit\n...\n...\n--class org.apache.hudi.bench.job.HudiTestSuiteJob \n--workload-yaml-path /path/to/your-workflow-dag.yaml\n...\n\nHey, I think it looks a little clearer", "url": "https://github.com/apache/hudi/pull/1191#discussion_r363601900", "createdAt": "2020-01-07T06:02:24Z", "author": {"login": "sev7e0"}, "path": "hudi-test-suite/README.md", "diffHunk": "@@ -0,0 +1,291 @@\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+\n+This page describes in detail how to run end to end tests on a hudi dataset that helps in improving our confidence \n+in a release as well as perform large scale performance benchmarks.  \n+\n+# Objectives\n+\n+1. Test with different versions of core libraries and components such as `hdfs`, `parquet`, `spark`, \n+`hive` and `avro`.\n+2. Generate different types of workloads across different dimensions such as `payload size`, `number of updates`, \n+`number of inserts`, `number of partitions`\n+3. Perform multiple types of operations such as `insert`, `bulk_insert`, `upsert`, `compact`, `query`\n+4. Support custom post process actions and validations\n+\n+# High Level Design\n+\n+The Hudi test suite runs as a long running spark job. The suite is divided into the following high level components : \n+\n+## Workload Generation\n+\n+This component does the work of generating the workload; `inserts`, `upserts` etc.\n+\n+## Workload Scheduling\n+\n+Depending on the type of workload generated, data is either ingested into the target hudi \n+dataset or the corresponding workload operation is executed. For example compaction does not necessarily need a workload\n+to be generated/ingested but can require an execution.\n+\n+## Other actions/operatons\n+\n+The test suite supports different types of operations besides ingestion such as Hive Query execution, Clean action etc.\n+\n+# Usage instructions\n+\n+\n+## Entry class to the test suite\n+\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteJob.java - Entry Point of the hudi test suite job. This \n+class wraps all the functionalities required to run a configurable integration suite.\n+```\n+\n+## Configurations required to run the job\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteConfig - Config class that drives the behavior of the \n+integration test suite. This class extends from com.uber.hoodie.utilities.DeltaStreamerConfig. Look at \n+link#HudiDeltaStreamer page to learn about all the available configs applicable to your test suite.\n+```\n+\n+## Generating a custom Workload Pattern\n+```\n+There are 2 ways to generate a workload pattern\n+1. Programatically\n+Choose to write up the entire DAG of operations programatically, take a look at WorkflowDagGenerator class.\n+Once you're ready with the DAG you want to execute, simply pass the class name as follows\n+spark-submit\n+...\n+...\n+--class org.apache.hudi.bench.job.HudiTestSuiteJob \n+--workload-generator-classname org.apache.hudi.bench.dag.scheduler.<your_workflowdaggenerator>\n+...\n+2. YAML file\n+Choose to write up the entire DAG of operations in YAML, take a look at complex-workload-dag-cow.yaml or \n+complex-workload-dag-mor.yaml.\n+Once you're ready with the DAG you want to execute, simply pass the yaml file path as follows\n+spark-submit\n+...\n+...\n+--class org.apache.hudi.bench.job.HudiTestSuiteJob \n+--workload-yaml-path /path/to/your-workflow-dag.yaml\n+...\n+```", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzY5MDM5Mw==", "bodyText": "Thanks for your suggestion. I have addressed it.", "url": "https://github.com/apache/hudi/pull/1191#discussion_r363690393", "createdAt": "2020-01-07T10:45:02Z", "author": {"login": "yanghua"}, "path": "hudi-test-suite/README.md", "diffHunk": "@@ -0,0 +1,291 @@\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+\n+This page describes in detail how to run end to end tests on a hudi dataset that helps in improving our confidence \n+in a release as well as perform large scale performance benchmarks.  \n+\n+# Objectives\n+\n+1. Test with different versions of core libraries and components such as `hdfs`, `parquet`, `spark`, \n+`hive` and `avro`.\n+2. Generate different types of workloads across different dimensions such as `payload size`, `number of updates`, \n+`number of inserts`, `number of partitions`\n+3. Perform multiple types of operations such as `insert`, `bulk_insert`, `upsert`, `compact`, `query`\n+4. Support custom post process actions and validations\n+\n+# High Level Design\n+\n+The Hudi test suite runs as a long running spark job. The suite is divided into the following high level components : \n+\n+## Workload Generation\n+\n+This component does the work of generating the workload; `inserts`, `upserts` etc.\n+\n+## Workload Scheduling\n+\n+Depending on the type of workload generated, data is either ingested into the target hudi \n+dataset or the corresponding workload operation is executed. For example compaction does not necessarily need a workload\n+to be generated/ingested but can require an execution.\n+\n+## Other actions/operatons\n+\n+The test suite supports different types of operations besides ingestion such as Hive Query execution, Clean action etc.\n+\n+# Usage instructions\n+\n+\n+## Entry class to the test suite\n+\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteJob.java - Entry Point of the hudi test suite job. This \n+class wraps all the functionalities required to run a configurable integration suite.\n+```\n+\n+## Configurations required to run the job\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteConfig - Config class that drives the behavior of the \n+integration test suite. This class extends from com.uber.hoodie.utilities.DeltaStreamerConfig. Look at \n+link#HudiDeltaStreamer page to learn about all the available configs applicable to your test suite.\n+```\n+\n+## Generating a custom Workload Pattern\n+```\n+There are 2 ways to generate a workload pattern\n+1. Programatically\n+Choose to write up the entire DAG of operations programatically, take a look at WorkflowDagGenerator class.\n+Once you're ready with the DAG you want to execute, simply pass the class name as follows\n+spark-submit\n+...\n+...\n+--class org.apache.hudi.bench.job.HudiTestSuiteJob \n+--workload-generator-classname org.apache.hudi.bench.dag.scheduler.<your_workflowdaggenerator>\n+...\n+2. YAML file\n+Choose to write up the entire DAG of operations in YAML, take a look at complex-workload-dag-cow.yaml or \n+complex-workload-dag-mor.yaml.\n+Once you're ready with the DAG you want to execute, simply pass the yaml file path as follows\n+spark-submit\n+...\n+...\n+--class org.apache.hudi.bench.job.HudiTestSuiteJob \n+--workload-yaml-path /path/to/your-workflow-dag.yaml\n+...\n+```", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzYwMTkwMA=="}, "originalCommit": null, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NDgxNjUyOnYy", "diffSide": "RIGHT", "path": "hudi-test-suite/README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxNzowNVrOFcIsdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxNzowNVrOFcIsdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTA0NjkwMQ==", "bodyText": "We need to change the package name here, like remove \"bench\"", "url": "https://github.com/apache/hudi/pull/1191#discussion_r365046901", "createdAt": "2020-01-10T02:17:05Z", "author": {"login": "n3nash"}, "path": "hudi-test-suite/README.md", "diffHunk": "@@ -0,0 +1,300 @@\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+\n+This page describes in detail how to run end to end tests on a hudi dataset that helps in improving our confidence \n+in a release as well as perform large scale performance benchmarks.  \n+\n+# Objectives\n+\n+1. Test with different versions of core libraries and components such as `hdfs`, `parquet`, `spark`, \n+`hive` and `avro`.\n+2. Generate different types of workloads across different dimensions such as `payload size`, `number of updates`, \n+`number of inserts`, `number of partitions`\n+3. Perform multiple types of operations such as `insert`, `bulk_insert`, `upsert`, `compact`, `query`\n+4. Support custom post process actions and validations\n+\n+# High Level Design\n+\n+The Hudi test suite runs as a long running spark job. The suite is divided into the following high level components : \n+\n+## Workload Generation\n+\n+This component does the work of generating the workload; `inserts`, `upserts` etc.\n+\n+## Workload Scheduling\n+\n+Depending on the type of workload generated, data is either ingested into the target hudi \n+dataset or the corresponding workload operation is executed. For example compaction does not necessarily need a workload\n+to be generated/ingested but can require an execution.\n+\n+## Other actions/operatons\n+\n+The test suite supports different types of operations besides ingestion such as Hive Query execution, Clean action etc.\n+\n+# Usage instructions\n+\n+\n+## Entry class to the test suite\n+\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteJob.java - Entry Point of the hudi test suite job. This ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NDgxNjYzOnYy", "diffSide": "RIGHT", "path": "hudi-test-suite/README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxNzoxMlrOFcIshg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxNzoxMlrOFcIshg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTA0NjkxOA==", "bodyText": "same, I would also re-check if the names of the classes are the same..", "url": "https://github.com/apache/hudi/pull/1191#discussion_r365046918", "createdAt": "2020-01-10T02:17:12Z", "author": {"login": "n3nash"}, "path": "hudi-test-suite/README.md", "diffHunk": "@@ -0,0 +1,300 @@\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+\n+This page describes in detail how to run end to end tests on a hudi dataset that helps in improving our confidence \n+in a release as well as perform large scale performance benchmarks.  \n+\n+# Objectives\n+\n+1. Test with different versions of core libraries and components such as `hdfs`, `parquet`, `spark`, \n+`hive` and `avro`.\n+2. Generate different types of workloads across different dimensions such as `payload size`, `number of updates`, \n+`number of inserts`, `number of partitions`\n+3. Perform multiple types of operations such as `insert`, `bulk_insert`, `upsert`, `compact`, `query`\n+4. Support custom post process actions and validations\n+\n+# High Level Design\n+\n+The Hudi test suite runs as a long running spark job. The suite is divided into the following high level components : \n+\n+## Workload Generation\n+\n+This component does the work of generating the workload; `inserts`, `upserts` etc.\n+\n+## Workload Scheduling\n+\n+Depending on the type of workload generated, data is either ingested into the target hudi \n+dataset or the corresponding workload operation is executed. For example compaction does not necessarily need a workload\n+to be generated/ingested but can require an execution.\n+\n+## Other actions/operatons\n+\n+The test suite supports different types of operations besides ingestion such as Hive Query execution, Clean action etc.\n+\n+# Usage instructions\n+\n+\n+## Entry class to the test suite\n+\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteJob.java - Entry Point of the hudi test suite job. This \n+class wraps all the functionalities required to run a configurable integration suite.\n+```\n+\n+## Configurations required to run the job\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteConfig - Config class that drives the behavior of the ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NDgxODA1OnYy", "diffSide": "RIGHT", "path": "hudi-test-suite/README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxODoyNFrOFcItdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxODoyNFrOFcItdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTA0NzE1OQ==", "bodyText": "You might have to re-run and update this with the new package name..", "url": "https://github.com/apache/hudi/pull/1191#discussion_r365047159", "createdAt": "2020-01-10T02:18:24Z", "author": {"login": "n3nash"}, "path": "hudi-test-suite/README.md", "diffHunk": "@@ -0,0 +1,300 @@\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+\n+This page describes in detail how to run end to end tests on a hudi dataset that helps in improving our confidence \n+in a release as well as perform large scale performance benchmarks.  \n+\n+# Objectives\n+\n+1. Test with different versions of core libraries and components such as `hdfs`, `parquet`, `spark`, \n+`hive` and `avro`.\n+2. Generate different types of workloads across different dimensions such as `payload size`, `number of updates`, \n+`number of inserts`, `number of partitions`\n+3. Perform multiple types of operations such as `insert`, `bulk_insert`, `upsert`, `compact`, `query`\n+4. Support custom post process actions and validations\n+\n+# High Level Design\n+\n+The Hudi test suite runs as a long running spark job. The suite is divided into the following high level components : \n+\n+## Workload Generation\n+\n+This component does the work of generating the workload; `inserts`, `upserts` etc.\n+\n+## Workload Scheduling\n+\n+Depending on the type of workload generated, data is either ingested into the target hudi \n+dataset or the corresponding workload operation is executed. For example compaction does not necessarily need a workload\n+to be generated/ingested but can require an execution.\n+\n+## Other actions/operatons\n+\n+The test suite supports different types of operations besides ingestion such as Hive Query execution, Clean action etc.\n+\n+# Usage instructions\n+\n+\n+## Entry class to the test suite\n+\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteJob.java - Entry Point of the hudi test suite job. This \n+class wraps all the functionalities required to run a configurable integration suite.\n+```\n+\n+## Configurations required to run the job\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteConfig - Config class that drives the behavior of the \n+integration test suite. This class extends from com.uber.hoodie.utilities.DeltaStreamerConfig. Look at \n+link#HudiDeltaStreamer page to learn about all the available configs applicable to your test suite.\n+```\n+\n+## Generating a custom Workload Pattern\n+\n+There are 2 ways to generate a workload pattern\n+\n+ 1.Programatically\n+\n+Choose to write up the entire DAG of operations programatically, take a look at `WorkflowDagGenerator` class.\n+Once you're ready with the DAG you want to execute, simply pass the class name as follows:\n+\n+```\n+spark-submit\n+...\n+...\n+--class org.apache.hudi.bench.job.HudiTestSuiteJob \n+--workload-generator-classname org.apache.hudi.bench.dag.scheduler.<your_workflowdaggenerator>\n+...\n+```\n+\n+ 2.YAML file\n+\n+Choose to write up the entire DAG of operations in YAML, take a look at `complex-workload-dag-cow.yaml` or \n+`complex-workload-dag-mor.yaml`.\n+Once you're ready with the DAG you want to execute, simply pass the yaml file path as follows:\n+\n+```\n+spark-submit\n+...\n+...\n+--class org.apache.hudi.bench.job.HudiTestSuiteJob \n+--workload-yaml-path /path/to/your-workflow-dag.yaml\n+...\n+```\n+\n+## Building the test suite\n+\n+The test suite can be found in the `hudi-bench` module. Use the `prepare_integration_suite.sh` script to build \n+the test suite, you can provide different parameters to the script.\n+\n+```\n+shell$ ./prepare_integration_suite.sh --help\n+Usage: prepare_integration_suite.sh\n+   --spark-command, prints the spark command\n+   -h, hdfs-version\n+   -s, spark version\n+   -p, parquet version\n+   -a, avro version\n+   -s, hive version\n+```\n+\n+```\n+shell$ ./prepare_integration_suite.sh\n+....\n+....\n+Final command : mvn clean install -DskipTests\n+[INFO] ------------------------------------------------------------------------\n+[INFO] Reactor Summary:\n+[INFO]\n+[INFO] Hudi ............................................... SUCCESS [  2.749 s]\n+[INFO] hudi-common ........................................ SUCCESS [ 12.711 s]\n+[INFO] hudi-timeline-service .............................. SUCCESS [  1.924 s]\n+[INFO] hudi-hadoop-mr ..................................... SUCCESS [  7.203 s]\n+[INFO] hudi-client ........................................ SUCCESS [ 10.486 s]\n+[INFO] hudi-hive .......................................... SUCCESS [  5.159 s]\n+[INFO] hudi-spark ......................................... SUCCESS [ 34.499 s]\n+[INFO] hudi-utilities ..................................... SUCCESS [  8.626 s]\n+[INFO] hudi-cli ........................................... SUCCESS [ 14.921 s]\n+[INFO] hudi-bench ......................................... SUCCESS [  7.706 s]", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI1NDgxOTc0OnYy", "diffSide": "RIGHT", "path": "hudi-test-suite/README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxOToyNlrOFcIuaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQwMjoxOToyNlrOFcIuaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTA0NzQwMA==", "bodyText": "please start your docker and try to run both these commands after renaming and package changes to make sure they run fine..", "url": "https://github.com/apache/hudi/pull/1191#discussion_r365047400", "createdAt": "2020-01-10T02:19:26Z", "author": {"login": "n3nash"}, "path": "hudi-test-suite/README.md", "diffHunk": "@@ -0,0 +1,300 @@\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+\n+This page describes in detail how to run end to end tests on a hudi dataset that helps in improving our confidence \n+in a release as well as perform large scale performance benchmarks.  \n+\n+# Objectives\n+\n+1. Test with different versions of core libraries and components such as `hdfs`, `parquet`, `spark`, \n+`hive` and `avro`.\n+2. Generate different types of workloads across different dimensions such as `payload size`, `number of updates`, \n+`number of inserts`, `number of partitions`\n+3. Perform multiple types of operations such as `insert`, `bulk_insert`, `upsert`, `compact`, `query`\n+4. Support custom post process actions and validations\n+\n+# High Level Design\n+\n+The Hudi test suite runs as a long running spark job. The suite is divided into the following high level components : \n+\n+## Workload Generation\n+\n+This component does the work of generating the workload; `inserts`, `upserts` etc.\n+\n+## Workload Scheduling\n+\n+Depending on the type of workload generated, data is either ingested into the target hudi \n+dataset or the corresponding workload operation is executed. For example compaction does not necessarily need a workload\n+to be generated/ingested but can require an execution.\n+\n+## Other actions/operatons\n+\n+The test suite supports different types of operations besides ingestion such as Hive Query execution, Clean action etc.\n+\n+# Usage instructions\n+\n+\n+## Entry class to the test suite\n+\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteJob.java - Entry Point of the hudi test suite job. This \n+class wraps all the functionalities required to run a configurable integration suite.\n+```\n+\n+## Configurations required to run the job\n+```\n+org.apache.hudi.bench.job.HudiTestSuiteConfig - Config class that drives the behavior of the \n+integration test suite. This class extends from com.uber.hoodie.utilities.DeltaStreamerConfig. Look at \n+link#HudiDeltaStreamer page to learn about all the available configs applicable to your test suite.\n+```\n+\n+## Generating a custom Workload Pattern\n+\n+There are 2 ways to generate a workload pattern\n+\n+ 1.Programatically\n+\n+Choose to write up the entire DAG of operations programatically, take a look at `WorkflowDagGenerator` class.\n+Once you're ready with the DAG you want to execute, simply pass the class name as follows:\n+\n+```\n+spark-submit\n+...\n+...\n+--class org.apache.hudi.bench.job.HudiTestSuiteJob \n+--workload-generator-classname org.apache.hudi.bench.dag.scheduler.<your_workflowdaggenerator>\n+...\n+```\n+\n+ 2.YAML file\n+\n+Choose to write up the entire DAG of operations in YAML, take a look at `complex-workload-dag-cow.yaml` or \n+`complex-workload-dag-mor.yaml`.\n+Once you're ready with the DAG you want to execute, simply pass the yaml file path as follows:\n+\n+```\n+spark-submit\n+...\n+...\n+--class org.apache.hudi.bench.job.HudiTestSuiteJob \n+--workload-yaml-path /path/to/your-workflow-dag.yaml\n+...\n+```\n+\n+## Building the test suite\n+\n+The test suite can be found in the `hudi-bench` module. Use the `prepare_integration_suite.sh` script to build \n+the test suite, you can provide different parameters to the script.\n+\n+```\n+shell$ ./prepare_integration_suite.sh --help\n+Usage: prepare_integration_suite.sh\n+   --spark-command, prints the spark command\n+   -h, hdfs-version\n+   -s, spark version\n+   -p, parquet version\n+   -a, avro version\n+   -s, hive version\n+```\n+\n+```\n+shell$ ./prepare_integration_suite.sh\n+....\n+....\n+Final command : mvn clean install -DskipTests\n+[INFO] ------------------------------------------------------------------------\n+[INFO] Reactor Summary:\n+[INFO]\n+[INFO] Hudi ............................................... SUCCESS [  2.749 s]\n+[INFO] hudi-common ........................................ SUCCESS [ 12.711 s]\n+[INFO] hudi-timeline-service .............................. SUCCESS [  1.924 s]\n+[INFO] hudi-hadoop-mr ..................................... SUCCESS [  7.203 s]\n+[INFO] hudi-client ........................................ SUCCESS [ 10.486 s]\n+[INFO] hudi-hive .......................................... SUCCESS [  5.159 s]\n+[INFO] hudi-spark ......................................... SUCCESS [ 34.499 s]\n+[INFO] hudi-utilities ..................................... SUCCESS [  8.626 s]\n+[INFO] hudi-cli ........................................... SUCCESS [ 14.921 s]\n+[INFO] hudi-bench ......................................... SUCCESS [  7.706 s]\n+[INFO] hudi-hadoop-mr-bundle .............................. SUCCESS [  1.873 s]\n+[INFO] hudi-hive-bundle ................................... SUCCESS [  1.508 s]\n+[INFO] hudi-spark-bundle .................................. SUCCESS [ 17.432 s]\n+[INFO] hudi-presto-bundle ................................. SUCCESS [  1.309 s]\n+[INFO] hudi-utilities-bundle .............................. SUCCESS [ 18.386 s]\n+[INFO] hudi-timeline-server-bundle ........................ SUCCESS [  8.600 s]\n+[INFO] hudi-bench-bundle .................................. SUCCESS [ 38.348 s]\n+[INFO] hudi-hadoop-docker ................................. SUCCESS [  2.053 s]\n+[INFO] hudi-hadoop-base-docker ............................ SUCCESS [  0.806 s]\n+[INFO] hudi-hadoop-namenode-docker ........................ SUCCESS [  0.302 s]\n+[INFO] hudi-hadoop-datanode-docker ........................ SUCCESS [  0.403 s]\n+[INFO] hudi-hadoop-history-docker ......................... SUCCESS [  0.447 s]\n+[INFO] hudi-hadoop-hive-docker ............................ SUCCESS [  1.534 s]\n+[INFO] hudi-hadoop-sparkbase-docker ....................... SUCCESS [  0.315 s]\n+[INFO] hudi-hadoop-sparkmaster-docker ..................... SUCCESS [  0.407 s]\n+[INFO] hudi-hadoop-sparkworker-docker ..................... SUCCESS [  0.447 s]\n+[INFO] hudi-hadoop-sparkadhoc-docker ...................... SUCCESS [  0.410 s]\n+[INFO] hudi-hadoop-presto-docker .......................... SUCCESS [  0.697 s]\n+[INFO] hudi-integ-test .................................... SUCCESS [01:02 min]\n+[INFO] ------------------------------------------------------------------------\n+[INFO] BUILD SUCCESS\n+[INFO] ------------------------------------------------------------------------\n+[INFO] Total time: 04:23 min\n+[INFO] Finished at: 2019-11-02T23:56:48-07:00\n+[INFO] Final Memory: 234M/1582M\n+[INFO] ------------------------------------------------------------------------\n+```\n+\n+## Running on the cluster or in your local machine\n+Copy over the necessary files and jars that are required to your cluster and then run the following spark-submit \n+command after replacing the correct values for the parameters. \n+NOTE : The properties-file should have all the necessary information required to ingest into a Hudi dataset. For more\n+ information on what properties need to be set, take a look at the test suite section under demo steps.\n+```\n+shell$ ./prepare_integration_suite.sh --spark-command\n+spark-submit --packages com.databricks:spark-avro_2.11:4.0.0 --master prepare_integration_suite.sh --deploy-mode\n+--properties-file  --class org.apache.hudi.bench.job.HudiTestSuiteJob target/hudi-bench-0.5.1-SNAPSHOT.jar \n+--source-class  --source-ordering-field  --input-base-path  --target-base-path  --target-table  --props  --storage-type  --payload-class  --workload-yaml-path  --input-file-size  --<deltastreamer-ingest>\n+```\n+\n+## Running through a test-case (local)\n+Take a look at the TestHudiTestSuiteJob to check how you can run the entire suite using JUnit.\n+\n+## Running an end to end test suite in Local Docker environment\n+\n+```\n+docker exec -it adhoc-2 /bin/bash\n+# COPY_ON_WRITE tables\n+=========================\n+## Run the following command to start the test suite\n+spark-submit \\ ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 182}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 75, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}