{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc1MTQ3NDk4", "number": 1333, "title": "[HUDI-589][DOCS] Fix querying_data page", "bodyText": "Added support matrix for COW and MOR tables\nChange reference from (views|pulls) to queries\nAnd minor restructuring\n\nTips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\n(For example: This pull request adds quick-start document.)\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-02-14T00:01:00Z", "url": "https://github.com/apache/hudi/pull/1333", "merged": true, "mergeCommit": {"oid": "840b07ee4452e1f0654a32cb32cd7bed3279edcf"}, "closed": true, "closedAt": "2020-03-02T19:09:42Z", "author": {"login": "bhasudha"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcEGVaiAFqTM1ODY4MTc0NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcJynIqAFqTM2NzQ1MjkxNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4NjgxNzQ0", "url": "https://github.com/apache/hudi/pull/1333#pullrequestreview-358681744", "createdAt": "2020-02-14T02:37:03Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjozNzowM1rOFpqEcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjo0NDoxMlrOFpqKAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTIwMg==", "bodyText": "please point to quick start or some example for this", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225202", "createdAt": "2020-02-14T02:37:03Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTQ3OQ==", "bodyText": "can we remove this line Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. .. you don't have to build it yourself per se..", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225479", "createdAt": "2020-02-14T02:38:22Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTYwMQ==", "bodyText": "use --jars --packages", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225601", "createdAt": "2020-02-14T02:38:54Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTg5OQ==", "bodyText": "For this section, can you take another stab.. it feels short and curt.. may be set some context on things like : Spark Datasources directly query underlying DFS data without Hive (and for this reason I think we should move SparkSQL up and place before this section)", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225899", "createdAt": "2020-02-14T02:40:34Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjAzOA==", "bodyText": "lets also spend some time setting context and explaining how this uses Spark/Hive integration", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226038", "createdAt": "2020-02-14T02:41:22Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjExMA==", "bodyText": "own parquet reader", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226110", "createdAt": "2020-02-14T02:41:42Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjIwNQ==", "bodyText": "By default : are you talking about copy_on_write tables?", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226205", "createdAt": "2020-02-14T02:42:14Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjQwMw==", "bodyText": "turning off convertMetastoreParquet please usethe exact and full config here within ``", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226403", "createdAt": "2020-02-14T02:43:04Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. \n+However, for MERGE_ON_READ tables which has both parquet and avro data, this default setting needs to be turned off using set `spark.sql.hive.convertMetastoreParquet=false`. \n+This will force Spark to fallback to using the Hive Serde to read the data (planning/executions is still Spark). \n \n ```java\n-Dataset<Row> hoodieROViewDF = spark.read().format(\"org.apache.hudi\")\n-// pass any path glob, can include hudi & non-hudi tables\n-.load(\"/glob/path/pattern\");\n+$ spark-shell --driver-class-path /etc/hive/conf  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.4 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g  --master yarn-client\n+\n+scala> sqlContext.sql(\"select count(*) from hudi_trips_mor_rt where datestr = '2016-10-02'\").show()\n+scala> sqlContext.sql(\"select count(*) from hudi_trips_mor_rt where datestr = '2016-10-02'\").show()\n ```\n- \n-### Snapshot query {#spark-snapshot-query}\n-Currently, near-real time data can only be queried as a Hive table in Spark using snapshot query mode. In order to do this, set `spark.sql.hive.convertMetastoreParquet=false`, forcing Spark to fallback \n-to using the Hive Serde to read the data (planning/executions is still Spark). \n \n-```java\n-$ spark-shell --jars hudi-spark-bundle_2.11-x.y.z-SNAPSHOT.jar --driver-class-path /etc/hive/conf  --packages org.apache.spark:spark-avro_2.11:2.4.4 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g  --master yarn-client\n+For COPY_ON_WRITE tables, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjU1Mg==", "bodyText": "Remove this section ?", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226552", "createdAt": "2020-02-14T02:43:50Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -145,8 +161,13 @@ Additionally, `HoodieReadClient` offers the following functionality using Hudi's\n | filterExists() | Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication |\n | checkExists(keys) | Check if the provided keys exist in a Hudi table |\n \n+### Read optimized query\n+\n+For read optimized queries, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjYyNA==", "bodyText": "COPY_ON_WRITE: typo", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226624", "createdAt": "2020-02-14T02:44:12Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -145,8 +161,13 @@ Additionally, `HoodieReadClient` offers the following functionality using Hudi's\n | filterExists() | Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication |\n | checkExists(keys) | Check if the provided keys exist in a Hudi table |\n \n+### Read optimized query\n+\n+For read optimized queries, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. \n+If using spark's built in support, additionally a path filter needs to be pushed into sparkContext as described earlier.\n \n ## Presto\n \n-Presto is a popular query engine, providing interactive query performance. Presto currently supports only read optimized queries on Hudi tables. \n-This requires the `hudi-presto-bundle` jar to be placed into `<presto_install>/plugin/hive-hadoop2/`, across the installation.\n+Presto is a popular query engine, providing interactive query performance. Presto currently supports snapshot queries on\n+COPY_On_WRITE and read optimized queries on MERGE_ON_READ Hudi tables. This requires the `hudi-presto-bundle` jar ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU4OTI0NzIw", "url": "https://github.com/apache/hudi/pull/1333#pullrequestreview-358924720", "createdAt": "2020-02-14T13:02:05Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMzowMjowNVrOFp13yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMzowMjowNVrOFp13yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxODU2OA==", "bodyText": "should we also mention the impala?", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379418568", "createdAt": "2020-02-14T13:02:05Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -9,7 +9,7 @@ last_modified_at: 2019-12-30T15:59:57-04:00\n \n Conceptually, Hudi stores data physically once on DFS, while providing 3 different ways of querying, as explained [before](/docs/concepts.html#query-types). \n Once the table is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n-bundle has been provided, the table can be queried by popular query engines like Hive, Spark and Presto.\n+bundle has been provided, the table can be queried by popular query engines like Hive, Spark datasource, Spark SQL and Presto.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "654a4e2e4014d8f0ff7bdba3594122db56c3bf24", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/654a4e2e4014d8f0ff7bdba3594122db56c3bf24", "committedDate": "2020-03-02T18:57:49Z", "message": "[HUDI-589][DOCS] Fix querying_data page\n\n- Added support matrix for COW and MOR tables\n- Change reference from (`views`|`pulls`) to `queries`\n- And minor restructuring"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "654a4e2e4014d8f0ff7bdba3594122db56c3bf24", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/654a4e2e4014d8f0ff7bdba3594122db56c3bf24", "committedDate": "2020-03-02T18:57:49Z", "message": "[HUDI-589][DOCS] Fix querying_data page\n\n- Added support matrix for COW and MOR tables\n- Change reference from (`views`|`pulls`) to `queries`\n- And minor restructuring"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY3NDUyOTE2", "url": "https://github.com/apache/hudi/pull/1333#pullrequestreview-367452916", "createdAt": "2020-03-02T19:09:24Z", "commit": {"oid": "654a4e2e4014d8f0ff7bdba3594122db56c3bf24"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3558, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}