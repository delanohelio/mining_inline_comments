{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ0MjgyODQ1", "number": 2366, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxMDo1NlrOFIVv4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQxMjozNzoxNVrOFI7Kmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0Mjg5MjQ4OnYy", "diffSide": "RIGHT", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxMDo1NlrOIKM08Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxMDo1NlrOIKM08Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NjgzMw==", "bodyText": "hmmm. slightly orthogonal, but the HoodieBaseFile itself should hand us a FileStatus object right? we should probably rethink the need for refreshing file status.", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547566833", "createdAt": "2020-12-23T00:10:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -391,27 +397,48 @@ public static FileStatus getFileStatus(HoodieBaseFile baseFile) throws IOExcepti\n     return grouped;\n   }\n \n+  public static Map<HoodieTableMetaClient, List<Path>> groupSnapshotPathsByMetaClient(\n+          Collection<HoodieTableMetaClient> metaClientList,\n+          List<Path> snapshotPaths\n+  ) {\n+    Map<HoodieTableMetaClient, List<Path>> grouped = new HashMap<>();\n+    metaClientList.forEach(metaClient -> grouped.put(metaClient, new ArrayList<>()));\n+    for (Path path : snapshotPaths) {\n+      // Find meta client associated with the input path\n+      metaClientList.stream().filter(metaClient -> path.toString().contains(metaClient.getBasePath()))\n+              .forEach(metaClient -> grouped.get(metaClient).add(path));\n+    }\n+    return grouped;\n+  }\n+\n   /**\n-   * Filters data files for a snapshot queried table.\n+   * Filters data files under @param paths for a snapshot queried table.\n    * @param job\n-   * @param metadata\n-   * @param fileStatuses\n+   * @param metaClient\n+   * @param paths\n    * @return\n    */\n   public static List<FileStatus> filterFileStatusForSnapshotMode(\n-      JobConf job, HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) throws IOException {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n+          JobConf job, HoodieTableMetaClient metaClient, List<Path> paths) throws IOException {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n+      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metaClient);\n     }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n+\n+    boolean useFileListingFromMetadata = job.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = job.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    HoodieTableFileSystemView fsView = FileSystemViewManager.createInMemoryFileSystemView(metaClient,\n+            useFileListingFromMetadata, verifyFileListing);\n+\n+    List<HoodieBaseFile> filteredBaseFiles = new ArrayList<>();\n+    for (Path p : paths) {\n+      String relativePartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), p);\n+      List<HoodieBaseFile> matched = fsView.getLatestBaseFiles(relativePartitionPath).collect(Collectors.toList());\n+      filteredBaseFiles.addAll(matched);\n+    }\n+\n+    LOG.info(\"Total paths to process after hoodie filter \" + filteredBaseFiles.size());\n     List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n+    for (HoodieBaseFile filteredFile : filteredBaseFiles) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0Mjg5NDE1OnYy", "diffSide": "RIGHT", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxMTo1MVrOIKM11Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxMTo1MVrOIKM11Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NzA2MQ==", "bodyText": "note to self: doing this by path is ok, since the FileSystemView internally caches per partition.", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547567061", "createdAt": "2020-12-23T00:11:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -391,27 +397,48 @@ public static FileStatus getFileStatus(HoodieBaseFile baseFile) throws IOExcepti\n     return grouped;\n   }\n \n+  public static Map<HoodieTableMetaClient, List<Path>> groupSnapshotPathsByMetaClient(\n+          Collection<HoodieTableMetaClient> metaClientList,\n+          List<Path> snapshotPaths\n+  ) {\n+    Map<HoodieTableMetaClient, List<Path>> grouped = new HashMap<>();\n+    metaClientList.forEach(metaClient -> grouped.put(metaClient, new ArrayList<>()));\n+    for (Path path : snapshotPaths) {\n+      // Find meta client associated with the input path\n+      metaClientList.stream().filter(metaClient -> path.toString().contains(metaClient.getBasePath()))\n+              .forEach(metaClient -> grouped.get(metaClient).add(path));\n+    }\n+    return grouped;\n+  }\n+\n   /**\n-   * Filters data files for a snapshot queried table.\n+   * Filters data files under @param paths for a snapshot queried table.\n    * @param job\n-   * @param metadata\n-   * @param fileStatuses\n+   * @param metaClient\n+   * @param paths\n    * @return\n    */\n   public static List<FileStatus> filterFileStatusForSnapshotMode(\n-      JobConf job, HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) throws IOException {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n+          JobConf job, HoodieTableMetaClient metaClient, List<Path> paths) throws IOException {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n+      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metaClient);\n     }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n+\n+    boolean useFileListingFromMetadata = job.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = job.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    HoodieTableFileSystemView fsView = FileSystemViewManager.createInMemoryFileSystemView(metaClient,\n+            useFileListingFromMetadata, verifyFileListing);\n+\n+    List<HoodieBaseFile> filteredBaseFiles = new ArrayList<>();\n+    for (Path p : paths) {\n+      String relativePartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), p);\n+      List<HoodieBaseFile> matched = fsView.getLatestBaseFiles(relativePartitionPath).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0Mjg5ODkxOnYy", "diffSide": "RIGHT", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxNDozN1rOIKM4bA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxNDozN1rOIKM4bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NzcyNA==", "bodyText": "makes sense", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547567724", "createdAt": "2020-12-23T00:14:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "diffHunk": "@@ -63,13 +69,25 @@\n     // TODO(vc): Should we handle also non-hoodie splits here?\n     Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByBasePath(conf, partitionsToParquetSplits.keySet());\n \n+    boolean useFileListingFromMetadata = conf.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = conf.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    // Create file system cache so metadata table is only instantiated once. Also can benefit normal file listing if\n+    // partition path is listed twice so file groups will already be loaded in file system\n+    Map<HoodieTableMetaClient, HoodieTableFileSystemView> fsCache = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0OTAyMjk5OnYy", "diffSide": "RIGHT", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQxMjozNzoxNVrOILG6GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQxMjozNzoxNVrOILG6GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODUxODQyNA==", "bodyText": "These 2 config options is every where, can we just pass the JobConf to the tool method FileSystemViewManager.createInMemoryFileSystemView and fetch them inside the method ? That would make the invocation more clean, IMO.", "url": "https://github.com/apache/hudi/pull/2366#discussion_r548518424", "createdAt": "2020-12-24T12:37:15Z", "author": {"login": "danny0405"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "diffHunk": "@@ -63,13 +69,25 @@\n     // TODO(vc): Should we handle also non-hoodie splits here?\n     Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByBasePath(conf, partitionsToParquetSplits.keySet());\n \n+    boolean useFileListingFromMetadata = conf.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = conf.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "originalPosition": 25}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4050, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}