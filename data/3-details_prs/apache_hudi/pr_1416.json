{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkwMDM5MDAx", "number": 1416, "title": "[HUDI-717] Fixed usage of HiveDriver for DDL statements for Hive 2.x", "bodyText": "What is the purpose of the pull request\nWhen using the HiveDriver mode in HudiHiveClient, Hive 2.x DDL operations like ALTER may fail. This is because Hive 2.x doesn't like db.table_name for operations.\nCommit 129e433 used the first way to fix this only for ALTER PARTITION statement. This is a comprehensive fix which fixes for all statements.\nBrief change log\nWhen using the HiveDriver mode in HudiHiveClient, Hive 2.x DDL operations like ALTER PARTITION may fail. This is because Hive 2.x doesn't like db.table_name for operations.\nIn this fix, we set the name of the database in the SessionState create for the Driver.\nAdded a unit test for the fix as well as additional unit tests for HoodieHiveClient.\nVerify this pull request\nThis pull request is already covered by existing tests, such as TestHiveSyncTool.java\nAdditional unit test cases have been added.\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-03-17T19:40:29Z", "url": "https://github.com/apache/hudi/pull/1416", "merged": true, "mergeCommit": {"oid": "6808559b018366b4bc6d47b40dbbe362f48f65d7"}, "closed": true, "closedAt": "2020-04-03T23:23:06Z", "author": {"login": "prashantwason"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcPOdsXgFqTM3Nzg4NTQ5MQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcUJaGIgFqTM4NzYzMTE2NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3ODg1NDkx", "url": "https://github.com/apache/hudi/pull/1416#pullrequestreview-377885491", "createdAt": "2020-03-19T16:25:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNjoyNTowOFrOF42WVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNjoyNTowOFrOF42WVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE1NTAzMQ==", "bodyText": "How does this work for non hdfs FS like cloud stores ?", "url": "https://github.com/apache/hudi/pull/1416#discussion_r395155031", "createdAt": "2020-03-19T16:25:08Z", "author": {"login": "bvaradar"}, "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -198,7 +198,8 @@ private String getPartitionClause(String partition) {\n     for (String partition : partitions) {\n       String partitionClause = getPartitionClause(partition);\n       Path partitionPath = FSUtils.getPartitionPath(syncConfig.basePath, partition);\n-      String fullPartitionPath = partitionPath.toUri().getScheme().equals(StorageSchemes.HDFS.getScheme())\n+      String partitionScheme = partitionPath.toUri().getScheme();\n+      String fullPartitionPath = StorageSchemes.HDFS.getScheme().equals(partitionScheme)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4MTE1NzA1", "url": "https://github.com/apache/hudi/pull/1416#pullrequestreview-378115705", "createdAt": "2020-03-19T21:44:30Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMTo0NDozMFrOF5Bdew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMTo1NTo1OVrOF5BwCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMzNzA4Mw==", "bodyText": "No, I think it does support cloud stores. This special handling was part of https://jira.apache.org/jira/browse/HUDI-325  to get correct behavior for HDFS  for alter partitions.  This should be fine.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r395337083", "createdAt": "2020-03-19T21:44:30Z", "author": {"login": "bvaradar"}, "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -198,7 +198,8 @@ private String getPartitionClause(String partition) {\n     for (String partition : partitions) {\n       String partitionClause = getPartitionClause(partition);\n       Path partitionPath = FSUtils.getPartitionPath(syncConfig.basePath, partition);\n-      String fullPartitionPath = partitionPath.toUri().getScheme().equals(StorageSchemes.HDFS.getScheme())\n+      String partitionScheme = partitionPath.toUri().getScheme();\n+      String fullPartitionPath = StorageSchemes.HDFS.getScheme().equals(partitionScheme)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE1NTAzMQ=="}, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM0MTgzMg==", "bodyText": "What is the reason for adding 1 to schema fields count ? Is it to account for partition field ? Can you add a comment here.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r395341832", "createdAt": "2020-03-19T21:55:59Z", "author": {"login": "bvaradar"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwMTI5Nzg4", "url": "https://github.com/apache/hudi/pull/1416#pullrequestreview-380129788", "createdAt": "2020-03-24T09:26:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQwOToyNjoyNVrOF6nhnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQwOToyNjoyNVrOF6nhnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzAwOTMwOA==", "bodyText": "Is this message correct ? We only added 1 partition right and thats why count went from 5 => 6 ?", "url": "https://github.com/apache/hudi/pull/1416#discussion_r397009308", "createdAt": "2020-03-24T09:26:25Z", "author": {"login": "umehrot2"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);\n+    assertEquals(\"Table partitions should match the number of partitions we wrote\", 5,\n+        hiveClientRT.scanTablePartitions(snapshotTableName).size());\n+\n+    // Now lets create more partitions and these are the only ones which needs to be synced\n+    DateTime dateTime = DateTime.now().plusDays(6);\n+    String commitTime2 = \"102\";\n+    String deltaCommitTime2 = \"103\";\n+\n+    TestUtil.addCOWPartitions(1, true, dateTime, commitTime2);\n+    TestUtil.addMORPartitions(1, true, false, dateTime, commitTime2, deltaCommitTime2);\n+    // Lets do the sync\n+    tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+    hiveClientRT = new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    // Schema being read from the log files\n+    assertEquals(\"Hive Schema should match the evolved table schema + partition field\",\n+        hiveClientRT.getTableSchema(snapshotTableName).size(), SchemaTestUtil.getEvolvedSchema().getFields().size() + 1);\n+    // Sync should add the one partition\n+    assertEquals(\"The 2 partitions we wrote should be added to hive\", 6, hiveClientRT.scanTablePartitions(snapshotTableName).size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 112}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwMTMzMjg5", "url": "https://github.com/apache/hudi/pull/1416#pullrequestreview-380133289", "createdAt": "2020-03-24T09:30:46Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQwOTozMDo0NlrOF6ns0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQwOTozMDo0NlrOF6ns0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzAxMjE3OQ==", "bodyText": "Is this the only reason to add this test, so that schema is read from base file ? Otherwise this seems exactly like testSyncMergeOnRead ? I guess I am also trying to understand how this test relates to the change you are making in this review.\nOn that note name of the test testSchemeFromMOR can probably be changed to something more meaningful to reflect what this test is trying to achieve differently.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r397012179", "createdAt": "2020-03-24T09:30:46Z", "author": {"login": "umehrot2"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "84b352b19b5cbab44ef4610722d47ee5f88296eb", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/84b352b19b5cbab44ef4610722d47ee5f88296eb", "committedDate": "2020-03-30T20:18:02Z", "message": "[HUDI-717] Fixed usage of HiveDriver for DDL statements.\n\nWhen using the HiveDriver mode in HudiHiveClient, Hive 2.x DDL operations like ALTER PARTITION may fail. This is because Hive 2.x doesn't like `db`.`table_name` for operations.\n\nIn this fix, we set the name of the database in the SessionState create for the Driver.\n\nAdded a unit test for the fix as well as additional unit tests for HoodieHiveClient."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "84b352b19b5cbab44ef4610722d47ee5f88296eb", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/84b352b19b5cbab44ef4610722d47ee5f88296eb", "committedDate": "2020-03-30T20:18:02Z", "message": "[HUDI-717] Fixed usage of HiveDriver for DDL statements.\n\nWhen using the HiveDriver mode in HudiHiveClient, Hive 2.x DDL operations like ALTER PARTITION may fail. This is because Hive 2.x doesn't like `db`.`table_name` for operations.\n\nIn this fix, we set the name of the database in the SessionState create for the Driver.\n\nAdded a unit test for the fix as well as additional unit tests for HoodieHiveClient."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NjMxMTY0", "url": "https://github.com/apache/hudi/pull/1416#pullrequestreview-387631164", "createdAt": "2020-04-03T23:22:13Z", "commit": {"oid": "84b352b19b5cbab44ef4610722d47ee5f88296eb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3943, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}