{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ0MzIzMDc1", "number": 1793, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQwMToxODoyM1rOELa12w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxODowMTo1M1rOENhReA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNDA5NTYzOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQwMToxODoyM1rOGs-0BQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQwMToxODoyM1rOGs-0BQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgxOTY1Mw==", "bodyText": "It seems that we don't need to unseal() as the sealed is false when using\n\n  \n    \n      hudi/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java\n    \n    \n        Lines 67 to 73\n      in\n      574dcf9\n    \n    \n    \n    \n\n        \n          \n            \n        \n\n        \n          \n           public HoodieRecord(HoodieKey key, T data) { \n        \n\n        \n          \n             this.key = key; \n        \n\n        \n          \n             this.data = data; \n        \n\n        \n          \n             this.currentLocation = null; \n        \n\n        \n          \n             this.newLocation = null; \n        \n\n        \n          \n             this.sealed = false;", "url": "https://github.com/apache/hudi/pull/1793#discussion_r449819653", "createdAt": "2020-07-05T01:18:23Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java", "diffHunk": "@@ -125,6 +125,9 @@ public HoodieGlobalBloomIndex(HoodieWriteConfig config) {\n           // Create an empty record to delete the record in the old partition\n           HoodieRecord<T> emptyRecord = new HoodieRecord(recordLocationHoodieKeyPair.get()._2,\n               new EmptyHoodieRecordPayload());\n+          emptyRecord.unseal();\n+          emptyRecord.setCurrentLocation(recordLocationHoodieKeyPair.get()._1());\n+          emptyRecord.seal();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNDA5NTY1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/simple/HoodieGlobalSimpleIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQwMToxODozNlrOGs-0Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQwMToxODozNlrOGs-0Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgxOTY1NQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1793#discussion_r449819655", "createdAt": "2020-07-05T01:18:36Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/simple/HoodieGlobalSimpleIndex.java", "diffHunk": "@@ -131,6 +131,9 @@ public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n             if (config.getGlobalSimpleIndexUpdatePartitionPath() && !(inputRecord.getPartitionPath().equals(partitionPath))) {\n               // Create an empty record to delete the record in the old partition\n               HoodieRecord<T> emptyRecord = new HoodieRecord(new HoodieKey(inputRecord.getRecordKey(), partitionPath), new EmptyHoodieRecordPayload());\n+              emptyRecord.unseal();\n+              emptyRecord.setCurrentLocation(location);\n+              emptyRecord.seal();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNDA5NTc5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQwMToxOToxOVrOGs-0GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQwMToxOToxOVrOGs-0GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgxOTY3Mg==", "bodyText": "minor: @throws Exception looks redundant without further info", "url": "https://github.com/apache/hudi/pull/1793#discussion_r449819672", "createdAt": "2020-07-05T01:19:19Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,173 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition\n+   * is deleted appropriately.\n+   * @throws Exception", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNjUwMDUwOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxMTozNDozN1rOGtTnoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxMTozNToxM1rOGtToqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MDU0NQ==", "bodyText": "lets rename this to deleteRecord", "url": "https://github.com/apache/hudi/pull/1793#discussion_r450160545", "createdAt": "2020-07-06T11:34:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java", "diffHunk": "@@ -125,6 +125,9 @@ public HoodieGlobalBloomIndex(HoodieWriteConfig config) {\n           // Create an empty record to delete the record in the old partition\n           HoodieRecord<T> emptyRecord = new HoodieRecord(recordLocationHoodieKeyPair.get()._2,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MDgwOQ==", "bodyText": "and taggedRecord to insert..  this makes what this block is doing clearer..", "url": "https://github.com/apache/hudi/pull/1793#discussion_r450160809", "createdAt": "2020-07-06T11:35:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java", "diffHunk": "@@ -125,6 +125,9 @@ public HoodieGlobalBloomIndex(HoodieWriteConfig config) {\n           // Create an empty record to delete the record in the old partition\n           HoodieRecord<T> emptyRecord = new HoodieRecord(recordLocationHoodieKeyPair.get()._2,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MDU0NQ=="}, "originalCommit": null, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNjUwNTQ2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxMTozNjoxMFrOGtTqeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxMjowMzo0NlrOGt7bFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MTI3NA==", "bodyText": "why is this relevant for this test?", "url": "https://github.com/apache/hudi/pull/1793#discussion_r450161274", "createdAt": "2020-07-06T11:36:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,173 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition\n+   * is deleted appropriately.\n+   * @throws Exception\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in\n+   * old partition is deleted appropriately.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType,\n+      HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // Force using older timeline layout", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDgxMjY5NQ==", "bodyText": "I don't know actually. I tried removing version_0 and the test fails. If we know why testUpsertsInternal in the same class does it (@bvaradar ), we might have an answer.", "url": "https://github.com/apache/hudi/pull/1793#discussion_r450812695", "createdAt": "2020-07-07T12:03:46Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,173 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition\n+   * is deleted appropriately.\n+   * @throws Exception\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in\n+   * old partition is deleted appropriately.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType,\n+      HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // Force using older timeline layout", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MTI3NA=="}, "originalCommit": null, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNjUwOTk5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQxMTozNzo0MlrOGtTtGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QxMjowMToyMVrOGt7V9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MTk0NA==", "bodyText": "can. we explicitly check for duplicates?", "url": "https://github.com/apache/hudi/pull/1793#discussion_r450161944", "createdAt": "2020-07-06T11:37:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,173 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition\n+   * is deleted appropriately.\n+   * @throws Exception\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in\n+   * old partition is deleted appropriately.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType,\n+      HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // Force using older timeline layout\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true).build())\n+        .withTimelineLayoutVersion(\n+            VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n \n     // Write 1 (only inserts)\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n+    // Check the entire dataset has all records still\n     String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+    Dataset<Row> rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    List<Pair<String, String>> actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n+\n+    // Check the entire dataset has all records still\n+    fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n+    for (int i = 0; i < fullPartitionPaths.length; i++) {\n+      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    }\n+\n+    rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    // Write 2 (updates)\n+    newCommitTime = \"003\";\n+    records = records.subList(5, 10);\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+      String partitionPath = rec.getPartitionPath();\n+      String newPartitionPath = null;\n+      if (partitionPath.equalsIgnoreCase(DEFAULT_FIRST_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_SECOND_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_SECOND_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_THIRD_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_THIRD_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_FIRST_PARTITION_PATH;\n+      } else {\n+        throw new IllegalStateException(\"Unknown partition path \" + rec.getPartitionPath());\n+      }\n+      recordsToUpsert.add(\n+          new HoodieRecord(new HoodieKey(rec.getRecordKey(), newPartitionPath),\n+              rec.getData()));\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(newPartitionPath, rec.getRecordKey()));\n+    }\n+\n+    writeRecords = jsc.parallelize(recordsToUpsert, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    statuses = result.collect();\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(updates1, fs);\n     // Check the entire dataset has all records still\n     fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+\n+    rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n+\n+    // verify all partitionpath, record key matches", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MjE3NA==", "bodyText": "and also atleast two files being present before triggering the update of the partition path", "url": "https://github.com/apache/hudi/pull/1793#discussion_r450162174", "createdAt": "2020-07-06T11:38:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,173 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition\n+   * is deleted appropriately.\n+   * @throws Exception\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in\n+   * old partition is deleted appropriately.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType,\n+      HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // Force using older timeline layout\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true).build())\n+        .withTimelineLayoutVersion(\n+            VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n \n     // Write 1 (only inserts)\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n+    // Check the entire dataset has all records still\n     String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+    Dataset<Row> rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    List<Pair<String, String>> actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n+\n+    // Check the entire dataset has all records still\n+    fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n+    for (int i = 0; i < fullPartitionPaths.length; i++) {\n+      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    }\n+\n+    rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    // Write 2 (updates)\n+    newCommitTime = \"003\";\n+    records = records.subList(5, 10);\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+      String partitionPath = rec.getPartitionPath();\n+      String newPartitionPath = null;\n+      if (partitionPath.equalsIgnoreCase(DEFAULT_FIRST_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_SECOND_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_SECOND_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_THIRD_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_THIRD_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_FIRST_PARTITION_PATH;\n+      } else {\n+        throw new IllegalStateException(\"Unknown partition path \" + rec.getPartitionPath());\n+      }\n+      recordsToUpsert.add(\n+          new HoodieRecord(new HoodieKey(rec.getRecordKey(), newPartitionPath),\n+              rec.getData()));\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(newPartitionPath, rec.getRecordKey()));\n+    }\n+\n+    writeRecords = jsc.parallelize(recordsToUpsert, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    statuses = result.collect();\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(updates1, fs);\n     // Check the entire dataset has all records still\n     fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+\n+    rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n+\n+    // verify all partitionpath, record key matches", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MTk0NA=="}, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDgwOTYwMg==", "bodyText": "I have refactored the test now. You can check it out. have added assertions to verify basefile counts.", "url": "https://github.com/apache/hudi/pull/1793#discussion_r450809602", "createdAt": "2020-07-07T11:57:59Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,173 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition\n+   * is deleted appropriately.\n+   * @throws Exception\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in\n+   * old partition is deleted appropriately.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType,\n+      HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // Force using older timeline layout\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true).build())\n+        .withTimelineLayoutVersion(\n+            VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n \n     // Write 1 (only inserts)\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n+    // Check the entire dataset has all records still\n     String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+    Dataset<Row> rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    List<Pair<String, String>> actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n+\n+    // Check the entire dataset has all records still\n+    fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n+    for (int i = 0; i < fullPartitionPaths.length; i++) {\n+      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    }\n+\n+    rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    // Write 2 (updates)\n+    newCommitTime = \"003\";\n+    records = records.subList(5, 10);\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+      String partitionPath = rec.getPartitionPath();\n+      String newPartitionPath = null;\n+      if (partitionPath.equalsIgnoreCase(DEFAULT_FIRST_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_SECOND_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_SECOND_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_THIRD_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_THIRD_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_FIRST_PARTITION_PATH;\n+      } else {\n+        throw new IllegalStateException(\"Unknown partition path \" + rec.getPartitionPath());\n+      }\n+      recordsToUpsert.add(\n+          new HoodieRecord(new HoodieKey(rec.getRecordKey(), newPartitionPath),\n+              rec.getData()));\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(newPartitionPath, rec.getRecordKey()));\n+    }\n+\n+    writeRecords = jsc.parallelize(recordsToUpsert, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    statuses = result.collect();\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(updates1, fs);\n     // Check the entire dataset has all records still\n     fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+\n+    rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n+\n+    // verify all partitionpath, record key matches", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MTk0NA=="}, "originalCommit": null, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDgxMTM4MA==", "bodyText": "We do assert with expected entries count. so duplicates should be taken care of in that assertion.", "url": "https://github.com/apache/hudi/pull/1793#discussion_r450811380", "createdAt": "2020-07-07T12:01:21Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,173 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition\n+   * is deleted appropriately.\n+   * @throws Exception\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in\n+   * old partition is deleted appropriately.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType,\n+      HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // Force using older timeline layout\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true).build())\n+        .withTimelineLayoutVersion(\n+            VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n \n     // Write 1 (only inserts)\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n+    // Check the entire dataset has all records still\n     String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+    Dataset<Row> rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    List<Pair<String, String>> actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n+\n+    // Check the entire dataset has all records still\n+    fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n+    for (int i = 0; i < fullPartitionPaths.length; i++) {\n+      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    }\n+\n+    rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    // Write 2 (updates)\n+    newCommitTime = \"003\";\n+    records = records.subList(5, 10);\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+      String partitionPath = rec.getPartitionPath();\n+      String newPartitionPath = null;\n+      if (partitionPath.equalsIgnoreCase(DEFAULT_FIRST_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_SECOND_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_SECOND_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_THIRD_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_THIRD_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_FIRST_PARTITION_PATH;\n+      } else {\n+        throw new IllegalStateException(\"Unknown partition path \" + rec.getPartitionPath());\n+      }\n+      recordsToUpsert.add(\n+          new HoodieRecord(new HoodieKey(rec.getRecordKey(), newPartitionPath),\n+              rec.getData()));\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(newPartitionPath, rec.getRecordKey()));\n+    }\n+\n+    writeRecords = jsc.parallelize(recordsToUpsert, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    statuses = result.collect();\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(updates1, fs);\n     // Check the entire dataset has all records still\n     fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+\n+    rows = HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+    actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n+\n+    // verify all partitionpath, record key matches", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDE2MTk0NA=="}, "originalCommit": null, "originalPosition": 217}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjA5MDE1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzoyMTo0NFrOGwOFmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzoyMTo0NFrOGwOFmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxNTY0Mw==", "bodyText": "nit: typo insertRecord", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453215643", "createdAt": "2020-07-11T17:21:44Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java", "diffHunk": "@@ -123,11 +123,13 @@ public HoodieGlobalBloomIndex(HoodieWriteConfig config) {\n         if (config.getBloomIndexUpdatePartitionPath()\n             && !recordLocationHoodieKeyPair.get()._2.getPartitionPath().equals(hoodieRecord.getPartitionPath())) {\n           // Create an empty record to delete the record in the old partition\n-          HoodieRecord<T> emptyRecord = new HoodieRecord(recordLocationHoodieKeyPair.get()._2,\n+          HoodieRecord<T> deleteRecord = new HoodieRecord(recordLocationHoodieKeyPair.get()._2,\n               new EmptyHoodieRecordPayload());\n+          deleteRecord.setCurrentLocation(recordLocationHoodieKeyPair.get()._1());\n+          deleteRecord.seal();\n           // Tag the incoming record for inserting to the new partition\n-          HoodieRecord<T> taggedRecord = HoodieIndexUtils.getTaggedRecord(hoodieRecord, Option.empty());\n-          return Arrays.asList(emptyRecord, taggedRecord).iterator();\n+          HoodieRecord<T> insetRecord = HoodieIndexUtils.getTaggedRecord(hoodieRecord, Option.empty());\n+          return Arrays.asList(deleteRecord, insetRecord).iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjA5MTAyOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzoyMzoxMVrOGwOGAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzoyMzoxMVrOGwOGAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxNTc0Ng==", "bodyText": "these 2 can be combined into a @ParameterizedTest with IndexType as argument", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453215746", "createdAt": "2020-07-11T17:23:11Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjEwMDkyOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzozNTo1M1rOGwOKrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzozNTo1M1rOGwOKrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxNjk0Mw==", "bodyText": "assertAll() works better for iterating assertions", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453216943", "createdAt": "2020-07-11T17:35:53Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjEwMTk3OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzozODowMVrOGwOLPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNDo1OTo1NFrOGwsfCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxNzA4NQ==", "bodyText": "result.collect() return value not used?", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453217085", "createdAt": "2020-07-11T17:38:01Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzcxMzY3NA==", "bodyText": "yes, we just need to trigger the action.", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453713674", "createdAt": "2020-07-13T14:59:54Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxNzA4NQ=="}, "originalCommit": null, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjEwODQzOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzo0NTo0NlrOGwOOPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzo0NTo0NlrOGwOOPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxNzg1Mg==", "bodyText": "these lines can be simplified to assertTrue(entrySet().stream().filter().count()>=1)", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453217852", "createdAt": "2020-07-11T17:45:46Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify that there are more than 1 basefiles per partition\n+    // we can't guarantee randomness in partitions where records are distributed. So, verify atleast one partition has more than 1 basefile.\n+    baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    boolean hasMoreThanOneBaseFile = false;\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      if (entry.getValue() > 1) {\n+        hasMoreThanOneBaseFile = true;\n+        break;\n+      }\n+    }\n+    assertTrue(hasMoreThanOneBaseFile, \"Atleast one partition should have more than 1 base file after 2nd batch of writes\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjExMzY5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzo1MjoxMlrOGwOQkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMjoyNjowMFrOGxBm8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxODQ0OQ==", "bodyText": "can this be replaced by using a new variable for expectedPartitionPathRecKeyPairs? a bit hard to track what is contained.", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453218449", "createdAt": "2020-07-11T17:52:12Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify that there are more than 1 basefiles per partition\n+    // we can't guarantee randomness in partitions where records are distributed. So, verify atleast one partition has more than 1 basefile.\n+    baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    boolean hasMoreThanOneBaseFile = false;\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      if (entry.getValue() > 1) {\n+        hasMoreThanOneBaseFile = true;\n+        break;\n+      }\n+    }\n+    assertTrue(hasMoreThanOneBaseFile, \"Atleast one partition should have more than 1 base file after 2nd batch of writes\");\n+\n+    // Write 3 (upserts to records from batch 1 with diff partition path)\n+    newCommitTime = \"003\";\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      // remove older entry from expected partition path record key pairs\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzcxNTkwNQ==", "bodyText": "don't get you. This tracks all partitionpath record key pairs so far. And in this block, for records for which partition path is getting updated, we need to remove those entries from this expected list and add updated entries. So, don't quite understand how introducing new variable will simplify things. would you mind clarifying.", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453715905", "createdAt": "2020-07-13T15:02:53Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify that there are more than 1 basefiles per partition\n+    // we can't guarantee randomness in partitions where records are distributed. So, verify atleast one partition has more than 1 basefile.\n+    baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    boolean hasMoreThanOneBaseFile = false;\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      if (entry.getValue() > 1) {\n+        hasMoreThanOneBaseFile = true;\n+        break;\n+      }\n+    }\n+    assertTrue(hasMoreThanOneBaseFile, \"Atleast one partition should have more than 1 base file after 2nd batch of writes\");\n+\n+    // Write 3 (upserts to records from batch 1 with diff partition path)\n+    newCommitTime = \"003\";\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      // remove older entry from expected partition path record key pairs\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxODQ0OQ=="}, "originalCommit": null, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1OTc2MA==", "bodyText": "ah ok i see. thanks for clarifying", "url": "https://github.com/apache/hudi/pull/1793#discussion_r454059760", "createdAt": "2020-07-14T02:26:00Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify that there are more than 1 basefiles per partition\n+    // we can't guarantee randomness in partitions where records are distributed. So, verify atleast one partition has more than 1 basefile.\n+    baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    boolean hasMoreThanOneBaseFile = false;\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      if (entry.getValue() > 1) {\n+        hasMoreThanOneBaseFile = true;\n+        break;\n+      }\n+    }\n+    assertTrue(hasMoreThanOneBaseFile, \"Atleast one partition should have more than 1 base file after 2nd batch of writes\");\n+\n+    // Write 3 (upserts to records from batch 1 with diff partition path)\n+    newCommitTime = \"003\";\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      // remove older entry from expected partition path record key pairs\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxODQ0OQ=="}, "originalCommit": null, "originalPosition": 195}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjExODQ3OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxNzo1ODoyOFrOGwOSwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNToxNToyNFrOGwtK3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxOTAwOQ==", "bodyText": "Looks like Set<Pair<String, String>> is better suited for expectedPartitionPathRecKeyPairs? so these can be simplified as assertEquals(expected, actual);", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453219009", "createdAt": "2020-07-11T17:58:28Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify that there are more than 1 basefiles per partition\n+    // we can't guarantee randomness in partitions where records are distributed. So, verify atleast one partition has more than 1 basefile.\n+    baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    boolean hasMoreThanOneBaseFile = false;\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      if (entry.getValue() > 1) {\n+        hasMoreThanOneBaseFile = true;\n+        break;\n+      }\n+    }\n+    assertTrue(hasMoreThanOneBaseFile, \"Atleast one partition should have more than 1 base file after 2nd batch of writes\");\n+\n+    // Write 3 (upserts to records from batch 1 with diff partition path)\n+    newCommitTime = \"003\";\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      // remove older entry from expected partition path record key pairs\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+      String partitionPath = rec.getPartitionPath();\n+      String newPartitionPath = null;\n+      if (partitionPath.equalsIgnoreCase(DEFAULT_FIRST_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_SECOND_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_SECOND_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_THIRD_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_THIRD_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_FIRST_PARTITION_PATH;\n+      } else {\n+        throw new IllegalStateException(\"Unknown partition path \" + rec.getPartitionPath());\n+      }\n+      recordsToUpsert.add(\n+          new HoodieRecord(new HoodieKey(rec.getRecordKey(), newPartitionPath),\n+              rec.getData()));\n+      // populate expected partition path and record keys\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(newPartitionPath, rec.getRecordKey()));\n+    }\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(updates1, fs);\n-    // Check the entire dataset has all records still\n-    fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n+    writeRecords = jsc.parallelize(recordsToUpsert, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    statuses = result.collect();\n+\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+  }\n+\n+  private void assertPartitionPathRecordKeys(String[] fullPartitionPaths, List<Pair<String, String>> expectedPartitionPathRecKeyPairs) {\n+    Dataset<Row> rows = getAllRows(fullPartitionPaths);\n+    List<Pair<String, String>> actualPartitionPathRecKeyPairs = getActualPartitionPathAndRecordKeys(rows);\n+    // verify all partitionpath, record key matches\n+    assertActualAndExpectedPartitionPathRecordKeyMatches(expectedPartitionPathRecKeyPairs, actualPartitionPathRecKeyPairs);\n+  }\n+\n+  private List<Pair<String, String>> getActualPartitionPathAndRecordKeys(Dataset<org.apache.spark.sql.Row> rows) {\n+    List<Pair<String, String>> actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n+    return actualPartitionPathRecKeyPairs;\n+  }\n+\n+  private Dataset<org.apache.spark.sql.Row> getAllRows(String[] fullPartitionPaths) {\n+    return HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+  }\n+\n+  private String[] getFullPartitionPaths() {\n+    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+    return fullPartitionPaths;\n+  }\n+\n+  private Map<String, Integer> getBaseFileCounts(String[] fullPartitionPaths) {\n+    return HoodieClientTestUtils.getBaseFileCountForPaths(basePath, fs, fullPartitionPaths);\n+  }\n+\n+  private void assertActualAndExpectedPartitionPathRecordKeyMatches(List<Pair<String, String>> expectedPartitionPathRecKeyPairs,\n+      List<Pair<String, String>> actualPartitionPathRecKeyPairs) {\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzcyNDg5Mg==", "bodyText": "I will fix the expected entries to be a Set, but can't fix the actual. we want to capture any duplicates if any. So, can't really use assertEquals(set, list).", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453724892", "createdAt": "2020-07-13T15:15:24Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify that there are more than 1 basefiles per partition\n+    // we can't guarantee randomness in partitions where records are distributed. So, verify atleast one partition has more than 1 basefile.\n+    baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    boolean hasMoreThanOneBaseFile = false;\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      if (entry.getValue() > 1) {\n+        hasMoreThanOneBaseFile = true;\n+        break;\n+      }\n+    }\n+    assertTrue(hasMoreThanOneBaseFile, \"Atleast one partition should have more than 1 base file after 2nd batch of writes\");\n+\n+    // Write 3 (upserts to records from batch 1 with diff partition path)\n+    newCommitTime = \"003\";\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      // remove older entry from expected partition path record key pairs\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+      String partitionPath = rec.getPartitionPath();\n+      String newPartitionPath = null;\n+      if (partitionPath.equalsIgnoreCase(DEFAULT_FIRST_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_SECOND_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_SECOND_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_THIRD_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_THIRD_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_FIRST_PARTITION_PATH;\n+      } else {\n+        throw new IllegalStateException(\"Unknown partition path \" + rec.getPartitionPath());\n+      }\n+      recordsToUpsert.add(\n+          new HoodieRecord(new HoodieKey(rec.getRecordKey(), newPartitionPath),\n+              rec.getData()));\n+      // populate expected partition path and record keys\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(newPartitionPath, rec.getRecordKey()));\n+    }\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(updates1, fs);\n-    // Check the entire dataset has all records still\n-    fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n+    writeRecords = jsc.parallelize(recordsToUpsert, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    statuses = result.collect();\n+\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+  }\n+\n+  private void assertPartitionPathRecordKeys(String[] fullPartitionPaths, List<Pair<String, String>> expectedPartitionPathRecKeyPairs) {\n+    Dataset<Row> rows = getAllRows(fullPartitionPaths);\n+    List<Pair<String, String>> actualPartitionPathRecKeyPairs = getActualPartitionPathAndRecordKeys(rows);\n+    // verify all partitionpath, record key matches\n+    assertActualAndExpectedPartitionPathRecordKeyMatches(expectedPartitionPathRecKeyPairs, actualPartitionPathRecKeyPairs);\n+  }\n+\n+  private List<Pair<String, String>> getActualPartitionPathAndRecordKeys(Dataset<org.apache.spark.sql.Row> rows) {\n+    List<Pair<String, String>> actualPartitionPathRecKeyPairs = new ArrayList<>();\n+    for (Row row : rows.collectAsList()) {\n+      actualPartitionPathRecKeyPairs\n+          .add(Pair.of(row.getAs(\"_hoodie_partition_path\"), row.getAs(\"_row_key\")));\n+    }\n+    return actualPartitionPathRecKeyPairs;\n+  }\n+\n+  private Dataset<org.apache.spark.sql.Row> getAllRows(String[] fullPartitionPaths) {\n+    return HoodieClientTestUtils\n+        .read(jsc, basePath, sqlContext, fs, fullPartitionPaths);\n+  }\n+\n+  private String[] getFullPartitionPaths() {\n+    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n     for (int i = 0; i < fullPartitionPaths.length; i++) {\n       fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n+    return fullPartitionPaths;\n+  }\n+\n+  private Map<String, Integer> getBaseFileCounts(String[] fullPartitionPaths) {\n+    return HoodieClientTestUtils.getBaseFileCountForPaths(basePath, fs, fullPartitionPaths);\n+  }\n+\n+  private void assertActualAndExpectedPartitionPathRecordKeyMatches(List<Pair<String, String>> expectedPartitionPathRecKeyPairs,\n+      List<Pair<String, String>> actualPartitionPathRecKeyPairs) {\n+    // verify all partitionpath, record key matches\n+    assertEquals(expectedPartitionPathRecKeyPairs.size(), actualPartitionPathRecKeyPairs.size());\n+    for (Pair<String, String> entry : actualPartitionPathRecKeyPairs) {\n+      assertTrue(expectedPartitionPathRecKeyPairs.contains(entry));\n+    }\n+\n+    for (Pair<String, String> entry : expectedPartitionPathRecKeyPairs) {\n+      assertTrue(actualPartitionPathRecKeyPairs.contains(entry));\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxOTAwOQ=="}, "originalCommit": null, "originalPosition": 272}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjEyMDg4OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxODowMTo1M1rOGwOT1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQxODowMTo1M1rOGwOT1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIxOTI4NQ==", "bodyText": "to align with junit assertions, can we make the expected variable 1st argument?", "url": "https://github.com/apache/hudi/pull/1793#discussion_r453219285", "createdAt": "2020-07-11T18:01:53Z", "author": {"login": "xushiyan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -399,56 +405,190 @@ public void testDeletes() throws Exception {\n   }\n \n   /**\n-   * Test update of a record to different partition with Global Index.\n+   * Tests when update partition path is set in global bloom, existing record in old partition is deleted appropriately.\n    */\n   @Test\n-  public void testUpsertToDiffPartitionGlobalIndex() throws Exception {\n-    HoodieWriteClient client = getHoodieWriteClient(getConfig(IndexType.GLOBAL_BLOOM), false);\n-    /**\n-     * Write 1 (inserts and deletes) Write actual 200 insert records and ignore 100 delete records\n-     */\n-    String newCommitTime = \"001\";\n-    List<HoodieRecord> inserts1 = dataGen.generateInserts(newCommitTime, 100);\n+  public void testUpsertsUpdatePartitionPathRegularGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_BLOOM, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n \n-    // Write 1 (only inserts)\n+  /**\n+   * Tests when update partition path is set in simple global bloom, existing record in old partition is deleted appropriately.\n+   */\n+  @Test\n+  public void testUpsertsUpdatePartitionPathSimpleGlobalBloom() throws Exception {\n+    testUpsertsUpdatePartitionPathGlobalBloom(IndexType.GLOBAL_SIMPLE, getConfig(),\n+        HoodieWriteClient::upsert);\n+  }\n+\n+  /**\n+   * This test ensures in a global bloom when update partition path is set to true in config, if an incoming record has mismatched partition\n+   * compared to whats in storage, then appropriate actions are taken. i.e. old record is deleted in old partition and new one is inserted\n+   * in the new partition.\n+   * test structure:\n+   * 1. insert 1 batch\n+   * 2. insert 2nd batch with larger no of records so that a new file group is created for partitions\n+   * 3. issue upserts to records from batch 1 with different partition path. This should ensure records from batch 1 are deleted and new\n+   * records are upserted to the new partition\n+   *\n+   * @param indexType index type to be tested for\n+   * @param config instance of {@link HoodieWriteConfig} to use\n+   * @param writeFn write function to be used for testing\n+   */\n+  private void testUpsertsUpdatePartitionPathGlobalBloom(IndexType indexType, HoodieWriteConfig config,\n+      Function3<JavaRDD<WriteStatus>, HoodieWriteClient, JavaRDD<HoodieRecord>, String> writeFn)\n+      throws Exception {\n+    // instantiate client\n+\n+    HoodieWriteConfig hoodieWriteConfig = getConfigBuilder()\n+        .withProps(config.getProps())\n+        .withCompactionConfig(\n+            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(10000).build())\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(indexType)\n+            .withBloomIndexUpdatePartitionPath(true)\n+            .withGlobalSimpleIndexUpdatePartitionPath(true)\n+            .build()).withTimelineLayoutVersion(VERSION_0).build();\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(), metaClient.getTableType(),\n+        metaClient.getTableConfig().getTableName(), metaClient.getArchivePath(),\n+        metaClient.getTableConfig().getPayloadClass(), VERSION_0);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Write 1\n+    String newCommitTime = \"001\";\n+    int numRecords = 10;\n     client.startCommitWithTime(newCommitTime);\n-    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(inserts1, 1);\n \n-    JavaRDD<WriteStatus> result = client.insert(writeRecords, newCommitTime);\n+    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n+    List<Pair<String, String>> expectedPartitionPathRecKeyPairs = new ArrayList<>();\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : records) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n+    JavaRDD<WriteStatus> result = writeFn.apply(client, writeRecords, newCommitTime);\n     List<WriteStatus> statuses = result.collect();\n-    assertNoWriteErrors(statuses);\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(inserts1, fs);\n-    String[] fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n-    for (int i = 0; i < fullPartitionPaths.length; i++) {\n-      fullPartitionPaths[i] = String.format(\"%s/%s/*\", basePath, dataGen.getPartitionPaths()[i]);\n+    // Check the entire dataset has all records\n+    String[] fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify one basefile per partition\n+    Map<String, Integer> baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      assertEquals(1, entry.getValue());\n     }\n-    assertEquals(100, HoodieClientTestUtils.read(jsc, basePath, sqlContext, fs, fullPartitionPaths).count(),\n-        \"Must contain 100 records\");\n \n-    /**\n-     * Write 2. Updates with different partition\n-     */\n-    newCommitTime = \"004\";\n+    // Write 2\n+    newCommitTime = \"002\";\n+    numRecords = 20; // so that a new file id is created\n     client.startCommitWithTime(newCommitTime);\n \n-    List<HoodieRecord> updates1 = dataGen.generateUpdatesWithDiffPartition(newCommitTime, inserts1);\n-    JavaRDD<HoodieRecord> updateRecords = jsc.parallelize(updates1, 1);\n+    List<HoodieRecord> recordsSecondBatch = dataGen.generateInserts(newCommitTime, numRecords);\n+    // populate expected partition path and record keys\n+    for (HoodieRecord rec : recordsSecondBatch) {\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+    }\n+    writeRecords = jsc.parallelize(recordsSecondBatch, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    result.collect();\n \n-    JavaRDD<WriteStatus> result1 = client.upsert(updateRecords, newCommitTime);\n-    List<WriteStatus> statuses1 = result1.collect();\n-    assertNoWriteErrors(statuses1);\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+\n+    // verify that there are more than 1 basefiles per partition\n+    // we can't guarantee randomness in partitions where records are distributed. So, verify atleast one partition has more than 1 basefile.\n+    baseFileCounts = getBaseFileCounts(fullPartitionPaths);\n+    boolean hasMoreThanOneBaseFile = false;\n+    for (Map.Entry<String, Integer> entry : baseFileCounts.entrySet()) {\n+      if (entry.getValue() > 1) {\n+        hasMoreThanOneBaseFile = true;\n+        break;\n+      }\n+    }\n+    assertTrue(hasMoreThanOneBaseFile, \"Atleast one partition should have more than 1 base file after 2nd batch of writes\");\n+\n+    // Write 3 (upserts to records from batch 1 with diff partition path)\n+    newCommitTime = \"003\";\n+\n+    // update to diff partition paths\n+    List<HoodieRecord> recordsToUpsert = new ArrayList<>();\n+    for (HoodieRecord rec : records) {\n+      // remove older entry from expected partition path record key pairs\n+      expectedPartitionPathRecKeyPairs\n+          .remove(Pair.of(rec.getPartitionPath(), rec.getRecordKey()));\n+      String partitionPath = rec.getPartitionPath();\n+      String newPartitionPath = null;\n+      if (partitionPath.equalsIgnoreCase(DEFAULT_FIRST_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_SECOND_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_SECOND_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_THIRD_PARTITION_PATH;\n+      } else if (partitionPath.equalsIgnoreCase(DEFAULT_THIRD_PARTITION_PATH)) {\n+        newPartitionPath = DEFAULT_FIRST_PARTITION_PATH;\n+      } else {\n+        throw new IllegalStateException(\"Unknown partition path \" + rec.getPartitionPath());\n+      }\n+      recordsToUpsert.add(\n+          new HoodieRecord(new HoodieKey(rec.getRecordKey(), newPartitionPath),\n+              rec.getData()));\n+      // populate expected partition path and record keys\n+      expectedPartitionPathRecKeyPairs.add(Pair.of(newPartitionPath, rec.getRecordKey()));\n+    }\n \n-    // check the partition metadata is written out\n-    assertPartitionMetadataForRecords(updates1, fs);\n-    // Check the entire dataset has all records still\n-    fullPartitionPaths = new String[dataGen.getPartitionPaths().length];\n+    writeRecords = jsc.parallelize(recordsToUpsert, 1);\n+    result = writeFn.apply(client, writeRecords, newCommitTime);\n+    statuses = result.collect();\n+\n+    // Check the entire dataset has all records\n+    fullPartitionPaths = getFullPartitionPaths();\n+    assertPartitionPathRecordKeys(fullPartitionPaths, expectedPartitionPathRecKeyPairs);\n+  }\n+\n+  private void assertPartitionPathRecordKeys(String[] fullPartitionPaths, List<Pair<String, String>> expectedPartitionPathRecKeyPairs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 227}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4531, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}