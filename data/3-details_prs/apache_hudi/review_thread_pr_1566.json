{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA5MTUwMzQ5", "number": 1566, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QyMzozMzo0OVrOD28b3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMzoyNzo0MFrOEqIX6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4OTM5ODcwOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QyMzozMzo0OVrOGM8EFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxNToxMTo1N1rOGNZBSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIyMDE4Mg==", "bodyText": "@pratyakshsharma : It looks like refreshSchemaProvider  not only refreshes schema-provider but also recreates Source and setup WriteClient\n@vinothchandar : Recreating DeltaSync each run would require to handle embedded timeline server reuse and async compaction triggering differently.  Another option is to have explicit refreshSchema() API in SchemaProvider (with default implementation (for compatibility) and implementing refresh in existing Schema Provider implementation) and have delta-streamer call this ? Let me know your thoughts on this ?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r416220182", "createdAt": "2020-04-27T23:33:49Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -162,18 +162,23 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche\n     this.fs = fs;\n     this.onInitializingHoodieWriteClient = onInitializingHoodieWriteClient;\n     this.props = props;\n-    this.schemaProvider = schemaProvider;\n \n     refreshTimeline();\n-\n     this.transformer = UtilHelpers.createTransformer(cfg.transformerClassNames);\n     this.keyGenerator = DataSourceUtils.createKeyGenerator(props);\n-\n-    this.formatAdapter = new SourceFormatAdapter(\n-        UtilHelpers.createSource(cfg.sourceClassName, props, jssc, sparkSession, schemaProvider));\n-\n     this.conf = conf;\n+    refreshSchemaProvider(schemaProvider);\n+  }\n \n+  /**\n+   * Very useful when DeltaStreamer is running in continuous mode.\n+   * @param schemaProvider\n+   * @throws IOException\n+   */\n+  public void refreshSchemaProvider(SchemaProvider schemaProvider) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dfaa70dc3bddebe875d1c194ac8916b260d0a3ba"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxOTA2NQ==", "bodyText": "right.. embedded server re-use is important. let's scratch my idea.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r416319065", "createdAt": "2020-04-28T04:33:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -162,18 +162,23 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche\n     this.fs = fs;\n     this.onInitializingHoodieWriteClient = onInitializingHoodieWriteClient;\n     this.props = props;\n-    this.schemaProvider = schemaProvider;\n \n     refreshTimeline();\n-\n     this.transformer = UtilHelpers.createTransformer(cfg.transformerClassNames);\n     this.keyGenerator = DataSourceUtils.createKeyGenerator(props);\n-\n-    this.formatAdapter = new SourceFormatAdapter(\n-        UtilHelpers.createSource(cfg.sourceClassName, props, jssc, sparkSession, schemaProvider));\n-\n     this.conf = conf;\n+    refreshSchemaProvider(schemaProvider);\n+  }\n \n+  /**\n+   * Very useful when DeltaStreamer is running in continuous mode.\n+   * @param schemaProvider\n+   * @throws IOException\n+   */\n+  public void refreshSchemaProvider(SchemaProvider schemaProvider) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIyMDE4Mg=="}, "originalCommit": {"oid": "dfaa70dc3bddebe875d1c194ac8916b260d0a3ba"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjY5NDYwMg==", "bodyText": "It looks like refreshSchemaProvider not only refreshes schema-provider but also recreates Source and setup WriteClient\n\nDo you see any side effects of doing this? @bvaradar\n\nhave delta-streamer call this ?\n\nThis call will happen exactly at the same point where I am calling refreshSchemaProvider in delta-streamer, right?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r416694602", "createdAt": "2020-04-28T15:11:57Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -162,18 +162,23 @@ public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, Sche\n     this.fs = fs;\n     this.onInitializingHoodieWriteClient = onInitializingHoodieWriteClient;\n     this.props = props;\n-    this.schemaProvider = schemaProvider;\n \n     refreshTimeline();\n-\n     this.transformer = UtilHelpers.createTransformer(cfg.transformerClassNames);\n     this.keyGenerator = DataSourceUtils.createKeyGenerator(props);\n-\n-    this.formatAdapter = new SourceFormatAdapter(\n-        UtilHelpers.createSource(cfg.sourceClassName, props, jssc, sparkSession, schemaProvider));\n-\n     this.conf = conf;\n+    refreshSchemaProvider(schemaProvider);\n+  }\n \n+  /**\n+   * Very useful when DeltaStreamer is running in continuous mode.\n+   * @param schemaProvider\n+   * @throws IOException\n+   */\n+  public void refreshSchemaProvider(SchemaProvider schemaProvider) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIyMDE4Mg=="}, "originalCommit": {"oid": "dfaa70dc3bddebe875d1c194ac8916b260d0a3ba"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzIzNzMwOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaSet.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMjozMjoxNVrOGWVhMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODoxMDo1MFrOGYX5dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NDQxNg==", "bodyText": "should we add serialVersionUID? If it is not specified and anything in the class imports is shaded -> it will affect autogenerated one. Which causes issues if there is more than 1 version of the class in the classpath which was shaded differently.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426074416", "createdAt": "2020-05-15T22:32:15Z", "author": {"login": "afilipchik"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaSet.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.schema;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaNormalization;\n+\n+import java.util.Set;\n+\n+/**\n+ * Tracks already processed schemas.\n+ */\n+public class SchemaSet implements Serializable {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE4NzEzOQ==", "bodyText": "@bvaradar to chime in here.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426187139", "createdAt": "2020-05-16T20:14:57Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaSet.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.schema;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaNormalization;\n+\n+import java.util.Set;\n+\n+/**\n+ * Tracks already processed schemas.\n+ */\n+public class SchemaSet implements Serializable {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NDQxNg=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIxMDU0OQ==", "bodyText": "I will add serialVersionUUID. Usually, serialversion mismatch is a clue to an underlying problem which is package version mismatch", "url": "https://github.com/apache/hudi/pull/1566#discussion_r428210549", "createdAt": "2020-05-20T18:10:50Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaSet.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.schema;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaNormalization;\n+\n+import java.util.Set;\n+\n+/**\n+ * Tracks already processed schemas.\n+ */\n+public class SchemaSet implements Serializable {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NDQxNg=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzI1MjcxOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMjo0MTo1OVrOGWVqgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODowNzoxM1rOGYXxFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NjgwMw==", "bodyText": "is it called only once during a run? Will it be an issue if it is called more than once and slightly different schema is returned?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426076803", "createdAt": "2020-05-15T22:41:59Z", "author": {"login": "afilipchik"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java", "diffHunk": "@@ -81,11 +66,22 @@ private static Schema getSchema(String registryUrl) throws IOException {\n \n   @Override\n   public Schema getSourceSchema() {\n-    return schema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE4NzA5Mw==", "bodyText": "It is already handled in the code, in case different schema is returned, writeClient is recreated and new avro schemas are registered with spark configuration.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426187093", "createdAt": "2020-05-16T20:14:24Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java", "diffHunk": "@@ -81,11 +66,22 @@ private static Schema getSchema(String registryUrl) throws IOException {\n \n   @Override\n   public Schema getSourceSchema() {\n-    return schema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NjgwMw=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIwODQwNg==", "bodyText": "this is called in every run and if we detect schema change, we register and recreate write client.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r428208406", "createdAt": "2020-05-20T18:07:13Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java", "diffHunk": "@@ -81,11 +66,22 @@ private static Schema getSchema(String registryUrl) throws IOException {\n \n   @Override\n   public Schema getSourceSchema() {\n-    return schema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NjgwMw=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzI1NDk0OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMjo0MzoxNVrOGWVrvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODowODo1NlrOGYX0pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NzExNw==", "bodyText": "it might result in target schema != source schema when targetRegistryUrl is not specified as schema might change between getSourceSchema, getTargetSchema calls. Is it a problem?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426077117", "createdAt": "2020-05-15T22:43:15Z", "author": {"login": "afilipchik"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java", "diffHunk": "@@ -81,11 +66,22 @@ private static Schema getSchema(String registryUrl) throws IOException {\n \n   @Override\n   public Schema getSourceSchema() {\n-    return schema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);\n+    try {\n+      return getSchema(registryUrl);\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\"Error reading source schema from registry :\" + registryUrl, ioe);\n+    }\n   }\n \n   @Override\n   public Schema getTargetSchema() {\n-    return targetSchema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);\n+    String targetRegistryUrl = config.getString(Config.TARGET_SCHEMA_REGISTRY_URL_PROP, registryUrl);\n+    try {\n+      return getSchema(targetRegistryUrl);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE4NTI1MQ==", "bodyText": "Should not be a problem I believe since sourceSchema is used for reading while targetSchema is used for writing. Any issues if at all should be handled in HoodieAvroUtils when we try to rewrite the record with targetSchema.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426185251", "createdAt": "2020-05-16T19:51:19Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java", "diffHunk": "@@ -81,11 +66,22 @@ private static Schema getSchema(String registryUrl) throws IOException {\n \n   @Override\n   public Schema getSourceSchema() {\n-    return schema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);\n+    try {\n+      return getSchema(registryUrl);\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\"Error reading source schema from registry :\" + registryUrl, ioe);\n+    }\n   }\n \n   @Override\n   public Schema getTargetSchema() {\n-    return targetSchema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);\n+    String targetRegistryUrl = config.getString(Config.TARGET_SCHEMA_REGISTRY_URL_PROP, registryUrl);\n+    try {\n+      return getSchema(targetRegistryUrl);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NzExNw=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIwOTMxNw==", "bodyText": "yes, target schema is allowed to be different than source schema due to transformations and this is fine.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r428209317", "createdAt": "2020-05-20T18:08:56Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaRegistryProvider.java", "diffHunk": "@@ -81,11 +66,22 @@ private static Schema getSchema(String registryUrl) throws IOException {\n \n   @Override\n   public Schema getSourceSchema() {\n-    return schema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);\n+    try {\n+      return getSchema(registryUrl);\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\"Error reading source schema from registry :\" + registryUrl, ioe);\n+    }\n   }\n \n   @Override\n   public Schema getTargetSchema() {\n-    return targetSchema;\n+    String registryUrl = config.getString(Config.SRC_SCHEMA_REGISTRY_URL_PROP);\n+    String targetRegistryUrl = config.getString(Config.TARGET_SCHEMA_REGISTRY_URL_PROP, registryUrl);\n+    try {\n+      return getSchema(targetRegistryUrl);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NzExNw=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzI1NTc4OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMjo0Mzo1NFrOGWVsPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMFQwNDowNjo1N1rOHU35hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NzI0NQ==", "bodyText": "would be great to have some documentation on why it is done this way.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426077245", "createdAt": "2020-05-15T22:43:54Z", "author": {"login": "afilipchik"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -464,21 +464,25 @@ private void shutdownCompactor(boolean error) {\n      */\n     protected Boolean onInitializingWriteClient(HoodieWriteClient writeClient) {\n       if (cfg.isAsyncCompactionEnabled()) {\n-        asyncCompactService = new AsyncCompactService(jssc, writeClient);\n-        // Enqueue existing pending compactions first\n-        HoodieTableMetaClient meta =\n-            new HoodieTableMetaClient(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath, true);\n-        List<HoodieInstant> pending = CompactionUtils.getPendingCompactionInstantTimes(meta);\n-        pending.forEach(hoodieInstant -> asyncCompactService.enqueuePendingCompaction(hoodieInstant));\n-        asyncCompactService.start((error) -> {\n-          // Shutdown DeltaSync\n-          shutdown(false);\n-          return true;\n-        });\n-        try {\n-          asyncCompactService.waitTillPendingCompactionsReducesTo(cfg.maxPendingCompactions);\n-        } catch (InterruptedException ie) {\n-          throw new HoodieException(ie);\n+        if (null != asyncCompactService) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE4NDgyOA==", "bodyText": "+1", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426184828", "createdAt": "2020-05-16T19:46:03Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -464,21 +464,25 @@ private void shutdownCompactor(boolean error) {\n      */\n     protected Boolean onInitializingWriteClient(HoodieWriteClient writeClient) {\n       if (cfg.isAsyncCompactionEnabled()) {\n-        asyncCompactService = new AsyncCompactService(jssc, writeClient);\n-        // Enqueue existing pending compactions first\n-        HoodieTableMetaClient meta =\n-            new HoodieTableMetaClient(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath, true);\n-        List<HoodieInstant> pending = CompactionUtils.getPendingCompactionInstantTimes(meta);\n-        pending.forEach(hoodieInstant -> asyncCompactService.enqueuePendingCompaction(hoodieInstant));\n-        asyncCompactService.start((error) -> {\n-          // Shutdown DeltaSync\n-          shutdown(false);\n-          return true;\n-        });\n-        try {\n-          asyncCompactService.waitTillPendingCompactionsReducesTo(cfg.maxPendingCompactions);\n-        } catch (InterruptedException ie) {\n-          throw new HoodieException(ie);\n+        if (null != asyncCompactService) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NzI0NQ=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTY0OTQxMw==", "bodyText": "Added", "url": "https://github.com/apache/hudi/pull/1566#discussion_r491649413", "createdAt": "2020-09-20T04:06:57Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -464,21 +464,25 @@ private void shutdownCompactor(boolean error) {\n      */\n     protected Boolean onInitializingWriteClient(HoodieWriteClient writeClient) {\n       if (cfg.isAsyncCompactionEnabled()) {\n-        asyncCompactService = new AsyncCompactService(jssc, writeClient);\n-        // Enqueue existing pending compactions first\n-        HoodieTableMetaClient meta =\n-            new HoodieTableMetaClient(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath, true);\n-        List<HoodieInstant> pending = CompactionUtils.getPendingCompactionInstantTimes(meta);\n-        pending.forEach(hoodieInstant -> asyncCompactService.enqueuePendingCompaction(hoodieInstant));\n-        asyncCompactService.start((error) -> {\n-          // Shutdown DeltaSync\n-          shutdown(false);\n-          return true;\n-        });\n-        try {\n-          asyncCompactService.waitTillPendingCompactionsReducesTo(cfg.maxPendingCompactions);\n-        } catch (InterruptedException ie) {\n-          throw new HoodieException(ie);\n+        if (null != asyncCompactService) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NzI0NQ=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzI1OTM5OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/Compactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMjo0NjowMlrOGWVucQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQxOTo0NDoyMVrOGWcPzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NzgwOQ==", "bodyText": "is it used anywhere?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426077809", "createdAt": "2020-05-15T22:46:02Z", "author": {"login": "afilipchik"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/Compactor.java", "diffHunk": "@@ -59,6 +60,10 @@ public void compact(HoodieInstant instant) throws IOException {\n           \"Compaction for instant (\" + instant + \") failed with write errors. Errors :\" + numWriteErrors);\n     }\n     // Commit compaction\n-    compactionClient.commitCompaction(instant.getTimestamp(), res, Option.empty());\n+    writeClient.commitCompaction(instant.getTimestamp(), res, Option.empty());\n+  }\n+\n+  public void updateWriteClient(HoodieWriteClient writeClient) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE4NDY1Mg==", "bodyText": "Yes, in HoodieDeltaStreamer.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426184652", "createdAt": "2020-05-16T19:44:21Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/Compactor.java", "diffHunk": "@@ -59,6 +60,10 @@ public void compact(HoodieInstant instant) throws IOException {\n           \"Compaction for instant (\" + instant + \") failed with write errors. Errors :\" + numWriteErrors);\n     }\n     // Commit compaction\n-    compactionClient.commitCompaction(instant.getTimestamp(), res, Option.empty());\n+    writeClient.commitCompaction(instant.getTimestamp(), res, Option.empty());\n+  }\n+\n+  public void updateWriteClient(HoodieWriteClient writeClient) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3NzgwOQ=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzI2Mzk2OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaSet.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMjo0ODo1MVrOGWVxJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxODowMzozN1rOGYXpQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3ODUwMQ==", "bodyText": "will this grow indefinitely? How would we remove old schema?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426078501", "createdAt": "2020-05-15T22:48:51Z", "author": {"login": "afilipchik"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaSet.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.schema;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaNormalization;\n+\n+import java.util.Set;\n+\n+/**\n+ * Tracks already processed schemas.\n+ */\n+public class SchemaSet implements Serializable {\n+\n+  private final Set<Long> processedSchema = new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE4NDc5MA==", "bodyText": "Personally I feel you do not have many schema evolutions in production environment. So not removing old schemas should not create any issues. @bvaradar to take the final call.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r426184790", "createdAt": "2020-05-16T19:45:42Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaSet.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.schema;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaNormalization;\n+\n+import java.util.Set;\n+\n+/**\n+ * Tracks already processed schemas.\n+ */\n+public class SchemaSet implements Serializable {\n+\n+  private final Set<Long> processedSchema = new HashSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3ODUwMQ=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIwNjQwMA==", "bodyText": "I think this is similar in scope to how sparkConf maintains avro schemas. In continuous mode, we reuse the same spark session. I think this is ok.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r428206400", "createdAt": "2020-05-20T18:03:37Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/SchemaSet.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.schema;\n+\n+import java.io.Serializable;\n+import java.util.HashSet;\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaNormalization;\n+\n+import java.util.Set;\n+\n+/**\n+ * Tracks already processed schemas.\n+ */\n+public class SchemaSet implements Serializable {\n+\n+  private final Set<Long> processedSchema = new HashSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3ODUwMQ=="}, "originalCommit": {"oid": "741db83307e63e9ee6fe9df1911f1bfc6394cf78"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwMTQ0ODE1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/Compactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNlQyMToyMToyOFrOHYiugQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxODo0NjozNFrOHZLO4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ5NjgzMw==", "bodyText": "Any specific reason for introducing a new variable here?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r495496833", "createdAt": "2020-09-26T21:21:28Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/Compactor.java", "diffHunk": "@@ -45,7 +45,8 @@ public Compactor(HoodieWriteClient compactionClient) {\n \n   public void compact(HoodieInstant instant) throws IOException {\n     LOG.info(\"Compactor executing compaction \" + instant);\n-    JavaRDD<WriteStatus> res = compactionClient.compact(instant.getTimestamp());\n+    HoodieWriteClient writeClient = compactionClient;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fe6e6502e2c8283df70f208d3777d04fc8f900"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2MDQ4MQ==", "bodyText": "This is to keep the client instance consistent between writing and committing.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r496160481", "createdAt": "2020-09-28T18:46:34Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/Compactor.java", "diffHunk": "@@ -45,7 +45,8 @@ public Compactor(HoodieWriteClient compactionClient) {\n \n   public void compact(HoodieInstant instant) throws IOException {\n     LOG.info(\"Compactor executing compaction \" + instant);\n-    JavaRDD<WriteStatus> res = compactionClient.compact(instant.getTimestamp());\n+    HoodieWriteClient writeClient = compactionClient;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ5NjgzMw=="}, "originalCommit": {"oid": "43fe6e6502e2c8283df70f208d3777d04fc8f900"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwMTQ2Mjg2OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNlQyMTo0Mjo1MlrOHYi1HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxODo0NTo0OFrOHZLNOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ5ODUyNQ==", "bodyText": "Just curious, can you please explain why do we need to call this function here? @bvaradar", "url": "https://github.com/apache/hudi/pull/1566#discussion_r495498525", "createdAt": "2020-09-26T21:42:52Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -536,14 +564,35 @@ public void syncHive(HiveConf conf) {\n    * SchemaProvider creation is a precursor to HoodieWriteClient and AsyncCompactor creation. This method takes care of\n    * this constraint.\n    */\n-  private void setupWriteClient() {\n-    LOG.info(\"Setting up Hoodie Write Client\");\n-    if ((null != schemaProvider) && (null == writeClient)) {\n-      registerAvroSchemas(schemaProvider);\n-      HoodieWriteConfig hoodieCfg = getHoodieClientConfig(schemaProvider);\n-      writeClient = new HoodieWriteClient<>(jssc, hoodieCfg, true);\n-      onInitializingHoodieWriteClient.apply(writeClient);\n+  public void setupWriteClient() throws IOException {\n+    if ((null != schemaProvider)) {\n+      Schema sourceSchema = schemaProvider.getSourceSchema();\n+      Schema targetSchema = schemaProvider.getTargetSchema();\n+      createNewWriteClient(sourceSchema, targetSchema);\n+    }\n+  }\n+\n+  private void createNewWriteClient(Schema sourceSchema, Schema targetSchema) throws IOException {\n+    LOG.info(\"Setting up new Hoodie Write Client\");\n+    registerAvroSchemas(sourceSchema, targetSchema);\n+    HoodieWriteConfig hoodieCfg = getHoodieClientConfig(targetSchema);\n+    if (hoodieCfg.isEmbeddedTimelineServerEnabled()) {\n+      if (!embeddedTimelineService.isPresent()) {\n+        embeddedTimelineService = EmbeddedTimelineServerHelper.createEmbeddedTimelineService(jssc.hadoopConfiguration(),\n+            jssc.getConf(), hoodieCfg);\n+      } else {\n+        EmbeddedTimelineServerHelper.updateWriteConfigWithTimelineServer(embeddedTimelineService.get(), hoodieCfg);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fe6e6502e2c8283df70f208d3777d04fc8f900"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ5OTExNg==", "bodyText": "As per my understanding, in every iteration of deltaStreamer, only the target schema can change, rest of the HoodieWriteConfig will remain same. Rather than creating a new hoodieCfg variable in this method every time schema changes, can we have a class level HoodieWriteConfig variable, where we only update the schema whenever it changes? This way we can do away with this call updateWriteConfigWithTimelineServer here. WDYT?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r495499116", "createdAt": "2020-09-26T21:51:01Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -536,14 +564,35 @@ public void syncHive(HiveConf conf) {\n    * SchemaProvider creation is a precursor to HoodieWriteClient and AsyncCompactor creation. This method takes care of\n    * this constraint.\n    */\n-  private void setupWriteClient() {\n-    LOG.info(\"Setting up Hoodie Write Client\");\n-    if ((null != schemaProvider) && (null == writeClient)) {\n-      registerAvroSchemas(schemaProvider);\n-      HoodieWriteConfig hoodieCfg = getHoodieClientConfig(schemaProvider);\n-      writeClient = new HoodieWriteClient<>(jssc, hoodieCfg, true);\n-      onInitializingHoodieWriteClient.apply(writeClient);\n+  public void setupWriteClient() throws IOException {\n+    if ((null != schemaProvider)) {\n+      Schema sourceSchema = schemaProvider.getSourceSchema();\n+      Schema targetSchema = schemaProvider.getTargetSchema();\n+      createNewWriteClient(sourceSchema, targetSchema);\n+    }\n+  }\n+\n+  private void createNewWriteClient(Schema sourceSchema, Schema targetSchema) throws IOException {\n+    LOG.info(\"Setting up new Hoodie Write Client\");\n+    registerAvroSchemas(sourceSchema, targetSchema);\n+    HoodieWriteConfig hoodieCfg = getHoodieClientConfig(targetSchema);\n+    if (hoodieCfg.isEmbeddedTimelineServerEnabled()) {\n+      if (!embeddedTimelineService.isPresent()) {\n+        embeddedTimelineService = EmbeddedTimelineServerHelper.createEmbeddedTimelineService(jssc.hadoopConfiguration(),\n+            jssc.getConf(), hoodieCfg);\n+      } else {\n+        EmbeddedTimelineServerHelper.updateWriteConfigWithTimelineServer(embeddedTimelineService.get(), hoodieCfg);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ5ODUyNQ=="}, "originalCommit": {"oid": "43fe6e6502e2c8283df70f208d3777d04fc8f900"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2MDA1Ng==", "bodyText": "There can be new fields in the source schema and the target schema can just reflect that. The write config are designed to be immutable with builder pattern. Also, some of the functions run in executor side. It is lot simpler by recreating the aggregate class (WriteClient) here.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r496160056", "createdAt": "2020-09-28T18:45:48Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -536,14 +564,35 @@ public void syncHive(HiveConf conf) {\n    * SchemaProvider creation is a precursor to HoodieWriteClient and AsyncCompactor creation. This method takes care of\n    * this constraint.\n    */\n-  private void setupWriteClient() {\n-    LOG.info(\"Setting up Hoodie Write Client\");\n-    if ((null != schemaProvider) && (null == writeClient)) {\n-      registerAvroSchemas(schemaProvider);\n-      HoodieWriteConfig hoodieCfg = getHoodieClientConfig(schemaProvider);\n-      writeClient = new HoodieWriteClient<>(jssc, hoodieCfg, true);\n-      onInitializingHoodieWriteClient.apply(writeClient);\n+  public void setupWriteClient() throws IOException {\n+    if ((null != schemaProvider)) {\n+      Schema sourceSchema = schemaProvider.getSourceSchema();\n+      Schema targetSchema = schemaProvider.getTargetSchema();\n+      createNewWriteClient(sourceSchema, targetSchema);\n+    }\n+  }\n+\n+  private void createNewWriteClient(Schema sourceSchema, Schema targetSchema) throws IOException {\n+    LOG.info(\"Setting up new Hoodie Write Client\");\n+    registerAvroSchemas(sourceSchema, targetSchema);\n+    HoodieWriteConfig hoodieCfg = getHoodieClientConfig(targetSchema);\n+    if (hoodieCfg.isEmbeddedTimelineServerEnabled()) {\n+      if (!embeddedTimelineService.isPresent()) {\n+        embeddedTimelineService = EmbeddedTimelineServerHelper.createEmbeddedTimelineService(jssc.hadoopConfiguration(),\n+            jssc.getConf(), hoodieCfg);\n+      } else {\n+        EmbeddedTimelineServerHelper.updateWriteConfigWithTimelineServer(embeddedTimelineService.get(), hoodieCfg);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ5ODUyNQ=="}, "originalCommit": {"oid": "43fe6e6502e2c8283df70f208d3777d04fc8f900"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjEyNzAwOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMzoyNToyM1rOHcK2Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQyMzo1MDo0MVrOHddxUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI5OTg1MA==", "bodyText": "I really dislike all the nested ifs-and null checks. :( there ought to be a better way of structuring this code overall. Side rant", "url": "https://github.com/apache/hudi/pull/1566#discussion_r499299850", "createdAt": "2020-10-04T23:25:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -244,6 +263,18 @@ private void refreshTimeline() throws IOException {\n         this.schemaProvider = srcRecordsWithCkpt.getKey();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0b3b64eae966fb35e92de7dc99a01b3d871ef37"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY1ODUxNA==", "bodyText": "Agree. the whole schema provider and write client handling needs to be refactored to be more readable.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r500658514", "createdAt": "2020-10-06T23:50:41Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -244,6 +263,18 @@ private void refreshTimeline() throws IOException {\n         this.schemaProvider = srcRecordsWithCkpt.getKey();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI5OTg1MA=="}, "originalCommit": {"oid": "a0b3b64eae966fb35e92de7dc99a01b3d871ef37"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjEyNzc0OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMzoyNjozOFrOHcK2aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQyMzo0ODo0OFrOHddvDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI5OTk0NA==", "bodyText": "rename: reInitWriteClient()", "url": "https://github.com/apache/hudi/pull/1566#discussion_r499299944", "createdAt": "2020-10-04T23:26:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -534,19 +565,33 @@ public void syncHive(HiveConf conf) {\n     syncHive();\n   }\n \n-  /**\n-   * Note that depending on configs and source-type, schemaProvider could either be eagerly or lazily created.\n-   * SchemaProvider creation is a precursor to HoodieWriteClient and AsyncCompactor creation. This method takes care of\n-   * this constraint.\n-   */\n-  private void setupWriteClient() {\n-    LOG.info(\"Setting up Hoodie Write Client\");\n-    if ((null != schemaProvider) && (null == writeClient)) {\n-      registerAvroSchemas(schemaProvider);\n-      HoodieWriteConfig hoodieCfg = getHoodieClientConfig(schemaProvider);\n-      writeClient = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jssc), hoodieCfg, true);\n-      onInitializingHoodieWriteClient.apply(writeClient);\n+  public void setupWriteClient() throws IOException {\n+    if ((null != schemaProvider)) {\n+      Schema sourceSchema = schemaProvider.getSourceSchema();\n+      Schema targetSchema = schemaProvider.getTargetSchema();\n+      createNewWriteClient(sourceSchema, targetSchema);\n+    }\n+  }\n+\n+  private void createNewWriteClient(Schema sourceSchema, Schema targetSchema) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0b3b64eae966fb35e92de7dc99a01b3d871ef37"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY1NzkzNA==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/1566#discussion_r500657934", "createdAt": "2020-10-06T23:48:48Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -534,19 +565,33 @@ public void syncHive(HiveConf conf) {\n     syncHive();\n   }\n \n-  /**\n-   * Note that depending on configs and source-type, schemaProvider could either be eagerly or lazily created.\n-   * SchemaProvider creation is a precursor to HoodieWriteClient and AsyncCompactor creation. This method takes care of\n-   * this constraint.\n-   */\n-  private void setupWriteClient() {\n-    LOG.info(\"Setting up Hoodie Write Client\");\n-    if ((null != schemaProvider) && (null == writeClient)) {\n-      registerAvroSchemas(schemaProvider);\n-      HoodieWriteConfig hoodieCfg = getHoodieClientConfig(schemaProvider);\n-      writeClient = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jssc), hoodieCfg, true);\n-      onInitializingHoodieWriteClient.apply(writeClient);\n+  public void setupWriteClient() throws IOException {\n+    if ((null != schemaProvider)) {\n+      Schema sourceSchema = schemaProvider.getSourceSchema();\n+      Schema targetSchema = schemaProvider.getTargetSchema();\n+      createNewWriteClient(sourceSchema, targetSchema);\n+    }\n+  }\n+\n+  private void createNewWriteClient(Schema sourceSchema, Schema targetSchema) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI5OTk0NA=="}, "originalCommit": {"oid": "a0b3b64eae966fb35e92de7dc99a01b3d871ef37"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjEyODQyOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNFQyMzoyNzo0MFrOHcK2uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQyMzo0ODo0NVrOHddu-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwMDAyNQ==", "bodyText": "any need to make this synchronized across threads?", "url": "https://github.com/apache/hudi/pull/1566#discussion_r499300025", "createdAt": "2020-10-04T23:27:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -624,21 +624,26 @@ private void shutdownCompactor(boolean error) {\n      */\n     protected Boolean onInitializingWriteClient(SparkRDDWriteClient writeClient) {\n       if (cfg.isAsyncCompactionEnabled()) {\n-        asyncCompactService = new SparkAsyncCompactService(new HoodieSparkEngineContext(jssc), writeClient);\n-        // Enqueue existing pending compactions first\n-        HoodieTableMetaClient meta =\n-            new HoodieTableMetaClient(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath, true);\n-        List<HoodieInstant> pending = CompactionUtils.getPendingCompactionInstantTimes(meta);\n-        pending.forEach(hoodieInstant -> asyncCompactService.enqueuePendingCompaction(hoodieInstant));\n-        asyncCompactService.start((error) -> {\n-          // Shutdown DeltaSync\n-          shutdown(false);\n-          return true;\n-        });\n-        try {\n-          asyncCompactService.waitTillPendingCompactionsReducesTo(cfg.maxPendingCompactions);\n-        } catch (InterruptedException ie) {\n-          throw new HoodieException(ie);\n+        if (null != asyncCompactService) {\n+          // Update the write client used by Async Compactor.\n+          asyncCompactService.updateWriteClient(writeClient);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0b3b64eae966fb35e92de7dc99a01b3d871ef37"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY1NzkxNQ==", "bodyText": "Should work as is but adding synchronized if we make future changes in a way where this would be not thread-safe.", "url": "https://github.com/apache/hudi/pull/1566#discussion_r500657915", "createdAt": "2020-10-06T23:48:45Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -624,21 +624,26 @@ private void shutdownCompactor(boolean error) {\n      */\n     protected Boolean onInitializingWriteClient(SparkRDDWriteClient writeClient) {\n       if (cfg.isAsyncCompactionEnabled()) {\n-        asyncCompactService = new SparkAsyncCompactService(new HoodieSparkEngineContext(jssc), writeClient);\n-        // Enqueue existing pending compactions first\n-        HoodieTableMetaClient meta =\n-            new HoodieTableMetaClient(new Configuration(jssc.hadoopConfiguration()), cfg.targetBasePath, true);\n-        List<HoodieInstant> pending = CompactionUtils.getPendingCompactionInstantTimes(meta);\n-        pending.forEach(hoodieInstant -> asyncCompactService.enqueuePendingCompaction(hoodieInstant));\n-        asyncCompactService.start((error) -> {\n-          // Shutdown DeltaSync\n-          shutdown(false);\n-          return true;\n-        });\n-        try {\n-          asyncCompactService.waitTillPendingCompactionsReducesTo(cfg.maxPendingCompactions);\n-        } catch (InterruptedException ie) {\n-          throw new HoodieException(ie);\n+        if (null != asyncCompactService) {\n+          // Update the write client used by Async Compactor.\n+          asyncCompactService.updateWriteClient(writeClient);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwMDAyNQ=="}, "originalCommit": {"oid": "a0b3b64eae966fb35e92de7dc99a01b3d871ef37"}, "originalPosition": 21}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4593, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}