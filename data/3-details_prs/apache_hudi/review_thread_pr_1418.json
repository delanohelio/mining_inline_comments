{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkwODg4MTI2", "number": 1418, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwNzoyNDo1M1rODpxSGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQxMDozMTo0OVrODrlRmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MTI1NjU4OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/config/AbstractConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwNzoyNDo1M1rOF5Jj9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMjozNjoyN1rOF5zDIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQ2OTgxNQ==", "bodyText": "Could only use SparkConfig to place all these methods if only consider spark free, not multi-engine", "url": "https://github.com/apache/hudi/pull/1418#discussion_r395469815", "createdAt": "2020-03-20T07:24:53Z", "author": {"login": "leesf"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/config/AbstractConfig.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.config;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Abstract config for multi-engine.\n+ */\n+public abstract class AbstractConfig<T> {\n+  protected Properties props;\n+\n+  public AbstractConfig(Properties props) {\n+    this.props = props;\n+  }\n+\n+  public abstract long getMaxMemoryAllowedForMerge(String maxMemoryFraction);\n+\n+  public abstract T getWriteStatusStorageLevel();\n+\n+  public abstract T getBloomIndexInputStorageLevel();\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5de4068333756f776c78653545d253d0a1b9b92f"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE0OTUzNg==", "bodyText": "I feel we can just make it Spark free for now.. Multi-engine can be dealt with separately..?", "url": "https://github.com/apache/hudi/pull/1418#discussion_r396149536", "createdAt": "2020-03-22T22:36:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/config/AbstractConfig.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.config;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Abstract config for multi-engine.\n+ */\n+public abstract class AbstractConfig<T> {\n+  protected Properties props;\n+\n+  public AbstractConfig(Properties props) {\n+    this.props = props;\n+  }\n+\n+  public abstract long getMaxMemoryAllowedForMerge(String maxMemoryFraction);\n+\n+  public abstract T getWriteStatusStorageLevel();\n+\n+  public abstract T getBloomIndexInputStorageLevel();\n+}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQ2OTgxNQ=="}, "originalCommit": {"oid": "5de4068333756f776c78653545d253d0a1b9b92f"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODYyNjI2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0Mzo1N1rOF7xKHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0Mzo1N1rOF7xKHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNTcwOA==", "bodyText": "single line? (also the usages above)", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398215708", "createdAt": "2020-03-25T22:43:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java", "diffHunk": "@@ -69,7 +70,8 @@ public HoodieBloomIndex(HoodieWriteConfig config) {\n \n     // Step 0: cache the input record RDD\n     if (config.getBloomIndexUseCaching()) {\n-      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+      StorageLevel storageLevel = ConfigUtils.getBloomIndexInputStorageLevel(config.getProps());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c65f1cec6ef42df3b05ddb727dd1a96070589be"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODYyOTcwOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/utils/ConfigUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0NToxNFrOF7xMEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0NToxNFrOF7xMEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNjIxMA==", "bodyText": "rename to SparkConfigUtils? to make it explicit?", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398216210", "createdAt": "2020-03-25T22:45:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/utils/ConfigUtils.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import org.apache.hudi.config.HoodieIndexConfig;\n+\n+import org.apache.spark.SparkEnv;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.util.Utils;\n+\n+import java.util.Properties;\n+\n+import static org.apache.hudi.config.HoodieMemoryConfig.DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n+import static org.apache.hudi.config.HoodieMemoryConfig.DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n+import static org.apache.hudi.config.HoodieWriteConfig.WRITE_STATUS_STORAGE_LEVEL;\n+\n+/**\n+ * Config utils.\n+ */\n+public class ConfigUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c65f1cec6ef42df3b05ddb727dd1a96070589be"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODYzMTE5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0NTo0OFrOF7xM9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0NTo0OFrOF7xM9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNjQzOA==", "bodyText": "remove if unused?", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398216438", "createdAt": "2020-03-25T22:45:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -565,6 +556,7 @@ public FileSystemViewStorageConfig getClientSpecifiedViewStorageConfig() {\n     private boolean isMemoryConfigSet = false;\n     private boolean isViewConfigSet = false;\n     private boolean isConsistencyGuardSet = false;\n+    private boolean isEngineConfigSet = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c65f1cec6ef42df3b05ddb727dd1a96070589be"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2ODYzNjU4OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo0Nzo1OVrOF7xQNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNDo1MjozOVrOF73Vbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNzI2OA==", "bodyText": "Okay.. if we call a Spark specific class here, then this does not achieve the purpose right..\ni.e you cannot move ConfigUtils to hudi-spark and keep config in hudi-writer-common without making hudi-writer-common depend on ConfigUtils?\nWe should change the caller of getMaxMemoryAllowedForMerge to use ConfigUtils.getXX()` just like how you did for storage level?", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398217268", "createdAt": "2020-03-25T22:47:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "diffHunk": "@@ -113,40 +112,8 @@ public Builder withWriteStatusFailureFraction(double failureFraction) {\n       return this;\n     }\n \n-    /**\n-     * Dynamic calculation of max memory to use for for spillable map. user.available.memory = spark.executor.memory *\n-     * (1 - spark.memory.fraction) spillable.available.memory = user.available.memory * hoodie.memory.fraction. Anytime\n-     * the spark.executor.memory or the spark.memory.fraction is changed, the memory used for spillable map changes\n-     * accordingly\n-     */\n     private long getMaxMemoryAllowedForMerge(String maxMemoryFraction) {\n-      final String SPARK_EXECUTOR_MEMORY_PROP = \"spark.executor.memory\";\n-      final String SPARK_EXECUTOR_MEMORY_FRACTION_PROP = \"spark.memory.fraction\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/memory/UnifiedMemoryManager.scala#L231} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION = \"0.6\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/SparkContext.scala#L471} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_MB = \"1024\"; // in MB\n-\n-      if (SparkEnv.get() != null) {\n-        // 1 GB is the default conf used by Spark, look at SparkContext.scala\n-        long executorMemoryInBytes = Utils.memoryStringToMb(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_MB)) * 1024 * 1024L;\n-        // 0.6 is the default value used by Spark,\n-        // look at {@link\n-        // https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L507}\n-        double memoryFraction = Double.parseDouble(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_FRACTION_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION));\n-        double maxMemoryFractionForMerge = Double.parseDouble(maxMemoryFraction);\n-        double userAvailableMemory = executorMemoryInBytes * (1 - memoryFraction);\n-        long maxMemoryForMerge = (long) Math.floor(userAvailableMemory * maxMemoryFractionForMerge);\n-        return Math.max(DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES, maxMemoryForMerge);\n-      } else {\n-        return DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n-      }\n+      return ConfigUtils.getMaxMemoryAllowedForMerge(props, maxMemoryFraction);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c65f1cec6ef42df3b05ddb727dd1a96070589be"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODMxNjkxMA==", "bodyText": "@leesf  I think this is still not addressed.. ?", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398316910", "createdAt": "2020-03-26T04:52:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "diffHunk": "@@ -113,40 +112,8 @@ public Builder withWriteStatusFailureFraction(double failureFraction) {\n       return this;\n     }\n \n-    /**\n-     * Dynamic calculation of max memory to use for for spillable map. user.available.memory = spark.executor.memory *\n-     * (1 - spark.memory.fraction) spillable.available.memory = user.available.memory * hoodie.memory.fraction. Anytime\n-     * the spark.executor.memory or the spark.memory.fraction is changed, the memory used for spillable map changes\n-     * accordingly\n-     */\n     private long getMaxMemoryAllowedForMerge(String maxMemoryFraction) {\n-      final String SPARK_EXECUTOR_MEMORY_PROP = \"spark.executor.memory\";\n-      final String SPARK_EXECUTOR_MEMORY_FRACTION_PROP = \"spark.memory.fraction\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/memory/UnifiedMemoryManager.scala#L231} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION = \"0.6\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/SparkContext.scala#L471} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_MB = \"1024\"; // in MB\n-\n-      if (SparkEnv.get() != null) {\n-        // 1 GB is the default conf used by Spark, look at SparkContext.scala\n-        long executorMemoryInBytes = Utils.memoryStringToMb(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_MB)) * 1024 * 1024L;\n-        // 0.6 is the default value used by Spark,\n-        // look at {@link\n-        // https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L507}\n-        double memoryFraction = Double.parseDouble(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_FRACTION_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION));\n-        double maxMemoryFractionForMerge = Double.parseDouble(maxMemoryFraction);\n-        double userAvailableMemory = executorMemoryInBytes * (1 - memoryFraction);\n-        long maxMemoryForMerge = (long) Math.floor(userAvailableMemory * maxMemoryFractionForMerge);\n-        return Math.max(DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES, maxMemoryForMerge);\n-      } else {\n-        return DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n-      }\n+      return ConfigUtils.getMaxMemoryAllowedForMerge(props, maxMemoryFraction);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNzI2OA=="}, "originalCommit": {"oid": "8c65f1cec6ef42df3b05ddb727dd1a96070589be"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2OTI5NzMxOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQwNDo1MzozMVrOF73WQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQxMDozMzowNFrOF8AjXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODMxNzEyMA==", "bodyText": "if the classes in config call the SparkConfigUtils, then we cannot claim its spark free right..\ncc @yanghua as well", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398317120", "createdAt": "2020-03-26T04:53:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "diffHunk": "@@ -113,52 +112,20 @@ public Builder withWriteStatusFailureFraction(double failureFraction) {\n       return this;\n     }\n \n-    /**\n-     * Dynamic calculation of max memory to use for for spillable map. user.available.memory = spark.executor.memory *\n-     * (1 - spark.memory.fraction) spillable.available.memory = user.available.memory * hoodie.memory.fraction. Anytime\n-     * the spark.executor.memory or the spark.memory.fraction is changed, the memory used for spillable map changes\n-     * accordingly\n-     */\n-    private long getMaxMemoryAllowedForMerge(String maxMemoryFraction) {\n-      final String SPARK_EXECUTOR_MEMORY_PROP = \"spark.executor.memory\";\n-      final String SPARK_EXECUTOR_MEMORY_FRACTION_PROP = \"spark.memory.fraction\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/memory/UnifiedMemoryManager.scala#L231} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION = \"0.6\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/SparkContext.scala#L471} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_MB = \"1024\"; // in MB\n-\n-      if (SparkEnv.get() != null) {\n-        // 1 GB is the default conf used by Spark, look at SparkContext.scala\n-        long executorMemoryInBytes = Utils.memoryStringToMb(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_MB)) * 1024 * 1024L;\n-        // 0.6 is the default value used by Spark,\n-        // look at {@link\n-        // https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L507}\n-        double memoryFraction = Double.parseDouble(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_FRACTION_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION));\n-        double maxMemoryFractionForMerge = Double.parseDouble(maxMemoryFraction);\n-        double userAvailableMemory = executorMemoryInBytes * (1 - memoryFraction);\n-        long maxMemoryForMerge = (long) Math.floor(userAvailableMemory * maxMemoryFractionForMerge);\n-        return Math.max(DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES, maxMemoryForMerge);\n-      } else {\n-        return DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n-      }\n-    }\n-\n     public HoodieMemoryConfig build() {\n       HoodieMemoryConfig config = new HoodieMemoryConfig(props);\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP),\n           MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP, DEFAULT_MAX_MEMORY_FRACTION_FOR_COMPACTION);\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FRACTION_FOR_MERGE_PROP),\n           MAX_MEMORY_FRACTION_FOR_MERGE_PROP, DEFAULT_MAX_MEMORY_FRACTION_FOR_MERGE);\n+      long maxMemoryAllowedForMerge =\n+          SparkConfigUtils.getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_MERGE_PROP));\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FOR_MERGE_PROP), MAX_MEMORY_FOR_MERGE_PROP,\n-          String.valueOf(getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_MERGE_PROP))));\n+          String.valueOf(maxMemoryAllowedForMerge));\n+      long maxMemoryAllowedForCompaction =\n+          SparkConfigUtils.getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "baed026d9f46420f9575741912ef8d6f84054a2d"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM0NTcwOQ==", "bodyText": "Yes, you are right. Maybe we have to extract these two lines into an extra method.", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398345709", "createdAt": "2020-03-26T06:40:11Z", "author": {"login": "yanghua"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "diffHunk": "@@ -113,52 +112,20 @@ public Builder withWriteStatusFailureFraction(double failureFraction) {\n       return this;\n     }\n \n-    /**\n-     * Dynamic calculation of max memory to use for for spillable map. user.available.memory = spark.executor.memory *\n-     * (1 - spark.memory.fraction) spillable.available.memory = user.available.memory * hoodie.memory.fraction. Anytime\n-     * the spark.executor.memory or the spark.memory.fraction is changed, the memory used for spillable map changes\n-     * accordingly\n-     */\n-    private long getMaxMemoryAllowedForMerge(String maxMemoryFraction) {\n-      final String SPARK_EXECUTOR_MEMORY_PROP = \"spark.executor.memory\";\n-      final String SPARK_EXECUTOR_MEMORY_FRACTION_PROP = \"spark.memory.fraction\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/memory/UnifiedMemoryManager.scala#L231} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION = \"0.6\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/SparkContext.scala#L471} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_MB = \"1024\"; // in MB\n-\n-      if (SparkEnv.get() != null) {\n-        // 1 GB is the default conf used by Spark, look at SparkContext.scala\n-        long executorMemoryInBytes = Utils.memoryStringToMb(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_MB)) * 1024 * 1024L;\n-        // 0.6 is the default value used by Spark,\n-        // look at {@link\n-        // https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L507}\n-        double memoryFraction = Double.parseDouble(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_FRACTION_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION));\n-        double maxMemoryFractionForMerge = Double.parseDouble(maxMemoryFraction);\n-        double userAvailableMemory = executorMemoryInBytes * (1 - memoryFraction);\n-        long maxMemoryForMerge = (long) Math.floor(userAvailableMemory * maxMemoryFractionForMerge);\n-        return Math.max(DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES, maxMemoryForMerge);\n-      } else {\n-        return DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n-      }\n-    }\n-\n     public HoodieMemoryConfig build() {\n       HoodieMemoryConfig config = new HoodieMemoryConfig(props);\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP),\n           MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP, DEFAULT_MAX_MEMORY_FRACTION_FOR_COMPACTION);\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FRACTION_FOR_MERGE_PROP),\n           MAX_MEMORY_FRACTION_FOR_MERGE_PROP, DEFAULT_MAX_MEMORY_FRACTION_FOR_MERGE);\n+      long maxMemoryAllowedForMerge =\n+          SparkConfigUtils.getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_MERGE_PROP));\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FOR_MERGE_PROP), MAX_MEMORY_FOR_MERGE_PROP,\n-          String.valueOf(getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_MERGE_PROP))));\n+          String.valueOf(maxMemoryAllowedForMerge));\n+      long maxMemoryAllowedForCompaction =\n+          SparkConfigUtils.getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODMxNzEyMA=="}, "originalCommit": {"oid": "baed026d9f46420f9575741912ef8d6f84054a2d"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODQ2NzkzNA==", "bodyText": "if the classes in config call the SparkConfigUtils, then we cannot claim its spark free right..\ncc @yanghua as well\n\nGet the point, updated the PR to make it totally free.", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398467934", "createdAt": "2020-03-26T10:33:04Z", "author": {"login": "leesf"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "diffHunk": "@@ -113,52 +112,20 @@ public Builder withWriteStatusFailureFraction(double failureFraction) {\n       return this;\n     }\n \n-    /**\n-     * Dynamic calculation of max memory to use for for spillable map. user.available.memory = spark.executor.memory *\n-     * (1 - spark.memory.fraction) spillable.available.memory = user.available.memory * hoodie.memory.fraction. Anytime\n-     * the spark.executor.memory or the spark.memory.fraction is changed, the memory used for spillable map changes\n-     * accordingly\n-     */\n-    private long getMaxMemoryAllowedForMerge(String maxMemoryFraction) {\n-      final String SPARK_EXECUTOR_MEMORY_PROP = \"spark.executor.memory\";\n-      final String SPARK_EXECUTOR_MEMORY_FRACTION_PROP = \"spark.memory.fraction\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/memory/UnifiedMemoryManager.scala#L231} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION = \"0.6\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/SparkContext.scala#L471} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_MB = \"1024\"; // in MB\n-\n-      if (SparkEnv.get() != null) {\n-        // 1 GB is the default conf used by Spark, look at SparkContext.scala\n-        long executorMemoryInBytes = Utils.memoryStringToMb(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_MB)) * 1024 * 1024L;\n-        // 0.6 is the default value used by Spark,\n-        // look at {@link\n-        // https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L507}\n-        double memoryFraction = Double.parseDouble(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_FRACTION_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION));\n-        double maxMemoryFractionForMerge = Double.parseDouble(maxMemoryFraction);\n-        double userAvailableMemory = executorMemoryInBytes * (1 - memoryFraction);\n-        long maxMemoryForMerge = (long) Math.floor(userAvailableMemory * maxMemoryFractionForMerge);\n-        return Math.max(DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES, maxMemoryForMerge);\n-      } else {\n-        return DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n-      }\n-    }\n-\n     public HoodieMemoryConfig build() {\n       HoodieMemoryConfig config = new HoodieMemoryConfig(props);\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP),\n           MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP, DEFAULT_MAX_MEMORY_FRACTION_FOR_COMPACTION);\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FRACTION_FOR_MERGE_PROP),\n           MAX_MEMORY_FRACTION_FOR_MERGE_PROP, DEFAULT_MAX_MEMORY_FRACTION_FOR_MERGE);\n+      long maxMemoryAllowedForMerge =\n+          SparkConfigUtils.getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_MERGE_PROP));\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FOR_MERGE_PROP), MAX_MEMORY_FOR_MERGE_PROP,\n-          String.valueOf(getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_MERGE_PROP))));\n+          String.valueOf(maxMemoryAllowedForMerge));\n+      long maxMemoryAllowedForCompaction =\n+          SparkConfigUtils.getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODMxNzEyMA=="}, "originalCommit": {"oid": "baed026d9f46420f9575741912ef8d6f84054a2d"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3MDI2MDc1OnYy", "diffSide": "LEFT", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQxMDozMTo0OVrOF8Agdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNlQxMDozMTo0OVrOF8Agdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODQ2NzE5MA==", "bodyText": "unused methods, remove them.", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398467190", "createdAt": "2020-03-26T10:31:49Z", "author": {"login": "leesf"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -503,22 +494,6 @@ public int getJmxPort() {\n   /**\n    * memory configs.\n    */\n-  public Double getMaxMemoryFractionPerPartitionMerge() {\n-    return Double.valueOf(props.getProperty(HoodieMemoryConfig.MAX_MEMORY_FRACTION_FOR_MERGE_PROP));\n-  }\n-\n-  public Double getMaxMemoryFractionPerCompaction() {\n-    return Double.valueOf(props.getProperty(HoodieMemoryConfig.MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP));\n-  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "52ad0ebaab4c41927975e5ff82997ab9d1515777"}, "originalPosition": 49}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4938, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}