{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM3NDY0NzQw", "number": 1752, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQyMTo1NTo0NlrOEHUecw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOTo1OTo1MVrOEJblmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MTEwOTYzOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/async/AsyncCompactService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQyMTo1NTo0NlrOGmognQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMFQyMTo1NTo0NlrOGmognQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE2Mjc4MQ==", "bodyText": "https://jira.apache.org/jira/browse/HUDI-1031 to add to docs", "url": "https://github.com/apache/hudi/pull/1752#discussion_r443162781", "createdAt": "2020-06-20T21:55:46Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/async/AsyncCompactService.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import org.apache.hudi.client.Compactor;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Async Compactor Service that runs in separate thread. Currently, only one compactor is allowed to run at any time.\n+ */\n+public class AsyncCompactService extends AbstractAsyncService {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(AsyncCompactService.class);\n+\n+  /**\n+   * This is the job pool used by async compaction.\n+   * In case of deltastreamer, Spark job scheduling configs are automatically set.\n+   * As the configs needs to be set before spark context is initiated, it is not\n+   * automated for Structured Streaming.\n+   * https://spark.apache.org/docs/latest/job-scheduling.html", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzEzNzMwOnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwNzo1NTowOFrOGp7QOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwNzo1NTowOFrOGp7QOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNTYwOA==", "bodyText": "scala has default values/optional parameters.. so instead of changing signature to pass empty everywhere.. we can just specify defaults here,", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446615608", "createdAt": "2020-06-28T07:55:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -48,7 +49,12 @@ private[hudi] object HoodieSparkSqlWriter {\n   def write(sqlContext: SQLContext,\n             mode: SaveMode,\n             parameters: Map[String, String],\n-            df: DataFrame): (Boolean, common.util.Option[String]) = {\n+            df: DataFrame,\n+            hoodieWriteClient: Option[HoodieWriteClient[HoodieRecordPayload[Nothing]]],", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzEzOTk1OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwNzo1ODo0MlrOGp7Rgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwNzo1ODo0MlrOGp7Rgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNTkzOA==", "bodyText": "is it possible that nothing is actually scheduled here, since there is nothiing to compact?", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446615938", "createdAt": "2020-06-28T07:58:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -279,6 +297,15 @@ private[hudi] object HoodieSparkSqlWriter {\n         log.info(\"Commit \" + instantTime + \" failed!\")\n       }\n \n+      val asyncCompactionEnabled = isAsyncCompactionEnabled(client, parameters, jsc.hadoopConfiguration())\n+      val compactionInstant : common.util.Option[java.lang.String] =\n+      if (asyncCompactionEnabled) {\n+        client.scheduleCompaction(common.util.Option.of(new util.HashMap[String, String](mapAsJavaMap(metaMap))))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzE0MTU0OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwODowMDozMlrOGp7SPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwODowMDozMlrOGp7SPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNjEyNA==", "bodyText": "what if the user sets the writeClient config for inline = false and does not set async compaction datasource option? should we control at a single level..", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446616124", "createdAt": "2020-06-28T08:00:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -303,6 +334,18 @@ private[hudi] object HoodieSparkSqlWriter {\n             }\n           })\n       }\n+      (false, common.util.Option.empty())\n+    }\n+  }\n+\n+  private def isAsyncCompactionEnabled(client: HoodieWriteClient[HoodieRecordPayload[Nothing]],\n+                                       parameters: Map[String, String], configuration: Configuration) : Boolean = {\n+    log.info(s\"Config.isInlineCompaction ? ${client.getConfig.isInlineCompaction}\")\n+    if (!client.getConfig.isInlineCompaction", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzE0NDQ3OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/java/org/apache/hudi/async/SparkStreamingWriterActivityDetector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwODowMzo0NVrOGp7TnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwODowMzo0NVrOGp7TnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNjQ3Nw==", "bodyText": "basic question..  if there are no writes , no compaction gets scheduled right? so async compaction is a no-op i.e it will check if there is some work to do, if not won't trigger anything?", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446616477", "createdAt": "2020-06-28T08:03:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/async/SparkStreamingWriterActivityDetector.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import java.util.function.Supplier;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * This class is used to detect activity of spark streaming writer. THis is used to decide if HoodieWriteClient\n+ * and async compactor needs to be closed. Spark Structured Streaming do not have explicit API on the Sink side to\n+ * determine if the stream is done. In this absence, async compactor proactively checks with the sink if it is\n+ * active. If there is no activity for sufficient period, async compactor shuts down. If the sink was indeed active,\n+ * a subsequent batch will re-trigger async compaction.\n+ */\n+public class SparkStreamingWriterActivityDetector {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzIyMDExOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/async/AsyncCompactService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOToyNjozOVrOGp742A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMToxMjo1OFrOG7xtOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjAwOA==", "bodyText": "move comments that refer to a sub-class impl to that class itself?", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446626008", "createdAt": "2020-06-28T09:26:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/async/AsyncCompactService.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import org.apache.hudi.client.Compactor;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Async Compactor Service that runs in separate thread. Currently, only one compactor is allowed to run at any time.\n+ */\n+public class AsyncCompactService extends AbstractAsyncService {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(AsyncCompactService.class);\n+\n+  /**\n+   * This is the job pool used by async compaction.\n+   * In case of deltastreamer, Spark job scheduling configs are automatically set.\n+   * As the configs needs to be set before spark context is initiated, it is not\n+   * automated for Structured Streaming.\n+   * https://spark.apache.org/docs/latest/job-scheduling.html\n+   */\n+  public static final String COMPACT_POOL_NAME = \"hoodiecompact\";\n+\n+  private final int maxConcurrentCompaction;\n+  private transient Compactor compactor;\n+  private transient JavaSparkContext jssc;\n+  private transient BlockingQueue<HoodieInstant> pendingCompactions = new LinkedBlockingQueue<>();\n+  private transient ReentrantLock queueLock = new ReentrantLock();\n+  private transient Condition consumed = queueLock.newCondition();\n+\n+  public AsyncCompactService(JavaSparkContext jssc, HoodieWriteClient client) {\n+    this.jssc = jssc;\n+    this.compactor = new Compactor(client, jssc);\n+    this.maxConcurrentCompaction = 1;\n+  }\n+\n+  /**\n+   * Enqueues new Pending compaction.\n+   */\n+  public void enqueuePendingCompaction(HoodieInstant instant) {\n+    pendingCompactions.add(instant);\n+  }\n+\n+  /**\n+   * Wait till outstanding pending compactions reduces to the passed in value.\n+   *\n+   * @param numPendingCompactions Maximum pending compactions allowed\n+   * @throws InterruptedException\n+   */\n+  public void waitTillPendingCompactionsReducesTo(int numPendingCompactions) throws InterruptedException {\n+    try {\n+      queueLock.lock();\n+      while (!isShutdown() && (pendingCompactions.size() > numPendingCompactions)) {\n+        consumed.await();\n+      }\n+    } finally {\n+      queueLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Fetch Next pending compaction if available.\n+   *\n+   * @return\n+   * @throws InterruptedException\n+   */\n+  private HoodieInstant fetchNextCompactionInstant() throws InterruptedException {\n+    LOG.info(\"Compactor waiting for next instant for compaction upto 60 seconds\");\n+    HoodieInstant instant = pendingCompactions.poll(10, TimeUnit.SECONDS);\n+    if (instant != null) {\n+      try {\n+        queueLock.lock();\n+        // Signal waiting thread\n+        consumed.signal();\n+      } finally {\n+        queueLock.unlock();\n+      }\n+    }\n+    return instant;\n+  }\n+\n+  /**\n+   * Start Compaction Service.\n+   */\n+  @Override\n+  protected Pair<CompletableFuture, ExecutorService> startService() {\n+    ExecutorService executor = Executors.newFixedThreadPool(maxConcurrentCompaction,\n+        r -> new Thread(r, \"async_compact_thread\"));\n+    return Pair.of(CompletableFuture.allOf(IntStream.range(0, maxConcurrentCompaction).mapToObj(i -> CompletableFuture.supplyAsync(() -> {\n+      try {\n+        // Set Compactor Pool Name for allowing users to prioritize compaction\n+        LOG.info(\"Setting Spark Pool name for compaction to \" + COMPACT_POOL_NAME);\n+        jssc.setLocalProperty(\"spark.scheduler.pool\", COMPACT_POOL_NAME);\n+\n+        while (!isShutdownRequested()) {\n+          final HoodieInstant instant = fetchNextCompactionInstant();\n+\n+          if (null != instant) {\n+            LOG.info(\"Starting Compaction for instant \" + instant);\n+            compactor.compact(instant);\n+            LOG.info(\"Finished Compaction for instant \" + instant);\n+          }\n+\n+          if (shouldStopCompactor()) {\n+            return true;\n+          }\n+        }\n+        LOG.info(\"Compactor shutting down properly!!\");\n+      } catch (InterruptedException ie) {\n+        LOG.warn(\"Compactor executor thread got interrupted exception. Stopping\", ie);\n+      } catch (IOException e) {\n+        LOG.error(\"Compactor executor failed\", e);\n+        throw new HoodieIOException(e.getMessage(), e);\n+      }\n+      return true;\n+    }, executor)).toArray(CompletableFuture[]::new)), executor);\n+  }\n+\n+  /**\n+   * Spark Structured Streaming Sink implementation do not have mechanism to know when the stream is shutdown.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMzU2Mg==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465333562", "createdAt": "2020-08-04T21:12:58Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/async/AsyncCompactService.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import org.apache.hudi.client.Compactor;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Async Compactor Service that runs in separate thread. Currently, only one compactor is allowed to run at any time.\n+ */\n+public class AsyncCompactService extends AbstractAsyncService {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(AsyncCompactService.class);\n+\n+  /**\n+   * This is the job pool used by async compaction.\n+   * In case of deltastreamer, Spark job scheduling configs are automatically set.\n+   * As the configs needs to be set before spark context is initiated, it is not\n+   * automated for Structured Streaming.\n+   * https://spark.apache.org/docs/latest/job-scheduling.html\n+   */\n+  public static final String COMPACT_POOL_NAME = \"hoodiecompact\";\n+\n+  private final int maxConcurrentCompaction;\n+  private transient Compactor compactor;\n+  private transient JavaSparkContext jssc;\n+  private transient BlockingQueue<HoodieInstant> pendingCompactions = new LinkedBlockingQueue<>();\n+  private transient ReentrantLock queueLock = new ReentrantLock();\n+  private transient Condition consumed = queueLock.newCondition();\n+\n+  public AsyncCompactService(JavaSparkContext jssc, HoodieWriteClient client) {\n+    this.jssc = jssc;\n+    this.compactor = new Compactor(client, jssc);\n+    this.maxConcurrentCompaction = 1;\n+  }\n+\n+  /**\n+   * Enqueues new Pending compaction.\n+   */\n+  public void enqueuePendingCompaction(HoodieInstant instant) {\n+    pendingCompactions.add(instant);\n+  }\n+\n+  /**\n+   * Wait till outstanding pending compactions reduces to the passed in value.\n+   *\n+   * @param numPendingCompactions Maximum pending compactions allowed\n+   * @throws InterruptedException\n+   */\n+  public void waitTillPendingCompactionsReducesTo(int numPendingCompactions) throws InterruptedException {\n+    try {\n+      queueLock.lock();\n+      while (!isShutdown() && (pendingCompactions.size() > numPendingCompactions)) {\n+        consumed.await();\n+      }\n+    } finally {\n+      queueLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Fetch Next pending compaction if available.\n+   *\n+   * @return\n+   * @throws InterruptedException\n+   */\n+  private HoodieInstant fetchNextCompactionInstant() throws InterruptedException {\n+    LOG.info(\"Compactor waiting for next instant for compaction upto 60 seconds\");\n+    HoodieInstant instant = pendingCompactions.poll(10, TimeUnit.SECONDS);\n+    if (instant != null) {\n+      try {\n+        queueLock.lock();\n+        // Signal waiting thread\n+        consumed.signal();\n+      } finally {\n+        queueLock.unlock();\n+      }\n+    }\n+    return instant;\n+  }\n+\n+  /**\n+   * Start Compaction Service.\n+   */\n+  @Override\n+  protected Pair<CompletableFuture, ExecutorService> startService() {\n+    ExecutorService executor = Executors.newFixedThreadPool(maxConcurrentCompaction,\n+        r -> new Thread(r, \"async_compact_thread\"));\n+    return Pair.of(CompletableFuture.allOf(IntStream.range(0, maxConcurrentCompaction).mapToObj(i -> CompletableFuture.supplyAsync(() -> {\n+      try {\n+        // Set Compactor Pool Name for allowing users to prioritize compaction\n+        LOG.info(\"Setting Spark Pool name for compaction to \" + COMPACT_POOL_NAME);\n+        jssc.setLocalProperty(\"spark.scheduler.pool\", COMPACT_POOL_NAME);\n+\n+        while (!isShutdownRequested()) {\n+          final HoodieInstant instant = fetchNextCompactionInstant();\n+\n+          if (null != instant) {\n+            LOG.info(\"Starting Compaction for instant \" + instant);\n+            compactor.compact(instant);\n+            LOG.info(\"Finished Compaction for instant \" + instant);\n+          }\n+\n+          if (shouldStopCompactor()) {\n+            return true;\n+          }\n+        }\n+        LOG.info(\"Compactor shutting down properly!!\");\n+      } catch (InterruptedException ie) {\n+        LOG.warn(\"Compactor executor thread got interrupted exception. Stopping\", ie);\n+      } catch (IOException e) {\n+        LOG.error(\"Compactor executor failed\", e);\n+        throw new HoodieIOException(e.getMessage(), e);\n+      }\n+      return true;\n+    }, executor)).toArray(CompletableFuture[]::new)), executor);\n+  }\n+\n+  /**\n+   * Spark Structured Streaming Sink implementation do not have mechanism to know when the stream is shutdown.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjAwOA=="}, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzIyMjE1OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/java/org/apache/hudi/async/SparkStreamingWriterActivityDetector.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOToyOTozMVrOGp753A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMToxMjo1MlrOG7xtCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI2OA==", "bodyText": "this does not mean there is no work for compaction right?", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446626268", "createdAt": "2020-06-28T09:29:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/async/SparkStreamingWriterActivityDetector.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import java.util.function.Supplier;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * This class is used to detect activity of spark streaming writer. THis is used to decide if HoodieWriteClient\n+ * and async compactor needs to be closed. Spark Structured Streaming do not have explicit API on the Sink side to\n+ * determine if the stream is done. In this absence, async compactor proactively checks with the sink if it is\n+ * active. If there is no activity for sufficient period, async compactor shuts down. If the sink was indeed active,\n+ * a subsequent batch will re-trigger async compaction.\n+ */\n+public class SparkStreamingWriterActivityDetector {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(SparkStreamingWriterActivityDetector.class);\n+\n+  private final Supplier<Long> lastStartBatchNanoTimeSupplier;\n+  private final Supplier<Long> lastEndBatchNanoTimeSupplier;\n+  private final long sinkInactivityTimeoutSecs;\n+\n+  private static final long SECS_TO_NANOS = 1000000000L;\n+\n+  public SparkStreamingWriterActivityDetector(\n+      Supplier<Long> lastStartBatchNanoTimeSupplier, Supplier<Long> lastEndBatchNanoTimeSupplier,\n+      long sinkInactivityTimeoutSecs) {\n+    this.lastStartBatchNanoTimeSupplier = lastStartBatchNanoTimeSupplier;\n+    this.lastEndBatchNanoTimeSupplier = lastEndBatchNanoTimeSupplier;\n+    this.sinkInactivityTimeoutSecs = sinkInactivityTimeoutSecs;\n+  }\n+\n+  /**\n+   * Detects if spark streaming write is still active based on time.\n+   * @return\n+   */\n+  public boolean hasRecentlyWritten() {\n+    long lastStartBatchTime = lastStartBatchNanoTimeSupplier.get();\n+    long lastEndBatchTime = lastEndBatchNanoTimeSupplier.get();\n+\n+    LOG.info(\"Checking if compactor needs to be stopped. \"\n+        + \"lastStartBatchTime=\" + lastStartBatchTime + \", lastEndBatchTime=\" + lastEndBatchTime\n+        + \", CurrTime=\" + System.nanoTime());\n+\n+    if (lastEndBatchTime - lastStartBatchTime < 0) {\n+      LOG.info(\"End Batch Time (\" + lastEndBatchTime + \") is less than Start Batch Time (\" + lastStartBatchTime + \")\"\n+          + \"Sink is running. So, no need to stop\");\n+      return true;\n+    }\n+\n+    long currTime = System.nanoTime();\n+    long elapsedTimeSecs = Double.valueOf(Math.ceil(1.0 * (currTime - lastEndBatchTime) / SECS_TO_NANOS)).longValue();\n+    if (elapsedTimeSecs > sinkInactivityTimeoutSecs) {\n+      LOG.warn(\"Streaming Sink has been idle for \" + elapsedTimeSecs + \" seconds\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMzUxNA==", "bodyText": "This code is deleted.", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465333514", "createdAt": "2020-08-04T21:12:52Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/async/SparkStreamingWriterActivityDetector.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import java.util.function.Supplier;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * This class is used to detect activity of spark streaming writer. THis is used to decide if HoodieWriteClient\n+ * and async compactor needs to be closed. Spark Structured Streaming do not have explicit API on the Sink side to\n+ * determine if the stream is done. In this absence, async compactor proactively checks with the sink if it is\n+ * active. If there is no activity for sufficient period, async compactor shuts down. If the sink was indeed active,\n+ * a subsequent batch will re-trigger async compaction.\n+ */\n+public class SparkStreamingWriterActivityDetector {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(SparkStreamingWriterActivityDetector.class);\n+\n+  private final Supplier<Long> lastStartBatchNanoTimeSupplier;\n+  private final Supplier<Long> lastEndBatchNanoTimeSupplier;\n+  private final long sinkInactivityTimeoutSecs;\n+\n+  private static final long SECS_TO_NANOS = 1000000000L;\n+\n+  public SparkStreamingWriterActivityDetector(\n+      Supplier<Long> lastStartBatchNanoTimeSupplier, Supplier<Long> lastEndBatchNanoTimeSupplier,\n+      long sinkInactivityTimeoutSecs) {\n+    this.lastStartBatchNanoTimeSupplier = lastStartBatchNanoTimeSupplier;\n+    this.lastEndBatchNanoTimeSupplier = lastEndBatchNanoTimeSupplier;\n+    this.sinkInactivityTimeoutSecs = sinkInactivityTimeoutSecs;\n+  }\n+\n+  /**\n+   * Detects if spark streaming write is still active based on time.\n+   * @return\n+   */\n+  public boolean hasRecentlyWritten() {\n+    long lastStartBatchTime = lastStartBatchNanoTimeSupplier.get();\n+    long lastEndBatchTime = lastEndBatchNanoTimeSupplier.get();\n+\n+    LOG.info(\"Checking if compactor needs to be stopped. \"\n+        + \"lastStartBatchTime=\" + lastStartBatchTime + \", lastEndBatchTime=\" + lastEndBatchTime\n+        + \", CurrTime=\" + System.nanoTime());\n+\n+    if (lastEndBatchTime - lastStartBatchTime < 0) {\n+      LOG.info(\"End Batch Time (\" + lastEndBatchTime + \") is less than Start Batch Time (\" + lastStartBatchTime + \")\"\n+          + \"Sink is running. So, no need to stop\");\n+      return true;\n+    }\n+\n+    long currTime = System.nanoTime();\n+    long elapsedTimeSecs = Double.valueOf(Math.ceil(1.0 * (currTime - lastEndBatchTime) / SECS_TO_NANOS)).longValue();\n+    if (elapsedTimeSecs > sinkInactivityTimeoutSecs) {\n+      LOG.warn(\"Streaming Sink has been idle for \" + elapsedTimeSecs + \" seconds\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI2OA=="}, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzIyOTMzOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOTozNzo1NVrOGp79XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTowOTowOVrOG7xl_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzE2NA==", "bodyText": "more importantly, we should also renable the test in TestDataSource", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446627164", "createdAt": "2020-06-28T09:37:55Z", "author": {"login": "vinothchandar"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java", "diffHunk": "@@ -58,6 +58,7 @@\n   protected static final String PRESTO_COORDINATOR = \"/presto-coordinator-1\";\n   protected static final String HOODIE_WS_ROOT = \"/var/hoodie/ws\";\n   protected static final String HOODIE_JAVA_APP = HOODIE_WS_ROOT + \"/hudi-spark/run_hoodie_app.sh\";\n+  protected static final String HOODIE_JAVA_STREAMING_APP = HOODIE_WS_ROOT + \"/hudi-spark/run_hoodie_streaming_app.sh\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMTcwOA==", "bodyText": "Enabled it after adding timed retry logic to wait for commits", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465331708", "createdAt": "2020-08-04T21:09:09Z", "author": {"login": "bvaradar"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java", "diffHunk": "@@ -58,6 +58,7 @@\n   protected static final String PRESTO_COORDINATOR = \"/presto-coordinator-1\";\n   protected static final String HOODIE_WS_ROOT = \"/var/hoodie/ws\";\n   protected static final String HOODIE_JAVA_APP = HOODIE_WS_ROOT + \"/hudi-spark/run_hoodie_app.sh\";\n+  protected static final String HOODIE_JAVA_STREAMING_APP = HOODIE_WS_ROOT + \"/hudi-spark/run_hoodie_streaming_app.sh\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzE2NA=="}, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzIzMzM4OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOTo0Mzo1MFrOGp7_dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMToxMjozNVrOG7xsfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzcwMg==", "bodyText": "just confirming that reuse of writeClient across batches is fine..", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446627702", "createdAt": "2020-06-28T09:43:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -38,46 +50,65 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n   private val retryIntervalMs = options(DataSourceWriteOptions.STREAMING_RETRY_INTERVAL_MS_OPT_KEY).toLong\n   private val ignoreFailedBatch = options(DataSourceWriteOptions.STREAMING_IGNORE_FAILED_BATCH_OPT_KEY).toBoolean\n \n+  private var isAsyncCompactorServiceShutdownAbnormally = false\n+\n   private val mode =\n     if (outputMode == OutputMode.Append()) {\n       SaveMode.Append\n     } else {\n       SaveMode.Overwrite\n     }\n \n-  override def addBatch(batchId: Long, data: DataFrame): Unit = {\n+  private var asyncCompactorService : AsyncCompactService = _\n+  private var writeClient : Option[HoodieWriteClient[HoodieRecordPayload[Nothing]]] = Option.empty\n+  private var lastStartBatchTimeNanos : lang.Long = System.nanoTime()\n+  private var lastEndBatchTimeNanos : lang.Long = System.nanoTime()\n+\n+  override def addBatch(batchId: Long, data: DataFrame): Unit = this.synchronized {\n+    if (isAsyncCompactorServiceShutdownAbnormally)  {\n+      throw new IllegalStateException(\"Async Compaction shutdown unexpectedly\")\n+    }\n+\n+    lastStartBatchTimeNanos = System.nanoTime()\n+\n     retry(retryCnt, retryIntervalMs)(\n       Try(\n         HoodieSparkSqlWriter.write(\n-          sqlContext,\n-          mode,\n-          options,\n-          data)\n+          sqlContext, mode, options, data, writeClient, Some(triggerAsyncCompactor))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMzM3Mg==", "bodyText": "Yes, this worked fine.", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465333372", "createdAt": "2020-08-04T21:12:35Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -38,46 +50,65 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n   private val retryIntervalMs = options(DataSourceWriteOptions.STREAMING_RETRY_INTERVAL_MS_OPT_KEY).toLong\n   private val ignoreFailedBatch = options(DataSourceWriteOptions.STREAMING_IGNORE_FAILED_BATCH_OPT_KEY).toBoolean\n \n+  private var isAsyncCompactorServiceShutdownAbnormally = false\n+\n   private val mode =\n     if (outputMode == OutputMode.Append()) {\n       SaveMode.Append\n     } else {\n       SaveMode.Overwrite\n     }\n \n-  override def addBatch(batchId: Long, data: DataFrame): Unit = {\n+  private var asyncCompactorService : AsyncCompactService = _\n+  private var writeClient : Option[HoodieWriteClient[HoodieRecordPayload[Nothing]]] = Option.empty\n+  private var lastStartBatchTimeNanos : lang.Long = System.nanoTime()\n+  private var lastEndBatchTimeNanos : lang.Long = System.nanoTime()\n+\n+  override def addBatch(batchId: Long, data: DataFrame): Unit = this.synchronized {\n+    if (isAsyncCompactorServiceShutdownAbnormally)  {\n+      throw new IllegalStateException(\"Async Compaction shutdown unexpectedly\")\n+    }\n+\n+    lastStartBatchTimeNanos = System.nanoTime()\n+\n     retry(retryCnt, retryIntervalMs)(\n       Try(\n         HoodieSparkSqlWriter.write(\n-          sqlContext,\n-          mode,\n-          options,\n-          data)\n+          sqlContext, mode, options, data, writeClient, Some(triggerAsyncCompactor))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzcwMg=="}, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzIzNjA1OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOTo0Njo0NVrOGp8AtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTowOTo1MlrOG7xnVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODAyMQ==", "bodyText": "this alone should be good enough to prevent the jvm from not hanging during exit? do we really need the laststart/lastend logic?", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446628021", "createdAt": "2020-06-28T09:46:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -111,12 +143,64 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n \n   @annotation.tailrec\n   private def retry[T](n: Int, waitInMillis: Long)(fn: => Try[T]): Try[T] = {\n+    lastStartBatchTimeNanos = System.nanoTime()\n     fn match {\n-      case x: util.Success[T] => x\n+      case x: Success[T] =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        x\n       case _ if n > 1 =>\n         Thread.sleep(waitInMillis)\n+        lastEndBatchTimeNanos = System.nanoTime()\n         retry(n - 1, waitInMillis * 2)(fn)\n-      case f => f\n+      case f =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        reset(false)\n+        f\n+    }\n+  }\n+\n+  protected def triggerAsyncCompactor(client: HoodieWriteClient[HoodieRecordPayload[Nothing]]): Unit = {\n+    log.info(\"Triggering Async compaction !!\")\n+    if (null == asyncCompactorService) {\n+      asyncCompactorService = new SparkStreamingAsyncCompactService(new JavaSparkContext(sqlContext.sparkContext),\n+        client, new SparkStreamingWriterActivityDetector(new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastStartBatchTimeNanos\n+        }, new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastEndBatchTimeNanos\n+        }, 10))\n+      asyncCompactorService.start(new Function[java.lang.Boolean, java.lang.Boolean] {\n+        override def apply(errored: lang.Boolean): lang.Boolean = {\n+          log.info(s\"Async Compactor shutdown. Errored ? $errored\")\n+          isAsyncCompactorServiceShutdownAbnormally = errored\n+          reset(false)\n+          log.info(\"Done resetting write client.\")\n+          true\n+        }\n+      })\n+\n+      // Add Shutdown Hook\n+      Runtime.getRuntime.addShutdownHook(new Thread(new Runnable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 162}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMjA1Mw==", "bodyText": "Yes, this and setting daemon mode was good enough", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465332053", "createdAt": "2020-08-04T21:09:52Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -111,12 +143,64 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n \n   @annotation.tailrec\n   private def retry[T](n: Int, waitInMillis: Long)(fn: => Try[T]): Try[T] = {\n+    lastStartBatchTimeNanos = System.nanoTime()\n     fn match {\n-      case x: util.Success[T] => x\n+      case x: Success[T] =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        x\n       case _ if n > 1 =>\n         Thread.sleep(waitInMillis)\n+        lastEndBatchTimeNanos = System.nanoTime()\n         retry(n - 1, waitInMillis * 2)(fn)\n-      case f => f\n+      case f =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        reset(false)\n+        f\n+    }\n+  }\n+\n+  protected def triggerAsyncCompactor(client: HoodieWriteClient[HoodieRecordPayload[Nothing]]): Unit = {\n+    log.info(\"Triggering Async compaction !!\")\n+    if (null == asyncCompactorService) {\n+      asyncCompactorService = new SparkStreamingAsyncCompactService(new JavaSparkContext(sqlContext.sparkContext),\n+        client, new SparkStreamingWriterActivityDetector(new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastStartBatchTimeNanos\n+        }, new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastEndBatchTimeNanos\n+        }, 10))\n+      asyncCompactorService.start(new Function[java.lang.Boolean, java.lang.Boolean] {\n+        override def apply(errored: lang.Boolean): lang.Boolean = {\n+          log.info(s\"Async Compactor shutdown. Errored ? $errored\")\n+          isAsyncCompactorServiceShutdownAbnormally = errored\n+          reset(false)\n+          log.info(\"Done resetting write client.\")\n+          true\n+        }\n+      })\n+\n+      // Add Shutdown Hook\n+      Runtime.getRuntime.addShutdownHook(new Thread(new Runnable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODAyMQ=="}, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzIzNjQ5OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOTo0NzozMlrOGp8A9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMToxMDozOVrOG7xovA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODA4NQ==", "bodyText": "Seems like this will happen each trigger/ not just first time?", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446628085", "createdAt": "2020-06-28T09:47:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -111,12 +143,64 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n \n   @annotation.tailrec\n   private def retry[T](n: Int, waitInMillis: Long)(fn: => Try[T]): Try[T] = {\n+    lastStartBatchTimeNanos = System.nanoTime()\n     fn match {\n-      case x: util.Success[T] => x\n+      case x: Success[T] =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        x\n       case _ if n > 1 =>\n         Thread.sleep(waitInMillis)\n+        lastEndBatchTimeNanos = System.nanoTime()\n         retry(n - 1, waitInMillis * 2)(fn)\n-      case f => f\n+      case f =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        reset(false)\n+        f\n+    }\n+  }\n+\n+  protected def triggerAsyncCompactor(client: HoodieWriteClient[HoodieRecordPayload[Nothing]]): Unit = {\n+    log.info(\"Triggering Async compaction !!\")\n+    if (null == asyncCompactorService) {\n+      asyncCompactorService = new SparkStreamingAsyncCompactService(new JavaSparkContext(sqlContext.sparkContext),\n+        client, new SparkStreamingWriterActivityDetector(new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastStartBatchTimeNanos\n+        }, new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastEndBatchTimeNanos\n+        }, 10))\n+      asyncCompactorService.start(new Function[java.lang.Boolean, java.lang.Boolean] {\n+        override def apply(errored: lang.Boolean): lang.Boolean = {\n+          log.info(s\"Async Compactor shutdown. Errored ? $errored\")\n+          isAsyncCompactorServiceShutdownAbnormally = errored\n+          reset(false)\n+          log.info(\"Done resetting write client.\")\n+          true\n+        }\n+      })\n+\n+      // Add Shutdown Hook\n+      Runtime.getRuntime.addShutdownHook(new Thread(new Runnable {\n+        override def run(): Unit = reset(true)\n+      }))\n+\n+      // First time, scan .hoodie folder and get all pending compactions\n+      val metaClient = new HoodieTableMetaClient(sqlContext.sparkContext.hadoopConfiguration,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMjQxMg==", "bodyText": "Now, only for the first time when async compactor is null.", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465332412", "createdAt": "2020-08-04T21:10:39Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -111,12 +143,64 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n \n   @annotation.tailrec\n   private def retry[T](n: Int, waitInMillis: Long)(fn: => Try[T]): Try[T] = {\n+    lastStartBatchTimeNanos = System.nanoTime()\n     fn match {\n-      case x: util.Success[T] => x\n+      case x: Success[T] =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        x\n       case _ if n > 1 =>\n         Thread.sleep(waitInMillis)\n+        lastEndBatchTimeNanos = System.nanoTime()\n         retry(n - 1, waitInMillis * 2)(fn)\n-      case f => f\n+      case f =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        reset(false)\n+        f\n+    }\n+  }\n+\n+  protected def triggerAsyncCompactor(client: HoodieWriteClient[HoodieRecordPayload[Nothing]]): Unit = {\n+    log.info(\"Triggering Async compaction !!\")\n+    if (null == asyncCompactorService) {\n+      asyncCompactorService = new SparkStreamingAsyncCompactService(new JavaSparkContext(sqlContext.sparkContext),\n+        client, new SparkStreamingWriterActivityDetector(new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastStartBatchTimeNanos\n+        }, new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastEndBatchTimeNanos\n+        }, 10))\n+      asyncCompactorService.start(new Function[java.lang.Boolean, java.lang.Boolean] {\n+        override def apply(errored: lang.Boolean): lang.Boolean = {\n+          log.info(s\"Async Compactor shutdown. Errored ? $errored\")\n+          isAsyncCompactorServiceShutdownAbnormally = errored\n+          reset(false)\n+          log.info(\"Done resetting write client.\")\n+          true\n+        }\n+      })\n+\n+      // Add Shutdown Hook\n+      Runtime.getRuntime.addShutdownHook(new Thread(new Runnable {\n+        override def run(): Unit = reset(true)\n+      }))\n+\n+      // First time, scan .hoodie folder and get all pending compactions\n+      val metaClient = new HoodieTableMetaClient(sqlContext.sparkContext.hadoopConfiguration,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODA4NQ=="}, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzIzNzE0OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/test/java/HoodieJavaStreamingApp.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOTo0ODoyMlrOGp8BSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTowNzozM1rOG7xi9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODE2OQ==", "bodyText": "why move to COW?", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446628169", "createdAt": "2020-06-28T09:48:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/test/java/HoodieJavaStreamingApp.java", "diffHunk": "@@ -68,7 +74,7 @@\n   private String tableName = \"hoodie_test\";\n \n   @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n-  private String tableType = HoodieTableType.MERGE_ON_READ.name();\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMDkzMg==", "bodyText": "Reverted.", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465330932", "createdAt": "2020-08-04T21:07:33Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/test/java/HoodieJavaStreamingApp.java", "diffHunk": "@@ -68,7 +74,7 @@\n   private String tableName = \"hoodie_test\";\n \n   @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n-  private String tableType = HoodieTableType.MERGE_ON_READ.name();\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODE2OQ=="}, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MzI0NjMzOnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOFQwOTo1OTo1MVrOGp8Frw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTowNzo1NVrOG7xjqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyOTI5NQ==", "bodyText": "https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-StreamingQueryManager.html seems like there are some listeners we can exploit to know of a StreamingQuery?", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446629295", "createdAt": "2020-06-28T09:59:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -38,46 +50,65 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n   private val retryIntervalMs = options(DataSourceWriteOptions.STREAMING_RETRY_INTERVAL_MS_OPT_KEY).toLong\n   private val ignoreFailedBatch = options(DataSourceWriteOptions.STREAMING_IGNORE_FAILED_BATCH_OPT_KEY).toBoolean\n \n+  private var isAsyncCompactorServiceShutdownAbnormally = false", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMTExMw==", "bodyText": "Thanks. Fixed by setting up daemon mode for async compactor thread", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465331113", "createdAt": "2020-08-04T21:07:55Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -38,46 +50,65 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n   private val retryIntervalMs = options(DataSourceWriteOptions.STREAMING_RETRY_INTERVAL_MS_OPT_KEY).toLong\n   private val ignoreFailedBatch = options(DataSourceWriteOptions.STREAMING_IGNORE_FAILED_BATCH_OPT_KEY).toBoolean\n \n+  private var isAsyncCompactorServiceShutdownAbnormally = false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyOTI5NQ=="}, "originalCommit": {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8"}, "originalPosition": 30}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4501, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}