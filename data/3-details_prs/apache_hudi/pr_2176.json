{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAzNDUwNzY3", "number": 2176, "title": "[HUDI-1327] Introduce base implementation of hudi-flink-client", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nProvide base implementation of hudi-flink-client\nsupport only COW table;\nsupport only insert and update\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-10-14T15:16:51Z", "url": "https://github.com/apache/hudi/pull/2176", "merged": true, "mergeCommit": {"oid": "4d05680038752077ceaebef261b66a5afc761e10"}, "closed": true, "closedAt": "2020-11-18T09:57:12Z", "author": {"login": "wangxianghu"}, "timelineItems": {"totalCount": 56, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdTZ57pABqjM4ODkxNjYxOTg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABddrIiegFqTUzMzI3NjE2NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0530bab8e0915a0b04fade2af3a9ec26e0ab129f", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/0530bab8e0915a0b04fade2af3a9ec26e0ab129f", "committedDate": "2020-10-14T15:11:24Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "cc59e8c86d3a494f768ff2935935000cb0c83926", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/cc59e8c86d3a494f768ff2935935000cb0c83926", "committedDate": "2020-10-17T12:12:11Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cc59e8c86d3a494f768ff2935935000cb0c83926", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/cc59e8c86d3a494f768ff2935935000cb0c83926", "committedDate": "2020-10-17T12:12:11Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "7abb6263e790fc08c62913e45c5ced67f735cd3a", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/7abb6263e790fc08c62913e45c5ced67f735cd3a", "committedDate": "2020-10-21T13:00:24Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7abb6263e790fc08c62913e45c5ced67f735cd3a", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/7abb6263e790fc08c62913e45c5ced67f735cd3a", "committedDate": "2020-10-21T13:00:24Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "ff73a101177ac1dbed4416b5e65f201ffb8ea9a5", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/ff73a101177ac1dbed4416b5e65f201ffb8ea9a5", "committedDate": "2020-10-21T13:46:20Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ff73a101177ac1dbed4416b5e65f201ffb8ea9a5", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/ff73a101177ac1dbed4416b5e65f201ffb8ea9a5", "committedDate": "2020-10-21T13:46:20Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "2a41baf77fb22d3ac6dba8c48e33b1a6e0844c05", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/2a41baf77fb22d3ac6dba8c48e33b1a6e0844c05", "committedDate": "2020-10-22T07:46:36Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2a41baf77fb22d3ac6dba8c48e33b1a6e0844c05", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/2a41baf77fb22d3ac6dba8c48e33b1a6e0844c05", "committedDate": "2020-10-22T07:46:36Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "f651233659b6e1550d0c2e54b659d99d7563ef82", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/f651233659b6e1550d0c2e54b659d99d7563ef82", "committedDate": "2020-10-26T03:21:22Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f651233659b6e1550d0c2e54b659d99d7563ef82", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/f651233659b6e1550d0c2e54b659d99d7563ef82", "committedDate": "2020-10-26T03:21:22Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "a6d804ef3d32a24505ec7e592aaa8fdb72e12cba", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a6d804ef3d32a24505ec7e592aaa8fdb72e12cba", "committedDate": "2020-10-26T07:43:37Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a6d804ef3d32a24505ec7e592aaa8fdb72e12cba", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a6d804ef3d32a24505ec7e592aaa8fdb72e12cba", "committedDate": "2020-10-26T07:43:37Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "40670f5bc8805f79eed14c6e7aca08bcb7cb7d53", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/40670f5bc8805f79eed14c6e7aca08bcb7cb7d53", "committedDate": "2020-11-03T02:06:16Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "40670f5bc8805f79eed14c6e7aca08bcb7cb7d53", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/40670f5bc8805f79eed14c6e7aca08bcb7cb7d53", "committedDate": "2020-11-03T02:06:16Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "e8b1d21c4b80ffce58bac20312bc96db226a73cd", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/e8b1d21c4b80ffce58bac20312bc96db226a73cd", "committedDate": "2020-11-04T07:53:35Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e8b1d21c4b80ffce58bac20312bc96db226a73cd", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/e8b1d21c4b80ffce58bac20312bc96db226a73cd", "committedDate": "2020-11-04T07:53:35Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "7c1912c346e1e1bf9dd428fe2e90f904ddbfe838", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/7c1912c346e1e1bf9dd428fe2e90f904ddbfe838", "committedDate": "2020-11-04T08:37:25Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7c1912c346e1e1bf9dd428fe2e90f904ddbfe838", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/7c1912c346e1e1bf9dd428fe2e90f904ddbfe838", "committedDate": "2020-11-04T08:37:25Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/0982523325a76046430bf3ed8c0e7360fca0a115", "committedDate": "2020-11-04T09:01:52Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI0OTE4ODE1", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-524918815", "createdAt": "2020-11-06T07:47:38Z", "commit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNzo0NzozOFrOHujQrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNzo0NzozOFrOHujQrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3NDI1NQ==", "bodyText": "Since this is the entry point, it would be better to add some description.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518574255", "createdAt": "2020-11-06T07:47:38Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI0OTE5Nzg0", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-524919784", "createdAt": "2020-11-06T07:49:33Z", "commit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNzo0OTozM1rOHujTrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNzo0OTozM1rOHujTrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3NTAyMw==", "bodyText": "Please add a description here, why we must hard-code this config option.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518575023", "createdAt": "2020-11-06T07:49:33Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI0OTU5OTgw", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-524959980", "createdAt": "2020-11-06T08:53:34Z", "commit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwODo1MzozNFrOHulNPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMDowMzoyOVrOHunpfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYwNjE0Mg==", "bodyText": "The indent is bad.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518606142", "createdAt": "2020-11-06T08:53:34Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new KafkaJson2HoodieRecord(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .setParallelism(1)\n+        .keyBy(HoodieRecord::getPartitionPath)\n+        .transform(WriteProcessOperator.NAME, TypeInformation.of(new TypeHint<Tuple3<String, List<WriteStatus>, Integer>>() {\n+        }), new WriteProcessOperator(new KeyedWriteProcessFunction())).name(\"write_process\").uid(\"write_process_uid\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYwNjU1Ng==", "bodyText": "Add a description about why hard code this parallelism.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518606556", "createdAt": "2020-11-06T08:54:20Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new KafkaJson2HoodieRecord(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .setParallelism(1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYyMDgxNA==", "bodyText": "Hudi write job via Flink? And can we add some variables to distinguish different hudi job? e.g. table name?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518620814", "createdAt": "2020-11-06T09:20:12Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new KafkaJson2HoodieRecord(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .setParallelism(1)\n+        .keyBy(HoodieRecord::getPartitionPath)\n+        .transform(WriteProcessOperator.NAME, TypeInformation.of(new TypeHint<Tuple3<String, List<WriteStatus>, Integer>>() {\n+        }), new WriteProcessOperator(new KeyedWriteProcessFunction())).name(\"write_process\").uid(\"write_process_uid\")\n+        .setParallelism(env.getParallelism())\n+        .addSink(new CommitSink()).name(\"commit_sink\").uid(\"commit_sink_uid\")\n+        .setParallelism(1);\n+\n+    env.execute(\"Hudi write via Flink\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYzMTM0Nw==", "bodyText": "keep the same style as others?  add the access modifier", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518631347", "createdAt": "2020-11-06T09:38:04Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieWriteConfig writeConfig;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  List<String> latestInstantList = new ArrayList<>(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYzMTM3NA==", "bodyText": "split the static fields and non-static fields", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518631374", "createdAt": "2020-11-06T09:38:07Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY0MjYxMA==", "bodyText": "It seems bufferedRecords sounds better?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518642610", "createdAt": "2020-11-06T09:57:26Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieWriteConfig writeConfig;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> records = new LinkedList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY0NjE0MQ==", "bodyText": "Why do two times call this method?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518646141", "createdAt": "2020-11-06T10:03:29Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieWriteConfig writeConfig;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> records = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      records.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), writeConfig);\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!records.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+    super.prepareSnapshotPreBarrier(checkpointId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 119}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/0982523325a76046430bf3ed8c0e7360fca0a115", "committedDate": "2020-11-04T09:01:52Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "735e85180bcc913d6076281a8f97df7d70efb06e", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/735e85180bcc913d6076281a8f97df7d70efb06e", "committedDate": "2020-11-08T13:05:34Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "735e85180bcc913d6076281a8f97df7d70efb06e", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/735e85180bcc913d6076281a8f97df7d70efb06e", "committedDate": "2020-11-08T13:05:34Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/f5b5a94c56f47282659dbc2535785fa1d38736b5", "committedDate": "2020-11-09T03:21:46Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/f5b5a94c56f47282659dbc2535785fa1d38736b5", "committedDate": "2020-11-09T03:21:46Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "bb491fdd707e47c5ef4073a79d2f3796bd5c7e4a", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/bb491fdd707e47c5ef4073a79d2f3796bd5c7e4a", "committedDate": "2020-11-09T05:44:50Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bb491fdd707e47c5ef4073a79d2f3796bd5c7e4a", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/bb491fdd707e47c5ef4073a79d2f3796bd5c7e4a", "committedDate": "2020-11-09T05:44:50Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "268e145358dc6f9ddabc32653498c741daf78d6e", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/268e145358dc6f9ddabc32653498c741daf78d6e", "committedDate": "2020-11-09T06:46:41Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "268e145358dc6f9ddabc32653498c741daf78d6e", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/268e145358dc6f9ddabc32653498c741daf78d6e", "committedDate": "2020-11-09T06:46:41Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "9bbaef1c6844bca962598ce37c42c8deb7fc9fc9", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/9bbaef1c6844bca962598ce37c42c8deb7fc9fc9", "committedDate": "2020-11-09T07:04:16Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9bbaef1c6844bca962598ce37c42c8deb7fc9fc9", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/9bbaef1c6844bca962598ce37c42c8deb7fc9fc9", "committedDate": "2020-11-09T07:04:16Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "a1badd5987c384f72fc9c6c37be6dd6a076d2cbb", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a1badd5987c384f72fc9c6c37be6dd6a076d2cbb", "committedDate": "2020-11-09T07:30:50Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a1badd5987c384f72fc9c6c37be6dd6a076d2cbb", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a1badd5987c384f72fc9c6c37be6dd6a076d2cbb", "committedDate": "2020-11-09T07:30:50Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/07097e7fec159248ae93f154b35e873b96cd2515", "committedDate": "2020-11-09T07:45:20Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1OTAxNTA2", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-525901506", "createdAt": "2020-11-09T03:23:56Z", "commit": {"oid": "735e85180bcc913d6076281a8f97df7d70efb06e"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwMzoyNDo0MFrOHvd01g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTo0MTowMFrOHvmR8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUzMzc4Mg==", "bodyText": "WDYT about renaming to KeyedWriteProcessOperator? Since you use KeyedWriteProcessFunction.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519533782", "createdAt": "2020-11-09T03:24:40Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/WriteProcessOperator.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class WriteProcessOperator extends KeyedProcessOperator<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU3MjkwNw==", "bodyText": "It would be better to add xxxFunction as the suffix. wdyt?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519572907", "createdAt": "2020-11-09T06:17:01Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/source/KafkaJson2HoodieRecord.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.source;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.keygen.KeyGenerator;\n+import org.apache.hudi.schema.FilebasedSchemaProvider;\n+import org.apache.hudi.util.AvroConvertor;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+public class KafkaJson2HoodieRecord implements MapFunction<String, HoodieRecord> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU3Mjk4NQ==", "bodyText": "Add a new empty line.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519572985", "createdAt": "2020-11-09T06:17:22Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/source/KafkaJson2HoodieRecord.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.source;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.keygen.KeyGenerator;\n+import org.apache.hudi.schema.FilebasedSchemaProvider;\n+import org.apache.hudi.util.AvroConvertor;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+public class KafkaJson2HoodieRecord implements MapFunction<String, HoodieRecord> {\n+\n+  private static Logger LOG = LoggerFactory.getLogger(KafkaJson2HoodieRecord.class);\n+\n+  private final HudiFlinkStreamer.Config cfg;\n+  private TypedProperties props;\n+  private KeyGenerator keyGenerator;\n+  private AvroConvertor convertor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYwMTA0NA==", "bodyText": "Can be a local variable.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519601044", "createdAt": "2020-11-09T07:30:44Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bbaef1c6844bca962598ce37c42c8deb7fc9fc9"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY1MDE0OQ==", "bodyText": "Never thrown exception.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519650149", "createdAt": "2020-11-09T09:05:58Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+\n+    indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(new org.apache.hadoop.conf.Configuration());\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(serializableHadoopConf, new FlinkTaskContextSupplier(getRuntimeContext()));\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(context, writeConfig);\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY1MzI1Nw==", "bodyText": "I am thinking one thing: if the interval of the checkpoint is too long. If this buffer would cause OOM?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519653257", "createdAt": "2020-11-09T09:11:14Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+\n+    indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(new org.apache.hadoop.conf.Configuration());\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(serializableHadoopConf, new FlinkTaskContextSupplier(getRuntimeContext()));\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(context, writeConfig);\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    // get latest requested instant\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    List<String> latestInstants = writeClient.getInflightsAndRequestedInstants(commitType);\n+    latestInstant = latestInstants.isEmpty() ? null : latestInstants.get(0);\n+\n+    if (output != null && latestInstant != null && records.size() > 0) {\n+      hasRecordsIn = true;\n+      String instantTimestamp = latestInstant;\n+      LOG.info(\"Upsert records, subtask id = [{}]  checkpoint_id = [{}}] instant = [{}], record size = [{}]\", indexOfThisSubtask, context.getCheckpointId(), instantTimestamp, records.size());\n+\n+      List<WriteStatus> writeStatus;\n+      switch (cfg.operation) {\n+        case INSERT:\n+          writeStatus = writeClient.insert(records, instantTimestamp);\n+          break;\n+        case UPSERT:\n+          writeStatus = writeClient.upsert(records, instantTimestamp);\n+          break;\n+        default:\n+          throw new HoodieFlinkStreamerException(\"Unknown operation : \" + cfg.operation);\n+      }\n+      output.collect(new Tuple3<>(instantTimestamp, writeStatus, indexOfThisSubtask));\n+      records.clear();\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext functionInitializationContext) throws Exception {\n+    // no operation\n+  }\n+\n+  @Override\n+  public void processElement(HoodieRecord hoodieRecord, Context context, Collector<Tuple3<String, List<WriteStatus>, Integer>> collector) throws Exception {\n+    records.add(hoodieRecord);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY1MzU5Mw==", "bodyText": "never used Exception.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519653593", "createdAt": "2020-11-09T09:11:47Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+\n+    indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(new org.apache.hadoop.conf.Configuration());\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(serializableHadoopConf, new FlinkTaskContextSupplier(getRuntimeContext()));\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(context, writeConfig);\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    // get latest requested instant\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    List<String> latestInstants = writeClient.getInflightsAndRequestedInstants(commitType);\n+    latestInstant = latestInstants.isEmpty() ? null : latestInstants.get(0);\n+\n+    if (output != null && latestInstant != null && records.size() > 0) {\n+      hasRecordsIn = true;\n+      String instantTimestamp = latestInstant;\n+      LOG.info(\"Upsert records, subtask id = [{}]  checkpoint_id = [{}}] instant = [{}], record size = [{}]\", indexOfThisSubtask, context.getCheckpointId(), instantTimestamp, records.size());\n+\n+      List<WriteStatus> writeStatus;\n+      switch (cfg.operation) {\n+        case INSERT:\n+          writeStatus = writeClient.insert(records, instantTimestamp);\n+          break;\n+        case UPSERT:\n+          writeStatus = writeClient.upsert(records, instantTimestamp);\n+          break;\n+        default:\n+          throw new HoodieFlinkStreamerException(\"Unknown operation : \" + cfg.operation);\n+      }\n+      output.collect(new Tuple3<>(instantTimestamp, writeStatus, indexOfThisSubtask));\n+      records.clear();\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext functionInitializationContext) throws Exception {\n+    // no operation\n+  }\n+\n+  @Override\n+  public void processElement(HoodieRecord hoodieRecord, Context context, Collector<Tuple3<String, List<WriteStatus>, Integer>> collector) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY1Mzc3NQ==", "bodyText": "never used exception", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519653775", "createdAt": "2020-11-09T09:12:04Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+\n+    indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(new org.apache.hadoop.conf.Configuration());\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(serializableHadoopConf, new FlinkTaskContextSupplier(getRuntimeContext()));\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(context, writeConfig);\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    // get latest requested instant\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    List<String> latestInstants = writeClient.getInflightsAndRequestedInstants(commitType);\n+    latestInstant = latestInstants.isEmpty() ? null : latestInstants.get(0);\n+\n+    if (output != null && latestInstant != null && records.size() > 0) {\n+      hasRecordsIn = true;\n+      String instantTimestamp = latestInstant;\n+      LOG.info(\"Upsert records, subtask id = [{}]  checkpoint_id = [{}}] instant = [{}], record size = [{}]\", indexOfThisSubtask, context.getCheckpointId(), instantTimestamp, records.size());\n+\n+      List<WriteStatus> writeStatus;\n+      switch (cfg.operation) {\n+        case INSERT:\n+          writeStatus = writeClient.insert(records, instantTimestamp);\n+          break;\n+        case UPSERT:\n+          writeStatus = writeClient.upsert(records, instantTimestamp);\n+          break;\n+        default:\n+          throw new HoodieFlinkStreamerException(\"Unknown operation : \" + cfg.operation);\n+      }\n+      output.collect(new Tuple3<>(instantTimestamp, writeStatus, indexOfThisSubtask));\n+      records.clear();\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext functionInitializationContext) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY2OTEwMg==", "bodyText": "It would be better to explain why do we need to implement the snapshotState method both in UDF and operator?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519669102", "createdAt": "2020-11-09T09:36:09Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/WriteProcessOperator.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class WriteProcessOperator extends KeyedProcessOperator<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> {\n+  public static final String NAME = \"WriteProcessOperator\";\n+  private static final Logger LOG = LoggerFactory.getLogger(WriteProcessOperator.class);\n+  private KeyedWriteProcessFunction function;\n+\n+  public WriteProcessOperator(KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> function) {\n+    super(function);\n+    this.function = (KeyedWriteProcessFunction) function;\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY3MDY4Ng==", "bodyText": "It would be better to make the value variable more reasonable.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519670686", "createdAt": "2020-11-09T09:38:33Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sink;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class CommitSink extends RichSinkFunction<Tuple3<String, List<WriteStatus>, Integer>> {\n+  private static final Logger LOG = LoggerFactory.getLogger(CommitSink.class);\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  private Map<String, List<List<WriteStatus>>> bufferedWriteStatus = new HashMap<>();\n+\n+  private Integer upsertParallelSize = 0;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+    upsertParallelSize = getRuntimeContext().getExecutionConfig().getParallelism();\n+\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(new HoodieFlinkEngineContext(new FlinkTaskContextSupplier(null)), writeConfig);\n+  }\n+\n+  @Override\n+  public void invoke(Tuple3<String, List<WriteStatus>, Integer> value, Context context) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY3MTcyOA==", "bodyText": "You do not use the returned value, right?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519671728", "createdAt": "2020-11-09T09:40:09Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sink;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class CommitSink extends RichSinkFunction<Tuple3<String, List<WriteStatus>, Integer>> {\n+  private static final Logger LOG = LoggerFactory.getLogger(CommitSink.class);\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  private Map<String, List<List<WriteStatus>>> bufferedWriteStatus = new HashMap<>();\n+\n+  private Integer upsertParallelSize = 0;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+    upsertParallelSize = getRuntimeContext().getExecutionConfig().getParallelism();\n+\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(new HoodieFlinkEngineContext(new FlinkTaskContextSupplier(null)), writeConfig);\n+  }\n+\n+  @Override\n+  public void invoke(Tuple3<String, List<WriteStatus>, Integer> value, Context context) throws Exception {\n+    LOG.info(\"Receive records, instantTime = [{}], subtaskId = [{}], records size = [{}]\", value.f0, value.f2, value.f1.size());\n+    try {\n+      if (bufferedWriteStatus.containsKey(value.f0)) {\n+        bufferedWriteStatus.get(value.f0).add(value.f1);\n+      } else {\n+        List<List<WriteStatus>> oneBatchData = new ArrayList<>(upsertParallelSize);\n+        oneBatchData.add(value.f1);\n+        bufferedWriteStatus.put(value.f0, oneBatchData);\n+      }\n+      // check and commit\n+      checkAndCommit(value.f0);\n+    } catch (Exception e) {\n+      LOG.error(\"Invoke sink error: \" + Thread.currentThread().getId() + \";\" + this);\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Check and commit if all subtask completed.\n+   *\n+   * @throws Exception\n+   */\n+  private boolean checkAndCommit(String instantTime) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY3MjMwNw==", "bodyText": "public -> private", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519672307", "createdAt": "2020-11-09T09:41:00Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sink;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class CommitSink extends RichSinkFunction<Tuple3<String, List<WriteStatus>, Integer>> {\n+  private static final Logger LOG = LoggerFactory.getLogger(CommitSink.class);\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  private Map<String, List<List<WriteStatus>>> bufferedWriteStatus = new HashMap<>();\n+\n+  private Integer upsertParallelSize = 0;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+    upsertParallelSize = getRuntimeContext().getExecutionConfig().getParallelism();\n+\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(new HoodieFlinkEngineContext(new FlinkTaskContextSupplier(null)), writeConfig);\n+  }\n+\n+  @Override\n+  public void invoke(Tuple3<String, List<WriteStatus>, Integer> value, Context context) throws Exception {\n+    LOG.info(\"Receive records, instantTime = [{}], subtaskId = [{}], records size = [{}]\", value.f0, value.f2, value.f1.size());\n+    try {\n+      if (bufferedWriteStatus.containsKey(value.f0)) {\n+        bufferedWriteStatus.get(value.f0).add(value.f1);\n+      } else {\n+        List<List<WriteStatus>> oneBatchData = new ArrayList<>(upsertParallelSize);\n+        oneBatchData.add(value.f1);\n+        bufferedWriteStatus.put(value.f0, oneBatchData);\n+      }\n+      // check and commit\n+      checkAndCommit(value.f0);\n+    } catch (Exception e) {\n+      LOG.error(\"Invoke sink error: \" + Thread.currentThread().getId() + \";\" + this);\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Check and commit if all subtask completed.\n+   *\n+   * @throws Exception\n+   */\n+  private boolean checkAndCommit(String instantTime) throws Exception {\n+    if (bufferedWriteStatus.get(instantTime).size() == upsertParallelSize) {\n+      LOG.info(\"Instant [{}] process complete, start commit\uff01\", instantTime);\n+      doCommit(instantTime);\n+      bufferedWriteStatus.clear();\n+      LOG.info(\"Instant [{}] commit completed!\", instantTime);\n+      return true;\n+    } else {\n+      LOG.info(\"Instant [{}], can not commit yet, subtask completed : [{}/{}]\", instantTime, bufferedWriteStatus.get(instantTime).size(), upsertParallelSize);\n+      return false;\n+    }\n+  }\n+\n+  public void doCommit(String instantTime) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 115}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/07097e7fec159248ae93f154b35e873b96cd2515", "committedDate": "2020-11-09T07:45:20Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "1a491c433401842b717c2d18ad5924b6343057b4", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/1a491c433401842b717c2d18ad5924b6343057b4", "committedDate": "2020-11-09T09:49:45Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1a491c433401842b717c2d18ad5924b6343057b4", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/1a491c433401842b717c2d18ad5924b6343057b4", "committedDate": "2020-11-09T09:49:45Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "0b4fce2397cab4a5c40731978a1ed0c51256a082", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/0b4fce2397cab4a5c40731978a1ed0c51256a082", "committedDate": "2020-11-09T09:53:03Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0b4fce2397cab4a5c40731978a1ed0c51256a082", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/0b4fce2397cab4a5c40731978a1ed0c51256a082", "committedDate": "2020-11-09T09:53:03Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "2158cc41339815985abfcc1b70c71c9bc285a2a2", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/2158cc41339815985abfcc1b70c71c9bc285a2a2", "committedDate": "2020-11-09T10:09:54Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2158cc41339815985abfcc1b70c71c9bc285a2a2", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/2158cc41339815985abfcc1b70c71c9bc285a2a2", "committedDate": "2020-11-09T10:09:54Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "51cca353c4bf63b7687d7715f44e629b14804a12", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/51cca353c4bf63b7687d7715f44e629b14804a12", "committedDate": "2020-11-09T10:40:15Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "51cca353c4bf63b7687d7715f44e629b14804a12", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/51cca353c4bf63b7687d7715f44e629b14804a12", "committedDate": "2020-11-09T10:40:15Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e", "committedDate": "2020-11-09T10:55:17Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2MTcyNjc4", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-526172678", "createdAt": "2020-11-09T11:45:21Z", "commit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMTo0NToyMVrOHvq4pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMTo1Nzo1NFrOHvrUEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc0Nzc0OQ==", "bodyText": "It seems that this variable: commitTimeout  means check retry number? If you think it a timeout solution, then it is not exact counting.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519747749", "createdAt": "2020-11-09T11:45:21Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc0ODg5Ng==", "bodyText": "IMO, checker is a noun. Can we name it to be doCheck or checkXXX ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519748896", "createdAt": "2020-11-09T11:47:29Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc1MDQ4MA==", "bodyText": "IMO, we can introduce both retry number and timeout conditions. WDYT?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519750480", "createdAt": "2020-11-09T11:50:26Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc0Nzc0OQ=="}, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc1MzIyNg==", "bodyText": "Chinese? Why this is a clear action to string buffer?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519753226", "createdAt": "2020-11-09T11:55:12Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {\n+      tryTimes++;\n+      StringBuffer sb = new StringBuffer();\n+      if (rollbackPendingCommits.contains(latestInstant)) {\n+        //\u6e05\u7a7a sb", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc1NDc2OQ==", "bodyText": "What's the result if the path exists? Adding a log warning message?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519754769", "createdAt": "2020-11-09T11:57:54Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {\n+      tryTimes++;\n+      StringBuffer sb = new StringBuffer();\n+      if (rollbackPendingCommits.contains(latestInstant)) {\n+        //\u6e05\u7a7a sb\n+        rollbackPendingCommits.forEach(x -> sb.append(x).append(\",\"));\n+\n+        LOG.warn(\"Latest transaction [{}] is not completed! unCompleted transaction:[{}],try times [{}]\", latestInstant, sb.toString(), tryTimes);\n+\n+        Thread.sleep(1000);\n+        rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+      } else {\n+        LOG.warn(\"Latest transaction [{}] is completed! Completed transaction, try times [{}]\", latestInstant, tryTimes);\n+        return;\n+      }\n+    }\n+    throw new InterruptedException(\"Last instant costs more than ten second, stop task now\");\n+  }\n+\n+\n+  /**\n+   * Create table if not exists.\n+   */\n+  private void initTable() throws IOException {\n+    if (!fs.exists(new Path(cfg.targetBasePath))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 203}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e", "committedDate": "2020-11-09T10:55:17Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "eed30ec812f384c3fc198166ed3eb9adcfffdcf3", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/eed30ec812f384c3fc198166ed3eb9adcfffdcf3", "committedDate": "2020-11-09T13:22:42Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "eed30ec812f384c3fc198166ed3eb9adcfffdcf3", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/eed30ec812f384c3fc198166ed3eb9adcfffdcf3", "committedDate": "2020-11-09T13:22:42Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "7fa6f8baf2f64f45c7200c73c88946967ef1307e", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/7fa6f8baf2f64f45c7200c73c88946967ef1307e", "committedDate": "2020-11-10T05:50:18Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7fa6f8baf2f64f45c7200c73c88946967ef1307e", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/7fa6f8baf2f64f45c7200c73c88946967ef1307e", "committedDate": "2020-11-10T05:50:18Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "2f80d223921d80520a92afeb5abbb672b28b2790", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/2f80d223921d80520a92afeb5abbb672b28b2790", "committedDate": "2020-11-10T05:53:00Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2f80d223921d80520a92afeb5abbb672b28b2790", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/2f80d223921d80520a92afeb5abbb672b28b2790", "committedDate": "2020-11-10T05:53:00Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "52854daf9eaac2b36b86d2eaff2b18203561f98b", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/52854daf9eaac2b36b86d2eaff2b18203561f98b", "committedDate": "2020-11-10T05:58:36Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "52854daf9eaac2b36b86d2eaff2b18203561f98b", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/52854daf9eaac2b36b86d2eaff2b18203561f98b", "committedDate": "2020-11-10T05:58:36Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/50a26bb33185788a1f4dc49b08a0a93189ead07d", "committedDate": "2020-11-11T09:19:33Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI4MDAzODUz", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-528003853", "createdAt": "2020-11-11T09:30:24Z", "commit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTozMDoyNFrOHxFOfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxMDowMjo1OFrOHxGYog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIyNzkwMA==", "bodyText": "we have TableSchemaResolver to handle the schema", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521227900", "createdAt": "2020-11-11T09:30:24Z", "author": {"login": "garyli1019"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/schema/FilebasedSchemaProvider.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.schema;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+\n+/**\n+ * A simple schema provider, that reads off files on DFS.\n+ */\n+public class FilebasedSchemaProvider extends SchemaProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIzMTIxNQ==", "bodyText": "would you add a bit more comments here? not quite familiar with Flink but a bit worried about if this will impact the performance", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521231215", "createdAt": "2020-11-11T09:36:15Z", "author": {"login": "garyli1019"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.KeyedWriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.JsonStringToHoodieRecordMapFunction;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+/**\n+ * An Utility which can incrementally consume data from Kafka and apply it to the target table.\n+ * currently, it only support MOR table and insert, upsert operation.\n+ */\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    // We use checkpoint to trigger write operation, including instant generating and committing,\n+    // There can only be one checkpoint at one time.\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // Read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new JsonStringToHoodieRecordMapFunction(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    // InstantGenerateOperator helps to emit globally unique instantTime, it must be executed in one parallelism\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .name(\"instant_generator\")\n+        .uid(\"instant_generator_id\")\n+        .setParallelism(1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIzMjQ1MQ==", "bodyText": "what is the diff between version 1 and 0?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521232451", "createdAt": "2020-11-11T09:38:23Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/upgrade/OneToZeroDowngradeHandler.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.MarkerFiles;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Downgrade handle to assist in downgrading hoodie table from version 1 to 0.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTI0Njg4Mg==", "bodyText": "Is this only dedup data in one batch but not upserting into the historical hudi table?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521246882", "createdAt": "2020-11-11T10:02:58Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 47}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/50a26bb33185788a1f4dc49b08a0a93189ead07d", "committedDate": "2020-11-11T09:19:33Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "b93e4f318ce65b6fd1c488d6c734df0fcb189be8", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/b93e4f318ce65b6fd1c488d6c734df0fcb189be8", "committedDate": "2020-11-11T10:05:03Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b93e4f318ce65b6fd1c488d6c734df0fcb189be8", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/b93e4f318ce65b6fd1c488d6c734df0fcb189be8", "committedDate": "2020-11-11T10:05:03Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "a330bbfb6c2f37af942bc5f284952198d1d9edd6", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a330bbfb6c2f37af942bc5f284952198d1d9edd6", "committedDate": "2020-11-11T10:05:43Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a330bbfb6c2f37af942bc5f284952198d1d9edd6", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a330bbfb6c2f37af942bc5f284952198d1d9edd6", "committedDate": "2020-11-11T10:05:43Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/96e8b469c8601bc3a43c67957b628f6e5b5aa390", "committedDate": "2020-11-11T10:59:35Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDc1MzM5", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530075339", "createdAt": "2020-11-13T13:49:48Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo0OTo0OFrOHyvAmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo0OTo0OFrOHyvAmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MTA0OQ==", "bodyText": "would be changed to RuntimeContext?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522961049", "createdAt": "2020-11-13T13:49:48Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.EngineProperty;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+\n+import java.util.function.Supplier;\n+\n+/**\n+ * Flink task context supplier.\n+ */\n+public class FlinkTaskContextSupplier extends TaskContextSupplier {\n+  private org.apache.flink.api.common.functions.RuntimeContext flinkRuntimeContext;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDc1NzQx", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530075741", "createdAt": "2020-11-13T13:50:19Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MDoyMFrOHyvB4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MDoyMFrOHyvB4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MTM3Ng==", "bodyText": "// TODO need to check again?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522961376", "createdAt": "2020-11-13T13:50:20Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.EngineProperty;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+\n+import java.util.function.Supplier;\n+\n+/**\n+ * Flink task context supplier.\n+ */\n+public class FlinkTaskContextSupplier extends TaskContextSupplier {\n+  private org.apache.flink.api.common.functions.RuntimeContext flinkRuntimeContext;\n+\n+  public FlinkTaskContextSupplier(RuntimeContext flinkRuntimeContext) {\n+    this.flinkRuntimeContext = flinkRuntimeContext;\n+  }\n+\n+  public RuntimeContext getFlinkRuntimeContext() {\n+    return flinkRuntimeContext;\n+  }\n+\n+  @Override\n+  public Supplier<Integer> getPartitionIdSupplier() {\n+    return () -> this.flinkRuntimeContext.getIndexOfThisSubtask();\n+  }\n+\n+  @Override\n+  public Supplier<Integer> getStageIdSupplier() {\n+    // need to check again", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDc2OTUw", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530076950", "createdAt": "2020-11-13T13:52:02Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MjowMlrOHyvFhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MjowMlrOHyvFhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MjMwOA==", "bodyText": "would be moved to super class ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522962308", "createdAt": "2020-11-13T13:52:02Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                Option<EmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  @Override\n+  protected HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createIndex(HoodieWriteConfig writeConfig) {\n+    return FlinkHoodieIndex.createIndex((HoodieFlinkEngineContext) context, config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieFlinkTable<T> table = HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS).bootstrap(context, extraMetadata);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 107}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDc3OTMx", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530077931", "createdAt": "2020-11-13T13:53:23Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MzoyNFrOHyvIcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MzoyNFrOHyvIcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MzA1OA==", "bodyText": "implement delete here?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522963058", "createdAt": "2020-11-13T13:53:24Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                Option<EmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  @Override\n+  protected HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createIndex(HoodieWriteConfig writeConfig) {\n+    return FlinkHoodieIndex.createIndex((HoodieFlinkEngineContext) context, config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieFlinkTable<T> table = HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS).bootstrap(context, extraMetadata);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.upsert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.INSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.INSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.insert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> bulkInsert(List<HoodieRecord<T>> records, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> bulkInsert(List<HoodieRecord<T>> records, String instantTime, Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> bulkInsertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime, Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> delete(List<HoodieKey> keys, String instantTime) {\n+    // TODO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 169}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDc5Mjg3", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530079287", "createdAt": "2020-11-13T13:55:17Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1NToxN1rOHyvMmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1NToxN1rOHyvMmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2NDEyMg==", "bodyText": "should we implement some index here ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522964122", "createdAt": "2020-11-13T13:55:17Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.state.FlinkInMemoryStateIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.List;\n+\n+/**\n+ * Base flink implementation of {@link HoodieIndex}.\n+ * @param <T> payload type\n+ */\n+public abstract class FlinkHoodieIndex<T extends HoodieRecordPayload> extends HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+  protected FlinkHoodieIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  public static FlinkHoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    // first use index class config to create index.\n+    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n+      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n+      if (!(instance instanceof HoodieIndex)) {\n+        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n+      }\n+      return (FlinkHoodieIndex) instance;\n+    }\n+    switch (config.getIndexType()) {\n+      case HBASE:\n+        // TODO\n+        return null;\n+      case INMEMORY:\n+        return new FlinkInMemoryStateIndex<>(context, config);\n+      case BLOOM:\n+        // TODO\n+        return null;\n+      case GLOBAL_BLOOM:\n+        // TODO\n+        return null;\n+      case SIMPLE:\n+        // TODO\n+        return null;\n+      case GLOBAL_SIMPLE:\n+        // TODO\n+        return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDgwMDA4", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530080008", "createdAt": "2020-11-13T13:56:15Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1NjoxNlrOHyvO1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1NjoxNlrOHyvO1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2NDY5Mg==", "bodyText": "throw exception here ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522964692", "createdAt": "2020-11-13T13:56:16Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(FlinkInMemoryStateIndex.class);\n+  private MapState<HoodieKey, HoodieRecordLocation> mapState;\n+\n+  public FlinkInMemoryStateIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    super(config);\n+    if (context.getRuntimeContext() != null) {\n+      MapStateDescriptor<HoodieKey, HoodieRecordLocation> indexStateDesc =\n+          new MapStateDescriptor<>(\"indexState\", TypeInformation.of(HoodieKey.class), TypeInformation.of(HoodieRecordLocation.class));\n+      if (context.getRuntimeContext() != null) {\n+        mapState = context.getRuntimeContext().getMapState(indexStateDesc);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records,\n+                                           HoodieEngineContext context,\n+                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) throws HoodieIndexException {\n+    return context.map(records, record -> {\n+      try {\n+        if (mapState.contains(record.getKey())) {\n+          record.unseal();\n+          record.setCurrentLocation(mapState.get(record.getKey()));\n+          record.seal();\n+        }\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"Tag record location failed, key = %s, %s\", record.getRecordKey(), e.getMessage()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDgwMzY1", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530080365", "createdAt": "2020-11-13T13:56:42Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1Njo0MlrOHyvP-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1Njo0MlrOHyvP-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2NDk4NQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522964985", "createdAt": "2020-11-13T13:56:42Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(FlinkInMemoryStateIndex.class);\n+  private MapState<HoodieKey, HoodieRecordLocation> mapState;\n+\n+  public FlinkInMemoryStateIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    super(config);\n+    if (context.getRuntimeContext() != null) {\n+      MapStateDescriptor<HoodieKey, HoodieRecordLocation> indexStateDesc =\n+          new MapStateDescriptor<>(\"indexState\", TypeInformation.of(HoodieKey.class), TypeInformation.of(HoodieRecordLocation.class));\n+      if (context.getRuntimeContext() != null) {\n+        mapState = context.getRuntimeContext().getMapState(indexStateDesc);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records,\n+                                           HoodieEngineContext context,\n+                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) throws HoodieIndexException {\n+    return context.map(records, record -> {\n+      try {\n+        if (mapState.contains(record.getKey())) {\n+          record.unseal();\n+          record.setCurrentLocation(mapState.get(record.getKey()));\n+          record.seal();\n+        }\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"Tag record location failed, key = %s, %s\", record.getRecordKey(), e.getMessage()));\n+      }\n+      return record;\n+    }, 0);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> updateLocation(List<WriteStatus> writeStatuses,\n+                                          HoodieEngineContext context,\n+                                          HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) throws HoodieIndexException {\n+    return context.map(writeStatuses, writeStatus -> {\n+      for (HoodieRecord record : writeStatus.getWrittenRecords()) {\n+        if (!writeStatus.isErrored(record.getKey())) {\n+          HoodieKey key = record.getKey();\n+          Option<HoodieRecordLocation> newLocation = record.getNewLocation();\n+          if (newLocation.isPresent()) {\n+            try {\n+              mapState.put(key, newLocation.get());\n+            } catch (Exception e) {\n+              LOG.error(String.format(\"Update record location failed, key = %s, %s\", record.getRecordKey(), e.getMessage()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDgzNzkx", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530083791", "createdAt": "2020-11-13T14:01:06Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/96e8b469c8601bc3a43c67957b628f6e5b5aa390", "committedDate": "2020-11-11T10:59:35Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "a137f1c13113341b914151fbb980141d0173be49", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a137f1c13113341b914151fbb980141d0173be49", "committedDate": "2020-11-14T14:48:12Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a137f1c13113341b914151fbb980141d0173be49", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a137f1c13113341b914151fbb980141d0173be49", "committedDate": "2020-11-14T14:48:12Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "647146c815c7b30f19981d2522d8a4c2dcc7f4e6", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/647146c815c7b30f19981d2522d8a4c2dcc7f4e6", "committedDate": "2020-11-16T06:31:43Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "647146c815c7b30f19981d2522d8a4c2dcc7f4e6", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/647146c815c7b30f19981d2522d8a4c2dcc7f4e6", "committedDate": "2020-11-16T06:31:43Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/b63e126c245ed063359c005dd7c1d2f79f9881f2", "committedDate": "2020-11-16T08:39:50Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMzI4Mzk0", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-530328394", "createdAt": "2020-11-13T18:59:25Z", "commit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "state": "APPROVED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODo1OToyNVrOHy7VLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDoyODo0MlrOH0j9PQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MjkyNA==", "bodyText": "can we reuse WriteOperationType, that already exists in hudi-common?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r523162924", "createdAt": "2020-11-13T18:59:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/constant/Operation.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.constant;\n+\n+public enum Operation {\n+  UPSERT, INSERT", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NTYxMg==", "bodyText": "can we reuse the WriteOperationType enum?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524875612", "createdAt": "2020-11-17T04:23:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/constant/Operation.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.constant;\n+\n+public enum Operation {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NjY5Ng==", "bodyText": "could we just call this hudi-flink instead of hudi-flink-writer. It gives us the change to also build the reading support into the same module/bundle?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524876696", "createdAt": "2020-11-17T04:27:01Z", "author": {"login": "vinothchandar"}, "path": "pom.xml", "diffHunk": "@@ -53,6 +53,8 @@\n     <module>hudi-integ-test</module>\n     <module>packaging/hudi-integ-test-bundle</module>\n     <module>hudi-examples</module>\n+    <module>hudi-flink-writer</module>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3Njk5Ng==", "bodyText": "In the future, I guess we can do more refactoring and make stuff work end-end with deltastreamer as well . We can provide a migration path for users then. For now, this is a solid approach to get stuff working end to end.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524876996", "createdAt": "2020-11-17T04:28:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.KeyedWriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.JsonStringToHoodieRecordMapFunction;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Properties;\n+\n+/**\n+ * An Utility which can incrementally consume data from Kafka and apply it to the target table.\n+ * currently, it only support MOR table and insert, upsert operation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NzExNw==", "bodyText": "can we make sure the program throws errors if using COW etc.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524877117", "createdAt": "2020-11-17T04:28:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.KeyedWriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.JsonStringToHoodieRecordMapFunction;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Properties;\n+\n+/**\n+ * An Utility which can incrementally consume data from Kafka and apply it to the target table.\n+ * currently, it only support MOR table and insert, upsert operation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyMDE0NTQy", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-532014542", "createdAt": "2020-11-17T04:32:03Z", "commit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDozMjowM1rOH0kA1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDozNzoyMlrOH0kGDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3ODAzOA==", "bodyText": "throw errors here, instead of returning null?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524878038", "createdAt": "2020-11-17T04:32:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.state.FlinkInMemoryStateIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.List;\n+\n+/**\n+ * Base flink implementation of {@link HoodieIndex}.\n+ * @param <T> payload type\n+ */\n+public abstract class FlinkHoodieIndex<T extends HoodieRecordPayload> extends HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+  protected FlinkHoodieIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  public static FlinkHoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    // first use index class config to create index.\n+    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n+      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n+      if (!(instance instanceof HoodieIndex)) {\n+        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n+      }\n+      return (FlinkHoodieIndex) instance;\n+    }\n+    switch (config.getIndexType()) {\n+      case HBASE:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3ODU4Mg==", "bodyText": "is this always in memory? or can it be rocksDB backed as well?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524878582", "createdAt": "2020-11-17T04:34:21Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3OTE5Mg==", "bodyText": "unused?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524879192", "createdAt": "2020-11-17T04:36:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3OTM3NQ==", "bodyText": "same comment. can we throw an Unsupported exception here instead of returning null.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524879375", "createdAt": "2020-11-17T04:37:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                Option<EmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  @Override\n+  protected HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createIndex(HoodieWriteConfig writeConfig) {\n+    return FlinkHoodieIndex.createIndex((HoodieFlinkEngineContext) context, config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieFlinkTable<T> table = HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.upsert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.INSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.INSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.insert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 135}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/b63e126c245ed063359c005dd7c1d2f79f9881f2", "committedDate": "2020-11-16T08:39:50Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "a7536d40e6977be59b957680728d3a1183260a4a", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a7536d40e6977be59b957680728d3a1183260a4a", "committedDate": "2020-11-17T06:24:18Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a7536d40e6977be59b957680728d3a1183260a4a", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/a7536d40e6977be59b957680728d3a1183260a4a", "committedDate": "2020-11-17T06:24:18Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "302222921fe4b4e634318e7a58f21266f87e9cc0", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/302222921fe4b4e634318e7a58f21266f87e9cc0", "committedDate": "2020-11-17T06:31:55Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "302222921fe4b4e634318e7a58f21266f87e9cc0", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/302222921fe4b4e634318e7a58f21266f87e9cc0", "committedDate": "2020-11-17T06:31:55Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "f083852c6ded288ba53adde753e173f81f520b3a", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/f083852c6ded288ba53adde753e173f81f520b3a", "committedDate": "2020-11-17T06:58:39Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83d7a0da9724741cad082e83b30986a4fe8ff4a1", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/83d7a0da9724741cad082e83b30986a4fe8ff4a1", "committedDate": "2020-11-17T08:29:47Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f083852c6ded288ba53adde753e173f81f520b3a", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/f083852c6ded288ba53adde753e173f81f520b3a", "committedDate": "2020-11-17T06:58:39Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}, "afterCommit": {"oid": "83d7a0da9724741cad082e83b30986a4fe8ff4a1", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/83d7a0da9724741cad082e83b30986a4fe8ff4a1", "committedDate": "2020-11-17T08:29:47Z", "message": "[HUDI-1327] Introduce base implemetation of hudi-flink-client"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzMTU0ODAz", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-533154803", "createdAt": "2020-11-18T07:11:29Z", "commit": {"oid": "83d7a0da9724741cad082e83b30986a4fe8ff4a1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzMjc2MTY0", "url": "https://github.com/apache/hudi/pull/2176#pullrequestreview-533276164", "createdAt": "2020-11-18T09:56:17Z", "commit": {"oid": "83d7a0da9724741cad082e83b30986a4fe8ff4a1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4171, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}