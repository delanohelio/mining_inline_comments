{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEzMjU4NDU3", "number": 1594, "title": "[HUDI-862] Migrate HUDI site blogs from Confluence to Jeykll.", "bodyText": "What is the purpose of the pull request\nBrief change log\n\nAdded a data file which saves author information (authors.yml)\nTheme changes:\n\n\nAuthor names added to blog pages. Author names link to author bio page on HUDI Confluence Wiki.\nDate added to blog posts\nBlog archive page has excerpts for better layout.\nAdded custom CSS for the above theme changes.\n\n\nAdded a new asset directory where the blog images are stored (docs/assets/images/blog)\nExported all blog posts from Confluence in markdown format.\n\nVerify this pull request\nPlease run the site in docker-compose and check the blog section.\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-05-05T00:55:17Z", "url": "https://github.com/apache/hudi/pull/1594", "merged": true, "mergeCommit": {"oid": "06149f3115136a9e65699b3d05e16a3b484af055"}, "closed": true, "closedAt": "2020-05-13T04:04:13Z", "author": {"login": "prashantwason"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABceJLPZAH2gAyNDEzMjU4NDU3OmIyMDE1NDE0OTIyZjk5Zjc3MGM2NGI5YjM4MzYzMDY2YmU4MTA5YzI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcgwCLigH2gAyNDEzMjU4NDU3OmQxNTMyMjU2MGM1ZjI5YmVkZDUwYTk0MmYyZDZhMTEyZTM0MWMwOGU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b2015414922f99f770c64b9b38363066be8109c2", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/b2015414922f99f770c64b9b38363066be8109c2", "committedDate": "2020-05-05T00:45:14Z", "message": "[HUDI-862] Migrate HUDI site blogs from Confluence to Jeykll.\n\n1. Added a data file which saves author information (authors.yml)\n2. Theme changes:\n  - Author names added to blog pages. Author names link to author bio page on HUDI Confluence Wiki.\n  - Date added to blog posts\n  - Blog archive page has excerpts for better layout.\n  - Added custom CSS for the above theme changes.\n3. Added a new asset directory where the blog images are stored (docs/assets/images/blog)\n4. Exported all blog posts from Confluence in markdown format."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1ODc0Nzcw", "url": "https://github.com/apache/hudi/pull/1594#pullrequestreview-405874770", "createdAt": "2020-05-05T14:59:03Z", "commit": {"oid": "b2015414922f99f770c64b9b38363066be8109c2"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo1OTowM1rOGQtkoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNTowMjo0NlrOGQtvWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3NzA1Ng==", "bodyText": "nice touch :)", "url": "https://github.com/apache/hudi/pull/1594#discussion_r420177056", "createdAt": "2020-05-05T14:59:03Z", "author": {"login": "vinothchandar"}, "path": "docs/_posts/2016-12-30-strata-talk-2017.md", "diffHunk": "@@ -1,8 +1,7 @@\n ---\n title:  \"Connect with us at Strata San Jose March 2017\"\n+author: admin", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2015414922f99f770c64b9b38363066be8109c2"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3NzkwOQ==", "bodyText": "could we get code markup for the code blocks?", "url": "https://github.com/apache/hudi/pull/1594#discussion_r420177909", "createdAt": "2020-05-05T15:00:12Z", "author": {"login": "vinothchandar"}, "path": "docs/_posts/2020-01-15-delete-support-in-hudi.md", "diffHunk": "@@ -0,0 +1,156 @@\n+---\n+title: \"Delete support in Hudi\"\n+excerpt: \"Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \u201chow to\u201d blog on how to delete records in hudi.\"\n+author: shivnarayan\n+---\n+\n+Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \"how to\" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.\n+\n+### Delete using RDD Level APIs\n+\n+If you have embedded  _HoodieWriteClient_ , then deletion is as simple as passing in a  _JavaRDD<HoodieKey>_ to the delete api.\n+\n+    // Fetch list of HoodieKeys from elsewhere that needs to be deleted\n+    // convert to JavaRDD if required. JavaRDD<HoodieKey> toBeDeletedKeys\n+    List<WriteStatus> statuses = writeClient.delete(toBeDeletedKeys, commitTime);\n+\n+### Deletion with Datasource\n+\n+Now we will walk through an example of how to perform deletes on a sample dataset using the Datasource API. Quick Start has the same example as below. Feel free to check it out.\n+\n+**Step 1** : Launch spark shell\n+\n+    bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle:0.5.1-incubating \\\n+        --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n+\n+**Step 2** : Import as required and set up table name, etc for sample dataset\n+\n+    import org.apache.hudi.QuickstartUtils._\n+    import scala.collection.JavaConversions._\n+    import org.apache.spark.sql.SaveMode._\n+    import org.apache.hudi.DataSourceReadOptions._\n+    import org.apache.hudi.DataSourceWriteOptions._\n+    import org.apache.hudi.config.HoodieWriteConfig._\n+     \n+    val tableName = \"hudi_cow_table\"\n+    val basePath = \"file:///tmp/hudi_cow_table\"\n+    val dataGen = new DataGenerator\n+\n+**Step 3** : Insert data. Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi dataset as below.\n+\n+    val inserts = convertToStringList(dataGen.generateInserts(10))\n+    val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+    df.write.format(\"org.apache.hudi\").\n+        options(getQuickstartWriteConfigs).\n+        option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+        option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+        option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+        option(TABLE_NAME, tableName).\n+        mode(Overwrite).\n+        save(basePath);\n+\n+**Step 4** : Query data. Load the data files into a DataFrame.\n+\n+    val roViewDF = spark.\n+        read.\n+        format(\"org.apache.hudi\").\n+        load(basePath + \"/*/*/*/*\")\n+    roViewDF.createOrReplaceTempView(\"hudi_ro_table\")\n+    spark.sql(\"select count(*) from hudi_ro_table\").show() // should return 10 (number of records inserted above)\n+    val riderValue = spark.sql(\"select distinct rider from hudi_ro_table\").show()\n+    // copy the value displayed to be used in next step\n+\n+**Step 5** : Fetch records that needs to be deleted, with the above rider value. This example is just to illustrate how to delete. In real world, use a select query using spark sql to fetch records that needs to be deleted and from the result we could invoke deletes as given below. Example rider value used is \"rider-213\".\n+\n+    val df = spark.sql(``\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-213'\"``)\n+\n+// Replace the above query with any other query that will fetch records to be deleted.\n+\n+**Step 6** : Issue deletes\n+\n+    val deletes = dataGen.generateDeletes(df.collectAsList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2015414922f99f770c64b9b38363066be8109c2"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3OTgwMA==", "bodyText": "https://github.com/apache/incubator-hudi/blame/asf-site/docs/_docs/1_1_quick_start_guide.md#L19  similar to the other pages.. ?", "url": "https://github.com/apache/hudi/pull/1594#discussion_r420179800", "createdAt": "2020-05-05T15:02:46Z", "author": {"login": "vinothchandar"}, "path": "docs/_posts/2020-01-15-delete-support-in-hudi.md", "diffHunk": "@@ -0,0 +1,156 @@\n+---\n+title: \"Delete support in Hudi\"\n+excerpt: \"Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \u201chow to\u201d blog on how to delete records in hudi.\"\n+author: shivnarayan\n+---\n+\n+Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \"how to\" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.\n+\n+### Delete using RDD Level APIs\n+\n+If you have embedded  _HoodieWriteClient_ , then deletion is as simple as passing in a  _JavaRDD<HoodieKey>_ to the delete api.\n+\n+    // Fetch list of HoodieKeys from elsewhere that needs to be deleted\n+    // convert to JavaRDD if required. JavaRDD<HoodieKey> toBeDeletedKeys\n+    List<WriteStatus> statuses = writeClient.delete(toBeDeletedKeys, commitTime);\n+\n+### Deletion with Datasource\n+\n+Now we will walk through an example of how to perform deletes on a sample dataset using the Datasource API. Quick Start has the same example as below. Feel free to check it out.\n+\n+**Step 1** : Launch spark shell\n+\n+    bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle:0.5.1-incubating \\\n+        --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n+\n+**Step 2** : Import as required and set up table name, etc for sample dataset\n+\n+    import org.apache.hudi.QuickstartUtils._\n+    import scala.collection.JavaConversions._\n+    import org.apache.spark.sql.SaveMode._\n+    import org.apache.hudi.DataSourceReadOptions._\n+    import org.apache.hudi.DataSourceWriteOptions._\n+    import org.apache.hudi.config.HoodieWriteConfig._\n+     \n+    val tableName = \"hudi_cow_table\"\n+    val basePath = \"file:///tmp/hudi_cow_table\"\n+    val dataGen = new DataGenerator\n+\n+**Step 3** : Insert data. Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi dataset as below.\n+\n+    val inserts = convertToStringList(dataGen.generateInserts(10))\n+    val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+    df.write.format(\"org.apache.hudi\").\n+        options(getQuickstartWriteConfigs).\n+        option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+        option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+        option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+        option(TABLE_NAME, tableName).\n+        mode(Overwrite).\n+        save(basePath);\n+\n+**Step 4** : Query data. Load the data files into a DataFrame.\n+\n+    val roViewDF = spark.\n+        read.\n+        format(\"org.apache.hudi\").\n+        load(basePath + \"/*/*/*/*\")\n+    roViewDF.createOrReplaceTempView(\"hudi_ro_table\")\n+    spark.sql(\"select count(*) from hudi_ro_table\").show() // should return 10 (number of records inserted above)\n+    val riderValue = spark.sql(\"select distinct rider from hudi_ro_table\").show()\n+    // copy the value displayed to be used in next step\n+\n+**Step 5** : Fetch records that needs to be deleted, with the above rider value. This example is just to illustrate how to delete. In real world, use a select query using spark sql to fetch records that needs to be deleted and from the result we could invoke deletes as given below. Example rider value used is \"rider-213\".\n+\n+    val df = spark.sql(``\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-213'\"``)\n+\n+// Replace the above query with any other query that will fetch records to be deleted.\n+\n+**Step 6** : Issue deletes\n+\n+    val deletes = dataGen.generateDeletes(df.collectAsList())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3NzkwOQ=="}, "originalCommit": {"oid": "b2015414922f99f770c64b9b38363066be8109c2"}, "originalPosition": 71}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6243c3a90ffe965327fac60e3b3301de2e2c9d73", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/6243c3a90ffe965327fac60e3b3301de2e2c9d73", "committedDate": "2020-05-13T01:25:14Z", "message": "Adding syntax highlighting to the blog posts"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "6243c3a90ffe965327fac60e3b3301de2e2c9d73", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/6243c3a90ffe965327fac60e3b3301de2e2c9d73", "committedDate": "2020-05-13T01:25:14Z", "message": "Adding syntax highlighting to the blog posts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ca0872eca928743f3709d2067eb4cc5c81697cd", "author": {"user": {"login": "lamberken", "name": "lamberken"}}, "url": "https://github.com/apache/hudi/commit/0ca0872eca928743f3709d2067eb4cc5c81697cd", "committedDate": "2020-05-13T02:17:49Z", "message": "Update 2020-04-27-apache-hudi-apache-zepplin.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d15322560c5f29bedd50a942f2d6a112e341c08e", "author": {"user": {"login": "lamberken", "name": "lamberken"}}, "url": "https://github.com/apache/hudi/commit/d15322560c5f29bedd50a942f2d6a112e341c08e", "committedDate": "2020-05-13T03:09:29Z", "message": "Add category"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3086, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}