{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0MjY5NTAw", "number": 2127, "title": "[HUDI-284] add more test for UpdateSchemaEvolution", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\n\nadd more test for schema evolution failed. future we can resolve some of the scenario.\n\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-09-28T16:38:19Z", "url": "https://github.com/apache/hudi/pull/2127", "merged": true, "mergeCommit": {"oid": "4d80e1e221b0ddfd542baeee9b6cbb3b28a88e68"}, "closed": true, "closedAt": "2020-10-19T14:38:05Z", "author": {"login": "lw309637554"}, "timelineItems": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdNeQYrAH2gAyNDk0MjY5NTAwOjRmNzQyZWY3ODFiMTdiOWM5ZjgwYjQzZmRlOTk0MzRiOWU5Zjg2MDc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdSWtVXABqjM4NzQ4MTQ5MTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "4f742ef781b17b9c9f80b43fde99434b9e9f8607", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/4f742ef781b17b9c9f80b43fde99434b9e9f8607", "committedDate": "2020-09-29T01:53:18Z", "message": "[HUDI-284] add more test for UpdateSchemaEvolution"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3c777e965b4c62e75dc82d8d9d05c0c550cdee35", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3c777e965b4c62e75dc82d8d9d05c0c550cdee35", "committedDate": "2020-09-28T16:35:46Z", "message": "[HUDI-284] add more test for UpdateSchemaEvolution"}, "afterCommit": {"oid": "4f742ef781b17b9c9f80b43fde99434b9e9f8607", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/4f742ef781b17b9c9f80b43fde99434b9e9f8607", "committedDate": "2020-09-29T01:53:18Z", "message": "[HUDI-284] add more test for UpdateSchemaEvolution"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNTUxNTQz", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-501551543", "createdAt": "2020-10-03T14:51:24Z", "commit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNDo1MToyNVrOHcB9jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNDo1NjoxMlrOHcB_Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDMxOA==", "bodyText": "these 3 local vars could be removed", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154318", "createdAt": "2020-10-03T14:51:25Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDQxOA==", "bodyText": "to avoid ambiguity, could we call this prepareFirstRecordCommit()?", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154418", "createdAt": "2020-10-03T14:52:46Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDU3NA==", "bodyText": "could we call them table and config instead of table2 and config2? since the method is split and they are the only vars left.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154574", "createdAt": "2020-10-03T14:54:09Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n \n     // Now try an update with an evolved schema\n     // Evolved schema does not have guarantee on preserving the original field ordering\n     final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+    final WriteStatus insertResult = prepareFirstCommitData(recordsStrs);\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n+    final HoodieSparkTable table2 = HoodieSparkTable.create(config2, context);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDczOA==", "bodyText": "seeing this is original code. i don't see why it needs to assert 1 equals to 1 returned eventually. could we simplify this by removing jsc.parallelize() and just retain the lambda block?", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154738", "createdAt": "2020-10-03T14:56:12Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n \n     // Now try an update with an evolved schema\n     // Evolved schema does not have guarantee on preserving the original field ordering\n     final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+    final WriteStatus insertResult = prepareFirstCommitData(recordsStrs);\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n+    final HoodieSparkTable table2 = HoodieSparkTable.create(config2, context);\n     assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 98}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/8852471a7cdef76ed1db2a4dc95b8239119356ac", "committedDate": "2020-10-03T17:24:30Z", "message": "[HUDI-284] merge master"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/cec3a1656cc19746fc020dfcd8a14a0ae98ac01c", "committedDate": "2020-10-03T14:42:17Z", "message": "[HUDI-284] merge master"}, "afterCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/8852471a7cdef76ed1db2a4dc95b8239119356ac", "committedDate": "2020-10-03T17:24:30Z", "message": "[HUDI-284] merge master"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNjcxODY5", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-501671869", "createdAt": "2020-10-05T00:06:24Z", "commit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDowNjoyNVrOHcLDDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNTo1NFrOHcLGdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwMzE4Mg==", "bodyText": "I think we can merge these two catch blocks? also it's probably better to move the LOG.error to LOG.debug and only log record data when such debug/tracing is enabled? This might just flood the logs.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499303182", "createdAt": "2020-10-05T00:06:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -254,6 +253,10 @@ public void write(GenericRecord oldRecord) {\n         LOG.error(\"Failed to merge old record into new file for key \" + key + \" from old file \" + getOldFilePath()\n             + \" to new file \" + newFilePath, e);\n         throw new HoodieUpsertException(errMsg, e);\n+      } catch (RuntimeException e) {\n+        LOG.error(\"Summary is \" + e.getMessage() + \", detail is schema mismatch when rewriting old record \" + oldRecord + \" from file \" + getOldFilePath()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDA1NA==", "bodyText": "we can reuse some code in this file by pulling the common structure into a helper function ?", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304054", "createdAt": "2020-10-05T00:15:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 101}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNjcyNjc2", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-501672676", "createdAt": "2020-10-05T00:15:44Z", "commit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNTo0NFrOHcLGYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNzoyNlrOHcLG8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDAzNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                jsc.parallelize(Arrays.asList(1)).map(x -> {\n          \n          \n            \n                jsc.parallelize(Arrays.asList(1)).foreach(x -> {", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304034", "createdAt": "2020-10-05T00:15:44Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDEyMA==", "bodyText": "Default sealed is false. we could skip unsealing.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304120", "createdAt": "2020-10-05T00:16:35Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDE3OQ==", "bodyText": "instead of creating a new Configuration, would it be better with table.getHadoopConf()?", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304179", "createdAt": "2020-10-05T00:17:26Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 287}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f8f53785681a0d6e0005dc4cb784daf96e724bc", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/9f8f53785681a0d6e0005dc4cb784daf96e724bc", "committedDate": "2020-10-05T14:44:32Z", "message": "Update hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java\n\nCo-authored-by: Raymond Xu <2701446+xushiyan@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "724429e7284e723b09d5aef898155f66982af15e", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/724429e7284e723b09d5aef898155f66982af15e", "committedDate": "2020-10-05T15:49:06Z", "message": "Revert \"Update hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java\"\n\nThis reverts commit 9f8f53785681a0d6e0005dc4cb784daf96e724bc."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyMTI0OTM1", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-502124935", "createdAt": "2020-10-05T14:38:45Z", "commit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDozODo0NVrOHcgIaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDozODo0NVrOHcgIaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0ODYxNw==", "bodyText": "make sense .done", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499648617", "createdAt": "2020-10-05T14:38:45Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -254,6 +253,10 @@ public void write(GenericRecord oldRecord) {\n         LOG.error(\"Failed to merge old record into new file for key \" + key + \" from old file \" + getOldFilePath()\n             + \" to new file \" + newFilePath, e);\n         throw new HoodieUpsertException(errMsg, e);\n+      } catch (RuntimeException e) {\n+        LOG.error(\"Summary is \" + e.getMessage() + \", detail is schema mismatch when rewriting old record \" + oldRecord + \" from file \" + getOldFilePath()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwMzE4Mg=="}, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MDYzMjMx", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-506063231", "createdAt": "2020-10-10T03:56:07Z", "commit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3NzkxNTgw", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-507791580", "createdAt": "2020-10-13T20:13:29Z", "commit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoxMzoyOVrOHg3mvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoxMzoyOVrOHg3mvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyNzUxNw==", "bodyText": "a old -> an old", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504227517", "createdAt": "2020-10-13T20:13:29Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3Nzk1NTM0", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-507795534", "createdAt": "2020-10-13T20:16:02Z", "commit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoxNjowM1rOHg3rsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoxNjowM1rOHg3rsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyODc4NQ==", "bodyText": "generateMultiRecordsForExampleSchema -> generateMultipleRecordsForExampleSchema?\n@lw309637554 I would leave it upto you to decide though.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504228785", "createdAt": "2020-10-13T20:16:03Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3Nzk5MDIy", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-507799022", "createdAt": "2020-10-13T20:21:08Z", "commit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMTowOFrOHg3_IA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMTowOFrOHg3_IA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzMzc2MA==", "bodyText": "oldrecords -> old records", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504233760", "createdAt": "2020-10-13T20:21:08Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, InvalidRecordException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 224}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3Nzk5ODU3", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-507799857", "createdAt": "2020-10-13T20:22:22Z", "commit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMjoyM1rOHg4D7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMjoyM1rOHg4D7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNDk4OQ==", "bodyText": "delete column ,Parquet/Avro -> delete column, Parquet/Avro", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504234989", "createdAt": "2020-10-13T20:22:23Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 209}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3ODAwNDc2", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-507800476", "createdAt": "2020-10-13T20:23:14Z", "commit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMzoxNFrOHg4HdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMzoxNFrOHg4HdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNTg5Mg==", "bodyText": "type ,org. -> type, org.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504235892", "createdAt": "2020-10-13T20:23:14Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, InvalidRecordException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, HoodieUpsertException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when change column type ,org.apache.parquet.avro.AvroConverters$FieldUTF8Converter\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 238}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3ODAyODUx", "url": "https://github.com/apache/hudi/pull/2127#pullrequestreview-507802851", "createdAt": "2020-10-13T20:26:50Z", "commit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyNjo1MFrOHg4QMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyNjo1MFrOHg4QMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzODEyOQ==", "bodyText": "can we make the schema files as .avsc? @lw309637554", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504238129", "createdAt": "2020-10-13T20:26:50Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-client-common/src/test/resources/exampleEvolvedSchemaDeleteColumn.txt", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 18}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3d637855f337aad14e59ff944068a31aa2529a5e", "committedDate": "2020-10-05T15:56:49Z", "message": "[HUDI-284] pulling the common structure"}, "afterCommit": {"oid": "81daca90c5fce73cc0634daca5d2cd70ed87df15", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/81daca90c5fce73cc0634daca5d2cd70ed87df15", "committedDate": "2020-10-14T03:33:08Z", "message": "[HUDI-284] pulling the common structure"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d34ac8fdb2641d143f27b2fa2cda95ca96c78f4c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d34ac8fdb2641d143f27b2fa2cda95ca96c78f4c", "committedDate": "2020-10-14T05:55:02Z", "message": "[HUDI-284] pulling the common structure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "81daca90c5fce73cc0634daca5d2cd70ed87df15", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/81daca90c5fce73cc0634daca5d2cd70ed87df15", "committedDate": "2020-10-14T03:33:08Z", "message": "[HUDI-284] pulling the common structure"}, "afterCommit": {"oid": "d34ac8fdb2641d143f27b2fa2cda95ca96c78f4c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d34ac8fdb2641d143f27b2fa2cda95ca96c78f4c", "committedDate": "2020-10-14T05:55:02Z", "message": "[HUDI-284] pulling the common structure"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4601, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}