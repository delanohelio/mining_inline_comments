{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNDQ1ODYw", "number": 2342, "title": "[RFC-15][HUDI-1325] Merge updates of unsynced instants to metadata table", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nCurrently every time an operation (i.e. commit, clean, rollback) completes we attempt to keep the metadata table up to date by creating an upsert on the metadata table with the same instant time. This way we can reference both datasets timelines and directly compare the instants on them to see if they are in sync.\nThere can be the possibility that the dataset timeline and the metadata table timeline become out of sync. When trying to read from the metadata table while the timeline is out of sync you would get incorrect values for getAllFilesInPartition and getAllPartitionPaths.\nThis change provides a way to overcome this scenario by reading unsynced timeline instants and merging it with existing metadata table records to get the most up to date state of the file system\nJIRA: https://issues.apache.org/jira/browse/HUDI-1325\nBrief change log\nTimeline Sync\n\nThe logic of converting timeline metadata to metadata table records was directly tied to the commit phase in HoodieBackedMetadataWriter. Refactored this logic to a utility class HoodieTableMetadataTimelineUtil\nCreated a scanner HoodieMetadataMergedInstantRecordScanner which handles conversion of timeline instants to metadata records and merges results\nAdded final step in AbstractHoodieTableMetadata.getMergedRecordByKey which uses the new scanner mentioned to fetch the HoodieRecord associated with the desired key from the unsynced timeline instants and merge it with the record from the metadata table\nWhen converting rollback operation to metadata table records there was logic that re-read from the metadata table to ensure any files being deleted as part of roll back existed.\n\n// Rollbacks deletes instants from timeline. The instant being rolled-back may not have been synced to the\n// metadata table. Hence, the deleted filed need to be checked against the metadata.\n\nThis doesn't make sense since all instants are processed in serial order so there would never be the case where a rollback was being written before an instant earlier on the timeline was already synced. Removed this logic because it created circular dependency when implementing timeline merging\n\nChanged the validate metadata step in tests to use the metadata reader HoodieBackedTableMetadata. By default when metadata writer HoodieBackedTableMetadataWriter is initialized it syncs all instants to the metadata table. By using the reader we can simulate metadata table being out of sync.\nModified initMetaClient in test base class to allow table type to be passed in since table type is always set as COPY_ON_WRITE if using this method to initialize the meta client\n\nRefactor\nFor the following reasons I modified the HoodieTableMetadata interface to be an AbstractHoodieMetadata class which contains the following shared functionality irresepective of how the metadata is stored  (as a Hoodie table, in some key/value store):\n\nFetching getAllPartitions and getAllFileInPartition from metadata should validate metadata flag is enabled and should default to file listing if any error occurs regardless of storage type\nDuring fetchAllPartitions and fetchAllFilesInPartition metrics should be published on operations and if validateLookUps is enabled, results returned from metadata should be compared against actual file listing results regardless of what storage type is\nIn getMergedRecordByKey regardless of how the key is fetched from storage the result is merged against unsynced timeline instants. This is why I introduced abstract getRecordByKeyFromMetadata which can be implemented by inheriting class which contains logic specific to storage type.\nMoved findInstantsToSync to AbstractHoodieTableMetadata. This method is needed regardless of storage type to find if metadata is in sync with timeline. However since how the last synced instant is stored can be dependent on storage type made this method abstract.\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\nTesting\nRan TestHoodieBackedMetadata which contains all tests related to metadata table\nAdded tests which use an unsynced client after commits, cleans, and restores have been performed to ensure updates not written to the metadata yet are still being reflected when reading from the metadata\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-12-16T21:21:04Z", "url": "https://github.com/apache/hudi/pull/2342", "merged": true, "mergeCommit": {"oid": "11661dc6aa03a8b8d39f823f9991ed8402a35ef1"}, "closed": true, "closedAt": "2020-12-28T17:52:03Z", "author": {"login": "rmpifer"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdnJ4fmAFqTU1NDk3MzY3MQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdqp6SvAFqTU1OTI0MTUzMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0OTczNjcx", "url": "https://github.com/apache/hudi/pull/2342#pullrequestreview-554973671", "createdAt": "2020-12-17T20:45:07Z", "commit": {"oid": "56717d03adf0391ed581fc3042eaff3a9bf733fb"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMDo0NTowN1rOIIIGEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMDo0ODo1OVrOIIIOGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5MjE0NA==", "bodyText": "I would prefer to still have an interface for HoodieTableMetadata and return that here. We can have the abstract class internal to the actual implementation?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r545392144", "createdAt": "2020-12-17T20:45:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -635,9 +635,9 @@ public boolean requireSortedRecords() {\n     return getBaseFileFormat() == HoodieFileFormat.HFILE;\n   }\n \n-  public HoodieTableMetadata metadata() {\n+  public AbstractHoodieTableMetadata metadata() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56717d03adf0391ed581fc3042eaff3a9bf733fb"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5NDIwMw==", "bodyText": "I think we can simply call this HoodieTableMetadataUtils ?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r545394203", "createdAt": "2020-12-17T20:48:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataTimelineUtil.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.metadata.AbstractHoodieTableMetadata.NON_PARTITIONED_NAME;\n+\n+/**\n+ * A utility to convert timeline information to metadata table records.\n+ */\n+public class HoodieTableMetadataTimelineUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56717d03adf0391ed581fc3042eaff3a9bf733fb"}, "originalPosition": 54}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "56717d03adf0391ed581fc3042eaff3a9bf733fb", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/56717d03adf0391ed581fc3042eaff3a9bf733fb", "committedDate": "2020-12-16T10:01:57Z", "message": "[RFC-15][HUDI-1325] Merge updates of unsynced instants to metadata table\n\nFix checkstyle violations\n\nFix rat licenses"}, "afterCommit": {"oid": "de31e8cf9ef6aa9d3fda663635a1a0ad06f8d2e0", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/de31e8cf9ef6aa9d3fda663635a1a0ad06f8d2e0", "committedDate": "2020-12-18T00:19:07Z", "message": "[RFC-15][HUDI-1325] Merge updates of unsynced instants to metadata table\n\nFix checkstyle violations\n\nFix rat licenses"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "de31e8cf9ef6aa9d3fda663635a1a0ad06f8d2e0", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/de31e8cf9ef6aa9d3fda663635a1a0ad06f8d2e0", "committedDate": "2020-12-18T00:19:07Z", "message": "[RFC-15][HUDI-1325] Merge updates of unsynced instants to metadata table\n\nFix checkstyle violations\n\nFix rat licenses"}, "afterCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/78253ff7484ed50217fb4a2c21ba554e988a8c99", "committedDate": "2020-12-18T01:11:03Z", "message": "[RFC-15][HUDI-1325] Merge updates of unsynced instants to metadata table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1OTgwODUw", "url": "https://github.com/apache/hudi/pull/2342#pullrequestreview-555980850", "createdAt": "2020-12-20T02:16:24Z", "commit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjoxNjoyNFrOIJAFKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjo0OTo0N1rOIJAPeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMwOTQxNw==", "bodyText": "I think we can keep the factory method in the interface?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546309417", "createdAt": "2020-12-20T02:16:24Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -266,7 +256,7 @@ private void initialize(JavaSparkContext jsc, HoodieTableMetaClient datasetMetaC\n   }\n \n   private void initTableMetadata() {\n-    this.metadata = new HoodieBackedTableMetadata(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),\n+    this.metadata = (HoodieBackedTableMetadata) AbstractHoodieTableMetadata.create(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMwOTc0Nw==", "bodyText": "won't metadata be synced in postCommit() of the last upsert above. wondering how this is actually not in sync", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546309747", "createdAt": "2020-12-20T02:20:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "diffHunk": "@@ -619,32 +618,97 @@ public void testMetadataMetrics() throws Exception {\n     }\n   }\n \n+  //@ParameterizedTest\n+  //@EnumSource(HoodieTableType.class)\n+  public void testMetadataOutOfSync(HoodieTableType tableType) throws Exception {\n+    init(HoodieTableType.COPY_ON_WRITE);\n+\n+    HoodieWriteClient unsyncedClient = new HoodieWriteClient<>(jsc, getWriteConfig(true, true));\n+\n+    // Enable metadata so table is initialized\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, true))) {\n+      // Perform Bulk Insert\n+      String newCommitTime = \"001\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+    }\n+\n+    // Perform commit operations with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // Perform Insert\n+      String newCommitTime = \"002\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Perform Upsert\n+      newCommitTime = \"003\";\n+      client.startCommitWithTime(newCommitTime);\n+      records = dataGen.generateUniqueUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Compaction\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"004\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMDUyMQ==", "bodyText": "just tableMetadata?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546310521", "createdAt": "2020-12-20T02:31:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "diffHunk": "@@ -619,32 +618,97 @@ public void testMetadataMetrics() throws Exception {\n     }\n   }\n \n+  //@ParameterizedTest\n+  //@EnumSource(HoodieTableType.class)\n+  public void testMetadataOutOfSync(HoodieTableType tableType) throws Exception {\n+    init(HoodieTableType.COPY_ON_WRITE);\n+\n+    HoodieWriteClient unsyncedClient = new HoodieWriteClient<>(jsc, getWriteConfig(true, true));\n+\n+    // Enable metadata so table is initialized\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, true))) {\n+      // Perform Bulk Insert\n+      String newCommitTime = \"001\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+    }\n+\n+    // Perform commit operations with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // Perform Insert\n+      String newCommitTime = \"002\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Perform Upsert\n+      newCommitTime = \"003\";\n+      client.startCommitWithTime(newCommitTime);\n+      records = dataGen.generateUniqueUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Compaction\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"004\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+\n+    // Perform clean operation with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // One more commit needed to trigger clean so upsert and compact\n+      String newCommitTime = \"005\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"006\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+\n+      // Clean\n+      newCommitTime = \"007\";\n+      client.clean(newCommitTime);\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+\n+    // Perform restore with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      client.restoreToInstant(\"004\");\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+  }\n+\n   /**\n    * Validate the metadata tables contents to ensure it matches what is on the file system.\n    *\n    * @throws IOException\n    */\n   private void validateMetadata(HoodieWriteClient client) throws IOException {\n     HoodieWriteConfig config = client.getConfig();\n-    HoodieBackedTableMetadataWriter metadataWriter = metadataWriter(client);\n-    assertNotNull(metadataWriter, \"MetadataWriter should have been initialized\");\n+\n+    HoodieBackedTableMetadata metadataReader = metadata(client);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTE4OQ==", "bodyText": "Given the only subclass of AbstractHoodieTableMetadata is this class and the abstract class already assumes the existence of the metadata table, I am not sure if the splitting is needed per se.\nI was under the impression that we would like to implement the timeline merging as another subclass. If we stick with the new scanner approach (which I actually have grown to like actually), we no longer need the abstract class right?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311189", "createdAt": "2020-12-20T02:39:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -66,24 +58,13 @@\n  * If the metadata table does not exist, RPC calls are used to retrieve file listings from the file system.\n  * No updates are applied to the table and it is not synced.\n  */\n-public class HoodieBackedTableMetadata implements HoodieTableMetadata {\n+public class HoodieBackedTableMetadata extends AbstractHoodieTableMetadata {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTIzMA==", "bodyText": "I assume all of this code, is just verbatim moved up to the base class ?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311230", "createdAt": "2020-12-20T02:39:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -117,172 +93,15 @@ public HoodieBackedTableMetadata(Configuration conf, String datasetBasePath, Str\n     } else {\n       LOG.info(\"Metadata table is disabled.\");\n     }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTU0NQ==", "bodyText": "left a comment around this scenario. there is one valid case here. good call out in the summary", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311545", "createdAt": "2020-12-20T02:44:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -569,114 +471,7 @@ public void update(HoodieRollbackMetadata rollbackMetadata, String instantTime)\n       return;\n     }\n \n-    Map<String, Map<String, Long>> partitionToAppendedFiles = new HashMap<>();\n-    Map<String, List<String>> partitionToDeletedFiles = new HashMap<>();\n-    processRollbackMetadata(rollbackMetadata, partitionToDeletedFiles, partitionToAppendedFiles);\n-    commitRollback(jsc, partitionToDeletedFiles, partitionToAppendedFiles, instantTime, \"Rollback\");\n-  }\n-\n-  /**\n-   * Extracts information about the deleted and append files from the {@code HoodieRollbackMetadata}.\n-   *\n-   * During a rollback files may be deleted (COW, MOR) or rollback blocks be appended (MOR only) to files. This\n-   * function will extract this change file for each partition.\n-   *\n-   * @param rollbackMetadata {@code HoodieRollbackMetadata}\n-   * @param partitionToDeletedFiles The {@code Map} to fill with files deleted per partition.\n-   * @param partitionToAppendedFiles The {@code Map} to fill with files appended per partition and their sizes.\n-   */\n-  private void processRollbackMetadata(HoodieRollbackMetadata rollbackMetadata,\n-                                       Map<String, List<String>> partitionToDeletedFiles,\n-                                       Map<String, Map<String, Long>> partitionToAppendedFiles) {\n-    rollbackMetadata.getPartitionMetadata().values().forEach(pm -> {\n-      final String partition = pm.getPartitionPath();\n-\n-      if (!pm.getSuccessDeleteFiles().isEmpty()) {\n-        if (!partitionToDeletedFiles.containsKey(partition)) {\n-          partitionToDeletedFiles.put(partition, new ArrayList<>());\n-        }\n-\n-        // Extract deleted file name from the absolute paths saved in getSuccessDeleteFiles()\n-        List<String> deletedFiles = pm.getSuccessDeleteFiles().stream().map(p -> new Path(p).getName())\n-            .collect(Collectors.toList());\n-        partitionToDeletedFiles.get(partition).addAll(deletedFiles);\n-      }\n-\n-      if (!pm.getAppendFiles().isEmpty()) {\n-        if (!partitionToAppendedFiles.containsKey(partition)) {\n-          partitionToAppendedFiles.put(partition, new HashMap<>());\n-        }\n-\n-        // Extract appended file name from the absolute paths saved in getAppendFiles()\n-        pm.getAppendFiles().forEach((path, size) -> {\n-          partitionToAppendedFiles.get(partition).merge(new Path(path).getName(), size, (oldSize, newSizeCopy) -> {\n-            return size + oldSize;\n-          });\n-        });\n-      }\n-    });\n-  }\n-\n-  /**\n-   * Create file delete records and commit.\n-   *\n-   * @param partitionToDeletedFiles {@code Map} of partitions and the deleted files\n-   * @param instantTime Timestamp at which the deletes took place\n-   * @param operation Type of the operation which caused the files to be deleted\n-   */\n-  private void commitRollback(JavaSparkContext jsc, Map<String, List<String>> partitionToDeletedFiles,\n-                              Map<String, Map<String, Long>> partitionToAppendedFiles, String instantTime,\n-                              String operation) {\n-    List<HoodieRecord> records = new LinkedList<>();\n-    int[] fileChangeCount = {0, 0}; // deletes, appends\n-\n-    partitionToDeletedFiles.forEach((partition, deletedFiles) -> {\n-      // Rollbacks deletes instants from timeline. The instant being rolled-back may not have been synced to the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 270}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTY0Mw==", "bodyText": "assume most of this code is just from the writer class verbatim", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311643", "createdAt": "2020-12-20T02:45:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.metadata.HoodieTableMetadata.NON_PARTITIONED_NAME;\n+\n+/**\n+ * A utility to convert timeline information to metadata table records.\n+ */\n+public class HoodieTableMetadataUtil {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieTableMetadataUtil.class);\n+\n+  /**\n+   * Converts a timeline instant to metadata table records.\n+   *\n+   * @param datasetMetaClient The meta client associated with the timeline instant\n+   * @param instant to fetch and convert to metadata table records\n+   * @return a list of metadata table records\n+   * @throws IOException\n+   */\n+  public static Option<List<HoodieRecord>> convertInstantToMetaRecords(HoodieTableMetaClient datasetMetaClient, HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTgzNQ==", "bodyText": "should we pass in the filter? guess it does not matter, since we ll read the instants in full anyway", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311835", "createdAt": "2020-12-20T02:47:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;\n+      }\n+    }\n+\n+    return metadataHoodieRecord;\n+  }\n+\n+  protected abstract Option<HoodieRecord<HoodieMetadataPayload>> getRecordByKeyFromMetadata(String key) throws IOException;\n+\n+  private void openTimelineScanner() throws IOException {\n+    if (timelineRecordScanner != null) {\n+      // Already opened\n+      return;\n+    }\n+\n+    HoodieTableMetaClient datasetMetaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+    List<HoodieInstant> unsyncedInstants = findInstantsToSync(datasetMetaClient);\n+    Schema schema = HoodieAvroUtils.addMetadataFields(HoodieMetadataRecord.getClassSchema());\n+    timelineRecordScanner =\n+            new HoodieMetadataMergedInstantRecordScanner(datasetMetaClient, unsyncedInstants, schema, MAX_MEMORY_SIZE_IN_BYTES, spillableMapDirectory, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMjA1OA==", "bodyText": "just return out of here, instead of reassigning to another important variable. may be easier to read.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546312058", "createdAt": "2020-12-20T02:49:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2ODIyMjUz", "url": "https://github.com/apache/hudi/pull/2342#pullrequestreview-556822253", "createdAt": "2020-12-22T05:10:03Z", "commit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwNToxMDowNFrOIJukuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwNjoxMTo0NVrOIJvmUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA3MTE2MQ==", "bodyText": "can be merged with the above function. I dont see it being called anywhere else.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547071161", "createdAt": "2020-12-22T05:10:04Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedInstantRecordScanner.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.DefaultSizeEstimator;\n+import org.apache.hudi.common.util.HoodieRecordSizeEstimator;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.ExternalSpillableMap;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+\n+/**\n+ * Provides functionality to convert timeline instants to table metadata records and then merge by key. Specify\n+ *  a filter to limit keys that are merged and stored in memory.\n+ */\n+public class HoodieMetadataMergedInstantRecordScanner {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieMetadataMergedInstantRecordScanner.class);\n+\n+  HoodieTableMetaClient metaClient;\n+  private List<HoodieInstant> instants;\n+  private Set<String> mergeKeyFilter;\n+  protected final ExternalSpillableMap<String, HoodieRecord<? extends HoodieRecordPayload>> records;\n+\n+  public HoodieMetadataMergedInstantRecordScanner(HoodieTableMetaClient metaClient, List<HoodieInstant> instants,\n+                                                  Schema readerSchema, Long maxMemorySizeInBytes,\n+                                                  String spillableMapBasePath, Set<String> mergeKeyFilter) throws IOException {\n+    this.metaClient = metaClient;\n+    this.instants = instants;\n+    this.mergeKeyFilter = mergeKeyFilter != null ? mergeKeyFilter : Collections.emptySet();\n+    this.records = new ExternalSpillableMap<>(maxMemorySizeInBytes, spillableMapBasePath, new DefaultSizeEstimator(),\n+            new HoodieRecordSizeEstimator(readerSchema));\n+\n+    scan();\n+  }\n+\n+  private void scan() {\n+    for (HoodieInstant instant : instants) {\n+      try {\n+        processInstant(instant);\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"Got exception when processing timeline instant %s\", instant.getTimestamp()), e);\n+        throw new HoodieException(String.format(\"Got exception when processing timeline instant %s\", instant.getTimestamp()), e);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts an instant to metadata table records and processes each record.\n+   *\n+   * @param instant\n+   * @throws IOException\n+   */\n+  private void processInstant(HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA3NjM5Nw==", "bodyText": "I think there needs to be a corresponding close() for the timelineRecordScanner too.\nOnce we commit an update to the table, the unsynced instants have changed.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547076397", "createdAt": "2020-12-22T05:30:30Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -372,19 +191,6 @@ protected void closeReaders() {\n     logRecordScanner = null;\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA4Nzk1NA==", "bodyText": "The reader-write pairs in metadata are related. So this can be simplified to be simply new HoodieBackedTableMetadata.\nHoodieBackedTableMetadata cannot work with any other implementaton of HoodieTableMetadata.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547087954", "createdAt": "2020-12-22T06:11:45Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -266,7 +256,7 @@ private void initialize(JavaSparkContext jsc, HoodieTableMetaClient datasetMetaC\n   }\n \n   private void initTableMetadata() {\n-    this.metadata = new HoodieBackedTableMetadata(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDQyMjAy", "url": "https://github.com/apache/hudi/pull/2342#pullrequestreview-557442202", "createdAt": "2020-12-23T00:32:55Z", "commit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDozMjo1NVrOIKNLYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDozNzoxM1rOIKNPnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MjU3OQ==", "bodyText": "Along with Prashant's comment we can just create HoodieBackedTableMetadata since HoodieBackedTableMetadaWriter should only be associated with this implementation", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547572579", "createdAt": "2020-12-23T00:32:55Z", "author": {"login": "rmpifer"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -266,7 +256,7 @@ private void initialize(JavaSparkContext jsc, HoodieTableMetaClient datasetMetaC\n   }\n \n   private void initTableMetadata() {\n-    this.metadata = new HoodieBackedTableMetadata(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),\n+    this.metadata = (HoodieBackedTableMetadata) AbstractHoodieTableMetadata.create(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMwOTQxNw=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MjU5MQ==", "bodyText": "Agreed. Updated this", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547572591", "createdAt": "2020-12-23T00:32:59Z", "author": {"login": "rmpifer"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -266,7 +256,7 @@ private void initialize(JavaSparkContext jsc, HoodieTableMetaClient datasetMetaC\n   }\n \n   private void initTableMetadata() {\n-    this.metadata = new HoodieBackedTableMetadata(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA4Nzk1NA=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MzEwMA==", "bodyText": "Sorry my comment was unclear. I created separate mergedRecord variable so I am not overriding metadataHoodieRecord", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547573100", "createdAt": "2020-12-23T00:35:04Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMjA1OA=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MzQzMg==", "bodyText": "Thanks for the callout. I added this to closeReaders()", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547573432", "createdAt": "2020-12-23T00:36:20Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -372,19 +191,6 @@ protected void closeReaders() {\n     logRecordScanner = null;\n   }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA3NjM5Nw=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MzY2Mw==", "bodyText": "Updated", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547573663", "createdAt": "2020-12-23T00:37:13Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedInstantRecordScanner.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.DefaultSizeEstimator;\n+import org.apache.hudi.common.util.HoodieRecordSizeEstimator;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.ExternalSpillableMap;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+\n+/**\n+ * Provides functionality to convert timeline instants to table metadata records and then merge by key. Specify\n+ *  a filter to limit keys that are merged and stored in memory.\n+ */\n+public class HoodieMetadataMergedInstantRecordScanner {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieMetadataMergedInstantRecordScanner.class);\n+\n+  HoodieTableMetaClient metaClient;\n+  private List<HoodieInstant> instants;\n+  private Set<String> mergeKeyFilter;\n+  protected final ExternalSpillableMap<String, HoodieRecord<? extends HoodieRecordPayload>> records;\n+\n+  public HoodieMetadataMergedInstantRecordScanner(HoodieTableMetaClient metaClient, List<HoodieInstant> instants,\n+                                                  Schema readerSchema, Long maxMemorySizeInBytes,\n+                                                  String spillableMapBasePath, Set<String> mergeKeyFilter) throws IOException {\n+    this.metaClient = metaClient;\n+    this.instants = instants;\n+    this.mergeKeyFilter = mergeKeyFilter != null ? mergeKeyFilter : Collections.emptySet();\n+    this.records = new ExternalSpillableMap<>(maxMemorySizeInBytes, spillableMapBasePath, new DefaultSizeEstimator(),\n+            new HoodieRecordSizeEstimator(readerSchema));\n+\n+    scan();\n+  }\n+\n+  private void scan() {\n+    for (HoodieInstant instant : instants) {\n+      try {\n+        processInstant(instant);\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"Got exception when processing timeline instant %s\", instant.getTimestamp()), e);\n+        throw new HoodieException(String.format(\"Got exception when processing timeline instant %s\", instant.getTimestamp()), e);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts an instant to metadata table records and processes each record.\n+   *\n+   * @param instant\n+   * @throws IOException\n+   */\n+  private void processInstant(HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA3MTE2MQ=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 82}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "06652a26bcbc4ca707333f841080aba6e0366971", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/06652a26bcbc4ca707333f841080aba6e0366971", "committedDate": "2020-12-28T05:30:50Z", "message": "[HUDI-1325] [RFC-15] Merge updates of unsynced instants to metadata table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/78253ff7484ed50217fb4a2c21ba554e988a8c99", "committedDate": "2020-12-18T01:11:03Z", "message": "[RFC-15][HUDI-1325] Merge updates of unsynced instants to metadata table"}, "afterCommit": {"oid": "06652a26bcbc4ca707333f841080aba6e0366971", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/06652a26bcbc4ca707333f841080aba6e0366971", "committedDate": "2020-12-28T05:30:50Z", "message": "[HUDI-1325] [RFC-15] Merge updates of unsynced instants to metadata table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MjQxNTMz", "url": "https://github.com/apache/hudi/pull/2342#pullrequestreview-559241533", "createdAt": "2020-12-28T17:51:50Z", "commit": {"oid": "06652a26bcbc4ca707333f841080aba6e0366971"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4020, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}