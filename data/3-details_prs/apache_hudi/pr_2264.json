{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI0MjQ0NDg0", "number": 2264, "title": "[HUDI-1406] Add date partition based source input selector for DeltaStreamer", "bodyText": "\u2026streamer\n\nAdds ability to list only recent date based partitions from source data.\n\nTips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\n(For example: This pull request adds quick-start document.)\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-11-19T21:31:23Z", "url": "https://github.com/apache/hudi/pull/2264", "merged": true, "mergeCommit": {"oid": "14d5d1100c69839d2bcfea26af0efdcd2057650d"}, "closed": true, "closedAt": "2020-12-17T11:59:31Z", "author": {"login": "bhasudha"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdeMKnQgBqjQwMTg1MTIwMDM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdnCRHNgFqTU1NDUzNjgyNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4cc886c688b9e1d718b956d4959c1344b4188881", "author": {"user": {"login": "bhasudha", "name": "Bhavani Sudha Saktheeswaran"}}, "url": "https://github.com/apache/hudi/commit/4cc886c688b9e1d718b956d4959c1344b4188881", "committedDate": "2020-11-19T21:28:42Z", "message": "[HUDI-1406] Add date partition based source input selector for Delta streamer\n\n- Adds ability to list only recent date based partitions from source data."}, "afterCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "author": {"user": {"login": "bhasudha", "name": "Bhavani Sudha Saktheeswaran"}}, "url": "https://github.com/apache/hudi/commit/12cf95c6446609db5a1838f4fa80b641b3d9a569", "committedDate": "2020-11-20T00:05:49Z", "message": "[HUDI-1406] Add date partition based source input selector for Delta streamer\n\n- Adds ability to list only recent date based partitions from source data."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2NzMxMzA3", "url": "https://github.com/apache/hudi/pull/2264#pullrequestreview-536731307", "createdAt": "2020-11-23T18:15:50Z", "commit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxODoxNTo1MFrOH4Zw2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxODo0MDoxNlrOH4al0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkwNDQxMQ==", "bodyText": "Can we just use log4j?  I think that's what we use elsewhere directly", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528904411", "createdAt": "2020-11-23T18:15:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMDUwNw==", "bodyText": ".source.dfs.datepartitioned.selector.depth", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528910507", "createdAt": "2020-11-23T18:26:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMDgyNw==", "bodyText": ".source.dfs.datepartitioned.selector.lookback.days", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528910827", "createdAt": "2020-11-23T18:27:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTAyMQ==", "bodyText": ".source.dfs.datepartitioned.selector.currentdate", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528911021", "createdAt": "2020-11-23T18:27:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTE4MA==", "bodyText": ".source.dfs.datepartitioned.selector.listparallelism", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528911180", "createdAt": "2020-11-23T18:27:53Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTQ5Mw==", "bodyText": "can we place the defaults adjacent to the property. so its easier to read.", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528911493", "createdAt": "2020-11-23T18:28:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTg4OQ==", "bodyText": "are you assuming a certain format for the current date to be specified in? would be good to doc/comment that. better have it in the property name", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528911889", "createdAt": "2020-11-23T18:29:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTAyMQ=="}, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNDQ3Ng==", "bodyText": "move to previous line?", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528914476", "createdAt": "2020-11-23T18:33:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));\n+      // Filter based on checkpoint & input size, if needed\n+      long currentBytes = 0;\n+      long maxModificationTime = Long.MIN_VALUE;\n+      List<FileStatus> filteredFiles = new ArrayList<>();\n+      for (FileStatus f : eligibleFiles) {\n+        if (currentBytes + f.getLen() >= sourceLimit) {\n+          // we have enough data, we are done\n+          break;\n+        }\n+\n+        maxModificationTime = f.getModificationTime();\n+        currentBytes += f.getLen();\n+        filteredFiles.add(f);\n+      }\n+\n+      // no data to read\n+      if (filteredFiles.isEmpty()) {\n+        return new ImmutablePair<>(\n+            Option.empty(), lastCheckpointStr.orElseGet(() -> String.valueOf(Long.MIN_VALUE)));\n+      }\n+\n+      // read the files out.\n+      String pathStr =\n+          filteredFiles.stream().map(f -> f.getPath().toString()).collect(Collectors.joining(\",\"));\n+\n+      return new ImmutablePair<>(Option.ofNullable(pathStr), String.valueOf(maxModificationTime));\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\n+          \"Unable to read from source from checkpoint: \" + lastCheckpointStr, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Prunes date level partitions to last few days configured by 'NUM_PREV_DAYS_TO_LIST' from\n+   * 'CURRENT_DATE'. Parallelizes listing by leveraging HoodieSparkEngineContext's methods.\n+   */\n+  public List<String> pruneDatePartitionPaths(\n+      HoodieSparkEngineContext context, FileSystem fs, String rootPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNTQ3NA==", "bodyText": "can we keep these in a single line?", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528915474", "createdAt": "2020-11-23T18:35:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));\n+      // Filter based on checkpoint & input size, if needed\n+      long currentBytes = 0;\n+      long maxModificationTime = Long.MIN_VALUE;\n+      List<FileStatus> filteredFiles = new ArrayList<>();\n+      for (FileStatus f : eligibleFiles) {\n+        if (currentBytes + f.getLen() >= sourceLimit) {\n+          // we have enough data, we are done\n+          break;\n+        }\n+\n+        maxModificationTime = f.getModificationTime();\n+        currentBytes += f.getLen();\n+        filteredFiles.add(f);\n+      }\n+\n+      // no data to read\n+      if (filteredFiles.isEmpty()) {\n+        return new ImmutablePair<>(\n+            Option.empty(), lastCheckpointStr.orElseGet(() -> String.valueOf(Long.MIN_VALUE)));\n+      }\n+\n+      // read the files out.\n+      String pathStr =\n+          filteredFiles.stream().map(f -> f.getPath().toString()).collect(Collectors.joining(\",\"));\n+\n+      return new ImmutablePair<>(Option.ofNullable(pathStr), String.valueOf(maxModificationTime));\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\n+          \"Unable to read from source from checkpoint: \" + lastCheckpointStr, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Prunes date level partitions to last few days configured by 'NUM_PREV_DAYS_TO_LIST' from\n+   * 'CURRENT_DATE'. Parallelizes listing by leveraging HoodieSparkEngineContext's methods.\n+   */\n+  public List<String> pruneDatePartitionPaths(\n+      HoodieSparkEngineContext context, FileSystem fs, String rootPath) {\n+    List<String> partitionPaths = new ArrayList<>();\n+    // get all partition paths before date partition level\n+    partitionPaths.add(rootPath);\n+    if (datePartitionDepth <= 0) {\n+      return partitionPaths;\n+    }\n+    SerializableConfiguration serializedConf = new SerializableConfiguration(fs.getConf());\n+    for (int i = 0; i < datePartitionDepth; i++) {\n+      partitionPaths =\n+          context.flatMap(\n+              partitionPaths,\n+              path -> {\n+                Path subDir = new Path(path);\n+                FileSystem fileSystem = subDir.getFileSystem(serializedConf.get());\n+                // skip files/dirs whose names start with (_, ., etc)\n+                FileStatus[] statuses =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNTk5MA==", "bodyText": "why collect this and then prune?  can't we prune also in parallel?", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528915990", "createdAt": "2020-11-23T18:36:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));\n+      // Filter based on checkpoint & input size, if needed\n+      long currentBytes = 0;\n+      long maxModificationTime = Long.MIN_VALUE;\n+      List<FileStatus> filteredFiles = new ArrayList<>();\n+      for (FileStatus f : eligibleFiles) {\n+        if (currentBytes + f.getLen() >= sourceLimit) {\n+          // we have enough data, we are done\n+          break;\n+        }\n+\n+        maxModificationTime = f.getModificationTime();\n+        currentBytes += f.getLen();\n+        filteredFiles.add(f);\n+      }\n+\n+      // no data to read\n+      if (filteredFiles.isEmpty()) {\n+        return new ImmutablePair<>(\n+            Option.empty(), lastCheckpointStr.orElseGet(() -> String.valueOf(Long.MIN_VALUE)));\n+      }\n+\n+      // read the files out.\n+      String pathStr =\n+          filteredFiles.stream().map(f -> f.getPath().toString()).collect(Collectors.joining(\",\"));\n+\n+      return new ImmutablePair<>(Option.ofNullable(pathStr), String.valueOf(maxModificationTime));\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\n+          \"Unable to read from source from checkpoint: \" + lastCheckpointStr, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Prunes date level partitions to last few days configured by 'NUM_PREV_DAYS_TO_LIST' from\n+   * 'CURRENT_DATE'. Parallelizes listing by leveraging HoodieSparkEngineContext's methods.\n+   */\n+  public List<String> pruneDatePartitionPaths(\n+      HoodieSparkEngineContext context, FileSystem fs, String rootPath) {\n+    List<String> partitionPaths = new ArrayList<>();\n+    // get all partition paths before date partition level\n+    partitionPaths.add(rootPath);\n+    if (datePartitionDepth <= 0) {\n+      return partitionPaths;\n+    }\n+    SerializableConfiguration serializedConf = new SerializableConfiguration(fs.getConf());\n+    for (int i = 0; i < datePartitionDepth; i++) {\n+      partitionPaths =\n+          context.flatMap(\n+              partitionPaths,\n+              path -> {\n+                Path subDir = new Path(path);\n+                FileSystem fileSystem = subDir.getFileSystem(serializedConf.get());\n+                // skip files/dirs whose names start with (_, ., etc)\n+                FileStatus[] statuses =\n+                    fileSystem.listStatus(\n+                        subDir,\n+                        file ->\n+                            IGNORE_FILEPREFIX_LIST.stream()\n+                                .noneMatch(pfx -> file.getName().startsWith(pfx)));\n+                List<String> res = new ArrayList<>();\n+                for (FileStatus status : statuses) {\n+                  res.add(status.getPath().toString());\n+                }\n+                return res.stream();\n+              },\n+              partitionsListParallelism);\n+    }\n+\n+    // Prune date partitions to last few days", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNjk1OA==", "bodyText": "Once again, this sorting can happen in parallel right? and just collect it finally", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528916958", "createdAt": "2020-11-23T18:38:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNzk2OQ==", "bodyText": "can we do this also in spark?", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528917969", "createdAt": "2020-11-23T18:40:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));\n+      // Filter based on checkpoint & input size, if needed\n+      long currentBytes = 0;\n+      long maxModificationTime = Long.MIN_VALUE;\n+      List<FileStatus> filteredFiles = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569"}, "originalPosition": 133}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cfc9d32078d819b0650e9addecbbb79309bf70d5", "author": {"user": {"login": "bhasudha", "name": "Bhavani Sudha Saktheeswaran"}}, "url": "https://github.com/apache/hudi/commit/cfc9d32078d819b0650e9addecbbb79309bf70d5", "committedDate": "2020-12-17T07:03:05Z", "message": "[HUDI-1406] Add date partition based source input selector for Delta streamer\n\n- Adds ability to list only recent date based partitions from source data."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "author": {"user": {"login": "bhasudha", "name": "Bhavani Sudha Saktheeswaran"}}, "url": "https://github.com/apache/hudi/commit/12cf95c6446609db5a1838f4fa80b641b3d9a569", "committedDate": "2020-11-20T00:05:49Z", "message": "[HUDI-1406] Add date partition based source input selector for Delta streamer\n\n- Adds ability to list only recent date based partitions from source data."}, "afterCommit": {"oid": "cfc9d32078d819b0650e9addecbbb79309bf70d5", "author": {"user": {"login": "bhasudha", "name": "Bhavani Sudha Saktheeswaran"}}, "url": "https://github.com/apache/hudi/commit/cfc9d32078d819b0650e9addecbbb79309bf70d5", "committedDate": "2020-12-17T07:03:05Z", "message": "[HUDI-1406] Add date partition based source input selector for Delta streamer\n\n- Adds ability to list only recent date based partitions from source data."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0NTM2ODI1", "url": "https://github.com/apache/hudi/pull/2264#pullrequestreview-554536825", "createdAt": "2020-12-17T11:58:47Z", "commit": {"oid": "cfc9d32078d819b0650e9addecbbb79309bf70d5"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4335, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}