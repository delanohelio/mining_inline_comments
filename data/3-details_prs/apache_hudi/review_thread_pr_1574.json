{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDExMDY3OTM2", "number": 1574, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzowMzozOVrOD5zjvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxODowOVrOD5z7jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTQwMTU5OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestHDFSParquetImportCommand.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzowMzozOVrOGRR6vQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzowMzozOVrOGRR6vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3MjU0MQ==", "bodyText": "use Files.exists(xx)?", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420772541", "createdAt": "2020-05-06T13:03:39Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestHDFSParquetImportCommand.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.utilities.HDFSParquetImporter;\n+import org.apache.hudi.utilities.TestHDFSParquetImporter;\n+import org.apache.hudi.utilities.TestHDFSParquetImporter.HoodieTripModel;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.jupiter.api.Assertions.assertAll;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link org.apache.hudi.cli.commands.HDFSParquetImportCommand}.\n+ */\n+public class ITTestHDFSParquetImportCommand extends AbstractShellIntegrationTest {\n+\n+  private Path sourcePath;\n+  private Path targetPath;\n+  private String tableName;\n+  private String schemaFile;\n+  private String tablePath;\n+\n+  private List<GenericRecord> insertData;\n+  private TestHDFSParquetImporter importer;\n+\n+  @BeforeEach\n+  public void init() throws IOException, ParseException {\n+    tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    sourcePath = new Path(basePath, \"source\");\n+    targetPath = new Path(tablePath);\n+    schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+    // create schema file\n+    try (FSDataOutputStream schemaFileOS = fs.create(new Path(schemaFile))) {\n+      schemaFileOS.write(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA.getBytes());\n+    }\n+\n+    importer = new TestHDFSParquetImporter();\n+    insertData = importer.createInsertRecords(sourcePath);\n+  }\n+\n+  /**\n+   * Test case for 'hdfsparquetimport' with insert.\n+   */\n+  @Test\n+  public void testConvertWithInsert() throws IOException {\n+    String command = String.format(\"hdfsparquetimport --srcPath %s --targetPath %s --tableName %s \"\n+        + \"--tableType %s --rowKeyField %s\" + \" --partitionPathField %s --parallelism %s \"\n+        + \"--schemaFilePath %s --format %s --sparkMemory %s --retry %s --sparkMaster %s\",\n+        sourcePath.toString(), targetPath.toString(), tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"_row_key\", \"timestamp\", \"1\", schemaFile, \"parquet\", \"2G\", \"1\", \"local\");\n+    CommandResult cr = getShell().executeCommand(command);\n+\n+    assertAll(\"Command run success\",\n+        () -> assertTrue(cr.isSuccess()),\n+        () -> assertEquals(\"Table imported to hoodie format\", cr.getResult().toString()));\n+\n+    // Check hudi table exist\n+    String metaPath = targetPath + File.separator + HoodieTableMetaClient.METAFOLDER_NAME;\n+    assertTrue(new File(metaPath).exists(), \"Hoodie table not exist.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTQ0NTA3OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxMzo1OFrOGRSVeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxOToxNzo0OFrOGSMtew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ==", "bodyText": "Please note that I am not talking about you here. But these array indexes containing numbers are very ugly and unreadable. We should think of a way to improve it.\nWe should parse all parameters and it is best to do:\n\nDefine appropriate variables to store each parameter to improve the readability of the code;\nRefactor it, yes, the parse of the parameters should be order-independent;\n\nWDYT? @hddong @vinothchandar", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420779385", "createdAt": "2020-05-06T13:13:58Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -82,17 +82,17 @@ public static void main(String[] args) throws Exception {\n         break;\n       case IMPORT:\n       case UPSERT:\n-        assert (args.length >= 12);\n+        assert (args.length >= 13);\n         String propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[11])) {\n-          propsFilePath = args[11];\n+        if (!StringUtils.isNullOrEmpty(args[12])) {\n+          propsFilePath = args[12];\n         }\n         List<String> configs = new ArrayList<>();\n-        if (args.length > 12) {\n-          configs.addAll(Arrays.asList(args).subList(12, args.length));\n+        if (args.length > 13) {\n+          configs.addAll(Arrays.asList(args).subList(13, args.length));\n         }\n-        returnCode = dataLoad(jsc, command, args[1], args[2], args[3], args[4], args[5], args[6],\n-            Integer.parseInt(args[7]), args[8], args[9], Integer.parseInt(args[10]), propsFilePath, configs);\n+        returnCode = dataLoad(jsc, command, args[3], args[4], args[5], args[6], args[7], args[8],", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM2NTg3MA==", "bodyText": "Please note that I am not talking about you here. But these array indexes containing numbers are very ugly and unreadable. We should think of a way to improve it.\nWe should parse all parameters and it is best to do:\n\nDefine appropriate variables to store each parameter to improve the readability of the code;\nRefactor it, yes, the parse of the parameters should be order-independent;\n\nWDYT? @hddong @vinothchandar\n\nAgree, and have a exist PR(#1174) by @pratyakshsharma", "url": "https://github.com/apache/hudi/pull/1574#discussion_r421365870", "createdAt": "2020-05-07T09:25:53Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -82,17 +82,17 @@ public static void main(String[] args) throws Exception {\n         break;\n       case IMPORT:\n       case UPSERT:\n-        assert (args.length >= 12);\n+        assert (args.length >= 13);\n         String propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[11])) {\n-          propsFilePath = args[11];\n+        if (!StringUtils.isNullOrEmpty(args[12])) {\n+          propsFilePath = args[12];\n         }\n         List<String> configs = new ArrayList<>();\n-        if (args.length > 12) {\n-          configs.addAll(Arrays.asList(args).subList(12, args.length));\n+        if (args.length > 13) {\n+          configs.addAll(Arrays.asList(args).subList(13, args.length));\n         }\n-        returnCode = dataLoad(jsc, command, args[1], args[2], args[3], args[4], args[5], args[6],\n-            Integer.parseInt(args[7]), args[8], args[9], Integer.parseInt(args[10]), propsFilePath, configs);\n+        returnCode = dataLoad(jsc, command, args[3], args[4], args[5], args[6], args[7], args[8],", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ=="}, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM5Mzg3OQ==", "bodyText": "Thanks for reminding me. It seems that PR is inactive?", "url": "https://github.com/apache/hudi/pull/1574#discussion_r421393879", "createdAt": "2020-05-07T10:12:44Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -82,17 +82,17 @@ public static void main(String[] args) throws Exception {\n         break;\n       case IMPORT:\n       case UPSERT:\n-        assert (args.length >= 12);\n+        assert (args.length >= 13);\n         String propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[11])) {\n-          propsFilePath = args[11];\n+        if (!StringUtils.isNullOrEmpty(args[12])) {\n+          propsFilePath = args[12];\n         }\n         List<String> configs = new ArrayList<>();\n-        if (args.length > 12) {\n-          configs.addAll(Arrays.asList(args).subList(12, args.length));\n+        if (args.length > 13) {\n+          configs.addAll(Arrays.asList(args).subList(13, args.length));\n         }\n-        returnCode = dataLoad(jsc, command, args[1], args[2], args[3], args[4], args[5], args[6],\n-            Integer.parseInt(args[7]), args[8], args[9], Integer.parseInt(args[10]), propsFilePath, configs);\n+        returnCode = dataLoad(jsc, command, args[3], args[4], args[5], args[6], args[7], args[8],", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ=="}, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNTgwMw==", "bodyText": "@yanghua Its been there for some time. Development was done, I was stuck at one of the test cases last when I worked on it. Will take a look at it soon :)", "url": "https://github.com/apache/hudi/pull/1574#discussion_r421735803", "createdAt": "2020-05-07T19:17:48Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -82,17 +82,17 @@ public static void main(String[] args) throws Exception {\n         break;\n       case IMPORT:\n       case UPSERT:\n-        assert (args.length >= 12);\n+        assert (args.length >= 13);\n         String propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[11])) {\n-          propsFilePath = args[11];\n+        if (!StringUtils.isNullOrEmpty(args[12])) {\n+          propsFilePath = args[12];\n         }\n         List<String> configs = new ArrayList<>();\n-        if (args.length > 12) {\n-          configs.addAll(Arrays.asList(args).subList(12, args.length));\n+        if (args.length > 13) {\n+          configs.addAll(Arrays.asList(args).subList(13, args.length));\n         }\n-        returnCode = dataLoad(jsc, command, args[1], args[2], args[3], args[4], args[5], args[6],\n-            Integer.parseInt(args[7]), args[8], args[9], Integer.parseInt(args[10]), propsFilePath, configs);\n+        returnCode = dataLoad(jsc, command, args[3], args[4], args[5], args[6], args[7], args[8],", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ=="}, "originalCommit": null, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTQ1MzcxOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HDFSParquetImportCommand.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxNTo1N1rOGRSaug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxNTo1N1rOGRSaug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4MDczMA==", "bodyText": "\"Spark Master \" -> \"Spark Master\" (remove the right empty backspace)", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420780730", "createdAt": "2020-05-06T13:15:57Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HDFSParquetImportCommand.java", "diffHunk": "@@ -57,6 +56,7 @@ public String convert(\n       @CliOption(key = \"schemaFilePath\", mandatory = true,\n           help = \"Path for Avro schema file\") final String schemaFilePath,\n       @CliOption(key = \"format\", mandatory = true, help = \"Format for the input data\") final String format,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxOTQ2MjU0OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HDFSParquetImportCommand.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxODowOVrOGRSgYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoxODowOVrOGRSgYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4MjE3OA==", "bodyText": "As a refactor suggestion, it would be better to define a data structure to store the cli args to avoid change the signature of the method frequently.", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420782178", "createdAt": "2020-05-06T13:18:09Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HDFSParquetImportCommand.java", "diffHunk": "@@ -78,8 +76,8 @@ public String convert(\n       cmd = SparkCommand.UPSERT.toString();\n     }\n \n-    sparkLauncher.addAppArgs(cmd, srcPath, targetPath, tableName, tableType, rowKeyField, partitionPathField,\n-        parallelism, schemaFilePath, sparkMemory, retry, propsFilePath);\n+    sparkLauncher.addAppArgs(cmd, master, sparkMemory, srcPath, targetPath, tableName, tableType, rowKeyField,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4602, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}