{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk0OTE1MDgx", "number": 1457, "title": "[HUDI-741] Added checks to validate Hoodie's schema evolution.", "bodyText": "What is the purpose of the pull request\nHUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by checking that the data written using the old schema can be read using the new schema.\nPlease see HUDI-741  for my detailed analysis of why this is required and how this is implemented.\nBrief change log\nCode changes:\n\nAdded a new config in HoodieWriteConfig to enable schema validation check (disabled by default)\nMoved code that reads schema from base/log files into hudi-common from hudi-hive-sync\nAdded a class org.apache.hudi.common.util.SchemaCompatibility which implements checks for schema evolution. This is based on org.apache.avro.SchemaCompatibility but performs HUDI specific checks.\n\nTesting changes:\n\nExtended HoodieTestDataGenerator to generate records using a custom Schema\nExtended TestHoodieClientBase to add insertBatch API which allows inserting a new batch of unique records into a HUDI table\nAdded a unit test to verify schema evolution for both COW and MOR tables.\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded unit test TestTableSchemaEvolution\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-03-27T18:44:27Z", "url": "https://github.com/apache/hudi/pull/1457", "merged": true, "mergeCommit": {"oid": "19d29ac7d0ac451324d25bfe85b51b1eca10bc67"}, "closed": true, "closedAt": "2020-04-16T06:35:00Z", "author": {"login": "prashantwason"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcSwZszAFqTM4Mzk2OTg4Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcYGyguAFqTM5NDMzNDU0Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgzOTY5ODg2", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-383969886", "createdAt": "2020-03-30T15:25:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNToyNTo1MlrOF9vRcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTo0MDoxMFrOF9v8FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MTk3MA==", "bodyText": "using the Handle classes at this level, breaks the layering.. Please refactor into a common helper.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400281970", "createdAt": "2020-03-30T15:25:52Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MjYyMQ==", "bodyText": "this whole block can be pulled into a private method like validateSchema() and reused? without being inlined into upsertRecordsInternal()", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400282621", "createdAt": "2020-03-30T15:26:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4Mzk4Mw==", "bodyText": "can this and the previous line be combined into one\nSchema tableSchema = schemaUtil.getLatestSchema()\n\nI don't think any other code cares about savedParquetSchema", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400283983", "createdAt": "2020-03-30T15:28:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NDQ2Ng==", "bodyText": "lets have this as a single ERROR statement?", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400284466", "createdAt": "2020-03-30T15:29:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTIyNg==", "bodyText": "can we handle this in code nicely above before try, instead of the catch block as an error case?", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400285226", "createdAt": "2020-03-30T15:30:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);\n+          LOG.warn(\"WriterSchema: \" + writerSchema);\n+          LOG.warn(\"Table latest schema: \" + savedSchema);\n+          throw new HoodieUpsertException(msg);\n+        }\n+      } catch (Exception e) {\n+        // If this is the first insert into the table then schema will not be present\n+        if (hoodieTable.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().countInstants() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTg4MA==", "bodyText": "so this is recaught below? IMO this method should just throw exception from one place..", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400285880", "createdAt": "2020-03-30T15:30:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);\n+          LOG.warn(\"WriterSchema: \" + writerSchema);\n+          LOG.warn(\"Table latest schema: \" + savedSchema);\n+          throw new HoodieUpsertException(msg);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NjQwOQ==", "bodyText": "rename to hoodie.avro.schema.validate ?", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400286409", "createdAt": "2020-03-30T15:31:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -52,6 +52,8 @@\n   private static final String TIMELINE_LAYOUT_VERSION = \"hoodie.timeline.layout.version\";\n   private static final String BASE_PATH_PROP = \"hoodie.base.path\";\n   private static final String AVRO_SCHEMA = \"hoodie.avro.schema\";\n+  private static final String SCHEMA_CHECK = \"hoodie.schema.check\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MDc1OQ==", "bodyText": "this needs to be attributed in LICENSE and NOTICE ..  Can we document all the changes? I am wondering if we can just wrap or extend the org.apache.avro.SchemaCompatibility class instead of copying in full..\ncc @bvaradar as well..", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400290759", "createdAt": "2020-03-30T15:37:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MTU5NA==", "bodyText": "lets move this to org.apache.hudi.common.avro", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400291594", "createdAt": "2020-03-30T15:38:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA==", "bodyText": "let's rename to TableSchemaReader and move to org.apache.hudi.common.table ? (I am trying to move us out of the habit of piling up more util classes)", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400292884", "createdAt": "2020-03-30T15:40:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaUtil.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Utilities to read Schema from data files and log files.\n+ */\n+public class SchemaUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MDEzMzM2", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-386013336", "createdAt": "2020-04-01T23:17:57Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoxNzo1OFrOF_V_DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoxNzo1OFrOF_V_DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDgxMg==", "bodyText": "nit : devolved/evolved", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401964812", "createdAt": "2020-04-01T23:17:58Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MDEzNDEw", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-386013410", "createdAt": "2020-04-01T23:18:09Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoxODowOVrOF_V_Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoxODowOVrOF_V_Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDg2Mw==", "bodyText": "same", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401964863", "createdAt": "2020-04-01T23:18:09Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed\n+    HoodieWriteConfig hoodieDevolvedWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+    client = getHoodieWriteClient(hoodieDevolvedWriteConfig, false);\n+    final List<HoodieRecord> failedRecords = dataGenDevolved.generateInserts(\"004\", numRecords);\n+    try {\n+      // We cannot use insertBatch directly here because we want to insert records\n+      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MDE0MzQ0", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-386014344", "createdAt": "2020-04-01T23:20:44Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoyMDo0NFrOF_WCgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoyMDo0NFrOF_WCgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NTY5OA==", "bodyText": "If you mean to say \"older schema\" or \"shorter schema\" or \"retracted schema\" - not sure if devolved is a work so I'm not sure are you evolving or retracting", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401965698", "createdAt": "2020-04-01T23:20:44Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed\n+    HoodieWriteConfig hoodieDevolvedWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+    client = getHoodieWriteClient(hoodieDevolvedWriteConfig, false);\n+    final List<HoodieRecord> failedRecords = dataGenDevolved.generateInserts(\"004\", numRecords);\n+    try {\n+      // We cannot use insertBatch directly here because we want to insert records\n+      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.\n+      writeBatch(client, \"005\", \"004\", Option.empty(), \"003\", numRecords,\n+          (String s, Integer a) -> failedRecords, HoodieWriteClient::insert, false, 0, 0, 0);\n+      fail(\"Insert with devolved scheme should fail\");\n+    } catch (HoodieInsertException ex) {\n+      // no new commit\n+      checkLatestDeltaCommit(\"004\");\n+      checkReadRecords(\"000\", numRecords);\n+      client.rollback(\"005\");\n+    }\n+\n+    // Update with devolved schema is also not allowed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 137}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MDE1NTk4", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-386015598", "createdAt": "2020-04-01T23:24:10Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoyNDoxMVrOF_WG0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoyNDoxMVrOF_WG0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NjgwMA==", "bodyText": "@prashantwason Can you please mark the lines/methods you have changed with may be MOD for future references ? Also, can you add 1-2 lines as to why that change is needed", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401966800", "createdAt": "2020-04-01T23:24:11Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/avro/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.avro;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.\n+ *\n+ * HUDI requires a Schema to be specified in HoodieWriteConfig and is used by the HoodieWriteClient to\n+ * create the records. The schema is also saved in the data files (parquet format) and log files (avro format).\n+ * Since a schema is required each time new data is ingested into a HUDI dataset, schema can be evolved over time.\n+ *\n+ * HUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by\n+ * checking that the data written using the old schema can be read using the new schema.\n+ *\n+ * New Schema is compatible only if:\n+ * 1. There is no change in schema\n+ * 2. A field has been added and it has a default value specified\n+ *\n+ * New Schema is incompatible if:\n+ * 1. A field has been deleted\n+ * 2. A field has been renamed (treated as delete + add)\n+ * 3. A field's type has changed to be incompatible with the older type\n+ */\n+public class SchemaCompatibility {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2MDE1ODMx", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-386015831", "createdAt": "2020-04-01T23:24:47Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwNTA1NDkx", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-390505491", "createdAt": "2020-04-09T05:47:14Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNTo0NzoxNVrOGDKZGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNTo0NzoxNVrOGDKZGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2OTE3Ng==", "bodyText": "there is a pending PR to move this code into the new table.action package.. FYI.. this may need to move there as well", "url": "https://github.com/apache/hudi/pull/1457#discussion_r405969176", "createdAt": "2020-04-09T05:47:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -487,6 +492,58 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n     return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, instantTime);\n   }\n \n+  /**\n+   * Ensure that the current writerSchema is compatible with the latest schema of this dataset.\n+   *\n+   * When inserting/updating data, we read records using the schema saved in the data/log files\n+   * and convert them to the GenericRecords with writerSchema. Hence, we need to ensure that\n+   * this conversion can take place without errors.\n+   *\n+   * @param hoodieTable The Hoodie Table\n+   * @param isUpsert If this is a check during upserts\n+   * @throws HoodieUpsertException If schema check fails during upserts\n+   * @throws HoodieInsertException If schema check fails during inserts\n+   */\n+  private void validateSchema(HoodieTable<T> hoodieTable, final boolean isUpsert)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwNTA2Nzc3", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-390506777", "createdAt": "2020-04-09T05:51:11Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwNTE3NzEx", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-390517711", "createdAt": "2020-04-09T06:21:14Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNjoyMToxNFrOGDLB0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNjoyMToxNFrOGDLB0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk3OTYwMQ==", "bodyText": "Since we log schema now in commit metadata, can't we just try to read it first. If it is not there, then we can try reading from parquet file.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r405979601", "createdAt": "2020-04-09T06:21:14Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.avro.SchemaCompatibility;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Helper class to read schema from data files and log files and to convert it between different formats.\n+ */\n+public class TableSchemaResolver {\n+\n+  private static final Logger LOG = LogManager.getLogger(TableSchemaResolver.class);\n+  private HoodieTableMetaClient metaClient;\n+\n+  public TableSchemaResolver(HoodieTableMetaClient metaClient) {\n+    this.metaClient = metaClient;\n+  }\n+\n+  /**\n+   * Gets the schema for a hoodie table. Depending on the type of table, read from any file written in the latest\n+   * commit. We will assume that the schema has not changed within a single atomic write.\n+   *\n+   * @return Parquet schema for this table\n+   * @throws Exception\n+   */\n+  public MessageType getDataSchema() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "4f1c1842659f9526e73fc122c77d0752f11a8edf", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/4f1c1842659f9526e73fc122c77d0752f11a8edf", "committedDate": "2020-04-15T21:39:38Z", "message": "[HUDI-741] Added checks to validate Hoodie's schema evolution.\n\nHUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by checking that the data written using the old schema can be read using the new schema.\n\nCode changes:\n\n1. Added a new config in HoodieWriteConfig to enable schema validation check (disabled by default)\n2. Moved code that reads schema from base/log files into hudi-common from hudi-hive-sync\n3. Added writerSchema to the extraMetadata of compaction commits in MOR table. This is same as that for commits on COW table.\n\nTesting changes:\n\n4. Extended TestHoodieClientBase to add insertBatch API which allows inserting a new batch of unique records into a HUDI table\n5. Added a unit test to verify schema evolution for both COW and MOR tables.\n6. Added unit tests for schema compatiblity checks."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "4f1c1842659f9526e73fc122c77d0752f11a8edf", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/4f1c1842659f9526e73fc122c77d0752f11a8edf", "committedDate": "2020-04-15T21:39:38Z", "message": "[HUDI-741] Added checks to validate Hoodie's schema evolution.\n\nHUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by checking that the data written using the old schema can be read using the new schema.\n\nCode changes:\n\n1. Added a new config in HoodieWriteConfig to enable schema validation check (disabled by default)\n2. Moved code that reads schema from base/log files into hudi-common from hudi-hive-sync\n3. Added writerSchema to the extraMetadata of compaction commits in MOR table. This is same as that for commits on COW table.\n\nTesting changes:\n\n4. Extended TestHoodieClientBase to add insertBatch API which allows inserting a new batch of unique records into a HUDI table\n5. Added a unit test to verify schema evolution for both COW and MOR tables.\n6. Added unit tests for schema compatiblity checks."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0MzM0NTQ3", "url": "https://github.com/apache/hudi/pull/1457#pullrequestreview-394334547", "createdAt": "2020-04-16T06:34:52Z", "commit": {"oid": "4f1c1842659f9526e73fc122c77d0752f11a8edf"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3275, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}