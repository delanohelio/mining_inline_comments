{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM3OTkzNzY0", "number": 1756, "title": "[HUDI-839] Introducing support for rollbacks using marker files", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nadd more tests for MarkerFiles,RollbackUtils, RollbackActionExecutor for markers and filelisting.\nalso fix the bugs for RollbackActionExecutor with markers mode or filelisting mode:\nin HoodieWriteClient.java should not deleteMarkerDir, the rollback with markerfiles mode will failed\nin ListingBasedRollbackHelper.java \"(path.toString().endsWith(HoodieFileFormat.HOODIE_LOG.getFileExtension()))\" can not check file is logfile\nin \"MarkerBasedRollbackStrategy.java\" baseCommitTime should use FSUtils.getCommitTime(baseFilePathForAppend.getName());\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-06-22T14:48:41Z", "url": "https://github.com/apache/hudi/pull/1756", "merged": true, "mergeCommit": {"oid": "1ec89e9a94161aa5d48b8f170bf67633b389d997"}, "closed": true, "closedAt": "2020-07-21T05:41:43Z", "author": {"login": "lw309637554"}, "timelineItems": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcnLtByAH2gAyNDM3OTkzNzY0OjkyMmIxZWZkYmQyZDRlMzMyOTcwNzZlZWE1ZmU0N2Q5NmEwYzI2MWM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc2-UExAH2gAyNDM3OTkzNzY0OmYzNmIyMzRlYWVjN2M5ZTdmNTc3ZjYzZjYzMWM2OWZjMWY1ZDYwNjU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "922b1efdbd2d4e33297076eea5fe47d96a0c261c", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/922b1efdbd2d4e33297076eea5fe47d96a0c261c", "committedDate": "2020-06-02T02:47:16Z", "message": "[HUDI-839] Introducing rollback strategy using marker files\n\n  - Initial commit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "470842d988f383f305c812a47e260e6bba51e993", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/470842d988f383f305c812a47e260e6bba51e993", "committedDate": "2020-06-11T17:10:21Z", "message": "Adding unit test for MarkerFiles"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "751f2baa3a8a562d53855c4d2c9d93daa20fde0c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/751f2baa3a8a562d53855c4d2c9d93daa20fde0c", "committedDate": "2020-06-22T14:15:36Z", "message": "[HUDI-839] Adding unit test for MarkerFiles,RollbackUtils, RollbackActionExecutor for markers and filelisting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "committedDate": "2020-06-22T16:15:00Z", "message": " [HUDI-839] Merge branch 'master'"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d7831d8172f3b886b5f077007f4c22e758055352", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d7831d8172f3b886b5f077007f4c22e758055352", "committedDate": "2020-06-22T14:46:06Z", "message": " [HUDI-839]  Merge branch 'master'"}, "afterCommit": {"oid": "e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/e55ce7cd424bf33aa0eb2420449c34a19ecfa891", "committedDate": "2020-06-22T16:15:00Z", "message": " [HUDI-839] Merge branch 'master'"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MjQ0MDk0", "url": "https://github.com/apache/hudi/pull/1756#pullrequestreview-436244094", "createdAt": "2020-06-24T00:20:32Z", "commit": {"oid": "e55ce7cd424bf33aa0eb2420449c34a19ecfa891"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwMDoyMDozMlrOGn-1Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwMDoyMDozMlrOGn-1Rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU3NzA5NA==", "bodyText": "cc @xushiyan I feel just making these two commits or this template code in some TestHelper.. will clean up code by a lot..", "url": "https://github.com/apache/hudi/pull/1756#discussion_r444577094", "createdAt": "2020-06-24T00:20:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/action/rollback/TestCopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+public class TestCopyOnWriteRollbackActionExecutor extends HoodieClientTestBase {\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initPath();\n+    initSparkContexts();\n+    //just generate tow partitions\n+    dataGen = new HoodieTestDataGenerator(new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n+    initFileSystem();\n+    initMetaClient();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  @Test\n+  public void testCopyOnWriteRollbackActionExecutorForFileListingAsGenerateFile() throws IOException {\n+    // Let's create some commit files and parquet files\n+    String commitTime1 = \"001\";\n+    String commitTime2 = \"002\";\n+    new File(basePath + \"/.hoodie\").mkdirs();\n+    HoodieTestDataGenerator.writePartitionMetadata(fs, new String[]{\"2015/03/16\", \"2015/03/17\", \"2016/03/15\"},\n+        basePath);\n+    HoodieTestUtils.createCommitFiles(basePath, commitTime1, commitTime2);\n+\n+    // Make commit1\n+    String file11 = HoodieTestUtils.createDataFile(basePath, \"2015/03/16\", commitTime1, \"id11\");\n+    HoodieTestUtils.createNewLogFile(fs, basePath, \"2015/03/16\",\n+        commitTime1, \"id11\", Option.of(3));\n+    String file12 = HoodieTestUtils.createDataFile(basePath, \"2015/03/17\", commitTime1, \"id12\");\n+\n+    // Make commit2\n+    String file21 = HoodieTestUtils.createDataFile(basePath, \"2015/03/16\", commitTime2, \"id21\");\n+    String file22 = HoodieTestUtils.createDataFile(basePath, \"2015/03/17\", commitTime2, \"id22\");\n+    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n+    HoodieTable table = this.getHoodieTable(metaClient, config);\n+    HoodieInstant needRollBackInstant = new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, \"002\");\n+\n+    // execute CopyOnWriteRollbackActionExecutor with filelisting mode\n+    CopyOnWriteRollbackActionExecutor copyOnWriteRollbackActionExecutor = new CopyOnWriteRollbackActionExecutor(jsc, config, table, \"003\", needRollBackInstant, true);\n+    assertFalse(copyOnWriteRollbackActionExecutor.getRollbackStrategy() instanceof MarkerBasedRollbackStrategy);\n+    List<HoodieRollbackStat> hoodieRollbackStats = copyOnWriteRollbackActionExecutor.executeRollback();\n+\n+    // assert hoodieRollbackStats\n+    assertEquals(hoodieRollbackStats.size(), 3);\n+    hoodieRollbackStats.forEach(stat -> {\n+      if (stat.getPartitionPath().equals(\"2015/03/16\")) {\n+        assertEquals(1, stat.getSuccessDeleteFiles().size());\n+        assertEquals(0, stat.getFailedDeleteFiles().size());\n+        assertEquals(null, stat.getCommandBlocksCount());\n+        assertEquals(\"file:\" + HoodieTestUtils.getDataFilePath(basePath, \"2015/03/16\", commitTime2, file21),\n+            stat.getSuccessDeleteFiles().get(0));\n+      } else if (stat.getPartitionPath().equals(\"2015/03/17\")) {\n+        assertEquals(1, stat.getSuccessDeleteFiles().size());\n+        assertEquals(0, stat.getFailedDeleteFiles().size());\n+        assertEquals(null, stat.getCommandBlocksCount());\n+        assertEquals(\"file:\" + HoodieTestUtils.getDataFilePath(basePath, \"2015/03/17\", commitTime2, file22),\n+            stat.getSuccessDeleteFiles().get(0));\n+      } else if (stat.getPartitionPath().equals(\"2015/03/17\")) {\n+        assertEquals(0, stat.getSuccessDeleteFiles().size());\n+        assertEquals(0, stat.getFailedDeleteFiles().size());\n+        assertEquals(null, stat.getCommandBlocksCount());\n+      }\n+    });\n+\n+    assertTrue(HoodieTestUtils.doesCommitExist(basePath, \"001\"));\n+    assertTrue(HoodieTestUtils.doesInflightExist(basePath, \"001\"));\n+    assertFalse(HoodieTestUtils.doesCommitExist(basePath, \"002\"));\n+    assertFalse(HoodieTestUtils.doesInflightExist(basePath, \"002\"));\n+    assertTrue(HoodieTestUtils.doesDataFileExist(basePath, \"2015/03/16\", commitTime1, file11)\n+        && HoodieTestUtils.doesDataFileExist(basePath, \"2015/03/17\", commitTime1, file12));\n+    assertFalse(HoodieTestUtils.doesDataFileExist(basePath, \"2015/03/16\", commitTime2, file21)\n+        || HoodieTestUtils.doesDataFileExist(basePath, \"2015/03/17\", commitTime2, file22));\n+  }\n+\n+  private void twoUpsertCommitDataRollBack(boolean isUsingMarkers) throws IOException {\n+    //1. prepare data\n+    HoodieWriteConfig cfg = getConfigBuilder().withRollbackUsingMarkers(isUsingMarkers).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e55ce7cd424bf33aa0eb2420449c34a19ecfa891"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MjQ0NjQy", "url": "https://github.com/apache/hudi/pull/1756#pullrequestreview-436244642", "createdAt": "2020-06-24T00:22:22Z", "commit": {"oid": "e55ce7cd424bf33aa0eb2420449c34a19ecfa891"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "37244033e285aa198ec1b420b3040d6c1beb706b", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/37244033e285aa198ec1b420b3040d6c1beb706b", "committedDate": "2020-06-25T17:24:00Z", "message": "[HUDI-839] merge latest master patch , add more test for rollback using markers, fix bugs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "37244033e285aa198ec1b420b3040d6c1beb706b", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/37244033e285aa198ec1b420b3040d6c1beb706b", "committedDate": "2020-06-25T17:24:00Z", "message": "[HUDI-839] merge latest master patch , add more test for rollback using markers, fix bugs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97380ea18311668d9eb3ee74998cbd526c8aafa0", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/97380ea18311668d9eb3ee74998cbd526c8aafa0", "committedDate": "2020-06-27T15:56:31Z", "message": "[HUDI-839] merge latest master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "713dd3176b3d49901d67d51ef38f0b7a67089360", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/713dd3176b3d49901d67d51ef38f0b7a67089360", "committedDate": "2020-07-02T08:06:03Z", "message": "[HUDI-839] clean marker file at pre commit for spark retries"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/9f57d75d73b81a31d7ece46a9134fcf26886dc31", "committedDate": "2020-07-02T13:09:35Z", "message": "[HUDI-839] merge latest master"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTc2MDQy", "url": "https://github.com/apache/hudi/pull/1756#pullrequestreview-442576042", "createdAt": "2020-07-04T00:03:12Z", "commit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQwMDowMzoxMlrOGs41bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQwMDo0Mzo0MlrOGtGEUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyMTcwOQ==", "bodyText": "I am wondering if we should do this always.. having this logic be rollback dependent can become hard to reason with in the long run", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449721709", "createdAt": "2020-07-04T00:03:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -332,9 +333,11 @@ public static SparkConf registerClasses(SparkConf conf) {\n   }\n \n   @Override\n-  protected void postCommit(HoodieCommitMetadata metadata, String instantTime,\n-      Option<Map<String, String>> extraMetadata) {\n+  protected void postCommit(HoodieTable<?> table, HoodieCommitMetadata metadata, String instantTime, Option<Map<String, String>> extraMetadata) {\n     try {\n+      if (!config.getRollBackUsingMarkers()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcyMTgyMg==", "bodyText": "Reflecting on this, probably good to rename MarkerType to IOType..", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449721822", "createdAt": "2020-07-04T00:04:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -278,6 +286,11 @@ public WriteStatus getWriteStatus() {\n     return writeStatus;\n   }\n \n+  @Override\n+  public MarkerFiles.MarkerType getIOType() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc4NjgyMw==", "bodyText": "probably need to ensure we are getting the base file format extension from the hoodieTable instance?", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449786823", "createdAt": "2020-07-04T16:34:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -113,8 +109,9 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n       partitionMetadata.trySave(getPartitionId());\n \n       oldFilePath = new Path(config.getBasePath() + \"/\" + partitionPath + \"/\" + latestValidFilePath);\n+      String newFileName = FSUtils.makeDataFileName(instantTime, writeToken, fileId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc4NzExMA==", "bodyText": "nit: we can just name this create()", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449787110", "createdAt": "2020-07-04T16:38:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -97,28 +98,9 @@ public Path makeNewPath(String partitionPath) {\n    *\n    * @param partitionPath Partition path\n    */\n-  protected void createMarkerFile(String partitionPath) {\n-    Path markerPath = makeNewMarkerPath(partitionPath);\n-    try {\n-      LOG.info(\"Creating Marker Path=\" + markerPath);\n-      fs.create(markerPath, false).close();\n-    } catch (IOException e) {\n-      throw new HoodieException(\"Failed to create marker file \" + markerPath, e);\n-    }\n-  }\n-\n-  /**\n-   * THe marker path will be <base-path>/.hoodie/.temp/<instant_ts>/2019/04/25/filename.\n-   */\n-  private Path makeNewMarkerPath(String partitionPath) {\n-    Path markerRootPath = new Path(hoodieTable.getMetaClient().getMarkerFolderPath(instantTime));\n-    Path path = FSUtils.getPartitionPath(markerRootPath, partitionPath);\n-    try {\n-      fs.mkdirs(path); // create a new partition as needed.\n-    } catch (IOException e) {\n-      throw new HoodieIOException(\"Failed to make dir \" + path, e);\n-    }\n-    return new Path(path.toString(), FSUtils.makeMarkerFile(instantTime, writeToken, fileId));\n+  protected void createMarkerFile(String partitionPath, String dataFileName) {\n+    MarkerFiles markerFiles = new MarkerFiles(hoodieTable, instantTime);\n+    markerFiles.createMarkerFile(partitionPath, dataFileName, getIOType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNjk1OA==", "bodyText": "these could be replaced with base format from the table object?", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449936958", "createdAt": "2020-07-06T00:30:35Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -54,29 +53,28 @@\n /**\n  * Performs Rollback of Hoodie Tables.\n  */\n-public class RollbackHelper implements Serializable {\n+public class ListingBasedRollbackHelper implements Serializable {\n \n-  private static final Logger LOG = LogManager.getLogger(RollbackHelper.class);\n+  private static final Logger LOG = LogManager.getLogger(ListingBasedRollbackHelper.class);\n \n   private final HoodieTableMetaClient metaClient;\n   private final HoodieWriteConfig config;\n \n-  public RollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config) {\n+  public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config) {\n     this.metaClient = metaClient;\n     this.config = config;\n   }\n \n   /**\n    * Performs all rollback actions that we have collected in parallel.\n    */\n-  public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<RollbackRequest> rollbackRequests) {\n+  public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n \n-    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n     SerializablePathFilter filter = (path) -> {\n-      if (path.toString().contains(basefileExtension)) {\n+      if (path.toString().endsWith(HoodieFileFormat.PARQUET.getFileExtension())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzIxMA==", "bodyText": "nit:extra line", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449937210", "createdAt": "2020-07-06T00:32:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/MarkerBasedRollbackStrategy.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Performs rollback using marker files generated during the write..\n+ */\n+public class MarkerBasedRollbackStrategy implements BaseRollbackActionExecutor.RollbackStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerBasedRollbackStrategy.class);\n+\n+  private final HoodieTable<?> table;\n+\n+  private final transient JavaSparkContext jsc;\n+\n+  private final HoodieWriteConfig config;\n+\n+  private final String basePath;\n+\n+  private final String instantTime;\n+\n+  public MarkerBasedRollbackStrategy(HoodieTable<?> table, JavaSparkContext jsc, HoodieWriteConfig config, String instantTime) {\n+    this.table = table;\n+    this.jsc = jsc;\n+    this.basePath = table.getMetaClient().getBasePath();\n+    this.config = config;\n+    this.instantTime = instantTime;\n+  }\n+\n+  private HoodieRollbackStat undoMerge(String mergedBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the merged base file:\" + mergedBaseFilePath);\n+    return deleteBaseFile(mergedBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat undoCreate(String createdBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the created base file:\" + createdBaseFilePath);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzc1Mw==", "bodyText": "these combinations added on Schema evolution test is a bit hard to understand. like why would be testing modes for rollbacks in a schema evolution test? any particular reason?", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449937753", "createdAt": "2020-07-06T00:37:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -408,6 +416,16 @@ public void testCopyOnWriteTable() throws Exception {\n     checkReadRecords(\"000\", 2 * numRecords);\n   }\n \n+  @Test\n+  public void testCopyOnWriteTableUsingFileListRollBack() throws Exception {\n+    testCopyOnWriteTable(false);\n+  }\n+\n+  @Test\n+  public void testCopyOnWriteTableUsingMarkersRollBack() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzg5NQ==", "bodyText": "Similar here.. this test probably does not need to test these two modes? for e.g: what additional testing are we getting over the test schema evolution test by doing this?", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449937895", "createdAt": "2020-07-06T00:38:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/index/TestHoodieIndex.java", "diffHunk": "@@ -328,6 +330,18 @@ public void testSimpleTagLocationAndUpdateWithRollback(IndexType indexType) thro\n     assert (javaRDD.filter(record -> record.getCurrentLocation() != null).collect().size() == 0);\n   }\n \n+  @ParameterizedTest\n+  @EnumSource(value = IndexType.class, names = {\"BLOOM\", \"GLOBAL_BLOOM\", \"SIMPLE\", \"GLOBAL_SIMPLE\"})", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzkzNw==", "bodyText": "same comment here..", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449937937", "createdAt": "2020-07-06T00:38:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/TestCleaner.java", "diffHunk": "@@ -904,6 +901,19 @@ public void testCleanMarkerDataFilesOnRollback() throws IOException {\n     assertEquals(0, getTotalTempFiles(), \"All temp files are deleted.\");\n   }\n \n+  /**\n+   * Test Cleaning functionality of table.rollback() API.\n+   */\n+  @Test\n+  public void testCleanMarkerDataFilesOnRollbackUsingFileList() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODAxNg==", "bodyText": "these may be worth adding in both modes..", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449938016", "createdAt": "2020-07-06T00:39:35Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/TestHoodieMergeOnReadTable.java", "diffHunk": "@@ -445,10 +442,20 @@ public void testCOWToMORConvertedTableRollback(HoodieFileFormat baseFileFormat)\n \n   @ParameterizedTest\n   @MethodSource(\"argumentsProvider\")\n-  public void testRollbackWithDeltaAndCompactionCommit(HoodieFileFormat baseFileFormat) throws Exception {\n+  public void testCOWToMORConvertedTableRollbackUsingFileList(HoodieFileFormat baseFileFormat) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODIyMg==", "bodyText": "typo:two", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449938222", "createdAt": "2020-07-06T00:41:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/action/rollback/TestCopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+public class TestCopyOnWriteRollbackActionExecutor extends HoodieClientTestBase {\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initPath();\n+    initSparkContexts();\n+    //just generate tow partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODUxMw==", "bodyText": "anyway to share code with the COW test?", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449938513", "createdAt": "2020-07-06T00:43:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/action/rollback/TestMergeOnReadRollbackActionExecutor.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+\n+public class TestMergeOnReadRollbackActionExecutor extends HoodieClientTestBase {\n+  @Override\n+  protected HoodieTableType getTableType() {\n+    return HoodieTableType.MERGE_ON_READ;\n+  }\n+\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initPath();\n+    initSparkContexts();\n+    //just generate tow partitions\n+    dataGen = new HoodieTestDataGenerator(new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n+    initFileSystem();\n+    initMetaClient();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  private void twoUpsertCommitDataRollBack(boolean isUsingMarkers) throws IOException, InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 73}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13543363004a757cc7bd7041a7ef81177cef0106", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/13543363004a757cc7bd7041a7ef81177cef0106", "committedDate": "2020-07-07T08:16:26Z", "message": "[HUDI-839] using fileformat with table.getBaseFileExtension"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d77c0e3d6cc8e2f7eb4ebe01533cfcf14d3c5c97", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d77c0e3d6cc8e2f7eb4ebe01533cfcf14d3c5c97", "committedDate": "2020-07-07T08:30:56Z", "message": "Merge branch 'master'  into HUDI-839-lw"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "d77c0e3d6cc8e2f7eb4ebe01533cfcf14d3c5c97", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d77c0e3d6cc8e2f7eb4ebe01533cfcf14d3c5c97", "committedDate": "2020-07-07T08:30:56Z", "message": "Merge branch 'master'  into HUDI-839-lw"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNzMzODM0", "url": "https://github.com/apache/hudi/pull/1756#pullrequestreview-442733834", "createdAt": "2020-07-06T01:50:29Z", "commit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQwMTo1MDoyOVrOGtGrjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wN1QwNjoyNjowNlrOGtwr4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk0ODU1Nw==", "bodyText": "ok", "url": "https://github.com/apache/hudi/pull/1756#discussion_r449948557", "createdAt": "2020-07-06T01:50:29Z", "author": {"login": "lw309637554"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/action/rollback/TestCopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+public class TestCopyOnWriteRollbackActionExecutor extends HoodieClientTestBase {\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initPath();\n+    initSparkContexts();\n+    //just generate tow partitions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzODIyMg=="}, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDYzMzkzNg==", "bodyText": "nit: we can just name this create()\n\nyes ,i will be better", "url": "https://github.com/apache/hudi/pull/1756#discussion_r450633936", "createdAt": "2020-07-07T06:18:17Z", "author": {"login": "lw309637554"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -97,28 +98,9 @@ public Path makeNewPath(String partitionPath) {\n    *\n    * @param partitionPath Partition path\n    */\n-  protected void createMarkerFile(String partitionPath) {\n-    Path markerPath = makeNewMarkerPath(partitionPath);\n-    try {\n-      LOG.info(\"Creating Marker Path=\" + markerPath);\n-      fs.create(markerPath, false).close();\n-    } catch (IOException e) {\n-      throw new HoodieException(\"Failed to create marker file \" + markerPath, e);\n-    }\n-  }\n-\n-  /**\n-   * THe marker path will be <base-path>/.hoodie/.temp/<instant_ts>/2019/04/25/filename.\n-   */\n-  private Path makeNewMarkerPath(String partitionPath) {\n-    Path markerRootPath = new Path(hoodieTable.getMetaClient().getMarkerFolderPath(instantTime));\n-    Path path = FSUtils.getPartitionPath(markerRootPath, partitionPath);\n-    try {\n-      fs.mkdirs(path); // create a new partition as needed.\n-    } catch (IOException e) {\n-      throw new HoodieIOException(\"Failed to make dir \" + path, e);\n-    }\n-    return new Path(path.toString(), FSUtils.makeMarkerFile(instantTime, writeToken, fileId));\n+  protected void createMarkerFile(String partitionPath, String dataFileName) {\n+    MarkerFiles markerFiles = new MarkerFiles(hoodieTable, instantTime);\n+    markerFiles.createMarkerFile(partitionPath, dataFileName, getIOType());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc4NzExMA=="}, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDYzNjc2OA==", "bodyText": "yes ,i will remove it", "url": "https://github.com/apache/hudi/pull/1756#discussion_r450636768", "createdAt": "2020-07-07T06:26:06Z", "author": {"login": "lw309637554"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -408,6 +416,16 @@ public void testCopyOnWriteTable() throws Exception {\n     checkReadRecords(\"000\", 2 * numRecords);\n   }\n \n+  @Test\n+  public void testCopyOnWriteTableUsingFileListRollBack() throws Exception {\n+    testCopyOnWriteTable(false);\n+  }\n+\n+  @Test\n+  public void testCopyOnWriteTableUsingMarkersRollBack() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNzc1Mw=="}, "originalCommit": {"oid": "9f57d75d73b81a31d7ece46a9134fcf26886dc31"}, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5MjUyMjQ4", "url": "https://github.com/apache/hudi/pull/1756#pullrequestreview-449252248", "createdAt": "2020-07-15T19:09:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOTowOTowMFrOGyMEXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOTowOTowMFrOGyMEXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI3OTcwOQ==", "bodyText": "@lw309637554 any reason for changing this to PARQUET? (or was that me?)", "url": "https://github.com/apache/hudi/pull/1756#discussion_r455279709", "createdAt": "2020-07-15T19:09:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -182,12 +162,12 @@ private HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRol\n    * Common method used for cleaning out parquet files under a partition path during rollback of a set of commits.\n    */\n   private Map<FileStatus, Boolean> deleteCleanedFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n-      Map<FileStatus, Boolean> results, String commit, String partitionPath) throws IOException {\n+                                                      String commit, String partitionPath) throws IOException {\n+    final Map<FileStatus, Boolean> results = new HashMap<>();\n     LOG.info(\"Cleaning path \" + partitionPath);\n     FileSystem fs = metaClient.getFs();\n-    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n     PathFilter filter = (path) -> {\n-      if (path.toString().contains(basefileExtension)) {\n+      if (path.toString().contains(HoodieFileFormat.PARQUET.getFileExtension())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 160}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "d66a9ec6adf000e4f3a8578aea9721d59b228fb9", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d66a9ec6adf000e4f3a8578aea9721d59b228fb9", "committedDate": "2020-07-16T05:54:23Z", "message": " [HUDI-839] revert the useless unit tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "d66a9ec6adf000e4f3a8578aea9721d59b228fb9", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d66a9ec6adf000e4f3a8578aea9721d59b228fb9", "committedDate": "2020-07-16T05:54:23Z", "message": " [HUDI-839] revert the useless unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f193a1155cf58842bb9d496a691579ba2ed71df7", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/f193a1155cf58842bb9d496a691579ba2ed71df7", "committedDate": "2020-07-19T22:41:08Z", "message": "Adding marker directory cleanup to timeline archival process\n\n - Added marker dir deletion after successful commit/rollback, individual files are not deleted during finalize\n - Fail safe for deleting marker directories, now during timeline archival process\n - Added check to ensure completed instants are not rolled back using marker based strategy. This will be incorrect\n - Reworked tests to rollback inflight instants, instead of completed instants whenever necessary\n - Added an unit test for MarkerBasedRollbackStrategy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/a0c4de4af17603e879e4bfdfc05846bdabecc6fa", "committedDate": "2020-07-19T22:52:20Z", "message": "Merge branch 'master' into pull/1756"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxMTcwMDY5", "url": "https://github.com/apache/hudi/pull/1756#pullrequestreview-451170069", "createdAt": "2020-07-19T22:54:31Z", "commit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQyMjo1NDozMVrOGzy-TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOVQyMzowMDo0NlrOGzzAvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTcwOQ==", "bodyText": "this PR will change behavior for marker dir deletion, with or without marker based rollback turned on.", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456965709", "createdAt": "2020-07-19T22:54:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -332,9 +333,12 @@ public static SparkConf registerClasses(SparkConf conf) {\n   }\n \n   @Override\n-  protected void postCommit(HoodieCommitMetadata metadata, String instantTime,\n-      Option<Map<String, String>> extraMetadata) {\n+  protected void postCommit(HoodieTable<?> table, HoodieCommitMetadata metadata, String instantTime, Option<Map<String, String>> extraMetadata) {\n     try {\n+\n+      // Delete the marker directory for the instant.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTgyOA==", "bodyText": "all of this stuff is now encapsulatd into a  MarkerFiles class", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456965828", "createdAt": "2020-07-19T22:55:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -97,28 +98,9 @@ public Path makeNewPath(String partitionPath) {\n    *\n    * @param partitionPath Partition path\n    */\n-  protected void createMarkerFile(String partitionPath) {\n-    Path markerPath = makeNewMarkerPath(partitionPath);\n-    try {\n-      LOG.info(\"Creating Marker Path=\" + markerPath);\n-      fs.create(markerPath, false).close();\n-    } catch (IOException e) {\n-      throw new HoodieException(\"Failed to create marker file \" + markerPath, e);\n-    }\n-  }\n-\n-  /**\n-   * THe marker path will be <base-path>/.hoodie/.temp/<instant_ts>/2019/04/25/filename.\n-   */\n-  private Path makeNewMarkerPath(String partitionPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTkzOQ==", "bodyText": "quick skim of changes here would be nice.", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456965939", "createdAt": "2020-07-19T22:56:43Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -410,72 +412,54 @@ public void deleteMarkerDir(String instantTs) {\n    * @param consistencyCheckEnabled Consistency Check Enabled\n    * @throws HoodieIOException\n    */\n-  protected void cleanFailedWrites(JavaSparkContext jsc, String instantTs, List<HoodieWriteStat> stats,\n-      boolean consistencyCheckEnabled) throws HoodieIOException {\n+  protected void reconcileAgainstMarkers(JavaSparkContext jsc,\n+                                         String instantTs,\n+                                         List<HoodieWriteStat> stats,\n+                                         boolean consistencyCheckEnabled) throws HoodieIOException {\n     try {\n       // Reconcile marker and data files with WriteStats so that partially written data-files due to failed\n       // (but succeeded on retry) tasks are removed.\n       String basePath = getMetaClient().getBasePath();\n-      FileSystem fs = getMetaClient().getFs();\n-      Path markerDir = new Path(metaClient.getMarkerFolderPath(instantTs));\n+      MarkerFiles markers = new MarkerFiles(this, instantTs);\n \n-      if (!fs.exists(markerDir)) {\n-        // Happens when all writes are appends\n+      if (!markers.doesMarkerDirExist()) {\n+        // can happen if it was an empty write say.\n         return;\n       }\n \n-      final String baseFileExtension = getBaseFileFormat().getFileExtension();\n-      List<String> invalidDataPaths = FSUtils.getAllDataFilesForMarkers(fs, basePath, instantTs, markerDir.toString(),\n-          baseFileExtension);\n-      List<String> validDataPaths = stats.stream().map(w -> String.format(\"%s/%s\", basePath, w.getPath()))\n-          .filter(p -> p.endsWith(baseFileExtension)).collect(Collectors.toList());\n+      // we are not including log appends here, since they are already fail-safe.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjA0Ng==", "bodyText": "during archival. either the commit instant or the corresponding rollback.. any left over marker dir will be deleted. or the archival will fail. there is a test added for this.", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456966046", "createdAt": "2020-07-19T22:57:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -264,6 +275,7 @@ public void archive(List<HoodieInstant> instants) throws HoodieCommitException {\n       List<IndexedRecord> records = new ArrayList<>();\n       for (HoodieInstant hoodieInstant : instants) {\n         try {\n+          deleteAnyLeftOverMarkerFiles(hoodieInstant);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjEwNA==", "bodyText": "@umehrot2 we can now add the parallelization changes here. for deletion of marker files.", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456966104", "createdAt": "2020-07-19T22:58:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/MarkerFiles.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operates on marker files for a given write action (commit, delta commit, compaction).\n+ */\n+public class MarkerFiles {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerFiles.class);\n+\n+  public static String stripMarkerSuffix(String path) {\n+    return path.substring(0, path.indexOf(HoodieTableMetaClient.MARKER_EXTN));\n+  }\n+\n+  private final String instantTime;\n+  private final FileSystem fs;\n+  private final Path markerDirPath;\n+  private final String basePath;\n+\n+  public MarkerFiles(FileSystem fs, String basePath, String markerFolderPath, String instantTime) {\n+    this.instantTime = instantTime;\n+    this.fs = fs;\n+    this.markerDirPath = new Path(markerFolderPath);\n+    this.basePath = basePath;\n+  }\n+\n+  public MarkerFiles(HoodieTable<?> table, String instantTime) {\n+    this(table.getMetaClient().getFs(),\n+        table.getMetaClient().getBasePath(),\n+        table.getMetaClient().getMarkerFolderPath(instantTime),\n+        instantTime);\n+  }\n+\n+  public void quietDeleteMarkerDir() {\n+    try {\n+      deleteMarkerDir();\n+    } catch (HoodieIOException ioe) {\n+      LOG.warn(\"Error deleting marker directory for instant \" + instantTime, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Delete Marker directory corresponding to an instant.\n+   */\n+  public boolean deleteMarkerDir() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjI3NQ==", "bodyText": "this is resilient already to attempting to delete an non-existent file.. marker file may not imply the data file is thre.", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456966275", "createdAt": "2020-07-19T23:00:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/MarkerBasedRollbackStrategy.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Performs rollback using marker files generated during the write..\n+ */\n+public class MarkerBasedRollbackStrategy implements BaseRollbackActionExecutor.RollbackStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerBasedRollbackStrategy.class);\n+\n+  private final HoodieTable<?> table;\n+\n+  private final transient JavaSparkContext jsc;\n+\n+  private final HoodieWriteConfig config;\n+\n+  private final String basePath;\n+\n+  private final String instantTime;\n+\n+  public MarkerBasedRollbackStrategy(HoodieTable<?> table, JavaSparkContext jsc, HoodieWriteConfig config, String instantTime) {\n+    this.table = table;\n+    this.jsc = jsc;\n+    this.basePath = table.getMetaClient().getBasePath();\n+    this.config = config;\n+    this.instantTime = instantTime;\n+  }\n+\n+  private HoodieRollbackStat undoMerge(String mergedBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the merged base file:\" + mergedBaseFilePath);\n+    return deleteBaseFile(mergedBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat undoCreate(String createdBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the created base file:\" + createdBaseFilePath);\n+    return deleteBaseFile(createdBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat deleteBaseFile(String baseFilePath) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjMzMg==", "bodyText": "checked that the log scanner can deal with spurious rollback blocks.. i.e rollbacks logged without any data blocks for that instant", "url": "https://github.com/apache/hudi/pull/1756#discussion_r456966332", "createdAt": "2020-07-19T23:00:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/MarkerBasedRollbackStrategy.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Performs rollback using marker files generated during the write..\n+ */\n+public class MarkerBasedRollbackStrategy implements BaseRollbackActionExecutor.RollbackStrategy {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerBasedRollbackStrategy.class);\n+\n+  private final HoodieTable<?> table;\n+\n+  private final transient JavaSparkContext jsc;\n+\n+  private final HoodieWriteConfig config;\n+\n+  private final String basePath;\n+\n+  private final String instantTime;\n+\n+  public MarkerBasedRollbackStrategy(HoodieTable<?> table, JavaSparkContext jsc, HoodieWriteConfig config, String instantTime) {\n+    this.table = table;\n+    this.jsc = jsc;\n+    this.basePath = table.getMetaClient().getBasePath();\n+    this.config = config;\n+    this.instantTime = instantTime;\n+  }\n+\n+  private HoodieRollbackStat undoMerge(String mergedBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the merged base file:\" + mergedBaseFilePath);\n+    return deleteBaseFile(mergedBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat undoCreate(String createdBaseFilePath) throws IOException {\n+    LOG.info(\"Rolling back by deleting the created base file:\" + createdBaseFilePath);\n+    return deleteBaseFile(createdBaseFilePath);\n+  }\n+\n+  private HoodieRollbackStat deleteBaseFile(String baseFilePath) throws IOException {\n+    Path fullDeletePath = new Path(basePath, baseFilePath);\n+    String partitionPath = FSUtils.getRelativePartitionPath(new Path(basePath), fullDeletePath.getParent());\n+    boolean isDeleted = table.getMetaClient().getFs().delete(fullDeletePath);\n+    return HoodieRollbackStat.newBuilder()\n+        .withPartitionPath(partitionPath)\n+        .withDeletedFileResult(baseFilePath, isDeleted)\n+        .build();\n+  }\n+\n+  private HoodieRollbackStat undoAppend(String appendBaseFilePath, HoodieInstant instantToRollback) throws IOException, InterruptedException {\n+    Path baseFilePathForAppend = new Path(basePath, appendBaseFilePath);\n+    String fileId = FSUtils.getFileIdFromFilePath(baseFilePathForAppend);\n+    String baseCommitTime = FSUtils.getCommitTime(baseFilePathForAppend.getName());\n+    String partitionPath = FSUtils.getRelativePartitionPath(new Path(basePath), new Path(basePath, appendBaseFilePath).getParent());\n+\n+    HoodieLogFormat.Writer writer = null;\n+    try {\n+      Path partitionFullPath = FSUtils.getPartitionPath(basePath, partitionPath);\n+\n+      if (!table.getMetaClient().getFs().exists(partitionFullPath)) {\n+        return HoodieRollbackStat.newBuilder()\n+            .withPartitionPath(partitionPath)\n+            .build();\n+      }\n+      writer = HoodieLogFormat.newWriterBuilder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 105}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "37c1febc23c88d27fccb42f7afc3acc6a4ee3d21", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/37c1febc23c88d27fccb42f7afc3acc6a4ee3d21", "committedDate": "2020-07-20T00:44:11Z", "message": "Fix compilation issue"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxMjY5NTc0", "url": "https://github.com/apache/hudi/pull/1756#pullrequestreview-451269574", "createdAt": "2020-07-20T03:02:37Z", "commit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwMzowMjozN1rOGz1Wpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwMzo0MzowNlrOGz2Q8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAwNDcxMQ==", "bodyText": "Ack.", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457004711", "createdAt": "2020-07-20T03:02:37Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -332,9 +333,12 @@ public static SparkConf registerClasses(SparkConf conf) {\n   }\n \n   @Override\n-  protected void postCommit(HoodieCommitMetadata metadata, String instantTime,\n-      Option<Map<String, String>> extraMetadata) {\n+  protected void postCommit(HoodieTable<?> table, HoodieCommitMetadata metadata, String instantTime, Option<Map<String, String>> extraMetadata) {\n     try {\n+\n+      // Delete the marker directory for the instant.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTcwOQ=="}, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAwNzA2NA==", "bodyText": "Is this needed since we already.have shouldRollbackUsingMarkers() ?", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457007064", "createdAt": "2020-07-20T03:09:04Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -632,6 +638,10 @@ public FileSystemViewStorageConfig getClientSpecifiedViewStorageConfig() {\n     return clientSpecifiedViewStorageConfig;\n   }\n \n+  public boolean getRollBackUsingMarkers() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37c1febc23c88d27fccb42f7afc3acc6a4ee3d21"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAxNjEyNA==", "bodyText": "This looks good to me.", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457016124", "createdAt": "2020-07-20T03:33:18Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -410,72 +412,54 @@ public void deleteMarkerDir(String instantTs) {\n    * @param consistencyCheckEnabled Consistency Check Enabled\n    * @throws HoodieIOException\n    */\n-  protected void cleanFailedWrites(JavaSparkContext jsc, String instantTs, List<HoodieWriteStat> stats,\n-      boolean consistencyCheckEnabled) throws HoodieIOException {\n+  protected void reconcileAgainstMarkers(JavaSparkContext jsc,\n+                                         String instantTs,\n+                                         List<HoodieWriteStat> stats,\n+                                         boolean consistencyCheckEnabled) throws HoodieIOException {\n     try {\n       // Reconcile marker and data files with WriteStats so that partially written data-files due to failed\n       // (but succeeded on retry) tasks are removed.\n       String basePath = getMetaClient().getBasePath();\n-      FileSystem fs = getMetaClient().getFs();\n-      Path markerDir = new Path(metaClient.getMarkerFolderPath(instantTs));\n+      MarkerFiles markers = new MarkerFiles(this, instantTs);\n \n-      if (!fs.exists(markerDir)) {\n-        // Happens when all writes are appends\n+      if (!markers.doesMarkerDirExist()) {\n+        // can happen if it was an empty write say.\n         return;\n       }\n \n-      final String baseFileExtension = getBaseFileFormat().getFileExtension();\n-      List<String> invalidDataPaths = FSUtils.getAllDataFilesForMarkers(fs, basePath, instantTs, markerDir.toString(),\n-          baseFileExtension);\n-      List<String> validDataPaths = stats.stream().map(w -> String.format(\"%s/%s\", basePath, w.getPath()))\n-          .filter(p -> p.endsWith(baseFileExtension)).collect(Collectors.toList());\n+      // we are not including log appends here, since they are already fail-safe.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NTkzOQ=="}, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAxNjg0Mg==", "bodyText": "Ack", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457016842", "createdAt": "2020-07-20T03:35:17Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -264,6 +275,7 @@ public void archive(List<HoodieInstant> instants) throws HoodieCommitException {\n       List<IndexedRecord> records = new ArrayList<>();\n       for (HoodieInstant hoodieInstant : instants) {\n         try {\n+          deleteAnyLeftOverMarkerFiles(hoodieInstant);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk2NjA0Ng=="}, "originalCommit": {"oid": "a0c4de4af17603e879e4bfdfc05846bdabecc6fa"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAxOTYzNQ==", "bodyText": "This is fine (backwards compatible) as we are not currently writing marker files for appends. right ?", "url": "https://github.com/apache/hudi/pull/1756#discussion_r457019635", "createdAt": "2020-07-20T03:43:06Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/MarkerFiles.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operates on marker files for a given write action (commit, delta commit, compaction).\n+ */\n+public class MarkerFiles {\n+\n+  private static final Logger LOG = LogManager.getLogger(MarkerFiles.class);\n+\n+  public static String stripMarkerSuffix(String path) {\n+    return path.substring(0, path.indexOf(HoodieTableMetaClient.MARKER_EXTN));\n+  }\n+\n+  private final String instantTime;\n+  private final FileSystem fs;\n+  private final Path markerDirPath;\n+  private final String basePath;\n+\n+  public MarkerFiles(FileSystem fs, String basePath, String markerFolderPath, String instantTime) {\n+    this.instantTime = instantTime;\n+    this.fs = fs;\n+    this.markerDirPath = new Path(markerFolderPath);\n+    this.basePath = basePath;\n+  }\n+\n+  public MarkerFiles(HoodieTable<?> table, String instantTime) {\n+    this(table.getMetaClient().getFs(),\n+        table.getMetaClient().getBasePath(),\n+        table.getMetaClient().getMarkerFolderPath(instantTime),\n+        instantTime);\n+  }\n+\n+  public void quietDeleteMarkerDir() {\n+    try {\n+      deleteMarkerDir();\n+    } catch (HoodieIOException ioe) {\n+      LOG.warn(\"Error deleting marker directory for instant \" + instantTime, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Delete Marker directory corresponding to an instant.\n+   */\n+  public boolean deleteMarkerDir() {\n+    try {\n+      boolean result = fs.delete(markerDirPath, true);\n+      if (result) {\n+        LOG.info(\"Removing marker directory at \" + markerDirPath);\n+      } else {\n+        LOG.info(\"No marker directory to delete at \" + markerDirPath);\n+      }\n+      return result;\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(ioe.getMessage(), ioe);\n+    }\n+  }\n+\n+  public boolean doesMarkerDirExist() throws IOException {\n+    return fs.exists(markerDirPath);\n+  }\n+\n+  public List<String> createdAndMergedDataPaths() throws IOException {\n+    List<String> dataFiles = new LinkedList<>();\n+    FSUtils.processFiles(fs, markerDirPath.toString(), (status) -> {\n+      String pathStr = status.getPath().toString();\n+      if (pathStr.contains(HoodieTableMetaClient.MARKER_EXTN) && !pathStr.endsWith(IOType.APPEND.name())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "37c1febc23c88d27fccb42f7afc3acc6a4ee3d21"}, "originalPosition": 100}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "37c1febc23c88d27fccb42f7afc3acc6a4ee3d21", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/37c1febc23c88d27fccb42f7afc3acc6a4ee3d21", "committedDate": "2020-07-20T00:44:11Z", "message": "Fix compilation issue"}, "afterCommit": {"oid": "3495237d4b41f5e9c6bbb79869e2fc9880833dd8", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/3495237d4b41f5e9c6bbb79869e2fc9880833dd8", "committedDate": "2020-07-20T07:51:12Z", "message": "Fix compilation issue & 1 unit test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3495237d4b41f5e9c6bbb79869e2fc9880833dd8", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/3495237d4b41f5e9c6bbb79869e2fc9880833dd8", "committedDate": "2020-07-20T07:51:12Z", "message": "Fix compilation issue & 1 unit test failure"}, "afterCommit": {"oid": "74df857678c8d3c92a18fb1e225fbe27be4da555", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/74df857678c8d3c92a18fb1e225fbe27be4da555", "committedDate": "2020-07-20T19:40:06Z", "message": "Fix compilation issue & 1 unit test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "74df857678c8d3c92a18fb1e225fbe27be4da555", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/74df857678c8d3c92a18fb1e225fbe27be4da555", "committedDate": "2020-07-20T19:40:06Z", "message": "Fix compilation issue & 1 unit test failure"}, "afterCommit": {"oid": "c6afcb1b90c733975ced4b3ac42eab2677ecc463", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/c6afcb1b90c733975ced4b3ac42eab2677ecc463", "committedDate": "2020-07-20T21:59:04Z", "message": "Fix compilation issue & 1 unit test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c6afcb1b90c733975ced4b3ac42eab2677ecc463", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/c6afcb1b90c733975ced4b3ac42eab2677ecc463", "committedDate": "2020-07-20T21:59:04Z", "message": "Fix compilation issue & 1 unit test failure"}, "afterCommit": {"oid": "6371ace3fdc1ecfe2edb5988c7593b78018c6fa3", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/6371ace3fdc1ecfe2edb5988c7593b78018c6fa3", "committedDate": "2020-07-20T22:13:32Z", "message": "Fix compilation issue & 1 unit test failure"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "551ce84f5de2f9a861940740adf2cfdca117310f", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/551ce84f5de2f9a861940740adf2cfdca117310f", "committedDate": "2020-07-20T23:34:18Z", "message": "Fix compilation issue & 1 unit test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6371ace3fdc1ecfe2edb5988c7593b78018c6fa3", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/6371ace3fdc1ecfe2edb5988c7593b78018c6fa3", "committedDate": "2020-07-20T22:13:32Z", "message": "Fix compilation issue & 1 unit test failure"}, "afterCommit": {"oid": "551ce84f5de2f9a861940740adf2cfdca117310f", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/551ce84f5de2f9a861940740adf2cfdca117310f", "committedDate": "2020-07-20T23:34:18Z", "message": "Fix compilation issue & 1 unit test failure"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7ab8f374f1ca32f1b006c4122e929d0fe417039", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/e7ab8f374f1ca32f1b006c4122e929d0fe417039", "committedDate": "2020-07-21T03:20:27Z", "message": "Fix small bug in test utils for casing test file correctly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f36b234eaec7c9e7f577f63f631c69fc1f5d6065", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/f36b234eaec7c9e7f577f63f631c69fc1f5d6065", "committedDate": "2020-07-21T04:14:02Z", "message": "Remove unused method on HoodieWriteConfig"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2841, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}