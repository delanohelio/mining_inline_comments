{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcwODEzMDQ5", "number": 1996, "title": "[BLOG] Async Compaction and Efficient Migration of large Parquet tables", "bodyText": "", "createdAt": "2020-08-20T10:21:40Z", "url": "https://github.com/apache/hudi/pull/1996", "merged": true, "mergeCommit": {"oid": "61305423b5b87b18ffd556e67f49ce8804a89dc8"}, "closed": true, "closedAt": "2020-09-01T19:27:18Z", "author": {"login": "bvaradar"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdBPACzgBqjM2ODE1NjEzNTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdEsgHXABqjM3MTY0Nzk5MDY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5eb7c64cb720d439332e2871a7df341fa789c7b8", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/5eb7c64cb720d439332e2871a7df341fa789c7b8", "committedDate": "2020-08-20T10:19:34Z", "message": "[BLOG] Efficient Migration of large Parquet tables"}, "afterCommit": {"oid": "729c0514a1dcc8771491eeb26dacb210b958a375", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/729c0514a1dcc8771491eeb26dacb210b958a375", "committedDate": "2020-08-22T01:19:31Z", "message": "[BLOG] Efficient Migration of large Parquet tables"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyOTI1Njgw", "url": "https://github.com/apache/hudi/pull/1996#pullrequestreview-472925680", "createdAt": "2020-08-22T13:19:18Z", "commit": {"oid": "3a100f3acecfd55e342b297a0755df52e710083e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMlQxMzoxOToxOFrOHFFLEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMlQxMzoxOToxOFrOHFFLEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTA4OTY4Mg==", "bodyText": "DeltaStreamer", "url": "https://github.com/apache/hudi/pull/1996#discussion_r475089682", "createdAt": "2020-08-22T13:19:18Z", "author": {"login": "leesf"}, "path": "docs/_posts/2020-08-21-async-compaction-deployment-model.md", "diffHunk": "@@ -0,0 +1,99 @@\n+---\n+title: \"Async Compaction Deployment Models\"\n+excerpt: \"Mechanisms for executing compaction jobs in Hudi asynchronously\"\n+author: vbalaji\n+category: blog\n+---\n+\n+We will look at different deployment models for executing compactions asynchronously.\n+\n+# Compaction\n+\n+For Merge-On-Read table, data is stored using a combination of columnar (e.g parquet) + row based (e.g avro) file formats. \n+Updates are logged to delta files & later compacted to produce new versions of columnar files synchronously or \n+asynchronously. One of th main motivations behind Merge-On-Read is to reduce data latency when ingesting records.\n+Hence, it makes sense to run compaction asynchronously without blocking ingestion.\n+\n+\n+# Async Compaction\n+\n+Async Compaction is performed in 2 steps:\n+\n+1. ***Compaction Scheduling***: This is done by the ingestion job. In this step, Hudi scans the partitions and selects **file \n+slices** to be compacted. A compaction plan is finally written to Hudi timeline.\n+1. ***Compaction Execution***: A separate process reads the compaction plan and performs compaction of file slices.\n+\n+  \n+# Deployment Models\n+\n+There are few ways by which we can execute compactions asynchronously. \n+\n+## Spark Structured Streaming\n+\n+With 0.6.0, we now have support for running async compactions in Spark \n+Structured Streaming jobs. Compactions are scheduled and executed asynchronously inside the \n+streaming job.  Async Compactions are enabled by default for structured streaming jobs\n+on Merge-On-Read table.\n+\n+Here is an example snippet in java\n+\n+```properties\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.HoodieDataSourceHelpers;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.spark.sql.streaming.OutputMode;\n+import org.apache.spark.sql.streaming.ProcessingTime;\n+\n+\n+ DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n+        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n+        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n+        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\")\n+        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\")\n+        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\")\n+        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, \"10\")\n+        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), \"true\")\n+        .option(HoodieWriteConfig.TABLE_NAME, tableName).option(\"checkpointLocation\", checkpointLocation)\n+        .outputMode(OutputMode.Append());\n+ writer.trigger(new ProcessingTime(30000)).start(tablePath);\n+```\n+\n+## DeltaStreaminer Continuous Mode", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a100f3acecfd55e342b297a0755df52e710083e"}, "originalPosition": 63}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3a100f3acecfd55e342b297a0755df52e710083e", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/3a100f3acecfd55e342b297a0755df52e710083e", "committedDate": "2020-08-22T10:19:49Z", "message": "[BLOG] Async Compaction Deployment Models"}, "afterCommit": {"oid": "fdc1eeb1c77442d1f8ffc4351e134979c2346ca8", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/fdc1eeb1c77442d1f8ffc4351e134979c2346ca8", "committedDate": "2020-08-24T00:02:47Z", "message": "[BLOG] Async Compaction Deployment Models"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8fadc62d88c9ea33f4c71c8bc2c0b86b70ac2163", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/8fadc62d88c9ea33f4c71c8bc2c0b86b70ac2163", "committedDate": "2020-09-01T19:23:31Z", "message": "[BLOG] Efficient Migration of large Parquet tables"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c693f173da922faf2491899a968023585d2d258c", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/c693f173da922faf2491899a968023585d2d258c", "committedDate": "2020-09-01T19:23:33Z", "message": "[BLOG] Async Compaction Deployment Models"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d3cfa1578f7efd8c969e265b6ef16129fd7ab061", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/d3cfa1578f7efd8c969e265b6ef16129fd7ab061", "committedDate": "2020-09-01T16:38:05Z", "message": "Merge branch 'asf-site' into blog"}, "afterCommit": {"oid": "c693f173da922faf2491899a968023585d2d258c", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/c693f173da922faf2491899a968023585d2d258c", "committedDate": "2020-09-01T19:23:33Z", "message": "[BLOG] Async Compaction Deployment Models"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4885, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}