{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc1MTQ3NDk4", "number": 1333, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjozNzowM1rODfw2bA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMzowMjowNVrODf4eXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMyODEyOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjozNzowM1rOFpqEcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzoyODozOFrOFtOwZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTIwMg==", "bodyText": "please point to quick start or some example for this", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225202", "createdAt": "2020-02-14T02:37:03Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjAwNQ==", "bodyText": "This is pointed in end of current paragraph already. (line 111)", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972005", "createdAt": "2020-02-23T07:28:38Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTIwMg=="}, "originalCommit": null, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMyOTgyOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjozODoyMlrOFpqFhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzozMjoyMFrOFtOxSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTQ3OQ==", "bodyText": "can we remove this line Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. .. you don't have to build it yourself per se..", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225479", "createdAt": "2020-02-14T02:38:22Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjIzMg==", "bodyText": "sure.", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972232", "createdAt": "2020-02-23T07:32:20Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTQ3OQ=="}, "originalCommit": null, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMzMDcxOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjozODo1NFrOFpqGAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzozMjoyN1rOFtOxSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTYwMQ==", "bodyText": "use --jars --packages", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225601", "createdAt": "2020-02-14T02:38:54Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjIzNQ==", "bodyText": "done.", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972235", "createdAt": "2020-02-23T07:32:27Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTYwMQ=="}, "originalCommit": null, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMzMjY4OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjo0MDozNFrOFpqHKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzo0Mjo1NFrOFtOzug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTg5OQ==", "bodyText": "For this section, can you take another stab.. it feels short and curt.. may be set some context on things like : Spark Datasources directly query underlying DFS data without Hive (and for this reason I think we should move SparkSQL up and place before this section)", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225899", "createdAt": "2020-02-14T02:40:34Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQyODc1OQ==", "bodyText": "+1", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379428759", "createdAt": "2020-02-14T13:27:43Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTg5OQ=="}, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3Mjg1OA==", "bodyText": "Agreed. I moved this section below Spark SQL. I need some help here with adding additional context though.", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972858", "createdAt": "2020-02-23T07:42:54Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTg5OQ=="}, "originalCommit": null, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMzMzYyOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjo0MToyMlrOFpqHtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzo0NDoxMVrOFtO0AQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjAzOA==", "bodyText": "lets also spend some time setting context and explaining how this uses Spark/Hive integration", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226038", "createdAt": "2020-02-14T02:41:22Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjkyOQ==", "bodyText": "Also need help here to add context on how Spark SQL integrates with Spark and Hive. Thanks!", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972929", "createdAt": "2020-02-23T07:44:11Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjAzOA=="}, "originalCommit": null, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMzNDA4OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjo0MTo0MlrOFpqH_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzozMjozN1rOFtOxUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjExMA==", "bodyText": "own parquet reader", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226110", "createdAt": "2020-02-14T02:41:42Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjI0MA==", "bodyText": "sure.", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972240", "createdAt": "2020-02-23T07:32:37Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjExMA=="}, "originalCommit": null, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMzNDY0OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjo0MjoxNFrOFpqIXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzoxOTo0MFrOFtOuFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjIwNQ==", "bodyText": "By default : are you talking about copy_on_write tables?", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226205", "createdAt": "2020-02-14T02:42:14Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MTQxNA==", "bodyText": "No. I meant Spark's way of handling things when I meant by default. I dint not refer to COPY_ON_Write table when I used 'by default'. If it causes ambiguity, we can rephrase it. let me know.", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382971414", "createdAt": "2020-02-23T07:19:40Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjIwNQ=="}, "originalCommit": null, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMzNTk2OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjo0MzowNFrOFpqJIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzozMjo0NlrOFtOxWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjQwMw==", "bodyText": "turning off convertMetastoreParquet please usethe exact and full config here within ``", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226403", "createdAt": "2020-02-14T02:43:04Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. \n+However, for MERGE_ON_READ tables which has both parquet and avro data, this default setting needs to be turned off using set `spark.sql.hive.convertMetastoreParquet=false`. \n+This will force Spark to fallback to using the Hive Serde to read the data (planning/executions is still Spark). \n \n ```java\n-Dataset<Row> hoodieROViewDF = spark.read().format(\"org.apache.hudi\")\n-// pass any path glob, can include hudi & non-hudi tables\n-.load(\"/glob/path/pattern\");\n+$ spark-shell --driver-class-path /etc/hive/conf  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.4 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g  --master yarn-client\n+\n+scala> sqlContext.sql(\"select count(*) from hudi_trips_mor_rt where datestr = '2016-10-02'\").show()\n+scala> sqlContext.sql(\"select count(*) from hudi_trips_mor_rt where datestr = '2016-10-02'\").show()\n ```\n- \n-### Snapshot query {#spark-snapshot-query}\n-Currently, near-real time data can only be queried as a Hive table in Spark using snapshot query mode. In order to do this, set `spark.sql.hive.convertMetastoreParquet=false`, forcing Spark to fallback \n-to using the Hive Serde to read the data (planning/executions is still Spark). \n \n-```java\n-$ spark-shell --jars hudi-spark-bundle_2.11-x.y.z-SNAPSHOT.jar --driver-class-path /etc/hive/conf  --packages org.apache.spark:spark-avro_2.11:2.4.4 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g  --master yarn-client\n+For COPY_ON_WRITE tables, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjI0OQ==", "bodyText": "okay sure.", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972249", "createdAt": "2020-02-23T07:32:46Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. \n+However, for MERGE_ON_READ tables which has both parquet and avro data, this default setting needs to be turned off using set `spark.sql.hive.convertMetastoreParquet=false`. \n+This will force Spark to fallback to using the Hive Serde to read the data (planning/executions is still Spark). \n \n ```java\n-Dataset<Row> hoodieROViewDF = spark.read().format(\"org.apache.hudi\")\n-// pass any path glob, can include hudi & non-hudi tables\n-.load(\"/glob/path/pattern\");\n+$ spark-shell --driver-class-path /etc/hive/conf  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.4 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g  --master yarn-client\n+\n+scala> sqlContext.sql(\"select count(*) from hudi_trips_mor_rt where datestr = '2016-10-02'\").show()\n+scala> sqlContext.sql(\"select count(*) from hudi_trips_mor_rt where datestr = '2016-10-02'\").show()\n ```\n- \n-### Snapshot query {#spark-snapshot-query}\n-Currently, near-real time data can only be queried as a Hive table in Spark using snapshot query mode. In order to do this, set `spark.sql.hive.convertMetastoreParquet=false`, forcing Spark to fallback \n-to using the Hive Serde to read the data (planning/executions is still Spark). \n \n-```java\n-$ spark-shell --jars hudi-spark-bundle_2.11-x.y.z-SNAPSHOT.jar --driver-class-path /etc/hive/conf  --packages org.apache.spark:spark-avro_2.11:2.4.4 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g  --master yarn-client\n+For COPY_ON_WRITE tables, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjQwMw=="}, "originalCommit": null, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMzNjk3OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjo0Mzo1MFrOFpqJuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzozMjo1MFrOFtOxXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjU1Mg==", "bodyText": "Remove this section ?", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226552", "createdAt": "2020-02-14T02:43:50Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -145,8 +161,13 @@ Additionally, `HoodieReadClient` offers the following functionality using Hudi's\n | filterExists() | Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication |\n | checkExists(keys) | Check if the provided keys exist in a Hudi table |\n \n+### Read optimized query\n+\n+For read optimized queries, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjI1NA==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972254", "createdAt": "2020-02-23T07:32:50Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -145,8 +161,13 @@ Additionally, `HoodieReadClient` offers the following functionality using Hudi's\n | filterExists() | Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication |\n | checkExists(keys) | Check if the provided keys exist in a Hudi table |\n \n+### Read optimized query\n+\n+For read optimized queries, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjU1Mg=="}, "originalCommit": null, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NjMzNzQ5OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQwMjo0NDoxMlrOFpqKAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzozMjo1M1rOFtOxYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjYyNA==", "bodyText": "COPY_ON_WRITE: typo", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226624", "createdAt": "2020-02-14T02:44:12Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -145,8 +161,13 @@ Additionally, `HoodieReadClient` offers the following functionality using Hudi's\n | filterExists() | Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication |\n | checkExists(keys) | Check if the provided keys exist in a Hudi table |\n \n+### Read optimized query\n+\n+For read optimized queries, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. \n+If using spark's built in support, additionally a path filter needs to be pushed into sparkContext as described earlier.\n \n ## Presto\n \n-Presto is a popular query engine, providing interactive query performance. Presto currently supports only read optimized queries on Hudi tables. \n-This requires the `hudi-presto-bundle` jar to be placed into `<presto_install>/plugin/hive-hadoop2/`, across the installation.\n+Presto is a popular query engine, providing interactive query performance. Presto currently supports snapshot queries on\n+COPY_On_WRITE and read optimized queries on MERGE_ON_READ Hudi tables. This requires the `hudi-presto-bundle` jar ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjI1Nw==", "bodyText": "will fix", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972257", "createdAt": "2020-02-23T07:32:53Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -145,8 +161,13 @@ Additionally, `HoodieReadClient` offers the following functionality using Hudi's\n | filterExists() | Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication |\n | checkExists(keys) | Check if the provided keys exist in a Hudi table |\n \n+### Read optimized query\n+\n+For read optimized queries, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. \n+If using spark's built in support, additionally a path filter needs to be pushed into sparkContext as described earlier.\n \n ## Presto\n \n-Presto is a popular query engine, providing interactive query performance. Presto currently supports only read optimized queries on Hudi tables. \n-This requires the `hudi-presto-bundle` jar to be placed into `<presto_install>/plugin/hive-hadoop2/`, across the installation.\n+Presto is a popular query engine, providing interactive query performance. Presto currently supports snapshot queries on\n+COPY_On_WRITE and read optimized queries on MERGE_ON_READ Hudi tables. This requires the `hudi-presto-bundle` jar ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjYyNA=="}, "originalCommit": null, "originalPosition": 197}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NzU3NzI3OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxMzowMjowNVrOFp13yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yM1QwNzoxMDoyN1rOFtOsCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxODU2OA==", "bodyText": "should we also mention the impala?", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379418568", "createdAt": "2020-02-14T13:02:05Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -9,7 +9,7 @@ last_modified_at: 2019-12-30T15:59:57-04:00\n \n Conceptually, Hudi stores data physically once on DFS, while providing 3 different ways of querying, as explained [before](/docs/concepts.html#query-types). \n Once the table is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n-bundle has been provided, the table can be queried by popular query engines like Hive, Spark and Presto.\n+bundle has been provided, the table can be queried by popular query engines like Hive, Spark datasource, Spark SQL and Presto.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTY0NjQ1OA==", "bodyText": "it's not released yet though..but we should file a ticket for doc-ing that nonetheless, when a impala release does happen", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379646458", "createdAt": "2020-02-14T21:12:41Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -9,7 +9,7 @@ last_modified_at: 2019-12-30T15:59:57-04:00\n \n Conceptually, Hudi stores data physically once on DFS, while providing 3 different ways of querying, as explained [before](/docs/concepts.html#query-types). \n Once the table is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n-bundle has been provided, the table can be queried by popular query engines like Hive, Spark and Presto.\n+bundle has been provided, the table can be queried by popular query engines like Hive, Spark datasource, Spark SQL and Presto.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxODU2OA=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MDg5MA==", "bodyText": "+1 Created https://issues.apache.org/jira/browse/HUDI-630", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382970890", "createdAt": "2020-02-23T07:10:27Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -9,7 +9,7 @@ last_modified_at: 2019-12-30T15:59:57-04:00\n \n Conceptually, Hudi stores data physically once on DFS, while providing 3 different ways of querying, as explained [before](/docs/concepts.html#query-types). \n Once the table is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n-bundle has been provided, the table can be queried by popular query engines like Hive, Spark and Presto.\n+bundle has been provided, the table can be queried by popular query engines like Hive, Spark datasource, Spark SQL and Presto.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxODU2OA=="}, "originalCommit": null, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4831, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}