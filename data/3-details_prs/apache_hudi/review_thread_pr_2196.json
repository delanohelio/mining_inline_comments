{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA3OTcwNDU2", "number": 2196, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzo1Mjo0MlrOEw4USw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwMjoxNToxMlrOE_a5Qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5Njg5ODAzOnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNzo1Mjo0MlrOHmtVrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNjowMzozNFrOHrVyDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM1MDc2Ng==", "bodyText": "I think this won't work. Insert overwrite only updates partitions that have records in the dataframe. Other partitions continue to have old data.\nBut, I like the idea. Maybe we can add additional configuration to insert_overwrite to mark old partitions as 'deleted'? This can be done in a way to support https://issues.apache.org/jira/browse/HUDI-1350.", "url": "https://github.com/apache/hudi/pull/2196#discussion_r510350766", "createdAt": "2020-10-22T17:52:42Z", "author": {"login": "satishkotha"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -93,6 +93,11 @@ private[hudi] object HoodieSparkSqlWriter {\n       operation = WriteOperationType.INSERT\n     }\n \n+    // If the mode is Overwrite, should use INSERT_OVERWRITE operation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc15e303e7c40fcff8f560e19183089766da43e6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDgzODk3OQ==", "bodyText": "@satishkotha  thanks,\n\ni think the method is ok . Because in HoodieSparkSqlWriter.scala , if we set  operation as replace action, then it also use  \"client.insertOverwrite(hoodieRecords, instantTime); \"  here will do as \" Insert overwrite only updates partitions\".\n\npublic static HoodieWriteResult doWriteOperation(SparkRDDWriteClient client, JavaRDD hoodieRecords,\nString instantTime, WriteOperationType operation) throws HoodieException {\nswitch (operation) {\ncase BULK_INSERT:\nOption userDefinedBulkInsertPartitioner =\ncreateUserDefinedBulkInsertPartitioner(client.getConfig());\nreturn new HoodieWriteResult(client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner));\ncase INSERT:\nreturn new HoodieWriteResult(client.insert(hoodieRecords, instantTime));\ncase UPSERT:\nreturn new HoodieWriteResult(client.upsert(hoodieRecords, instantTime));\ncase INSERT_OVERWRITE:\nreturn client.insertOverwrite(hoodieRecords, instantTime);\ndefault:\n\nhttps://issues.apache.org/jira/browse/HUDI-1350. is another job.", "url": "https://github.com/apache/hudi/pull/2196#discussion_r510838979", "createdAt": "2020-10-23T12:10:53Z", "author": {"login": "lw309637554"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -93,6 +93,11 @@ private[hudi] object HoodieSparkSqlWriter {\n       operation = WriteOperationType.INSERT\n     }\n \n+    // If the mode is Overwrite, should use INSERT_OVERWRITE operation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM1MDc2Ng=="}, "originalCommit": {"oid": "fc15e303e7c40fcff8f560e19183089766da43e6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MjY2NA==", "bodyText": "@lw309637554  can you add below test:\nstep1: Write N records to hoodie table for partition1\nstep2: Write N more records using SaveMode.Overwrite for partition2\nstep3: Query for all the rows from hoodie table. We should only see N records for partition2.\nIf i understand your code correctly, with this change, we will see 2N records", "url": "https://github.com/apache/hudi/pull/2196#discussion_r513672664", "createdAt": "2020-10-28T18:30:10Z", "author": {"login": "satishkotha"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -93,6 +93,11 @@ private[hudi] object HoodieSparkSqlWriter {\n       operation = WriteOperationType.INSERT\n     }\n \n+    // If the mode is Overwrite, should use INSERT_OVERWRITE operation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM1MDc2Ng=="}, "originalCommit": {"oid": "fc15e303e7c40fcff8f560e19183089766da43e6"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNzY5Mg==", "bodyText": "@satishkotha thanks, have add the unit test\nand it is okay", "url": "https://github.com/apache/hudi/pull/2196#discussion_r515207692", "createdAt": "2020-10-30T16:03:34Z", "author": {"login": "lw309637554"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -93,6 +93,11 @@ private[hudi] object HoodieSparkSqlWriter {\n       operation = WriteOperationType.INSERT\n     }\n \n+    // If the mode is Overwrite, should use INSERT_OVERWRITE operation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM1MDc2Ng=="}, "originalCommit": {"oid": "fc15e303e7c40fcff8f560e19183089766da43e6"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NDU1MDkzOnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMjo0NTo1MVrOHtsbOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNjoxNDo0NVrOHuK58A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzY3NTgzNQ==", "bodyText": "tmpTable only registered inputDF2, so you will not get data for partition1 even if we do SaveMode.Append in line 206? Don't you need to read back all data from table? Can you please fix test setup?", "url": "https://github.com/apache/hudi/pull/2196#discussion_r517675835", "createdAt": "2020-11-04T22:45:51Z", "author": {"login": "satishkotha"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -156,6 +162,76 @@ class TestCOWDataSource extends HoodieClientTestBase {\n     assertEquals(100, timeTravelDF.count()) // 100 initial inserts must be pulled\n   }\n \n+  @Test def testOverWriteModeUseReplaceAction(): Unit = {\n+    val records1 = recordsToStrings(dataGen.generateInserts(\"001\", 5)).toList\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(records1, 2))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Append)\n+      .save(basePath)\n+\n+    val records2 = recordsToStrings(dataGen.generateInserts(\"002\", 5)).toList\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+    inputDF2.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+\n+    val metaClient = new HoodieTableMetaClient(spark.sparkContext.hadoopConfiguration, basePath, true)\n+    val commits =  metaClient.getActiveTimeline.filterCompletedInstants().getInstants.toArray\n+      .map(instant => (instant.asInstanceOf[HoodieInstant]).getAction)\n+    assertEquals(2, commits.size)\n+    assertEquals(\"commit\", commits(0))\n+    assertEquals(\"replacecommit\", commits(1))\n+  }\n+\n+  @Test def testOverWriteModeUseReplaceActionOnDisJointPartitions(): Unit = {\n+    // step1: Write 5 records to hoodie table for partition1 DEFAULT_FIRST_PARTITION_PATH\n+    val records1 = recordsToStrings(dataGen.generateInsertsForPartition(\"001\", 5, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(records1, 2))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Append)\n+      .save(basePath)\n+\n+    // step2: Write 7 more records using SaveMode.Overwrite for partition2 DEFAULT_SECOND_PARTITION_PATH\n+    val records2 = recordsToStrings(dataGen.generateInsertsForPartition(\"002\", 7, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH)).toList\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+    inputDF2.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    inputDF2.registerTempTable(\"tmpTable\")\n+\n+    // step3: Query the rows count from hoodie table  for partition1 DEFAULT_FIRST_PARTITION_PATH\n+    val recordCountForParititon1 =  spark.sql(String.format(\"select count(*) from tmpTable where partition = '%s'\", HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).collect()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17bfea4716c4ece0b45c8867f848bc937939cd1c"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE3NTIxNg==", "bodyText": "will fix it", "url": "https://github.com/apache/hudi/pull/2196#discussion_r518175216", "createdAt": "2020-11-05T16:14:45Z", "author": {"login": "lw309637554"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -156,6 +162,76 @@ class TestCOWDataSource extends HoodieClientTestBase {\n     assertEquals(100, timeTravelDF.count()) // 100 initial inserts must be pulled\n   }\n \n+  @Test def testOverWriteModeUseReplaceAction(): Unit = {\n+    val records1 = recordsToStrings(dataGen.generateInserts(\"001\", 5)).toList\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(records1, 2))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Append)\n+      .save(basePath)\n+\n+    val records2 = recordsToStrings(dataGen.generateInserts(\"002\", 5)).toList\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+    inputDF2.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+\n+    val metaClient = new HoodieTableMetaClient(spark.sparkContext.hadoopConfiguration, basePath, true)\n+    val commits =  metaClient.getActiveTimeline.filterCompletedInstants().getInstants.toArray\n+      .map(instant => (instant.asInstanceOf[HoodieInstant]).getAction)\n+    assertEquals(2, commits.size)\n+    assertEquals(\"commit\", commits(0))\n+    assertEquals(\"replacecommit\", commits(1))\n+  }\n+\n+  @Test def testOverWriteModeUseReplaceActionOnDisJointPartitions(): Unit = {\n+    // step1: Write 5 records to hoodie table for partition1 DEFAULT_FIRST_PARTITION_PATH\n+    val records1 = recordsToStrings(dataGen.generateInsertsForPartition(\"001\", 5, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(records1, 2))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Append)\n+      .save(basePath)\n+\n+    // step2: Write 7 more records using SaveMode.Overwrite for partition2 DEFAULT_SECOND_PARTITION_PATH\n+    val records2 = recordsToStrings(dataGen.generateInsertsForPartition(\"002\", 7, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH)).toList\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+    inputDF2.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    inputDF2.registerTempTable(\"tmpTable\")\n+\n+    // step3: Query the rows count from hoodie table  for partition1 DEFAULT_FIRST_PARTITION_PATH\n+    val recordCountForParititon1 =  spark.sql(String.format(\"select count(*) from tmpTable where partition = '%s'\", HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).collect()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzY3NTgzNQ=="}, "originalCommit": {"oid": "17bfea4716c4ece0b45c8867f848bc937939cd1c"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NDU5MDQ2OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMzowMDoyOFrOHtsyAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMzowMDoyOFrOHtsyAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzY4MTY2Ng==", "bodyText": "I ran a similar test using quick start setup guide:\n\ningest 10 records for partition \"2020/03/11\"\n`scala> val inserts = convertToStringList(dataGen.generateInserts(10))\ninserts: java.util.List[String] = [{\"ts\": 0, \"uuid\": \"299d5202-1ea0-4918-9d2f-2365bc1c2402\", \"rider\": \"rider-213\", \"driver\": \"driver-213\", \"begin_lat\": 0.4726905879569653, \"begin_lon\": 0.46157858450465483, \"end_lat\": 0.754803407008858, \"end_lon\": 0.9671159942018241, \"fare\": 34.158284716382845, \"partitionpath\": \"2020/03/11\"}, {\"ts\": 0, \"uuid\": \"0fc23a14-c815-4b09-bff1-c6193a6de5b7\", \"rider\": \"rider-213\", \"driver\": \"driver-213\", \"begin_lat\": 0.6100070562136587, \"begin_lon\": 0.8779402295427752, \"end_lat\": 0.3407870505929602, \"end_lon\": 0.5030798142293655, \"fare\": 43.4923811219014, \"partitionpath\": \"2020/03/11\"}, {\"ts\": 0, \"uuid\": \"7136e8f8-ed82-4fc4-b60d-f7367f7be791\", \"rider\": \"rider-213\", \"driver\": \"driver-213\", \"begin_lat\": 0.5731835407930634, \"begin_lon\": 0.4923479652912024, \"end_lat\":...\nscala> val df = spark.read.json(spark.sparkContext.parallelize(inserts, 1))\nwarning: there was one deprecation warning; re-run with -deprecation for details\ndf: org.apache.spark.sql.DataFrame = [begin_lat: double, begin_lon: double ... 8 more fields]\n\nscala> df.write.format(\"org.apache.hudi\").\n|     options(getQuickstartWriteConfigs).\n|     option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n|     option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n|     option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n|     option(TABLE_NAME, tableName).\n|     mode(Overwrite).\n|     save(basePath);`\n\nQuery the data back and see that records are written correctly\n`scala> val tripsSnapshotDF = spark.\n|     read.\n|     format(\"org.apache.hudi\").\n|     load(basePath + \"////\")\n20/11/04 14:51:53 WARN DefaultSource: Loading Base File Only View.\ntripsSnapshotDF: org.apache.spark.sql.DataFrame = [_hoodie_commit_time: string, _hoodie_commit_seqno: string ... 13 more fields]\n\nscala> tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\")\nscala>\nscala> spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show()\n+-------------------+--------------------+----------------------+---------+----------+------------------+\n|_hoodie_commit_time|  _hoodie_record_key|_hoodie_partition_path|    rider|    driver|              fare|\n+-------------------+--------------------+----------------------+---------+----------+------------------+\n|     20201104145141|299d5202-1ea0-491...|            2020/03/11|rider-213|driver-213|34.158284716382845|\n|     20201104145141|0fc23a14-c815-4b0...|            2020/03/11|rider-213|driver-213|  43.4923811219014|\n|     20201104145141|7136e8f8-ed82-4fc...|            2020/03/11|rider-213|driver-213| 64.27696295884016|\n|     20201104145141|5ffa488e-d75e-4ef...|            2020/03/11|rider-213|driver-213| 93.56018115236618|\n|     20201104145141|cf09166f-dc3f-45e...|            2020/03/11|rider-213|driver-213|17.851135255091155|\n|     20201104145141|6f522490-e29e-419...|            2020/03/11|rider-213|driver-213|19.179139106643607|\n|     20201104145141|db97e3ef-ad7a-4e8...|            2020/03/11|rider-213|driver-213| 33.92216483948643|\n|     20201104145141|a42d7c22-d0bf-4b9...|            2020/03/11|rider-213|driver-213| 66.62084366450246|\n|     20201104145141|94154d3e-c3da-436...|            2020/03/11|rider-213|driver-213| 41.06290929046368|\n|     20201104145141|618b3f38-bb71-402...|            2020/03/11|rider-213|driver-213| 27.79478688582596|\n+-------------------+--------------------+----------------------+---------+----------+------------------+\n`\n\nUse insert_overwrite and write to new partition (This is with master, not using your change. So I still have insert_overwrite operation with Append mode)\n`scala>  val dataGen = new DataGenerator(Array(\"2020/09/11\"))\ndataGen: org.apache.hudi.QuickstartUtils.DataGenerator = org.apache.hudi.QuickstartUtils$DataGenerator@f00b18a\n\nscala> val inserts2 = convertToStringList(dataGen.generateInserts(1))\nscala> val df = spark.read.json(spark.sparkContext.parallelize(inserts2, 1))\nscala> df.write.format(\"org.apache.hudi\").\n|     options(getQuickstartWriteConfigs).\n|     option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n|     option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n|     option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n|     option(TABLE_NAME, tableName).\n|     mode(Append).\n| option(OPERATION_OPT_KEY, \"insert_overwrite\").\n|      save(basePath);4. Query the data backscala> spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show()\n+-------------------+--------------------+----------------------+---------+----------+------------------+\n|_hoodie_commit_time|  _hoodie_record_key|_hoodie_partition_path|    rider|    driver|              fare|\n+-------------------+--------------------+----------------------+---------+----------+------------------+\n|     20201104145258|299d5202-1ea0-491...|            2020/03/11|rider-213|driver-213|34.158284716382845|\n|     20201104145258|0fc23a14-c815-4b0...|            2020/03/11|rider-213|driver-213|  43.4923811219014|\n|     20201104145258|7136e8f8-ed82-4fc...|            2020/03/11|rider-213|driver-213| 64.27696295884016|\n|     20201104145258|5ffa488e-d75e-4ef...|            2020/03/11|rider-213|driver-213| 93.56018115236618|\n|     20201104145258|cf09166f-dc3f-45e...|            2020/03/11|rider-213|driver-213|17.851135255091155|\n|     20201104145258|6f522490-e29e-419...|            2020/03/11|rider-213|driver-213|19.179139106643607|\n|     20201104145258|db97e3ef-ad7a-4e8...|            2020/03/11|rider-213|driver-213| 33.92216483948643|\n|     20201104145258|a42d7c22-d0bf-4b9...|            2020/03/11|rider-213|driver-213| 66.62084366450246|\n|     20201104145258|94154d3e-c3da-436...|            2020/03/11|rider-213|driver-213| 41.06290929046368|\n|     20201104145258|618b3f38-bb71-402...|            2020/03/11|rider-213|driver-213| 27.79478688582596|\n|     20201104145348|4dac8aa3-b8fa-410...|            2020/09/11|rider-284|driver-284|49.527694252432056|\n+-------------------+--------------------+----------------------+---------+----------+------------------+\n`\nAs you can see in step4, we see all 11 records. With 'SaveMode.Overwrite' we should only see 1 record. Hope this is clear.", "url": "https://github.com/apache/hudi/pull/2196#discussion_r517681666", "createdAt": "2020-11-04T23:00:28Z", "author": {"login": "satishkotha"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -156,6 +162,76 @@ class TestCOWDataSource extends HoodieClientTestBase {\n     assertEquals(100, timeTravelDF.count()) // 100 initial inserts must be pulled\n   }\n \n+  @Test def testOverWriteModeUseReplaceAction(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17bfea4716c4ece0b45c8867f848bc937939cd1c"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDYyNDkwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMToyNTozM1rOH2CyiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMToyNTozM1rOH2CyiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQzMDg1Nw==", "bodyText": "This can be time consuming for large tables. Is it possible to parallelize this?\n\nget all partitions and then\nenginecontext.parallelize(#paritions)\nfor each partition, get all latest file ids\n\nI think we dont need to add new filesystem apis with this approach", "url": "https://github.com/apache/hudi/pull/2196#discussion_r526430857", "createdAt": "2020-11-18T21:25:33Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  public SparkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,\n+                                                       HoodieWriteConfig config, HoodieTable table,\n+                                                       String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(context, config, table, instantTime, inputRecordsRDD, WriteOperationType.INSERT_OVERWRITE_TABLE);\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(JavaRDD<WriteStatus> writeStatuses) {\n+    Map<String, List<String>> result =  table.getSliceView().getLatestFileSlices().distinct()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "305f3208abf34f57c47c5795da7ebb49ad66f430"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0OTM2Mzg2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwMjoxNToxMlrOH9HnPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxNTo1MjozN1rOH9hqoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg0OTkxNw==", "bodyText": "Can we use function style here instead of needing to case to PairFunction ? Something like partitionPathRdd.mapToPair( () -> ...) ?", "url": "https://github.com/apache/hudi/pull/2196#discussion_r533849917", "createdAt": "2020-12-02T02:15:12Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFunction;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  public SparkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,\n+                                                       HoodieWriteConfig config, HoodieTable table,\n+                                                       String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(context, config, table, instantTime, inputRecordsRDD, WriteOperationType.INSERT_OVERWRITE_TABLE);\n+  }\n+\n+  protected List<String> getAllExistingFileIds(String partitionPath) {\n+    return table.getSliceView().getLatestFileSlices(partitionPath)\n+        .map(fg -> fg.getFileId()).distinct().collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(JavaRDD<WriteStatus> writeStatuses) {\n+    Map<String, List<String>> partitionToExistingFileIds = new HashMap<>();\n+    try {\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(table.getMetaClient().getFs(),\n+          table.getMetaClient().getBasePath(), false);\n+      JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+      if (partitionPaths != null && partitionPaths.size() > 0) {\n+        context.setJobStatus(this.getClass().getSimpleName(), \"Getting ExistingFileIds of all partitions\");\n+        JavaRDD<String> partitionPathRdd = jsc.parallelize(partitionPaths, partitionPaths.size());\n+        partitionToExistingFileIds = partitionPathRdd.mapToPair((PairFunction<String, String, List<String>>)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0da0ca747eb297f21e3e51cbd571cc8a680b18b0"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI3Njc2OQ==", "bodyText": "ok", "url": "https://github.com/apache/hudi/pull/2196#discussion_r534276769", "createdAt": "2020-12-02T15:52:37Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkInsertOverwriteTableCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFunction;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  public SparkInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,\n+                                                       HoodieWriteConfig config, HoodieTable table,\n+                                                       String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(context, config, table, instantTime, inputRecordsRDD, WriteOperationType.INSERT_OVERWRITE_TABLE);\n+  }\n+\n+  protected List<String> getAllExistingFileIds(String partitionPath) {\n+    return table.getSliceView().getLatestFileSlices(partitionPath)\n+        .map(fg -> fg.getFileId()).distinct().collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(JavaRDD<WriteStatus> writeStatuses) {\n+    Map<String, List<String>> partitionToExistingFileIds = new HashMap<>();\n+    try {\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(table.getMetaClient().getFs(),\n+          table.getMetaClient().getBasePath(), false);\n+      JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+      if (partitionPaths != null && partitionPaths.size() > 0) {\n+        context.setJobStatus(this.getClass().getSimpleName(), \"Getting ExistingFileIds of all partitions\");\n+        JavaRDD<String> partitionPathRdd = jsc.parallelize(partitionPaths, partitionPaths.size());\n+        partitionToExistingFileIds = partitionPathRdd.mapToPair((PairFunction<String, String, List<String>>)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg0OTkxNw=="}, "originalCommit": {"oid": "0da0ca747eb297f21e3e51cbd571cc8a680b18b0"}, "originalPosition": 66}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4118, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}