{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEzMjU4NDU3", "number": 1594, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo1OTowM1rOD5cWYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNTowMDoxMlrOD5cYVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNTU5OTA2OnYy", "diffSide": "RIGHT", "path": "docs/_posts/2016-12-30-strata-talk-2017.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo1OTowM1rOGQtkoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo1OTowM1rOGQtkoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3NzA1Ng==", "bodyText": "nice touch :)", "url": "https://github.com/apache/hudi/pull/1594#discussion_r420177056", "createdAt": "2020-05-05T14:59:03Z", "author": {"login": "vinothchandar"}, "path": "docs/_posts/2016-12-30-strata-talk-2017.md", "diffHunk": "@@ -1,8 +1,7 @@\n ---\n title:  \"Connect with us at Strata San Jose March 2017\"\n+author: admin", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2015414922f99f770c64b9b38363066be8109c2"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxNTYwNDA3OnYy", "diffSide": "RIGHT", "path": "docs/_posts/2020-01-15-delete-support-in-hudi.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNTowMDoxMlrOGQtn9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNTowMjo0NlrOGQtvWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3NzkwOQ==", "bodyText": "could we get code markup for the code blocks?", "url": "https://github.com/apache/hudi/pull/1594#discussion_r420177909", "createdAt": "2020-05-05T15:00:12Z", "author": {"login": "vinothchandar"}, "path": "docs/_posts/2020-01-15-delete-support-in-hudi.md", "diffHunk": "@@ -0,0 +1,156 @@\n+---\n+title: \"Delete support in Hudi\"\n+excerpt: \"Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \u201chow to\u201d blog on how to delete records in hudi.\"\n+author: shivnarayan\n+---\n+\n+Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \"how to\" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.\n+\n+### Delete using RDD Level APIs\n+\n+If you have embedded  _HoodieWriteClient_ , then deletion is as simple as passing in a  _JavaRDD<HoodieKey>_ to the delete api.\n+\n+    // Fetch list of HoodieKeys from elsewhere that needs to be deleted\n+    // convert to JavaRDD if required. JavaRDD<HoodieKey> toBeDeletedKeys\n+    List<WriteStatus> statuses = writeClient.delete(toBeDeletedKeys, commitTime);\n+\n+### Deletion with Datasource\n+\n+Now we will walk through an example of how to perform deletes on a sample dataset using the Datasource API. Quick Start has the same example as below. Feel free to check it out.\n+\n+**Step 1** : Launch spark shell\n+\n+    bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle:0.5.1-incubating \\\n+        --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n+\n+**Step 2** : Import as required and set up table name, etc for sample dataset\n+\n+    import org.apache.hudi.QuickstartUtils._\n+    import scala.collection.JavaConversions._\n+    import org.apache.spark.sql.SaveMode._\n+    import org.apache.hudi.DataSourceReadOptions._\n+    import org.apache.hudi.DataSourceWriteOptions._\n+    import org.apache.hudi.config.HoodieWriteConfig._\n+     \n+    val tableName = \"hudi_cow_table\"\n+    val basePath = \"file:///tmp/hudi_cow_table\"\n+    val dataGen = new DataGenerator\n+\n+**Step 3** : Insert data. Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi dataset as below.\n+\n+    val inserts = convertToStringList(dataGen.generateInserts(10))\n+    val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+    df.write.format(\"org.apache.hudi\").\n+        options(getQuickstartWriteConfigs).\n+        option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+        option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+        option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+        option(TABLE_NAME, tableName).\n+        mode(Overwrite).\n+        save(basePath);\n+\n+**Step 4** : Query data. Load the data files into a DataFrame.\n+\n+    val roViewDF = spark.\n+        read.\n+        format(\"org.apache.hudi\").\n+        load(basePath + \"/*/*/*/*\")\n+    roViewDF.createOrReplaceTempView(\"hudi_ro_table\")\n+    spark.sql(\"select count(*) from hudi_ro_table\").show() // should return 10 (number of records inserted above)\n+    val riderValue = spark.sql(\"select distinct rider from hudi_ro_table\").show()\n+    // copy the value displayed to be used in next step\n+\n+**Step 5** : Fetch records that needs to be deleted, with the above rider value. This example is just to illustrate how to delete. In real world, use a select query using spark sql to fetch records that needs to be deleted and from the result we could invoke deletes as given below. Example rider value used is \"rider-213\".\n+\n+    val df = spark.sql(``\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-213'\"``)\n+\n+// Replace the above query with any other query that will fetch records to be deleted.\n+\n+**Step 6** : Issue deletes\n+\n+    val deletes = dataGen.generateDeletes(df.collectAsList())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2015414922f99f770c64b9b38363066be8109c2"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3OTgwMA==", "bodyText": "https://github.com/apache/incubator-hudi/blame/asf-site/docs/_docs/1_1_quick_start_guide.md#L19  similar to the other pages.. ?", "url": "https://github.com/apache/hudi/pull/1594#discussion_r420179800", "createdAt": "2020-05-05T15:02:46Z", "author": {"login": "vinothchandar"}, "path": "docs/_posts/2020-01-15-delete-support-in-hudi.md", "diffHunk": "@@ -0,0 +1,156 @@\n+---\n+title: \"Delete support in Hudi\"\n+excerpt: \"Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \u201chow to\u201d blog on how to delete records in hudi.\"\n+author: shivnarayan\n+---\n+\n+Deletes are supported at a record level in Hudi with 0.5.1 release. This blog is a \"how to\" blog on how to delete records in hudi. Deletes can be done with 3 flavors: Hudi RDD APIs, with Spark data source and with DeltaStreamer.\n+\n+### Delete using RDD Level APIs\n+\n+If you have embedded  _HoodieWriteClient_ , then deletion is as simple as passing in a  _JavaRDD<HoodieKey>_ to the delete api.\n+\n+    // Fetch list of HoodieKeys from elsewhere that needs to be deleted\n+    // convert to JavaRDD if required. JavaRDD<HoodieKey> toBeDeletedKeys\n+    List<WriteStatus> statuses = writeClient.delete(toBeDeletedKeys, commitTime);\n+\n+### Deletion with Datasource\n+\n+Now we will walk through an example of how to perform deletes on a sample dataset using the Datasource API. Quick Start has the same example as below. Feel free to check it out.\n+\n+**Step 1** : Launch spark shell\n+\n+    bin/spark-shell --packages org.apache.hudi:hudi-spark-bundle:0.5.1-incubating \\\n+        --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n+\n+**Step 2** : Import as required and set up table name, etc for sample dataset\n+\n+    import org.apache.hudi.QuickstartUtils._\n+    import scala.collection.JavaConversions._\n+    import org.apache.spark.sql.SaveMode._\n+    import org.apache.hudi.DataSourceReadOptions._\n+    import org.apache.hudi.DataSourceWriteOptions._\n+    import org.apache.hudi.config.HoodieWriteConfig._\n+     \n+    val tableName = \"hudi_cow_table\"\n+    val basePath = \"file:///tmp/hudi_cow_table\"\n+    val dataGen = new DataGenerator\n+\n+**Step 3** : Insert data. Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi dataset as below.\n+\n+    val inserts = convertToStringList(dataGen.generateInserts(10))\n+    val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+    df.write.format(\"org.apache.hudi\").\n+        options(getQuickstartWriteConfigs).\n+        option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+        option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+        option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+        option(TABLE_NAME, tableName).\n+        mode(Overwrite).\n+        save(basePath);\n+\n+**Step 4** : Query data. Load the data files into a DataFrame.\n+\n+    val roViewDF = spark.\n+        read.\n+        format(\"org.apache.hudi\").\n+        load(basePath + \"/*/*/*/*\")\n+    roViewDF.createOrReplaceTempView(\"hudi_ro_table\")\n+    spark.sql(\"select count(*) from hudi_ro_table\").show() // should return 10 (number of records inserted above)\n+    val riderValue = spark.sql(\"select distinct rider from hudi_ro_table\").show()\n+    // copy the value displayed to be used in next step\n+\n+**Step 5** : Fetch records that needs to be deleted, with the above rider value. This example is just to illustrate how to delete. In real world, use a select query using spark sql to fetch records that needs to be deleted and from the result we could invoke deletes as given below. Example rider value used is \"rider-213\".\n+\n+    val df = spark.sql(``\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-213'\"``)\n+\n+// Replace the above query with any other query that will fetch records to be deleted.\n+\n+**Step 6** : Issue deletes\n+\n+    val deletes = dataGen.generateDeletes(df.collectAsList())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3NzkwOQ=="}, "originalCommit": {"oid": "b2015414922f99f770c64b9b38363066be8109c2"}, "originalPosition": 71}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4623, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}