{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxMDEyMjQw", "number": 2254, "title": "[HUDI-1350] Support Partition level delete API in HUDI", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nSupport Partition level delete API in HUDI  base on replace action. the spark datasource and hudi cli can use this api\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-11-14T14:19:45Z", "url": "https://github.com/apache/hudi/pull/2254", "merged": true, "mergeCommit": {"oid": "e177466fd266ebf3a8d371ce1bf2ecf3bdfe90ed"}, "closed": true, "closedAt": "2020-12-28T23:01:06Z", "author": {"login": "lw309637554"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdccj4fgH2gAyNTIxMDEyMjQwOmEyNzhhNTRhZjFkZjQ1ZTdhNzIzY2MxYjVlZDI1OTUzYTQ2NjNjMGU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdqP0M_gH2gAyNTIxMDEyMjQwOmE3ZTBjODE4OWRlZTkxZWMyMjdkNzljMWExMTQ2MWFlNGNiZTNmNjM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a278a54af1df45e7a723cc1b5ed25953a4663c0e", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/a278a54af1df45e7a723cc1b5ed25953a4663c0e", "committedDate": "2020-11-14T14:23:39Z", "message": "[HUDI-1350] Support Partition level delete API in HUDI"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "27c93a71669cbc180c2826c1cb74e74e5956deb5", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/27c93a71669cbc180c2826c1cb74e74e5956deb5", "committedDate": "2020-11-14T14:14:48Z", "message": "[HUDI-1350] Support Partition level delete API in HUDI"}, "afterCommit": {"oid": "a278a54af1df45e7a723cc1b5ed25953a4663c0e", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/a278a54af1df45e7a723cc1b5ed25953a4663c0e", "committedDate": "2020-11-14T14:23:39Z", "message": "[HUDI-1350] Support Partition level delete API in HUDI"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ea8fa9d9a9708564c7cf0f9b2ee92824f668bdf6", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/ea8fa9d9a9708564c7cf0f9b2ee92824f668bdf6", "committedDate": "2020-11-14T14:24:34Z", "message": "Merge branch 'master' into HUDI-1350"}, "afterCommit": {"oid": "9a5371dfa45c31aad012fdf229977f54d0cb68ab", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/9a5371dfa45c31aad012fdf229977f54d0cb68ab", "committedDate": "2020-11-17T05:04:03Z", "message": "Merge branch 'master' into  HUDI-1350"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9a5371dfa45c31aad012fdf229977f54d0cb68ab", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/9a5371dfa45c31aad012fdf229977f54d0cb68ab", "committedDate": "2020-11-17T05:04:03Z", "message": "Merge branch 'master' into  HUDI-1350"}, "afterCommit": {"oid": "92c114ade6b1bab838030e900793ff1c2cc02de6", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/92c114ade6b1bab838030e900793ff1c2cc02de6", "committedDate": "2020-11-18T02:14:53Z", "message": "Merge  branch 'master' into  HUDI-1350"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "533af553618371427e85de214b96669c8d249bab", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/533af553618371427e85de214b96669c8d249bab", "committedDate": "2020-11-18T14:53:35Z", "message": "Merge  branch 'master' into  HUDI-1350"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "92c114ade6b1bab838030e900793ff1c2cc02de6", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/92c114ade6b1bab838030e900793ff1c2cc02de6", "committedDate": "2020-11-18T02:14:53Z", "message": "Merge  branch 'master' into  HUDI-1350"}, "afterCommit": {"oid": "533af553618371427e85de214b96669c8d249bab", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/533af553618371427e85de214b96669c8d249bab", "committedDate": "2020-11-18T14:53:35Z", "message": "Merge  branch 'master' into  HUDI-1350"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7cdd2db01d83a6127f1b967c039d34895d987123", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/7cdd2db01d83a6127f1b967c039d34895d987123", "committedDate": "2020-11-19T16:19:13Z", "message": "Merge remote-tracking branch 'upstream/master' into HUDI-1350"}, "afterCommit": {"oid": "3af20d869ffeb3216d14b41a5cd59ae1c0f87d1c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3af20d869ffeb3216d14b41a5cd59ae1c0f87d1c", "committedDate": "2020-11-20T00:30:48Z", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1350"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a279a39a37f48d261834705e58be220bdbcc804c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/a279a39a37f48d261834705e58be220bdbcc804c", "committedDate": "2020-11-20T01:37:21Z", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1350"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3af20d869ffeb3216d14b41a5cd59ae1c0f87d1c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3af20d869ffeb3216d14b41a5cd59ae1c0f87d1c", "committedDate": "2020-11-20T00:30:48Z", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1350"}, "afterCommit": {"oid": "a279a39a37f48d261834705e58be220bdbcc804c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/a279a39a37f48d261834705e58be220bdbcc804c", "committedDate": "2020-11-20T01:37:21Z", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1350"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c", "committedDate": "2020-12-09T15:20:10Z", "message": "Merge remote-tracking branch 'upstream/master' into HUDI-1350"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Njg0NDgw", "url": "https://github.com/apache/hudi/pull/2254#pullrequestreview-549684480", "createdAt": "2020-12-10T23:09:02Z", "commit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowOTowMlrOIDhp0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQyMDo1NjowOFrOIFoYLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2ODAxOQ==", "bodyText": "can you parallelize this? It is very similar to what you did for 'insert_overwrite_table'. Try to reuse the code from there if possible", "url": "https://github.com/apache/hudi/pull/2254#discussion_r540568019", "createdAt": "2020-12-10T23:09:02Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));\n+    result.setWriteStatuses(jsc.emptyRDD());\n+    Map<String, List<String>> partitionToReplaceFileIds = partitions.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc2OTg3Ng==", "bodyText": "Do you think using JavaRDD for partitions makes more sense (given all other APIs are using RDD)?", "url": "https://github.com/apache/hudi/pull/2254#discussion_r542769876", "createdAt": "2020-12-14T20:51:02Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -157,6 +157,15 @@ private synchronized FileSystemViewManager getViewManager() {\n    */\n   public abstract HoodieWriteMetadata<O> delete(HoodieEngineContext context, String instantTime, K keys);\n \n+  /**\n+   * Deletes all data of partitions.\n+   * @param context    HoodieEngineContext\n+   * @param instantTime Instant Time for the action\n+   * @param partitions   {@link List} of partition to be deleted\n+   * @return HoodieWriteMetadata\n+   */\n+  public abstract HoodieWriteMetadata deletePartitions(HoodieEngineContext context, String instantTime, List<String> partitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc3NTM0Mg==", "bodyText": "could you try to simplify this test to avoid code duplication. Also, see if its possible to add similar test for MOR tables.", "url": "https://github.com/apache/hudi/pull/2254#discussion_r542775342", "createdAt": "2020-12-14T20:56:08Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -999,6 +999,110 @@ private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int\n     verifyRecordsWritten(commitTime2, inserts2, statuses);\n   }\n \n+  /**\n+   * Test scenario of writing fewer file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithFewerRecordsFirstPartition() throws Exception {\n+    verifyDeletePartitionsHandling(1000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithSimilarNumberOfRecords() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing more file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingHandlingWithFewerRecordsSecondThirdPartition() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 1000, 1000);\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records for first partition.\n+   *  2) Do write2 (upsert) with 'batch2RecordsCount' number of records for second partition.\n+   *  3) Do write3 (upsert) with 'batch3RecordsCount' number of records for third partition.\n+   *  4) delete first partition and check result.\n+   *  5) delete second and third partition and check result.\n+   *\n+   */\n+  private void verifyDeletePartitionsHandling(int batch1RecordsCount, int batch2RecordsCount, int batch3RecordsCount) throws Exception {\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000, false);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator();\n+\n+    // Do Inserts for DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime1 = \"001\";\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInsertsForPartition(commitTime1, batch1RecordsCount, DEFAULT_FIRST_PARTITION_PATH);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batch1Buckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyRecordsWritten(commitTime1, inserts1, statuses);\n+\n+    // Do Inserts for DEFAULT_SECOND_PARTITION_PATH\n+    String commitTime2 = \"002\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0OTQwOTc0", "url": "https://github.com/apache/hudi/pull/2254#pullrequestreview-554940974", "createdAt": "2020-12-17T19:57:01Z", "commit": {"oid": "17fbc00fa912a74221fdd2c5b5d2f931dc9cee9d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxOTo1NzowMVrOIIGbBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxOTo1NzowMVrOIIGbBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM2NDc0MA==", "bodyText": "you may want to assert that other partitions still have base files.", "url": "https://github.com/apache/hudi/pull/2254#discussion_r545364740", "createdAt": "2020-12-17T19:57:01Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -999,6 +999,103 @@ private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int\n     verifyRecordsWritten(commitTime2, inserts2, statuses);\n   }\n \n+  /**\n+   * Test scenario of writing fewer file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithFewerRecordsFirstPartition() throws Exception {\n+    verifyDeletePartitionsHandling(1000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithSimilarNumberOfRecords() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing more file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingHandlingWithFewerRecordsSecondThirdPartition() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 1000, 1000);\n+  }\n+\n+  private Set<String> insertPartitionRecordsWithCommit(SparkRDDWriteClient client, int recordsCount, String commitTime1, String partitionPath) {\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInsertsForPartition(commitTime1, recordsCount, partitionPath);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batchBuckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyRecordsWritten(commitTime1, inserts1, statuses);\n+    return batchBuckets;\n+  }\n+\n+  private Set<String> deletePartitionWithCommit(SparkRDDWriteClient client, String commitTime, List<String> deletePartitionPath) {\n+    client.startCommitWithTime(commitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n+    HoodieWriteResult writeResult = client.deletePartitions(deletePartitionPath, commitTime);\n+    Set<String> deletePartitionReplaceFileIds =\n+        writeResult.getPartitionToReplaceFileIds().entrySet()\n+            .stream().flatMap(entry -> entry.getValue().stream()).collect(Collectors.toSet());\n+    return deletePartitionReplaceFileIds;\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records for first partition.\n+   *  2) Do write2 (upsert) with 'batch2RecordsCount' number of records for second partition.\n+   *  3) Do write3 (upsert) with 'batch3RecordsCount' number of records for third partition.\n+   *  4) delete first partition and check result.\n+   *  5) delete second and third partition and check result.\n+   *\n+   */\n+  private void verifyDeletePartitionsHandling(int batch1RecordsCount, int batch2RecordsCount, int batch3RecordsCount) throws Exception {\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000, false);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator();\n+\n+    // Do Inserts for DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime1 = \"001\";\n+    Set<String> batch1Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch1RecordsCount, commitTime1, DEFAULT_FIRST_PARTITION_PATH);\n+\n+    // Do Inserts for DEFAULT_SECOND_PARTITION_PATH\n+    String commitTime2 = \"002\";\n+    Set<String> batch2Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch2RecordsCount, commitTime2, DEFAULT_SECOND_PARTITION_PATH);\n+\n+    // Do Inserts for DEFAULT_THIRD_PARTITION_PATH\n+    String commitTime3 = \"003\";\n+    Set<String> batch3Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch3RecordsCount, commitTime3, DEFAULT_THIRD_PARTITION_PATH);\n+\n+    // delete DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime4 = \"004\";\n+    Set<String> deletePartitionReplaceFileIds1 =\n+        deletePartitionWithCommit(client, commitTime4, Arrays.asList(DEFAULT_FIRST_PARTITION_PATH));\n+    assertEquals(batch1Buckets, deletePartitionReplaceFileIds1);\n+    List<HoodieBaseFile> baseFiles = HoodieClientTestUtils.getLatestBaseFiles(basePath, fs,\n+        String.format(\"%s/%s/*\", basePath, DEFAULT_FIRST_PARTITION_PATH));\n+    assertEquals(0, baseFiles.size());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17fbc00fa912a74221fdd2c5b5d2f931dc9cee9d"}, "originalPosition": 93}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "549eec5e1ee38ca9f03421b25698c10cd0d65dce", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/549eec5e1ee38ca9f03421b25698c10cd0d65dce", "committedDate": "2020-12-23T15:33:04Z", "message": "[HUDI-1350] Support Partition level delete API in HUDI base InsertOverwriteCommitAction"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "17fbc00fa912a74221fdd2c5b5d2f931dc9cee9d", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/17fbc00fa912a74221fdd2c5b5d2f931dc9cee9d", "committedDate": "2020-12-16T16:21:08Z", "message": "[HUDI-1350] Support Partition level delete API in HUDI base InsertOverwriteCommitAction"}, "afterCommit": {"oid": "549eec5e1ee38ca9f03421b25698c10cd0d65dce", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/549eec5e1ee38ca9f03421b25698c10cd0d65dce", "committedDate": "2020-12-23T15:33:04Z", "message": "[HUDI-1350] Support Partition level delete API in HUDI base InsertOverwriteCommitAction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eecd8a746d51f01eb669be6c4e9076f8b47512e9", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/eecd8a746d51f01eb669be6c4e9076f8b47512e9", "committedDate": "2020-12-23T16:33:56Z", "message": "Merge remote-tracking branch 'upstream/master' into HUDI-1350"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4OTA2NTA4", "url": "https://github.com/apache/hudi/pull/2254#pullrequestreview-558906508", "createdAt": "2020-12-27T06:29:48Z", "commit": {"oid": "eecd8a746d51f01eb669be6c4e9076f8b47512e9"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QwNjoyOTo0OFrOILotxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QwNjozMToxNFrOILouEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA3MjMyNw==", "bodyText": "can we please use the HoodieTimer class here", "url": "https://github.com/apache/hudi/pull/2254#discussion_r549072327", "createdAt": "2020-12-27T06:29:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime,null, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eecd8a746d51f01eb669be6c4e9076f8b47512e9"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA3MjQwMQ==", "bodyText": "so we are saying the files have been replaced with an empty file list?", "url": "https://github.com/apache/hudi/pull/2254#discussion_r549072401", "createdAt": "2020-12-27T06:31:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime,null, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));\n+    result.setWriteStatuses(jsc.emptyRDD());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eecd8a746d51f01eb669be6c4e9076f8b47512e9"}, "originalPosition": 59}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7e0c8189dee91ec227d79c1a11461ae4cbe3f63", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/a7e0c8189dee91ec227d79c1a11461ae4cbe3f63", "committedDate": "2020-12-27T11:27:39Z", "message": "[HUDI-1350] Support Partition level delete API in HUDI base InsertOverwriteCommitAction"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4314, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}