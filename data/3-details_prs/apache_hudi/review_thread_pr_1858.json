{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU0NzI2ODM5", "number": 1858, "reviewThreads": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMDoyNDoyNVrOEQyHVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxOTowNToyMVrOEWmnsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDMzNzUxOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMDoyNDoyNVrOG1Ib2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMTo1NjowNVrOG1LPVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NTkxNQ==", "bodyText": "@vinothchandar : is this the right place to call upgrade/downgrade. If not, please advise.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458365915", "createdAt": "2020-07-21T20:24:25Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -190,6 +192,7 @@ public HoodieMetrics getMetrics() {\n    */\n   protected HoodieTable getTableAndInitCtx(WriteOperationType operationType) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n+    mayBeUpradeOrDowngrade(metaClient);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMTg2Mw==", "bodyText": "looks ok. Mostly sure", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458411863", "createdAt": "2020-07-21T21:56:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -190,6 +192,7 @@ public HoodieMetrics getMetrics() {\n    */\n   protected HoodieTable getTableAndInitCtx(WriteOperationType operationType) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n+    mayBeUpradeOrDowngrade(metaClient);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NTkxNQ=="}, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDM0NDc2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMDoyNjoyOFrOG1IgTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMTo1Njo0MVrOG1LQPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NzA1NA==", "bodyText": "@vinothchandar : after updating the hoodie.properties file, I haven't reloaded the meta client as of now. It is just the table version in memory that has changed and no other code blocks should access table.version. Do you think is reload of metaclient mandatory if we update hoodie.properties w/ new table version?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458367054", "createdAt": "2020-07-21T20:26:28Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.exception.HoodieException;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeHelper {\n+\n+  public static final String HOODIE_ORIG_PROPERTY_FILE = \"hoodie.properties.orig\";\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie verion was upgraded to 0.6.0, there are some upgrade steps need\n+   * to be executed before doing any writes.\n+   * Similarly, if a dataset was created using 0.6.0 and then hoodie was downgraded, some downgrade steps need to be executed before proceeding w/ any writes.\n+   * On a high level, these are the steps performed\n+   * Step1 : Understand current hoodie version and table version from hoodie.properties file\n+   * Step2 : Fix any residues from previous upgrade/downgrade\n+   * Step3 : Check for version upgrade/downgrade.\n+   * Step4 : If upgrade/downgrade is required, perform the steps required for the same.\n+   * Step5 : Copy hoodie.properties -> hoodie.properties.orig\n+   * Step6 : Update hoodie.properties file with current table version\n+   * Step7 : Delete hoodie.properties.orig\n+   * </p>\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @throws IOException\n+   */\n+  public static void doUpgradeOrDowngrade(HoodieTableMetaClient metaClient) throws IOException {\n+    // Fetch version from property file and current version\n+    HoodieTableVersion versionFromPropertyFile = metaClient.getTableConfig().getHoodieTableVersionFromPropertyFile();\n+    HoodieTableVersion currentVersion = metaClient.getTableConfig().getCurrentHoodieTableVersion();\n+\n+    Path metaPath = new Path(metaClient.getMetaPath());\n+    Path originalHoodiePropertyPath = getOrigHoodiePropertyFilePath(metaPath.toString());\n+\n+    boolean updateTableVersionInPropertyFile = false;\n+\n+    if (metaClient.getFs().exists(originalHoodiePropertyPath)) {\n+      // if hoodie.properties.orig exists, rename to hoodie.properties and skip upgrade/downgrade step\n+      metaClient.getFs().rename(originalHoodiePropertyPath, getHoodiePropertyFilePath(metaPath.toString()));\n+      updateTableVersionInPropertyFile = true;\n+    } else {\n+      // upgrade or downgrade if there is a version mismatch\n+      if (versionFromPropertyFile != currentVersion) {\n+        updateTableVersionInPropertyFile = true;\n+        if (versionFromPropertyFile == HoodieTableVersion.PRE_ZERO_SIZE_ZERO && currentVersion == HoodieTableVersion.ZERO_SIX_ZERO) {\n+          upgradeFromOlderToZeroSixZero();\n+        } else if (versionFromPropertyFile == HoodieTableVersion.ZERO_SIX_ZERO && currentVersion == HoodieTableVersion.PRE_ZERO_SIZE_ZERO) {\n+          downgradeFromZeroSixZeroToPreZeroSixZero();\n+        } else {\n+          throw new HoodieException(\"Illegal state wrt table versions. Version from proerpty file \" + versionFromPropertyFile + \" and current version \" + currentVersion);\n+        }\n+      }\n+    }\n+\n+    /**\n+     * If table version needs to be updated in hoodie.properties file.\n+     * Step1: Copy hoodie.properties to hoodie.properties.orig\n+     * Step2: add table.version to hoodie.properties\n+     * Step3: delete hoodie.properties.orig\n+     */\n+    if (updateTableVersionInPropertyFile) {\n+      updateTableVersionInMetaPath(metaClient);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMjA5Mg==", "bodyText": "yes. we need to reload if upgrade was done. to be safe", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458412092", "createdAt": "2020-07-21T21:56:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.exception.HoodieException;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeHelper {\n+\n+  public static final String HOODIE_ORIG_PROPERTY_FILE = \"hoodie.properties.orig\";\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie verion was upgraded to 0.6.0, there are some upgrade steps need\n+   * to be executed before doing any writes.\n+   * Similarly, if a dataset was created using 0.6.0 and then hoodie was downgraded, some downgrade steps need to be executed before proceeding w/ any writes.\n+   * On a high level, these are the steps performed\n+   * Step1 : Understand current hoodie version and table version from hoodie.properties file\n+   * Step2 : Fix any residues from previous upgrade/downgrade\n+   * Step3 : Check for version upgrade/downgrade.\n+   * Step4 : If upgrade/downgrade is required, perform the steps required for the same.\n+   * Step5 : Copy hoodie.properties -> hoodie.properties.orig\n+   * Step6 : Update hoodie.properties file with current table version\n+   * Step7 : Delete hoodie.properties.orig\n+   * </p>\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @throws IOException\n+   */\n+  public static void doUpgradeOrDowngrade(HoodieTableMetaClient metaClient) throws IOException {\n+    // Fetch version from property file and current version\n+    HoodieTableVersion versionFromPropertyFile = metaClient.getTableConfig().getHoodieTableVersionFromPropertyFile();\n+    HoodieTableVersion currentVersion = metaClient.getTableConfig().getCurrentHoodieTableVersion();\n+\n+    Path metaPath = new Path(metaClient.getMetaPath());\n+    Path originalHoodiePropertyPath = getOrigHoodiePropertyFilePath(metaPath.toString());\n+\n+    boolean updateTableVersionInPropertyFile = false;\n+\n+    if (metaClient.getFs().exists(originalHoodiePropertyPath)) {\n+      // if hoodie.properties.orig exists, rename to hoodie.properties and skip upgrade/downgrade step\n+      metaClient.getFs().rename(originalHoodiePropertyPath, getHoodiePropertyFilePath(metaPath.toString()));\n+      updateTableVersionInPropertyFile = true;\n+    } else {\n+      // upgrade or downgrade if there is a version mismatch\n+      if (versionFromPropertyFile != currentVersion) {\n+        updateTableVersionInPropertyFile = true;\n+        if (versionFromPropertyFile == HoodieTableVersion.PRE_ZERO_SIZE_ZERO && currentVersion == HoodieTableVersion.ZERO_SIX_ZERO) {\n+          upgradeFromOlderToZeroSixZero();\n+        } else if (versionFromPropertyFile == HoodieTableVersion.ZERO_SIX_ZERO && currentVersion == HoodieTableVersion.PRE_ZERO_SIZE_ZERO) {\n+          downgradeFromZeroSixZeroToPreZeroSixZero();\n+        } else {\n+          throw new HoodieException(\"Illegal state wrt table versions. Version from proerpty file \" + versionFromPropertyFile + \" and current version \" + currentVersion);\n+        }\n+      }\n+    }\n+\n+    /**\n+     * If table version needs to be updated in hoodie.properties file.\n+     * Step1: Copy hoodie.properties to hoodie.properties.orig\n+     * Step2: add table.version to hoodie.properties\n+     * Step3: delete hoodie.properties.orig\n+     */\n+    if (updateTableVersionInPropertyFile) {\n+      updateTableVersionInMetaPath(metaClient);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NzA1NA=="}, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTI4NDQ2OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMzoxOToyMlrOG1ROJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwMDo1NDowN1rOG3BISA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwOTg2Mg==", "bodyText": "@vinothchandar : sorry forgot to ask this question earlier. May I know how to fetch current hoodie version in use?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458509862", "createdAt": "2020-07-22T03:19:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -151,6 +154,27 @@ public HoodieTableType getTableType() {\n         : Option.empty();\n   }\n \n+  /**\n+   * @return the table version from .hoodie properties file.\n+   */\n+  public HoodieTableVersion getHoodieTableVersionFromPropertyFile() {\n+    if (props.contains(HOODIE_TABLE_VERSION_PROP_NAME)) {\n+      String propValue = props.getProperty(HOODIE_TABLE_VERSION_PROP_NAME);\n+      if (propValue.equals(HoodieTableVersion.ZERO_SIX_ZERO.version)) {\n+        return HoodieTableVersion.ZERO_SIX_ZERO;\n+      }\n+    }\n+    return DEFAULT_TABLE_VERSION;\n+  }\n+\n+  /**\n+   * @return the current hoodie table version.\n+   */\n+  public HoodieTableVersion getCurrentHoodieTableVersion() {\n+    // TODO: fetch current version dynamically", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcyOTIxNQ==", "bodyText": "By reading hoodie.properties we need to treat everything below 0.6.0 as V_PRE_0.6.0 . Cannot deduce the actual jars per se.\nWe can also make these version numbers 0,1,2 instead of PRE_0.6.0, 0.6.0, 0.7.0 and so on. ?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458729215", "createdAt": "2020-07-22T11:40:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -151,6 +154,27 @@ public HoodieTableType getTableType() {\n         : Option.empty();\n   }\n \n+  /**\n+   * @return the table version from .hoodie properties file.\n+   */\n+  public HoodieTableVersion getHoodieTableVersionFromPropertyFile() {\n+    if (props.contains(HOODIE_TABLE_VERSION_PROP_NAME)) {\n+      String propValue = props.getProperty(HOODIE_TABLE_VERSION_PROP_NAME);\n+      if (propValue.equals(HoodieTableVersion.ZERO_SIX_ZERO.version)) {\n+        return HoodieTableVersion.ZERO_SIX_ZERO;\n+      }\n+    }\n+    return DEFAULT_TABLE_VERSION;\n+  }\n+\n+  /**\n+   * @return the current hoodie table version.\n+   */\n+  public HoodieTableVersion getCurrentHoodieTableVersion() {\n+    // TODO: fetch current version dynamically", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwOTg2Mg=="}, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczMTc5Mg==", "bodyText": "let me try to rephrase. Lets say a dataset was created in 0.6.0 and then user moves to hoodie version 0.6.1 and when he launches hoodie for first time, how in this code we will get to know that this is using version 0.6.1? hoodie.properties will be having 0.6.0 only right.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458731792", "createdAt": "2020-07-22T11:45:13Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -151,6 +154,27 @@ public HoodieTableType getTableType() {\n         : Option.empty();\n   }\n \n+  /**\n+   * @return the table version from .hoodie properties file.\n+   */\n+  public HoodieTableVersion getHoodieTableVersionFromPropertyFile() {\n+    if (props.contains(HOODIE_TABLE_VERSION_PROP_NAME)) {\n+      String propValue = props.getProperty(HOODIE_TABLE_VERSION_PROP_NAME);\n+      if (propValue.equals(HoodieTableVersion.ZERO_SIX_ZERO.version)) {\n+        return HoodieTableVersion.ZERO_SIX_ZERO;\n+      }\n+    }\n+    return DEFAULT_TABLE_VERSION;\n+  }\n+\n+  /**\n+   * @return the current hoodie table version.\n+   */\n+  public HoodieTableVersion getCurrentHoodieTableVersion() {\n+    // TODO: fetch current version dynamically", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwOTg2Mg=="}, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MzM2OA==", "bodyText": "HoodieTableVersion or someplace we need to ahve a CURR_VERSION variable that gets bumped to 0.6.1 .\nMore I think about this. I think its better to name the versions 0,1,2... and so on, instead of release numbers. we may not bump this up every release . only when upgrade/downgrade is necessary.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r460343368", "createdAt": "2020-07-25T00:54:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -151,6 +154,27 @@ public HoodieTableType getTableType() {\n         : Option.empty();\n   }\n \n+  /**\n+   * @return the table version from .hoodie properties file.\n+   */\n+  public HoodieTableVersion getHoodieTableVersionFromPropertyFile() {\n+    if (props.contains(HOODIE_TABLE_VERSION_PROP_NAME)) {\n+      String propValue = props.getProperty(HOODIE_TABLE_VERSION_PROP_NAME);\n+      if (propValue.equals(HoodieTableVersion.ZERO_SIX_ZERO.version)) {\n+        return HoodieTableVersion.ZERO_SIX_ZERO;\n+      }\n+    }\n+    return DEFAULT_TABLE_VERSION;\n+  }\n+\n+  /**\n+   * @return the current hoodie table version.\n+   */\n+  public HoodieTableVersion getCurrentHoodieTableVersion() {\n+    // TODO: fetch current version dynamically", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwOTg2Mg=="}, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTIyOTUyOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0Njo0OFrOG32Opg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0Njo0OFrOG32Opg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxMzM1MA==", "bodyText": "@vinothchandar : I have added a flag here to say where delete has to be done or just stats need to be collected. Since I don't want to duplicate code, tried my best to re-use. If you can think of any other ways, lmk.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461213350", "createdAt": "2020-07-27T22:46:48Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "diffHunk": "@@ -159,24 +161,32 @@ private void rollBackIndex() {\n     LOG.info(\"Index rolled back for commits \" + instantToRollback);\n   }\n \n-  public List<HoodieRollbackStat> doRollbackAndGetStats() {\n-    final String instantTimeToRollback = instantToRollback.getTimestamp();\n-    final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())\n-        && !instantToRollback.isCompleted();\n-    validateSavepointRollbacks();\n-    if (!isPendingCompaction) {\n-      validateRollbackCommitSequence();\n-    }\n-\n-    try {\n-      List<HoodieRollbackStat> stats = executeRollback();\n-      LOG.info(\"Rolled back inflight instant \" + instantTimeToRollback);\n+  public List<HoodieRollbackStat> mayBeRollbackAndGetStats(boolean doDelete) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTIzMDgyOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0NzoyNlrOG32PcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0NzoyNlrOG32PcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxMzU1Mg==", "bodyText": "this is the else part where we just collect stats.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461213552", "createdAt": "2020-07-27T22:47:26Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "diffHunk": "@@ -159,24 +161,32 @@ private void rollBackIndex() {\n     LOG.info(\"Index rolled back for commits \" + instantToRollback);\n   }\n \n-  public List<HoodieRollbackStat> doRollbackAndGetStats() {\n-    final String instantTimeToRollback = instantToRollback.getTimestamp();\n-    final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())\n-        && !instantToRollback.isCompleted();\n-    validateSavepointRollbacks();\n-    if (!isPendingCompaction) {\n-      validateRollbackCommitSequence();\n-    }\n-\n-    try {\n-      List<HoodieRollbackStat> stats = executeRollback();\n-      LOG.info(\"Rolled back inflight instant \" + instantTimeToRollback);\n+  public List<HoodieRollbackStat> mayBeRollbackAndGetStats(boolean doDelete) {\n+    if(doDelete) {\n+      final String instantTimeToRollback = instantToRollback.getTimestamp();\n+      final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())\n+          && !instantToRollback.isCompleted();\n+      validateSavepointRollbacks();\n       if (!isPendingCompaction) {\n-        rollBackIndex();\n+        validateRollbackCommitSequence();\n+      }\n+\n+      try {\n+        List<HoodieRollbackStat> stats = executeRollback(doDelete);\n+        LOG.info(\"Rolled back inflight instant \" + instantTimeToRollback);\n+        if (!isPendingCompaction) {\n+          rollBackIndex();\n+        }\n+        return stats;\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Unable to execute rollback \", e);\n+      }\n+    } else{\n+      try {\n+        return executeRollback(doDelete);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTIzMjQ0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0ODoxMFrOG32QXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0ODoxMFrOG32QXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxMzc5MQ==", "bodyText": "and if doDelete is false, we call into executeRollbackUsingFileListing directly.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461213791", "createdAt": "2020-07-27T22:48:10Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -80,10 +82,16 @@ public CopyOnWriteRollbackActionExecutor(JavaSparkContext jsc,\n     if (!resolvedInstant.isRequested()) {\n       // delete all the data files for this commit\n       LOG.info(\"Clean out all base files generated for commit: \" + resolvedInstant);\n-      stats = getRollbackStrategy().execute(resolvedInstant);\n+      if(doDelete) {\n+        stats = getRollbackStrategy().execute(resolvedInstant);\n+      } else{\n+        stats = executeRollbackUsingFileListing(resolvedInstant, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTIzNjEyOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0OTo0NFrOG32SpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMTozNzo0NFrOG35cRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNDM3Mw==", "bodyText": "disintegrated ListingBasedRollbackHelper into two apis, performRollback and collectRollbackStats where first calls into 2nd. Incase of actual rollback, we call performRollback and incase of collecting stats for upgrade, we call into collectRollbackStats. I am repurposing HoodieRollbackStat to hold the info on file path to be rolledback.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461214373", "createdAt": "2020-07-27T22:49:44Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -100,8 +108,13 @@ public CopyOnWriteRollbackActionExecutor(JavaSparkContext jsc,\n   }\n \n   @Override\n-  protected List<HoodieRollbackStat> executeRollbackUsingFileListing(HoodieInstant instantToRollback) {\n+  protected List<HoodieRollbackStat> executeRollbackUsingFileListing(HoodieInstant instantToRollback, boolean doDelete) {\n     List<ListingBasedRollbackRequest> rollbackRequests = generateRollbackRequestsByListing();\n-    return new ListingBasedRollbackHelper(table.getMetaClient(), config).performRollback(jsc, instantToRollback, rollbackRequests);\n+    ListingBasedRollbackHelper listingBasedRollbackHelper = new ListingBasedRollbackHelper(table.getMetaClient(), config);\n+    if(doDelete) {\n+      return listingBasedRollbackHelper.performRollback(jsc, instantToRollback, rollbackRequests);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI2NTk5MA==", "bodyText": "as discussed, we can just call the collectRollbackStats directly, assuming listing based rollback strategy.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461265990", "createdAt": "2020-07-28T01:37:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -100,8 +108,13 @@ public CopyOnWriteRollbackActionExecutor(JavaSparkContext jsc,\n   }\n \n   @Override\n-  protected List<HoodieRollbackStat> executeRollbackUsingFileListing(HoodieInstant instantToRollback) {\n+  protected List<HoodieRollbackStat> executeRollbackUsingFileListing(HoodieInstant instantToRollback, boolean doDelete) {\n     List<ListingBasedRollbackRequest> rollbackRequests = generateRollbackRequestsByListing();\n-    return new ListingBasedRollbackHelper(table.getMetaClient(), config).performRollback(jsc, instantToRollback, rollbackRequests);\n+    ListingBasedRollbackHelper listingBasedRollbackHelper = new ListingBasedRollbackHelper(table.getMetaClient(), config);\n+    if(doDelete) {\n+      return listingBasedRollbackHelper.performRollback(jsc, instantToRollback, rollbackRequests);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNDM3Mw=="}, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTIzNzc1OnYy", "diffSide": "LEFT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1MDoxOFrOG32TnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMTozODowMFrOG35ckQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNDYyMA==", "bodyText": "this filter was used only within one method and hence moved it within the resp method.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461214620", "createdAt": "2020-07-27T22:50:18Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -68,34 +69,38 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n    * Performs all rollback actions that we have collected in parallel.\n    */\n   public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n-    SerializablePathFilter filter = (path) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI2NjA2NQ==", "bodyText": "ack", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461266065", "createdAt": "2020-07-28T01:38:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -68,34 +69,38 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n    * Performs all rollback actions that we have collected in parallel.\n    */\n   public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n-    SerializablePathFilter filter = (path) -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNDYyMA=="}, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTI0MTQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1MTo1N1rOG32VyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMTozODoyNFrOG35c9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNTE3Ng==", "bodyText": "can you help me understand how to differentiate between CREATE and MERGE in these code blocks.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461215176", "createdAt": "2020-07-27T22:51:57Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -130,39 +137,55 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n               1L\n           );\n           return new Tuple2<>(rollbackRequest.getPartitionPath(),\n-                  HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n-                          .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n         }\n         default:\n           throw new IllegalStateException(\"Unknown Rollback action \" + rollbackRequest);\n       }\n-    }).reduceByKey(RollbackUtils::mergeRollbackStat).map(Tuple2::_2).collect();\n+    });\n   }\n \n \n-\n   /**\n    * Common method used for cleaning out base files under a partition path during rollback of a set of commits.\n    */\n-  private Map<FileStatus, Boolean> deleteCleanedFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n-                                                      String partitionPath, PathFilter filter) throws IOException {\n+  private Map<FileStatus, Boolean> deleteBaseAndLogFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI2NjE2NA==", "bodyText": "its hard. but would nt MERGE work for both in terms of actually performing a correct rollback?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461266164", "createdAt": "2020-07-28T01:38:24Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -130,39 +137,55 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n               1L\n           );\n           return new Tuple2<>(rollbackRequest.getPartitionPath(),\n-                  HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n-                          .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n         }\n         default:\n           throw new IllegalStateException(\"Unknown Rollback action \" + rollbackRequest);\n       }\n-    }).reduceByKey(RollbackUtils::mergeRollbackStat).map(Tuple2::_2).collect();\n+    });\n   }\n \n \n-\n   /**\n    * Common method used for cleaning out base files under a partition path during rollback of a set of commits.\n    */\n-  private Map<FileStatus, Boolean> deleteCleanedFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n-                                                      String partitionPath, PathFilter filter) throws IOException {\n+  private Map<FileStatus, Boolean> deleteBaseAndLogFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNTE3Ng=="}, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTI0Mzc2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1Mjo1M1rOG32XFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwMTo0MDoxM1rOG35e_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNTUwOA==", "bodyText": "incase of just collecting stats, all files are added to success list.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461215508", "createdAt": "2020-07-27T22:52:53Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -130,39 +137,55 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n               1L\n           );\n           return new Tuple2<>(rollbackRequest.getPartitionPath(),\n-                  HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n-                          .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n         }\n         default:\n           throw new IllegalStateException(\"Unknown Rollback action \" + rollbackRequest);\n       }\n-    }).reduceByKey(RollbackUtils::mergeRollbackStat).map(Tuple2::_2).collect();\n+    });\n   }\n \n \n-\n   /**\n    * Common method used for cleaning out base files under a partition path during rollback of a set of commits.\n    */\n-  private Map<FileStatus, Boolean> deleteCleanedFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n-                                                      String partitionPath, PathFilter filter) throws IOException {\n+  private Map<FileStatus, Boolean> deleteBaseAndLogFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n+      String commit, String partitionPath, boolean doDelete) throws IOException {\n     LOG.info(\"Cleaning path \" + partitionPath);\n+    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n+    SerializablePathFilter filter = (path) -> {\n+      if (path.toString().endsWith(basefileExtension)) {\n+        String fileCommitTime = FSUtils.getCommitTime(path.getName());\n+        return commit.equals(fileCommitTime);\n+      } else if (FSUtils.isLogFile(path)) {\n+        // Since the baseCommitTime is the only commit for new log files, it's okay here\n+        String fileCommitTime = FSUtils.getBaseCommitTimeFromLogPath(path);\n+        return commit.equals(fileCommitTime);\n+      }\n+      return false;\n+    };\n+\n     final Map<FileStatus, Boolean> results = new HashMap<>();\n     FileSystem fs = metaClient.getFs();\n     FileStatus[] toBeDeleted = fs.listStatus(FSUtils.getPartitionPath(config.getBasePath(), partitionPath), filter);\n     for (FileStatus file : toBeDeleted) {\n-      boolean success = fs.delete(file.getPath(), false);\n-      results.put(file, success);\n-      LOG.info(\"Delete file \" + file.getPath() + \"\\t\" + success);\n+      if(doDelete) {\n+        boolean success = fs.delete(file.getPath(), false);\n+        results.put(file, success);\n+        LOG.info(\"Delete file \" + file.getPath() + \"\\t\" + success);\n+      } else{\n+        results.put(file, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI2NjY4NQ==", "bodyText": "lets add a test , that has inflight commit, with few marker files deleted. and then we show that the code can correctly upgrade and perform a rollback using marker based strategy", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461266685", "createdAt": "2020-07-28T01:40:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -130,39 +137,55 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n               1L\n           );\n           return new Tuple2<>(rollbackRequest.getPartitionPath(),\n-                  HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n-                          .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n         }\n         default:\n           throw new IllegalStateException(\"Unknown Rollback action \" + rollbackRequest);\n       }\n-    }).reduceByKey(RollbackUtils::mergeRollbackStat).map(Tuple2::_2).collect();\n+    });\n   }\n \n \n-\n   /**\n    * Common method used for cleaning out base files under a partition path during rollback of a set of commits.\n    */\n-  private Map<FileStatus, Boolean> deleteCleanedFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n-                                                      String partitionPath, PathFilter filter) throws IOException {\n+  private Map<FileStatus, Boolean> deleteBaseAndLogFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n+      String commit, String partitionPath, boolean doDelete) throws IOException {\n     LOG.info(\"Cleaning path \" + partitionPath);\n+    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n+    SerializablePathFilter filter = (path) -> {\n+      if (path.toString().endsWith(basefileExtension)) {\n+        String fileCommitTime = FSUtils.getCommitTime(path.getName());\n+        return commit.equals(fileCommitTime);\n+      } else if (FSUtils.isLogFile(path)) {\n+        // Since the baseCommitTime is the only commit for new log files, it's okay here\n+        String fileCommitTime = FSUtils.getBaseCommitTimeFromLogPath(path);\n+        return commit.equals(fileCommitTime);\n+      }\n+      return false;\n+    };\n+\n     final Map<FileStatus, Boolean> results = new HashMap<>();\n     FileSystem fs = metaClient.getFs();\n     FileStatus[] toBeDeleted = fs.listStatus(FSUtils.getPartitionPath(config.getBasePath(), partitionPath), filter);\n     for (FileStatus file : toBeDeleted) {\n-      boolean success = fs.delete(file.getPath(), false);\n-      results.put(file, success);\n-      LOG.info(\"Delete file \" + file.getPath() + \"\\t\" + success);\n+      if(doDelete) {\n+        boolean success = fs.delete(file.getPath(), false);\n+        results.put(file, success);\n+        LOG.info(\"Delete file \" + file.getPath() + \"\\t\" + success);\n+      } else{\n+        results.put(file, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNTUwOA=="}, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTI0NTY4OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/HoodieRollbackStat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1Mzo1MlrOG32YWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1Mzo1MlrOG32YWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNTgzMw==", "bodyText": "have added this to hold file status fully to be used for upgrade", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461215833", "createdAt": "2020-07-27T22:53:52Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/HoodieRollbackStat.java", "diffHunk": "@@ -39,12 +40,15 @@\n   // Count of HoodieLogFile to commandBlocks written for a particular rollback\n   private final Map<FileStatus, Long> commandBlocksCount;\n \n+  private final List<FileStatus> filesToRollback;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTI0NzY5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1NDo0OVrOG32Zqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1NDo0OVrOG32Zqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNjE3MA==", "bodyText": "yet to figure out how to differentiate CREATE and MERGE from fileStatus", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461216170", "createdAt": "2020-07-27T22:54:49Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeHelper {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngradeHelper.class);\n+  public static final String HOODIE_ORIG_PROPERTY_FILE = \"hoodie.properties.orig\";\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths. Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3),\n+   * and Hoodie version was upgraded to 0.6.0, Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded to pre 0.6.0 or to Hoodie table version 0,\n+   * then some downgrade steps need to be executed before proceeding w/ any writes.\n+   * On a high level, these are the steps performed\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Fix any residues from previous upgrade/downgrade\n+   * Step3 : If there are no residues, Check for version upgrade/downgrade. If version mismatch, perform upgrade/downgrade.\n+   * Step4 : If there are residues, clean them up and skip upgrade/downgrade since those steps would have been completed last time.\n+   * Step5 : Copy hoodie.properties -> hoodie.properties.orig\n+   * Step6 : Update hoodie.properties file with current table version\n+   * Step7 : Delete hoodie.properties.orig\n+   * </p>\n+   *\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @param toVersion version to which upgrade or downgrade has to be done.\n+   */\n+  public static void doUpgradeOrDowngrade(HoodieTableMetaClient metaClient, HoodieTableVersion toVersion, HoodieWriteConfig config, JavaSparkContext jsc) throws IOException {\n+    // Fetch version from property file and current version\n+    HoodieTableVersion versionFromPropertyFile = metaClient.getTableConfig().getHoodieTableVersionFromPropertyFile();\n+\n+    Path metaPath = new Path(metaClient.getMetaPath());\n+    Path originalHoodiePropertyFile = getOrigHoodiePropertyFilePath(metaPath.toString());\n+\n+    boolean updateTableVersionInPropertyFile = false;\n+\n+    if (metaClient.getFs().exists(originalHoodiePropertyFile)) {\n+      // if hoodie.properties.orig exists, rename to hoodie.properties and skip upgrade/downgrade step\n+      metaClient.getFs().rename(originalHoodiePropertyFile, getHoodiePropertyFilePath(metaPath.toString()));\n+      updateTableVersionInPropertyFile = true;\n+    } else {\n+      // upgrade or downgrade if there is a version mismatch\n+      if (versionFromPropertyFile != toVersion) {\n+        updateTableVersionInPropertyFile = true;\n+        if (versionFromPropertyFile == HoodieTableVersion.ONE && toVersion == HoodieTableVersion.ZERO) {\n+          upgradeFromZeroToOne(config, jsc.hadoopConfiguration(), jsc);\n+        } else if (versionFromPropertyFile == HoodieTableVersion.ZERO && toVersion == HoodieTableVersion.ONE) {\n+          downgradeFromOneToZero();\n+        } else {\n+          throw new HoodieException(\"Illegal state wrt table versions. Version from proerpty file \" + versionFromPropertyFile + \" and current version \" + toVersion);\n+        }\n+      }\n+    }\n+\n+    /**\n+     * If table version needs to be updated in hoodie.properties file.\n+     * Step1: Copy hoodie.properties to hoodie.properties.orig\n+     * Step2: add table.version to hoodie.properties\n+     * Step3: delete hoodie.properties.orig\n+     */\n+    if (updateTableVersionInPropertyFile) {\n+      updateTableVersionInHoodiePropertyFile(metaClient, toVersion);\n+    }\n+  }\n+\n+  /**\n+   * Upgrade steps to be done to upgrade from hoodie table version 0 to 1.\n+   */\n+  private static void upgradeFromZeroToOne(HoodieWriteConfig config, Configuration hadoopConf, JavaSparkContext jsc) {\n+    // fetch pending commit info\n+    HoodieTable table = HoodieTable.create(config, hadoopConf);\n+    HoodieTimeline inflightTimeline = table.getMetaClient().getCommitsTimeline().filterPendingExcludingCompaction();\n+    List<String> commits = inflightTimeline.getReverseOrderedInstants().map(HoodieInstant::getTimestamp)\n+        .collect(Collectors.toList());\n+    for (String commit : commits) {\n+      // for every pending commit, delete old marker files and re-SparkMaincreate marker files in new format\n+      recreateMarkerFiles(commit, table, jsc);\n+    }\n+  }\n+\n+  public static void recreateMarkerFiles(final String commitInstantTime, HoodieTable table, JavaSparkContext jsc) throws HoodieRollbackException {\n+    try {\n+      Option<HoodieInstant> commitInstantOpt = Option.fromJavaOptional(table.getActiveTimeline().getCommitsTimeline().getInstants()\n+          .filter(instant -> HoodieActiveTimeline.EQUALS.test(instant.getTimestamp(), commitInstantTime))\n+          .findFirst());\n+      if (commitInstantOpt.isPresent()) {\n+        MarkerFiles markerFiles = new MarkerFiles(table, commitInstantTime);\n+        markerFiles.quietDeleteMarkerDir();\n+\n+        List<HoodieRollbackStat> rollbackStats = new CopyOnWriteRollbackActionExecutor(jsc, table.getConfig(), table, \"\", commitInstantOpt.get(), false).mayBeRollbackAndGetStats(false);\n+\n+        for (HoodieRollbackStat rollbackStat : rollbackStats) {\n+          for (FileStatus fileStatus : rollbackStat.getFilesToRollback()) {\n+            String path = fileStatus.getPath().toString();\n+            String dataFileName = path.substring(path.lastIndexOf(\"/\") + 1);\n+            markerFiles.create(rollbackStat.getPartitionPath(), dataFileName, path.endsWith(table.getBaseFileExtension()) ? IOType.CREATE : IOType.MERGE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 143}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3OTI0OTAwOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1NToxOFrOG32acQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1NToxOFrOG32acQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNjM2OQ==", "bodyText": "I assume every entry in commandBlocks will be an APPEND. Correct me if I am wrong.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461216369", "createdAt": "2020-07-27T22:55:18Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeHelper {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngradeHelper.class);\n+  public static final String HOODIE_ORIG_PROPERTY_FILE = \"hoodie.properties.orig\";\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths. Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3),\n+   * and Hoodie version was upgraded to 0.6.0, Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded to pre 0.6.0 or to Hoodie table version 0,\n+   * then some downgrade steps need to be executed before proceeding w/ any writes.\n+   * On a high level, these are the steps performed\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Fix any residues from previous upgrade/downgrade\n+   * Step3 : If there are no residues, Check for version upgrade/downgrade. If version mismatch, perform upgrade/downgrade.\n+   * Step4 : If there are residues, clean them up and skip upgrade/downgrade since those steps would have been completed last time.\n+   * Step5 : Copy hoodie.properties -> hoodie.properties.orig\n+   * Step6 : Update hoodie.properties file with current table version\n+   * Step7 : Delete hoodie.properties.orig\n+   * </p>\n+   *\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @param toVersion version to which upgrade or downgrade has to be done.\n+   */\n+  public static void doUpgradeOrDowngrade(HoodieTableMetaClient metaClient, HoodieTableVersion toVersion, HoodieWriteConfig config, JavaSparkContext jsc) throws IOException {\n+    // Fetch version from property file and current version\n+    HoodieTableVersion versionFromPropertyFile = metaClient.getTableConfig().getHoodieTableVersionFromPropertyFile();\n+\n+    Path metaPath = new Path(metaClient.getMetaPath());\n+    Path originalHoodiePropertyFile = getOrigHoodiePropertyFilePath(metaPath.toString());\n+\n+    boolean updateTableVersionInPropertyFile = false;\n+\n+    if (metaClient.getFs().exists(originalHoodiePropertyFile)) {\n+      // if hoodie.properties.orig exists, rename to hoodie.properties and skip upgrade/downgrade step\n+      metaClient.getFs().rename(originalHoodiePropertyFile, getHoodiePropertyFilePath(metaPath.toString()));\n+      updateTableVersionInPropertyFile = true;\n+    } else {\n+      // upgrade or downgrade if there is a version mismatch\n+      if (versionFromPropertyFile != toVersion) {\n+        updateTableVersionInPropertyFile = true;\n+        if (versionFromPropertyFile == HoodieTableVersion.ONE && toVersion == HoodieTableVersion.ZERO) {\n+          upgradeFromZeroToOne(config, jsc.hadoopConfiguration(), jsc);\n+        } else if (versionFromPropertyFile == HoodieTableVersion.ZERO && toVersion == HoodieTableVersion.ONE) {\n+          downgradeFromOneToZero();\n+        } else {\n+          throw new HoodieException(\"Illegal state wrt table versions. Version from proerpty file \" + versionFromPropertyFile + \" and current version \" + toVersion);\n+        }\n+      }\n+    }\n+\n+    /**\n+     * If table version needs to be updated in hoodie.properties file.\n+     * Step1: Copy hoodie.properties to hoodie.properties.orig\n+     * Step2: add table.version to hoodie.properties\n+     * Step3: delete hoodie.properties.orig\n+     */\n+    if (updateTableVersionInPropertyFile) {\n+      updateTableVersionInHoodiePropertyFile(metaClient, toVersion);\n+    }\n+  }\n+\n+  /**\n+   * Upgrade steps to be done to upgrade from hoodie table version 0 to 1.\n+   */\n+  private static void upgradeFromZeroToOne(HoodieWriteConfig config, Configuration hadoopConf, JavaSparkContext jsc) {\n+    // fetch pending commit info\n+    HoodieTable table = HoodieTable.create(config, hadoopConf);\n+    HoodieTimeline inflightTimeline = table.getMetaClient().getCommitsTimeline().filterPendingExcludingCompaction();\n+    List<String> commits = inflightTimeline.getReverseOrderedInstants().map(HoodieInstant::getTimestamp)\n+        .collect(Collectors.toList());\n+    for (String commit : commits) {\n+      // for every pending commit, delete old marker files and re-SparkMaincreate marker files in new format\n+      recreateMarkerFiles(commit, table, jsc);\n+    }\n+  }\n+\n+  public static void recreateMarkerFiles(final String commitInstantTime, HoodieTable table, JavaSparkContext jsc) throws HoodieRollbackException {\n+    try {\n+      Option<HoodieInstant> commitInstantOpt = Option.fromJavaOptional(table.getActiveTimeline().getCommitsTimeline().getInstants()\n+          .filter(instant -> HoodieActiveTimeline.EQUALS.test(instant.getTimestamp(), commitInstantTime))\n+          .findFirst());\n+      if (commitInstantOpt.isPresent()) {\n+        MarkerFiles markerFiles = new MarkerFiles(table, commitInstantTime);\n+        markerFiles.quietDeleteMarkerDir();\n+\n+        List<HoodieRollbackStat> rollbackStats = new CopyOnWriteRollbackActionExecutor(jsc, table.getConfig(), table, \"\", commitInstantOpt.get(), false).mayBeRollbackAndGetStats(false);\n+\n+        for (HoodieRollbackStat rollbackStat : rollbackStats) {\n+          for (FileStatus fileStatus : rollbackStat.getFilesToRollback()) {\n+            String path = fileStatus.getPath().toString();\n+            String dataFileName = path.substring(path.lastIndexOf(\"/\") + 1);\n+            markerFiles.create(rollbackStat.getPartitionPath(), dataFileName, path.endsWith(table.getBaseFileExtension()) ? IOType.CREATE : IOType.MERGE);\n+          }\n+          for (FileStatus fileStatus : rollbackStat.getCommandBlocksCount().keySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MTAxMTg0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMDo0MjoyMFrOG4G7DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMDo0MjoyMFrOG4G7DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ4Njg2MQ==", "bodyText": "@vinothchandar : forgot to remind you yesterday when we discussed to move the collectStats method to a separate class and call directly for upgrade. These validations steps (validateSavepointRollbacks, validateRollbackCommitSequence) might be required as well right ? So, few bits and pieces in this class is required in upgrade step as well.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461486861", "createdAt": "2020-07-28T10:42:20Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "diffHunk": "@@ -159,24 +161,32 @@ private void rollBackIndex() {\n     LOG.info(\"Index rolled back for commits \" + instantToRollback);\n   }\n \n-  public List<HoodieRollbackStat> doRollbackAndGetStats() {\n-    final String instantTimeToRollback = instantToRollback.getTimestamp();\n-    final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())\n-        && !instantToRollback.isCompleted();\n-    validateSavepointRollbacks();\n-    if (!isPendingCompaction) {\n-      validateRollbackCommitSequence();\n-    }\n-\n-    try {\n-      List<HoodieRollbackStat> stats = executeRollback();\n-      LOG.info(\"Rolled back inflight instant \" + instantTimeToRollback);\n+  public List<HoodieRollbackStat> mayBeRollbackAndGetStats(boolean doDelete) {\n+    if(doDelete) {\n+      final String instantTimeToRollback = instantToRollback.getTimestamp();\n+      final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NzI1NzE1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxODo1MFrOG5CWQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxODo1MFrOG5CWQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ2MDQ4MA==", "bodyText": "no changes in this file. just formatting changes.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r462460480", "createdAt": "2020-07-29T17:18:50Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "diffHunk": "@@ -59,31 +61,31 @@\n   protected final boolean useMarkerBasedStrategy;\n \n   public BaseRollbackActionExecutor(JavaSparkContext jsc,\n-                                    HoodieWriteConfig config,\n-                                    HoodieTable<?> table,\n-                                    String instantTime,\n-                                    HoodieInstant instantToRollback,\n-                                    boolean deleteInstants) {\n+      HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04accb1729fc6e4f10e0785f89b550c4e76f75b0"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjM1MzYwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMzoyNzowOFrOG7L3ZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOFQxNzoxNzowMVrOG91CuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMzU3Mg==", "bodyText": "rename: UpgradeDowngradeUtil.migrate(..)", "url": "https://github.com/apache/hudi/pull/1858#discussion_r464713572", "createdAt": "2020-08-03T23:27:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -329,9 +341,34 @@ private static int deleteSavepoint(JavaSparkContext jsc, String savepointTime, S\n     }\n   }\n \n+  /**\n+   * Upgrade or downgrade hoodie table.\n+   * @param jsc instance of {@link JavaSparkContext} to use.\n+   * @param basePath base path of the dataset.\n+   * @param toVersion version to which upgrade/downgrade to be done.\n+   * @return 0 if success, else -1.\n+   * @throws Exception\n+   */\n+  protected static int upgradeOrDowngradeHoodieDataset(JavaSparkContext jsc, String basePath, String toVersion) throws Exception {\n+    HoodieWriteConfig config = getWriteConfig(basePath);\n+    HoodieTableMetaClient metaClient = ClientUtils.createMetaClient(jsc.hadoopConfiguration(), config, false);\n+    try {\n+      UpgradeDowngradeUtil.doUpgradeOrDowngrade(metaClient, HoodieTableVersion.valueOf(toVersion), config, jsc, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njk4OTM3Ng==", "bodyText": "I am not sure if migrate will be the right terminology to use here. Isn't migrate used to move from one system to another? This is more of an upgrade version or downgrade version right within the same system(hudi).", "url": "https://github.com/apache/hudi/pull/1858#discussion_r466989376", "createdAt": "2020-08-07T11:46:25Z", "author": {"login": "nsivabalan"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -329,9 +341,34 @@ private static int deleteSavepoint(JavaSparkContext jsc, String savepointTime, S\n     }\n   }\n \n+  /**\n+   * Upgrade or downgrade hoodie table.\n+   * @param jsc instance of {@link JavaSparkContext} to use.\n+   * @param basePath base path of the dataset.\n+   * @param toVersion version to which upgrade/downgrade to be done.\n+   * @return 0 if success, else -1.\n+   * @throws Exception\n+   */\n+  protected static int upgradeOrDowngradeHoodieDataset(JavaSparkContext jsc, String basePath, String toVersion) throws Exception {\n+    HoodieWriteConfig config = getWriteConfig(basePath);\n+    HoodieTableMetaClient metaClient = ClientUtils.createMetaClient(jsc.hadoopConfiguration(), config, false);\n+    try {\n+      UpgradeDowngradeUtil.doUpgradeOrDowngrade(metaClient, HoodieTableVersion.valueOf(toVersion), config, jsc, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMzU3Mg=="}, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzQ4NTM2OA==", "bodyText": "not really. migrate is a general term. :)", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467485368", "createdAt": "2020-08-08T17:17:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -329,9 +341,34 @@ private static int deleteSavepoint(JavaSparkContext jsc, String savepointTime, S\n     }\n   }\n \n+  /**\n+   * Upgrade or downgrade hoodie table.\n+   * @param jsc instance of {@link JavaSparkContext} to use.\n+   * @param basePath base path of the dataset.\n+   * @param toVersion version to which upgrade/downgrade to be done.\n+   * @return 0 if success, else -1.\n+   * @throws Exception\n+   */\n+  protected static int upgradeOrDowngradeHoodieDataset(JavaSparkContext jsc, String basePath, String toVersion) throws Exception {\n+    HoodieWriteConfig config = getWriteConfig(basePath);\n+    HoodieTableMetaClient metaClient = ClientUtils.createMetaClient(jsc.hadoopConfiguration(), config, false);\n+    try {\n+      UpgradeDowngradeUtil.doUpgradeOrDowngrade(metaClient, HoodieTableVersion.valueOf(toVersion), config, jsc, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMzU3Mg=="}, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjM2MDg1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMzozMDo1NVrOG7L7ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQwNjoyNjozMVrOG94kbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNDY4Mg==", "bodyText": "we should do this no matter, whether rollback using markers is on /off", "url": "https://github.com/apache/hudi/pull/1858#discussion_r464714682", "createdAt": "2020-08-03T23:30:55Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -186,10 +188,14 @@ public HoodieMetrics getMetrics() {\n    * Get HoodieTable and init {@link Timer.Context}.\n    *\n    * @param operationType write operation type\n+   * @param instantTime current inflight instant time\n    * @return HoodieTable\n    */\n-  protected HoodieTable getTableAndInitCtx(WriteOperationType operationType) {\n+  protected HoodieTable getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n+    if (config.shouldRollbackUsingMarkers()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njk5MTE4Nw==", "bodyText": "this was my thinking behind this guard. If someone wishes to stay with list based rollback, why execute this upgrade this which specifically does some work to assist in marker based rollback which will never be used since marked based rollback is not going to be used at all. I am not very strong on this though. But tests need some fixes though as I rely on creating commits and marker files using client at first, by disabling marker based rollback. If we remove this guard, then tests need to manually create all data files and marker files. I am not saying that as a reason to keep this guard, just saying we have some extra work to be done.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r466991187", "createdAt": "2020-08-07T11:50:32Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -186,10 +188,14 @@ public HoodieMetrics getMetrics() {\n    * Get HoodieTable and init {@link Timer.Context}.\n    *\n    * @param operationType write operation type\n+   * @param instantTime current inflight instant time\n    * @return HoodieTable\n    */\n-  protected HoodieTable getTableAndInitCtx(WriteOperationType operationType) {\n+  protected HoodieTable getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n+    if (config.shouldRollbackUsingMarkers()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNDY4Mg=="}, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU0MzE1MA==", "bodyText": "tests actually pass. I have changes where this solely gated by the table version and not any other config.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467543150", "createdAt": "2020-08-09T06:26:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -186,10 +188,14 @@ public HoodieMetrics getMetrics() {\n    * Get HoodieTable and init {@link Timer.Context}.\n    *\n    * @param operationType write operation type\n+   * @param instantTime current inflight instant time\n    * @return HoodieTable\n    */\n-  protected HoodieTable getTableAndInitCtx(WriteOperationType operationType) {\n+  protected HoodieTable getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n+    if (config.shouldRollbackUsingMarkers()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNDY4Mg=="}, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjM2MzgxOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMzozMjoyMlrOG7L9bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMTo1MDo0MVrOG9W4qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNTExNw==", "bodyText": "is this just moving code in bulk?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r464715117", "createdAt": "2020-08-03T23:32:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java", "diffHunk": "@@ -63,4 +84,156 @@ static HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRoll\n     return new HoodieRollbackStat(stat1.getPartitionPath(), successDeleteFiles, failedDeleteFiles, commandBlocksCount);\n   }\n \n+  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njk5MTI3NQ==", "bodyText": "yes", "url": "https://github.com/apache/hudi/pull/1858#discussion_r466991275", "createdAt": "2020-08-07T11:50:41Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java", "diffHunk": "@@ -63,4 +84,156 @@ static HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRoll\n     return new HoodieRollbackStat(stat1.getPartitionPath(), successDeleteFiles, failedDeleteFiles, commandBlocksCount);\n   }\n \n+  /**", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNTExNw=="}, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwNjgxMzU1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngradeUtil.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDozODoyOFrOG72G2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOFQxNzoxNToyMFrOG91CQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNTY1OA==", "bodyText": "@nsivabalan Can a user control the hoodie layout version manually from the HoodieWriteConfig. Say, choose the older timeline layout for 0.6.0 in which case there is no need to upgrade ?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r465405658", "createdAt": "2020-08-05T00:38:28Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngradeUtil.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTg0NDc4MQ==", "bodyText": "why feel apprehensive about upgrading? I mean, why do you want to upgrade to 0.6.0 but want to avoid the upgrade step? IMO, we should not have such knobs. But we do have knobs though, the marker based rollbacks. If you disable marker based rollback, this upgrade step may not take place. But this part of code is yet to be reviewed which I coded it up. Wanted to see if @bvaradar and other reviewers agrees that the upgrade step should be guarded by the marker based rollback config.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r465844781", "createdAt": "2020-08-05T16:17:26Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngradeUtil.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNTY1OA=="}, "originalCommit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTg3NDIwNA==", "bodyText": "@nsivabalan Can you point me to the part of the code that you are referring to ?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r465874204", "createdAt": "2020-08-05T17:04:07Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngradeUtil.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNTY1OA=="}, "originalCommit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzQ4NTI1MA==", "bodyText": "We should upgrade no matter what - regardless of marker based rollback being on/off, regardless of timeline layout version. as of now, timeline layout version is a config that can be controlled manually. in a future version, say 1->2 we can force migrate timeline line layout if need be.\nMeta point is: making the decision to upgrade/downgrade based on write config is a problematic thing IMO. It makes reasoning with actions done during upgrade be based on configs, which can be changed in subsequent writes. We need to be able to reason about state of dataset after upgrade, and convince ourselves that any config change after that point would work well.\ni.e even if markerBasedRollback=disabled during upgrade, we prepare the dataset such that it can be turned on at any given point from there on.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467485250", "createdAt": "2020-08-08T17:15:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngradeUtil.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNTY1OA=="}, "originalCommit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMjI0NDY3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwODo1OToyM1rOG8pyBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwODo1OToyM1rOG8pyBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI1MjI5NA==", "bodyText": "we should keep these to just HoodieClientTestUtils. since markers are just an artifact of the client", "url": "https://github.com/apache/hudi/pull/1858#discussion_r466252294", "createdAt": "2020-08-06T08:59:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java", "diffHunk": "@@ -279,6 +287,23 @@ public static String createDataFile(String basePath, String partitionPath, Strin\n     return fileID;\n   }\n \n+  public static void createMarkerFile(String basePath, String partitionPath, String instantTime, String dataFileName) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea18853c5d5c52145eabf56b99e7e639beb5e5db"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMTEzMTM5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNDoxMjo0N1rOG97WHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxOToyODoyMlrOG99Pzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU4ODYzNw==", "bodyText": "this needs fixing. This will issue commit via client and since you have removed the guard to execute upgrade (even if marker based is disabled), upgrade would have already been executed. if I am not wrong, below command of\nUpgradeDowngrade.run(metaClient, HoodieTableVersion.ONE, cfg, jsc, null); \n\nis a no op since already the table version is upgraded.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467588637", "createdAt": "2020-08-09T14:12:47Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,405 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";\n+\n+  public static Stream<Arguments> configParams() {\n+    Object[][] data = new Object[][] {\n+            {true, HoodieTableType.COPY_ON_WRITE}, {false, HoodieTableType.COPY_ON_WRITE},\n+            {true, HoodieTableType.MERGE_ON_READ}, {false, HoodieTableType.MERGE_ON_READ}\n+    };\n+    return Stream.of(data).map(Arguments::of);\n+  }\n+\n+  @Test\n+  public void testLeftOverUpdatedPropFileCleanup() throws IOException {\n+    testUpgradeInternal(true, true, HoodieTableType.MERGE_ON_READ);\n+  }\n+\n+  @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @MethodSource(\"configParams\")\n+  public void testUpgrade(boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    testUpgradeInternal(false, deletePartialMarkerFiles, tableType);\n+  }\n+\n+  public void testUpgradeInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    // init config, table and client.\n+    Map<String, String> params = new HashMap<>();\n+    if (tableType == HoodieTableType.MERGE_ON_READ) {\n+      params.put(HOODIE_TABLE_TYPE_PROP_NAME, HoodieTableType.MERGE_ON_READ.name());\n+      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n+    HoodieWriteClient client = getHoodieWriteClient(cfg);\n+\n+    // prepare data. Make 2 commits, in which 2nd is not committed.\n+    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n+    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n+    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU4ODY5Nw==", "bodyText": "this is what I mentioned earlier that tests need quite a bit of fixing if you remove that guard :)", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467588697", "createdAt": "2020-08-09T14:13:30Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,405 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";\n+\n+  public static Stream<Arguments> configParams() {\n+    Object[][] data = new Object[][] {\n+            {true, HoodieTableType.COPY_ON_WRITE}, {false, HoodieTableType.COPY_ON_WRITE},\n+            {true, HoodieTableType.MERGE_ON_READ}, {false, HoodieTableType.MERGE_ON_READ}\n+    };\n+    return Stream.of(data).map(Arguments::of);\n+  }\n+\n+  @Test\n+  public void testLeftOverUpdatedPropFileCleanup() throws IOException {\n+    testUpgradeInternal(true, true, HoodieTableType.MERGE_ON_READ);\n+  }\n+\n+  @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @MethodSource(\"configParams\")\n+  public void testUpgrade(boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    testUpgradeInternal(false, deletePartialMarkerFiles, tableType);\n+  }\n+\n+  public void testUpgradeInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    // init config, table and client.\n+    Map<String, String> params = new HashMap<>();\n+    if (tableType == HoodieTableType.MERGE_ON_READ) {\n+      params.put(HOODIE_TABLE_TYPE_PROP_NAME, HoodieTableType.MERGE_ON_READ.name());\n+      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n+    HoodieWriteClient client = getHoodieWriteClient(cfg);\n+\n+    // prepare data. Make 2 commits, in which 2nd is not committed.\n+    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n+    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n+    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU4ODYzNw=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxMzg5Mw==", "bodyText": "nvm. I overlooked this line\nmetaClient.getTableConfig().setTableVersion(HoodieTableVersion.ZERO);\n\nTests are fine.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467613893", "createdAt": "2020-08-09T18:26:56Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,405 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";\n+\n+  public static Stream<Arguments> configParams() {\n+    Object[][] data = new Object[][] {\n+            {true, HoodieTableType.COPY_ON_WRITE}, {false, HoodieTableType.COPY_ON_WRITE},\n+            {true, HoodieTableType.MERGE_ON_READ}, {false, HoodieTableType.MERGE_ON_READ}\n+    };\n+    return Stream.of(data).map(Arguments::of);\n+  }\n+\n+  @Test\n+  public void testLeftOverUpdatedPropFileCleanup() throws IOException {\n+    testUpgradeInternal(true, true, HoodieTableType.MERGE_ON_READ);\n+  }\n+\n+  @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @MethodSource(\"configParams\")\n+  public void testUpgrade(boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    testUpgradeInternal(false, deletePartialMarkerFiles, tableType);\n+  }\n+\n+  public void testUpgradeInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    // init config, table and client.\n+    Map<String, String> params = new HashMap<>();\n+    if (tableType == HoodieTableType.MERGE_ON_READ) {\n+      params.put(HOODIE_TABLE_TYPE_PROP_NAME, HoodieTableType.MERGE_ON_READ.name());\n+      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n+    HoodieWriteClient client = getHoodieWriteClient(cfg);\n+\n+    // prepare data. Make 2 commits, in which 2nd is not committed.\n+    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n+    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n+    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU4ODYzNw=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxOTc5MQ==", "bodyText": "This is what I was trying to convey. As part of this call, (2 commits), already upgrade step would have been executed. But, we also reset hoodie table version and call upgrade explicitly after this. So, should be fine. Not sure if you are aware of this. Will let you take a call. If you are ok, patch is good to be merged.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467619791", "createdAt": "2020-08-09T19:28:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,405 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";\n+\n+  public static Stream<Arguments> configParams() {\n+    Object[][] data = new Object[][] {\n+            {true, HoodieTableType.COPY_ON_WRITE}, {false, HoodieTableType.COPY_ON_WRITE},\n+            {true, HoodieTableType.MERGE_ON_READ}, {false, HoodieTableType.MERGE_ON_READ}\n+    };\n+    return Stream.of(data).map(Arguments::of);\n+  }\n+\n+  @Test\n+  public void testLeftOverUpdatedPropFileCleanup() throws IOException {\n+    testUpgradeInternal(true, true, HoodieTableType.MERGE_ON_READ);\n+  }\n+\n+  @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @MethodSource(\"configParams\")\n+  public void testUpgrade(boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    testUpgradeInternal(false, deletePartialMarkerFiles, tableType);\n+  }\n+\n+  public void testUpgradeInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    // init config, table and client.\n+    Map<String, String> params = new HashMap<>();\n+    if (tableType == HoodieTableType.MERGE_ON_READ) {\n+      params.put(HOODIE_TABLE_TYPE_PROP_NAME, HoodieTableType.MERGE_ON_READ.name());\n+      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n+    HoodieWriteClient client = getHoodieWriteClient(cfg);\n+\n+    // prepare data. Make 2 commits, in which 2nd is not committed.\n+    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n+    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n+    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU4ODYzNw=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMTIzNjgzOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestUpgradeDowngradeCommand.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNjoxNDowNVrOG98IBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNzoyNDo1MlrOG98iKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMTQxNA==", "bodyText": "probably you might have to follow something like this in TestUpgradeDowngrade.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467601414", "createdAt": "2020-08-09T16:14:05Z", "author": {"login": "nsivabalan"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestUpgradeDowngradeCommand.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.testutils.AbstractShellIntegrationTest;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Properties;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+/**\n+ * Tests {@link UpgradeOrDowngradeCommand}.\n+ */\n+public class TestUpgradeDowngradeCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @BeforeEach\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    //Create some commits files and parquet files\n+    String commitTime1 = \"100\";\n+    String commitTime2 = \"101\";\n+    HoodieTestDataGenerator.writePartitionMetadata(fs, HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS, tablePath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwODEwNA==", "bodyText": "not following. In general, this test can be much simplified. see TestMarkerBasedRollbackStrategy . I did not have time to fix this.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467608104", "createdAt": "2020-08-09T17:24:52Z", "author": {"login": "vinothchandar"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestUpgradeDowngradeCommand.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.testutils.AbstractShellIntegrationTest;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Properties;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+/**\n+ * Tests {@link UpgradeOrDowngradeCommand}.\n+ */\n+public class TestUpgradeDowngradeCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @BeforeEach\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    //Create some commits files and parquet files\n+    String commitTime1 = \"100\";\n+    String commitTime2 = \"101\";\n+    HoodieTestDataGenerator.writePartitionMetadata(fs, HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS, tablePath);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMTQxNA=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMTI1MTU0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNjozMjo0NFrOG98PFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNzoyMzoxMlrOG98hmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzIyMA==", "bodyText": "updated/upgraded(file is named as HOODIE_UPDATED_PROPERTY_FILE). Lets use same terminology everywhere. Ignore addressing renaming/java docs/refactoring comments for now. Let's get the patch in for now. But leaving comments so that I can take it up after 0.6.0 release.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467603220", "createdAt": "2020-08-09T16:32:44Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Date;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngrade {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngrade.class);\n+  public static final String HOODIE_UPDATED_PROPERTY_FILE = \"hoodie.properties.updated\";\n+\n+  private HoodieTableMetaClient metaClient;\n+  private HoodieWriteConfig config;\n+  private JavaSparkContext jsc;\n+  private transient FileSystem fs;\n+  private Path updatedPropsFilePath;\n+  private Path propsFilePath;\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   *\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie version was upgraded to 0.6.0,\n+   * Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded\n+   * to pre 0.6.0 or to Hoodie table version 0, then some downgrade steps need to be executed before proceeding w/ any writes.\n+   *\n+   * On a high level, these are the steps performed\n+   *\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Delete any left over .upgraded from previous upgrade/downgrade", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwNzk2Mw==", "bodyText": "thats a typo. let me fix the comments. Code does have the same terminology", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467607963", "createdAt": "2020-08-09T17:23:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Date;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngrade {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngrade.class);\n+  public static final String HOODIE_UPDATED_PROPERTY_FILE = \"hoodie.properties.updated\";\n+\n+  private HoodieTableMetaClient metaClient;\n+  private HoodieWriteConfig config;\n+  private JavaSparkContext jsc;\n+  private transient FileSystem fs;\n+  private Path updatedPropsFilePath;\n+  private Path propsFilePath;\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   *\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie version was upgraded to 0.6.0,\n+   * Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded\n+   * to pre 0.6.0 or to Hoodie table version 0, then some downgrade steps need to be executed before proceeding w/ any writes.\n+   *\n+   * On a high level, these are the steps performed\n+   *\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Delete any left over .upgraded from previous upgrade/downgrade", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzIyMA=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMTI1Mjc0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNjozNDoyNVrOG98PqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxODoyMjoxMVrOG9826Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzM2OQ==", "bodyText": "why use same name for this method and for the other method too?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467603369", "createdAt": "2020-08-09T16:34:25Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Date;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngrade {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngrade.class);\n+  public static final String HOODIE_UPDATED_PROPERTY_FILE = \"hoodie.properties.updated\";\n+\n+  private HoodieTableMetaClient metaClient;\n+  private HoodieWriteConfig config;\n+  private JavaSparkContext jsc;\n+  private transient FileSystem fs;\n+  private Path updatedPropsFilePath;\n+  private Path propsFilePath;\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   *\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie version was upgraded to 0.6.0,\n+   * Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded\n+   * to pre 0.6.0 or to Hoodie table version 0, then some downgrade steps need to be executed before proceeding w/ any writes.\n+   *\n+   * On a high level, these are the steps performed\n+   *\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Delete any left over .upgraded from previous upgrade/downgrade\n+   * Step3 : If version are different, perform upgrade/downgrade.\n+   * Step4 : Copy hoodie.properties -> hoodie.properties.upgraded with the version updated\n+   * Step6 : Rename hoodie.properties.updated to hoodie.properties\n+   * </p>\n+   *\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @param toVersion version to which upgrade or downgrade has to be done.\n+   * @param config instance of {@link HoodieWriteConfig} to use.\n+   * @param jsc instance of {@link JavaSparkContext} to use.\n+   * @param instantTime current instant time that should not be touched.\n+   */\n+  public static void run(HoodieTableMetaClient metaClient, HoodieTableVersion toVersion, HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwNzkxNA==", "bodyText": "why not?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467607914", "createdAt": "2020-08-09T17:22:36Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Date;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngrade {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngrade.class);\n+  public static final String HOODIE_UPDATED_PROPERTY_FILE = \"hoodie.properties.updated\";\n+\n+  private HoodieTableMetaClient metaClient;\n+  private HoodieWriteConfig config;\n+  private JavaSparkContext jsc;\n+  private transient FileSystem fs;\n+  private Path updatedPropsFilePath;\n+  private Path propsFilePath;\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   *\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie version was upgraded to 0.6.0,\n+   * Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded\n+   * to pre 0.6.0 or to Hoodie table version 0, then some downgrade steps need to be executed before proceeding w/ any writes.\n+   *\n+   * On a high level, these are the steps performed\n+   *\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Delete any left over .upgraded from previous upgrade/downgrade\n+   * Step3 : If version are different, perform upgrade/downgrade.\n+   * Step4 : Copy hoodie.properties -> hoodie.properties.upgraded with the version updated\n+   * Step6 : Rename hoodie.properties.updated to hoodie.properties\n+   * </p>\n+   *\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @param toVersion version to which upgrade or downgrade has to be done.\n+   * @param config instance of {@link HoodieWriteConfig} to use.\n+   * @param jsc instance of {@link JavaSparkContext} to use.\n+   * @param instantTime current instant time that should not be touched.\n+   */\n+  public static void run(HoodieTableMetaClient metaClient, HoodieTableVersion toVersion, HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzM2OQ=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxMzQxNw==", "bodyText": "you could use. but at first sight, I got confused that we can calling the same method within run().", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467613417", "createdAt": "2020-08-09T18:22:11Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Date;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngrade {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngrade.class);\n+  public static final String HOODIE_UPDATED_PROPERTY_FILE = \"hoodie.properties.updated\";\n+\n+  private HoodieTableMetaClient metaClient;\n+  private HoodieWriteConfig config;\n+  private JavaSparkContext jsc;\n+  private transient FileSystem fs;\n+  private Path updatedPropsFilePath;\n+  private Path propsFilePath;\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   *\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie version was upgraded to 0.6.0,\n+   * Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded\n+   * to pre 0.6.0 or to Hoodie table version 0, then some downgrade steps need to be executed before proceeding w/ any writes.\n+   *\n+   * On a high level, these are the steps performed\n+   *\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Delete any left over .upgraded from previous upgrade/downgrade\n+   * Step3 : If version are different, perform upgrade/downgrade.\n+   * Step4 : Copy hoodie.properties -> hoodie.properties.upgraded with the version updated\n+   * Step6 : Rename hoodie.properties.updated to hoodie.properties\n+   * </p>\n+   *\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @param toVersion version to which upgrade or downgrade has to be done.\n+   * @param config instance of {@link HoodieWriteConfig} to use.\n+   * @param jsc instance of {@link JavaSparkContext} to use.\n+   * @param instantTime current instant time that should not be touched.\n+   */\n+  public static void run(HoodieTableMetaClient metaClient, HoodieTableVersion toVersion, HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzM2OQ=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMTI1NzYyOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNjo0MDoyMlrOG98R_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNjo0MDoyMlrOG98R_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzk2Nw==", "bodyText": "comments need fixing.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467603967", "createdAt": "2020-08-09T16:40:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,405 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";\n+\n+  public static Stream<Arguments> configParams() {\n+    Object[][] data = new Object[][] {\n+            {true, HoodieTableType.COPY_ON_WRITE}, {false, HoodieTableType.COPY_ON_WRITE},\n+            {true, HoodieTableType.MERGE_ON_READ}, {false, HoodieTableType.MERGE_ON_READ}\n+    };\n+    return Stream.of(data).map(Arguments::of);\n+  }\n+\n+  @Test\n+  public void testLeftOverUpdatedPropFileCleanup() throws IOException {\n+    testUpgradeInternal(true, true, HoodieTableType.MERGE_ON_READ);\n+  }\n+\n+  @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @MethodSource(\"configParams\")\n+  public void testUpgrade(boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    testUpgradeInternal(false, deletePartialMarkerFiles, tableType);\n+  }\n+\n+  public void testUpgradeInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    // init config, table and client.\n+    Map<String, String> params = new HashMap<>();\n+    if (tableType == HoodieTableType.MERGE_ON_READ) {\n+      params.put(HOODIE_TABLE_TYPE_PROP_NAME, HoodieTableType.MERGE_ON_READ.name());\n+      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n+    HoodieWriteClient client = getHoodieWriteClient(cfg);\n+\n+    // prepare data. Make 2 commits, in which 2nd is not committed.\n+    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n+    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n+    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n+\n+    HoodieTable<?> table = this.getHoodieTable(metaClient, cfg);\n+    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n+\n+    // delete one of the marker files in 2nd commit if need be.\n+    MarkerFiles markerFiles = new MarkerFiles(table, commitInstant.getTimestamp());\n+    List<String> markerPaths = markerFiles.allMarkerFilePaths();\n+    if (deletePartialMarkerFiles) {\n+      String toDeleteMarkerFile = markerPaths.get(0);\n+      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n+      markerPaths.remove(toDeleteMarkerFile);\n+    }\n+\n+    // set hoodie.table.version to 0 in hoodie.properties file\n+    metaClient.getTableConfig().setTableVersion(HoodieTableVersion.ZERO);\n+\n+    // if induce residues are set, copy property file to orig file.\n+    if (induceResiduesFromPrevUpgrade) {\n+      createResidualFile();\n+    }\n+\n+    // should re-create marker files for 2nd commit since its pending. If there was any residues, no upgrade steps should happen except for updating the hoodie.table.version", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMTI2MzA3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNjo0NzoyNFrOG98Ulw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxODoyMToxN1rOG982mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwNDYzMQ==", "bodyText": "sorry, I don't get why we need this here. If properties contain table type and table name, why bail out?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467604631", "createdAt": "2020-08-09T16:47:24Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -96,6 +97,8 @@ public HoodieTableConfig(FileSystem fs, String metaPath, String payloadClassName\n       throw new HoodieIOException(\"Could not load Hoodie properties from \" + propertyPath, e);\n     }\n     this.props = props;\n+    ValidationUtils.checkArgument(props.containsKey(HOODIE_TABLE_TYPE_PROP_NAME) && props.containsKey(HOODIE_TABLE_NAME_PROP_NAME),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwODgyNA==", "bodyText": "these properties are written always. so this indicates a corrupted file (with high probability) . Actually what we discussed is that there wont be a corrupted/partial hoodie.properties file given even s3 (or gcs) guarantees partial writes won't be visible. So this is just additional safety", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467608824", "createdAt": "2020-08-09T17:32:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -96,6 +97,8 @@ public HoodieTableConfig(FileSystem fs, String metaPath, String payloadClassName\n       throw new HoodieIOException(\"Could not load Hoodie properties from \" + propertyPath, e);\n     }\n     this.props = props;\n+    ValidationUtils.checkArgument(props.containsKey(HOODIE_TABLE_TYPE_PROP_NAME) && props.containsKey(HOODIE_TABLE_NAME_PROP_NAME),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwNDYzMQ=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwOTU3NQ==", "bodyText": "sorry, I don't get why we need this here. If properties contain table type and table name, why bail out?\n\nwe bail out if these props are not present.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467609575", "createdAt": "2020-08-09T17:40:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -96,6 +97,8 @@ public HoodieTableConfig(FileSystem fs, String metaPath, String payloadClassName\n       throw new HoodieIOException(\"Could not load Hoodie properties from \" + propertyPath, e);\n     }\n     this.props = props;\n+    ValidationUtils.checkArgument(props.containsKey(HOODIE_TABLE_TYPE_PROP_NAME) && props.containsKey(HOODIE_TABLE_NAME_PROP_NAME),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwNDYzMQ=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxMzMzOA==", "bodyText": "ok, was confused as to how this related to this patch of upgrade/downgrade.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467613338", "createdAt": "2020-08-09T18:21:17Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -96,6 +97,8 @@ public HoodieTableConfig(FileSystem fs, String metaPath, String payloadClassName\n       throw new HoodieIOException(\"Could not load Hoodie properties from \" + propertyPath, e);\n     }\n     this.props = props;\n+    ValidationUtils.checkArgument(props.containsKey(HOODIE_TABLE_TYPE_PROP_NAME) && props.containsKey(HOODIE_TABLE_NAME_PROP_NAME),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwNDYzMQ=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMTM2ODgxOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxOTowNToyMVrOG99G0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxOTowNToyMVrOG99G0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxNzQ5MA==", "bodyText": "minor: there are only 2 args.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467617490", "createdAt": "2020-08-09T19:05:21Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ca5adc0039c68235ee08623336b4d90d6f1f338"}, "originalPosition": 75}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4340, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}