{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkxMzEyNjcz", "number": 1421, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwMjowNzowM1rODpu4TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMDo1NzozMFrODp0ygQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MDg2Mjg0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwMjowNzowM1rOF5F8gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwNDoxOTowMVrOF5HVUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDU2Mg==", "bodyText": "I think its a reasonable thing to parallelize this.. Listing of cleaning etc has been parallelized like this before. Should be safe to do.\nAlso can we pull this block into a method? getSmallFiles(partitionPaths)", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395410562", "createdAt": "2020-03-20T02:07:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQzMzI5Ng==", "bodyText": "good point, will move to method like getSmallFilesForPartitions(partitionPaths, jsc)", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395433296", "createdAt": "2020-03-20T04:19:01Z", "author": {"login": "ffcchi"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDU2Mg=="}, "originalCommit": null, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MDg2MzQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwMjowNzozOFrOF5F82Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwNDoxNzo0OFrOF5HUkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDY0OQ==", "bodyText": "Can we change that code so that it always lists in parallel.. i.e no need for the fallback in this if block...", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395410649", "createdAt": "2020-03-20T02:07:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {\n+        //Parellelize the GetSmallFile Operation by using RDDs\n+        List<String> partitionPathsList = new ArrayList<>(partitionPaths);\n+        JavaRDD<String> partitionPathRdds = jsc.parallelize(partitionPathsList, partitionPathsList.size());\n+        List<Tuple2<String, List<SmallFile>>> partitionSmallFileTuples =\n+                partitionPathRdds.map(it -> new Tuple2<String, List<SmallFile>>(it, getSmallFiles(it))).collect();\n+\n+        for (Tuple2<String, List<SmallFile>> tuple : partitionSmallFileTuples) {\n+          partitionSmallFilesMap.put(tuple._1, tuple._2);\n+        }\n+      }\n+\n       for (String partitionPath : partitionPaths) {\n         WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n         if (pStat.getNumInserts() > 0) {\n \n-          List<SmallFile> smallFiles = getSmallFiles(partitionPath);\n+          List<SmallFile> smallFiles;\n+          if (partitionSmallFilesMap.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQzMzEwNA==", "bodyText": "sounds good. will do", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395433104", "createdAt": "2020-03-20T04:17:48Z", "author": {"login": "ffcchi"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {\n+        //Parellelize the GetSmallFile Operation by using RDDs\n+        List<String> partitionPathsList = new ArrayList<>(partitionPaths);\n+        JavaRDD<String> partitionPathRdds = jsc.parallelize(partitionPathsList, partitionPathsList.size());\n+        List<Tuple2<String, List<SmallFile>>> partitionSmallFileTuples =\n+                partitionPathRdds.map(it -> new Tuple2<String, List<SmallFile>>(it, getSmallFiles(it))).collect();\n+\n+        for (Tuple2<String, List<SmallFile>> tuple : partitionSmallFileTuples) {\n+          partitionSmallFilesMap.put(tuple._1, tuple._2);\n+        }\n+      }\n+\n       for (String partitionPath : partitionPaths) {\n         WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n         if (pStat.getNumInserts() > 0) {\n \n-          List<SmallFile> smallFiles = getSmallFiles(partitionPath);\n+          List<SmallFile> smallFiles;\n+          if (partitionSmallFilesMap.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDY0OQ=="}, "originalCommit": null, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MDg2NTYyOnYy", "diffSide": "LEFT", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwMjowOTozM1rOF5F-MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQwNDoxNjo1N1rOF5HUDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDk5Mw==", "bodyText": "guess this is not needed anymore since we aggregate it into a map already.. (never liked this line. so lgtm)", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395410993", "createdAt": "2020-03-20T02:09:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java", "diffHunk": "@@ -374,16 +374,12 @@ public void finalizeWrite(JavaSparkContext jsc, String instantTs, List<HoodieWri\n             sf.location = new HoodieRecordLocation(FSUtils.getCommitTime(filename), FSUtils.getFileId(filename));\n             sf.sizeBytes = getTotalFileSize(smallFileSlice);\n             smallFileLocations.add(sf);\n-            // Update the global small files list\n-            smallFiles.add(sf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQzMjk3Mw==", "bodyText": "the UpsertPartitioner actually holds a global list small files for all partitions. since the work will be distributed to Task nodes, the method can't  the small file to global list anymore. So I moved the logic of updating that global list out side of the parallelism section.", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395432973", "createdAt": "2020-03-20T04:16:57Z", "author": {"login": "ffcchi"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java", "diffHunk": "@@ -374,16 +374,12 @@ public void finalizeWrite(JavaSparkContext jsc, String instantTs, List<HoodieWri\n             sf.location = new HoodieRecordLocation(FSUtils.getCommitTime(filename), FSUtils.getFileId(filename));\n             sf.sizeBytes = getTotalFileSize(smallFileSlice);\n             smallFileLocations.add(sf);\n-            // Update the global small files list\n-            smallFiles.add(sf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDk5Mw=="}, "originalCommit": null, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MTcyNjUxOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMDoyMToyOVrOF5OLww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNDo1OTowN1rOF5XQYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU0NTUzOQ==", "bodyText": "Do we really need all this passing around of jsc object ? We can just directly pass it from within this function right, as its inherited.", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395545539", "createdAt": "2020-03-20T10:21:29Z", "author": {"login": "umehrot2"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -486,11 +486,11 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n     return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, commitTime);\n   }\n \n-  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile) {\n+  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile, JavaSparkContext jsc) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY2NDIxMQ==", "bodyText": "nice catch, the HoodieWriterClient do have jsc already.  will update", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395664211", "createdAt": "2020-03-20T14:14:52Z", "author": {"login": "ffcchi"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -486,11 +486,11 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n     return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, commitTime);\n   }\n \n-  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile) {\n+  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile, JavaSparkContext jsc) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU0NTUzOQ=="}, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NDE3OA==", "bodyText": "+1", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395694178", "createdAt": "2020-03-20T14:59:07Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -486,11 +486,11 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n     return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, commitTime);\n   }\n \n-  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile) {\n+  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile, JavaSparkContext jsc) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU0NTUzOQ=="}, "originalCommit": null, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MTc3NTQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMDozODowMFrOF5Orpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMDozODowMFrOF5Orpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU1MzcwMw==", "bodyText": "nit: probably remove this comment", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395553703", "createdAt": "2020-03-20T10:38:00Z", "author": {"login": "umehrot2"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {\n+        //Parellelize the GetSmallFile Operation by using RDDs", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MTgzMTA1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMDo1NzozMFrOF5PPGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMDo1NzozMFrOF5PPGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU2Mjc3OQ==", "bodyText": "You may want to refactor this to something like:\npartitionSmallFilesMap = partitionPathRdds.mapToPair((PairFunction<String, String, List<SmallFile>>) \n  partitionPath -> new Tuple2<>(partitionPath, getSmallFiles(partitionPath))).collectAsMap();", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395562779", "createdAt": "2020-03-20T10:57:30Z", "author": {"login": "umehrot2"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {\n+        //Parellelize the GetSmallFile Operation by using RDDs\n+        List<String> partitionPathsList = new ArrayList<>(partitionPaths);\n+        JavaRDD<String> partitionPathRdds = jsc.parallelize(partitionPathsList, partitionPathsList.size());\n+        List<Tuple2<String, List<SmallFile>>> partitionSmallFileTuples =\n+                partitionPathRdds.map(it -> new Tuple2<String, List<SmallFile>>(it, getSmallFiles(it))).collect();\n+\n+        for (Tuple2<String, List<SmallFile>> tuple : partitionSmallFileTuples) {\n+          partitionSmallFilesMap.put(tuple._1, tuple._2);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 61}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4942, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}