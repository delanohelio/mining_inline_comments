{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcwOTgwNjM3", "number": 1306, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoxOTozN1rODdJd7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoyMDo0NVrODdJfFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODkwNDEyOnYy", "diffSide": "RIGHT", "path": "docs/_docs/1_1_quick_start_guide.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoxOTozN1rOFlleSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoxOTozN1rOFlleSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NTU5Mg==", "bodyText": "hudi_ro_table -> hudi_trips_snapshot here as well.", "url": "https://github.com/apache/hudi/pull/1306#discussion_r374955592", "createdAt": "2020-02-04T22:19:37Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -176,28 +176,28 @@ Delete records for the HoodieKeys passed in.\n \n ```scala\n // fetch total records count\n-spark.sql(\"select uuid, partitionPath from hudi_ro_table\").count()\n+spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n // fetch two records to be deleted\n-val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table\").limit(2)\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").limit(2)\n \n // issue deletes\n val deletes = dataGen.generateDeletes(ds.collectAsList())\n val df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\n-df.write.format(\"org.apache.hudi\").\n-options(getQuickstartWriteConfigs).\n-option(OPERATION_OPT_KEY,\"delete\").\n-option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n-option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n-option(TABLE_NAME, tableName).\n-mode(Append).\n-save(basePath);\n+df.write.format(\"hudi\").\n+  options(getQuickstartWriteConfigs).\n+  option(OPERATION_OPT_KEY,\"delete\").\n+  option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+  option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(TABLE_NAME, tableName).\n+  mode(Append).\n+  save(basePath)\n \n // run the same read query as above.\n val roAfterDeleteViewDF = spark.\n-    read.\n-    format(\"org.apache.hudi\").\n-    load(basePath + \"/*/*/*/*\")\n+  read.\n+  format(\"hudi\").\n+  load(basePath + \"/*/*/*/*\")\n roAfterDeleteViewDF.registerTempTable(\"hudi_ro_table\")\n // fetch should return (total - 2) records\n spark.sql(\"select uuid, partitionPath from hudi_ro_table\").count()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3890d0f5fc45eb1b90c430f97c7d12d935d4282e"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODkwNzExOnYy", "diffSide": "RIGHT", "path": "docs/_docs/1_1_quick_start_guide.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoyMDo0NVrOFllgQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwNToyNToyNlrOFlsI6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NjA5OA==", "bodyText": "consider changing hudi_ro_table -> hudi_trips_snapshot here as well for consistency ?", "url": "https://github.com/apache/hudi/pull/1306#discussion_r374956098", "createdAt": "2020-02-04T22:20:45Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -176,28 +176,28 @@ Delete records for the HoodieKeys passed in.\n \n ```scala\n // fetch total records count\n-spark.sql(\"select uuid, partitionPath from hudi_ro_table\").count()\n+spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n // fetch two records to be deleted\n-val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table\").limit(2)\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").limit(2)\n \n // issue deletes\n val deletes = dataGen.generateDeletes(ds.collectAsList())\n val df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\n-df.write.format(\"org.apache.hudi\").\n-options(getQuickstartWriteConfigs).\n-option(OPERATION_OPT_KEY,\"delete\").\n-option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n-option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n-option(TABLE_NAME, tableName).\n-mode(Append).\n-save(basePath);\n+df.write.format(\"hudi\").\n+  options(getQuickstartWriteConfigs).\n+  option(OPERATION_OPT_KEY,\"delete\").\n+  option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+  option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(TABLE_NAME, tableName).\n+  mode(Append).\n+  save(basePath)\n \n // run the same read query as above.\n val roAfterDeleteViewDF = spark.\n-    read.\n-    format(\"org.apache.hudi\").\n-    load(basePath + \"/*/*/*/*\")\n+  read.\n+  format(\"hudi\").\n+  load(basePath + \"/*/*/*/*\")\n roAfterDeleteViewDF.registerTempTable(\"hudi_ro_table\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3890d0f5fc45eb1b90c430f97c7d12d935d4282e"}, "originalPosition": 163}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTA2NDgwOQ==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/1306#discussion_r375064809", "createdAt": "2020-02-05T05:25:26Z", "author": {"login": "lamberken"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -176,28 +176,28 @@ Delete records for the HoodieKeys passed in.\n \n ```scala\n // fetch total records count\n-spark.sql(\"select uuid, partitionPath from hudi_ro_table\").count()\n+spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n // fetch two records to be deleted\n-val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table\").limit(2)\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").limit(2)\n \n // issue deletes\n val deletes = dataGen.generateDeletes(ds.collectAsList())\n val df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\n-df.write.format(\"org.apache.hudi\").\n-options(getQuickstartWriteConfigs).\n-option(OPERATION_OPT_KEY,\"delete\").\n-option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n-option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n-option(TABLE_NAME, tableName).\n-mode(Append).\n-save(basePath);\n+df.write.format(\"hudi\").\n+  options(getQuickstartWriteConfigs).\n+  option(OPERATION_OPT_KEY,\"delete\").\n+  option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+  option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(TABLE_NAME, tableName).\n+  mode(Append).\n+  save(basePath)\n \n // run the same read query as above.\n val roAfterDeleteViewDF = spark.\n-    read.\n-    format(\"org.apache.hudi\").\n-    load(basePath + \"/*/*/*/*\")\n+  read.\n+  format(\"hudi\").\n+  load(basePath + \"/*/*/*/*\")\n roAfterDeleteViewDF.registerTempTable(\"hudi_ro_table\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NjA5OA=="}, "originalCommit": {"oid": "3890d0f5fc45eb1b90c430f97c7d12d935d4282e"}, "originalPosition": 163}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 39, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}