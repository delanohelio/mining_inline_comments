{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI5NzE3NzUz", "number": 1711, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxMTo0MTo0MFrOEDaBDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxMTo0MTo0MFrOEDaBDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMDA3NDM3OnYy", "diffSide": "RIGHT", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxMTo0MTo0MFrOGgZ-IQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxMjowMjoyMFrOGgajGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjYzMzEyMQ==", "bodyText": "what do we use in the merged record reader? cc @bvaradar", "url": "https://github.com/apache/hudi/pull/1711#discussion_r436633121", "createdAt": "2020-06-08T11:41:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java", "diffHunk": "@@ -82,7 +82,7 @@ public RealtimeUnmergedRecordReader(HoodieRealtimeFileSplit split, JobConf job,\n         false, jobConf.getInt(MAX_DFS_STREAM_BUFFER_SIZE_PROP, DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE), record -> {\n           // convert Hoodie log record to Hadoop AvroWritable and buffer\n           GenericRecord rec = (GenericRecord) record.getData().getInsertValue(getReaderSchema()).get();\n-          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getWriterSchema());\n+          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getHiveSchema());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "132c811c1a8d3fc6cc2bac2ca7b4ffc0e86e024b"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjYzNjQzMQ==", "bodyText": "@vinothchandar This is a bug when using RealtimeUnmergedRecordReader, merged record reader works fine. unmerged record reader should use hive schema rather than writer schema here.", "url": "https://github.com/apache/hudi/pull/1711#discussion_r436636431", "createdAt": "2020-06-08T11:48:53Z", "author": {"login": "leesf"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java", "diffHunk": "@@ -82,7 +82,7 @@ public RealtimeUnmergedRecordReader(HoodieRealtimeFileSplit split, JobConf job,\n         false, jobConf.getInt(MAX_DFS_STREAM_BUFFER_SIZE_PROP, DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE), record -> {\n           // convert Hoodie log record to Hadoop AvroWritable and buffer\n           GenericRecord rec = (GenericRecord) record.getData().getInsertValue(getReaderSchema()).get();\n-          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getWriterSchema());\n+          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getHiveSchema());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjYzMzEyMQ=="}, "originalCommit": {"oid": "132c811c1a8d3fc6cc2bac2ca7b4ffc0e86e024b"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjY0MjU4Ng==", "bodyText": "sg.", "url": "https://github.com/apache/hudi/pull/1711#discussion_r436642586", "createdAt": "2020-06-08T12:02:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java", "diffHunk": "@@ -82,7 +82,7 @@ public RealtimeUnmergedRecordReader(HoodieRealtimeFileSplit split, JobConf job,\n         false, jobConf.getInt(MAX_DFS_STREAM_BUFFER_SIZE_PROP, DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE), record -> {\n           // convert Hoodie log record to Hadoop AvroWritable and buffer\n           GenericRecord rec = (GenericRecord) record.getData().getInsertValue(getReaderSchema()).get();\n-          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getWriterSchema());\n+          ArrayWritable aWritable = (ArrayWritable) avroToArrayWritable(rec, getHiveSchema());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjYzMzEyMQ=="}, "originalCommit": {"oid": "132c811c1a8d3fc6cc2bac2ca7b4ffc0e86e024b"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4472, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}