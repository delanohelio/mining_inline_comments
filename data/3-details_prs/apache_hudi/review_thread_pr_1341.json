{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc2ODUzMDEy", "number": 1341, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowODozOFrODg4ugg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxOTozMToxNlrODjNaVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODEwNDM0OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrintHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowODozOFrOFrVy2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxODo1Njo1OFrOFsfQcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MDE3MQ==", "bodyText": "Please desist from using any Guava APIs.", "url": "https://github.com/apache/hudi/pull/1341#discussion_r380990171", "createdAt": "2020-02-18T23:08:38Z", "author": {"login": "smarthi"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrintHelper.java", "diffHunk": "@@ -18,13 +18,16 @@\n \n package org.apache.hudi.cli;\n \n+import com.google.common.base.Strings;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE5Mzc3OQ==", "bodyText": "GTK. thanks!", "url": "https://github.com/apache/hudi/pull/1341#discussion_r382193779", "createdAt": "2020-02-20T18:56:58Z", "author": {"login": "satishkotha"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrintHelper.java", "diffHunk": "@@ -18,13 +18,16 @@\n \n package org.apache.hudi.cli;\n \n+import com.google.common.base.Strings;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MDE3MQ=="}, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODEwNjEwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrintHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzowOTozMVrOFrVz6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxODo1NzowOFrOFsfQsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MDQ0MA==", "bodyText": "Replace this with StringUtils.isNullOrEmpty() pending PR# 1159", "url": "https://github.com/apache/hudi/pull/1341#discussion_r380990440", "createdAt": "2020-02-18T23:09:31Z", "author": {"login": "smarthi"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrintHelper.java", "diffHunk": "@@ -57,11 +60,38 @@ public static String print(String[] header, String[][] rows) {\n    */\n   public static String print(TableHeader rowHeader, Map<String, Function<Object, String>> fieldNameToConverterMap,\n       String sortByField, boolean isDescending, Integer limit, boolean headerOnly, List<Comparable[]> rows) {\n+    return print(rowHeader, fieldNameToConverterMap, sortByField, isDescending, limit, headerOnly, rows, \"\");\n+  }\n+\n+  /**\n+   * Serialize Table to printable string and also export a temporary view to easily write sql queries.\n+   *\n+   * Ideally, exporting view needs to be outside PrintHelper, but all commands use this. So this is easy\n+   * way to add support for all commands\n+   *\n+   * @param rowHeader Row Header\n+   * @param fieldNameToConverterMap Field Specific Converters\n+   * @param sortByField Sorting field\n+   * @param isDescending Order\n+   * @param limit Limit\n+   * @param headerOnly Headers only\n+   * @param rows List of rows\n+   * @param tempTableName table name to export\n+   * @return Serialized form for printing\n+   */\n+  public static String print(TableHeader rowHeader, Map<String, Function<Object, String>> fieldNameToConverterMap,\n+      String sortByField, boolean isDescending, Integer limit, boolean headerOnly, List<Comparable[]> rows,\n+      String tempTableName) {\n \n     if (headerOnly) {\n       return HoodiePrintHelper.print(rowHeader);\n     }\n \n+    if (!Strings.isNullOrEmpty(tempTableName)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE5Mzg0MQ==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/1341#discussion_r382193841", "createdAt": "2020-02-20T18:57:08Z", "author": {"login": "satishkotha"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/HoodiePrintHelper.java", "diffHunk": "@@ -57,11 +60,38 @@ public static String print(String[] header, String[][] rows) {\n    */\n   public static String print(TableHeader rowHeader, Map<String, Function<Object, String>> fieldNameToConverterMap,\n       String sortByField, boolean isDescending, Integer limit, boolean headerOnly, List<Comparable[]> rows) {\n+    return print(rowHeader, fieldNameToConverterMap, sortByField, isDescending, limit, headerOnly, rows, \"\");\n+  }\n+\n+  /**\n+   * Serialize Table to printable string and also export a temporary view to easily write sql queries.\n+   *\n+   * Ideally, exporting view needs to be outside PrintHelper, but all commands use this. So this is easy\n+   * way to add support for all commands\n+   *\n+   * @param rowHeader Row Header\n+   * @param fieldNameToConverterMap Field Specific Converters\n+   * @param sortByField Sorting field\n+   * @param isDescending Order\n+   * @param limit Limit\n+   * @param headerOnly Headers only\n+   * @param rows List of rows\n+   * @param tempTableName table name to export\n+   * @return Serialized form for printing\n+   */\n+  public static String print(TableHeader rowHeader, Map<String, Function<Object, String>> fieldNameToConverterMap,\n+      String sortByField, boolean isDescending, Integer limit, boolean headerOnly, List<Comparable[]> rows,\n+      String tempTableName) {\n \n     if (headerOnly) {\n       return HoodiePrintHelper.print(rowHeader);\n     }\n \n+    if (!Strings.isNullOrEmpty(tempTableName)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MDQ0MA=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODEyMzc2OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempTableUtil.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzoxODowMVrOFrV-rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxODo1ODoxMlrOFsfSzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MzE5OA==", "bodyText": "Is this gonna be coupled to Spark? It's best not to add more Spark specific code given that there is work being undertaken to support other engines like flink on Hudi.  Can this be abstracted out to be engine agnostic ?", "url": "https://github.com/apache/hudi/pull/1341#discussion_r380993198", "createdAt": "2020-02-18T23:18:01Z", "author": {"login": "smarthi"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempTableUtil.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.utils;\n+\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public class TempTableUtil {\n+  private static final Logger LOG = LogManager.getLogger(TempTableUtil.class);\n+\n+  private JavaSparkContext jsc;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE5NDM4Mg==", "bodyText": "This is just another utility similar to how we setup spark context in tests. Created an abstraction. Let me know if you have any more concrete suggestions", "url": "https://github.com/apache/hudi/pull/1341#discussion_r382194382", "createdAt": "2020-02-20T18:58:12Z", "author": {"login": "satishkotha"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempTableUtil.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.utils;\n+\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public class TempTableUtil {\n+  private static final Logger LOG = LogManager.getLogger(TempTableUtil.class);\n+\n+  private JavaSparkContext jsc;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MzE5OA=="}, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODEyNDY1OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempTableUtil.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQyMzoxODoyOVrOFrV_Qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxODo1OTo0OVrOFsfWFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MzM0Ng==", "bodyText": "Please use Generic types if possible.", "url": "https://github.com/apache/hudi/pull/1341#discussion_r380993346", "createdAt": "2020-02-18T23:18:29Z", "author": {"login": "smarthi"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempTableUtil.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.utils;\n+\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public class TempTableUtil {\n+  private static final Logger LOG = LogManager.getLogger(TempTableUtil.class);\n+\n+  private JavaSparkContext jsc;\n+  private SQLContext sqlContext;\n+\n+  public TempTableUtil(String appName) {\n+    try {\n+      SparkConf sparkConf = new SparkConf().setAppName(appName)\n+              .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").setMaster(\"local[8]\");\n+      jsc = new JavaSparkContext(sparkConf);\n+      jsc.setLogLevel(\"ERROR\");\n+\n+      sqlContext = new SQLContext(jsc);\n+    } catch (Throwable ex) {\n+      // log full stack trace and rethrow. Without this its difficult to debug failures, if any\n+      LOG.error(\"unable to initialize spark context \", ex);\n+      throw new HoodieException(ex);\n+    }\n+  }\n+\n+  public void write(String tableName, List<String> headers, List<List<Comparable>> rows) {\n+    try {\n+      if (headers.isEmpty() || rows.isEmpty()) {\n+        return;\n+      }\n+\n+      if (rows.stream().filter(row -> row.size() != headers.size()).count() > 0) {\n+        throw new HoodieException(\"Invalid row, does not match headers \" + headers.size() + \" \" + rows.size());\n+      }\n+\n+      // replace all whitespaces in headers to make it easy to write sql queries\n+      List<String> headersNoSpaces = headers.stream().map(title -> title.replaceAll(\"\\\\s+\",\"\"))\n+              .collect(Collectors.toList());\n+\n+      // generate schema for table\n+      StructType structType = new StructType();\n+      for (int i = 0; i < headersNoSpaces.size(); i++) {\n+        // try guessing data type from column data.\n+        DataType headerDataType = getDataType(rows.get(0).get(i));\n+        structType = structType.add(DataTypes.createStructField(headersNoSpaces.get(i), headerDataType, true));\n+      }\n+      List<Row> records = rows.stream().map(row -> RowFactory.create(row.toArray(new Comparable[row.size()])))\n+              .collect(Collectors.toList());\n+      Dataset<Row> dataset = this.sqlContext.createDataFrame(records, structType);\n+      dataset.createOrReplaceTempView(tableName);\n+      System.out.println(\"Wrote table view: \" + tableName);\n+    } catch (Throwable ex) {\n+      // log full stack trace and rethrow. Without this its difficult to debug failures, if any\n+      LOG.error(\"unable to write \", ex);\n+      throw new HoodieException(ex);\n+    }\n+  }\n+\n+  public void runQuery(String sqlText) {\n+    try {\n+      this.sqlContext.sql(sqlText).show(Integer.MAX_VALUE, false);\n+    } catch (Throwable ex) {\n+      // log full stack trace and rethrow. Without this its difficult to debug failures, if any\n+      LOG.error(\"unable to read \", ex);\n+      throw new HoodieException(ex);\n+    }\n+  }\n+\n+  public void deleteTable(String tableName) {\n+    try {\n+      sqlContext.sql(\"DROP TABLE IF EXISTS \" + tableName);\n+    } catch (Throwable ex) {\n+      // log full stack trace and rethrow. Without this its difficult to debug failures, if any\n+      LOG.error(\"unable to initialize spark context \", ex);\n+      throw new HoodieException(ex);\n+    }\n+  }\n+\n+  private DataType getDataType(Comparable comparable) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjE5NTIyMA==", "bodyText": "Not sure what you mean. This is dynamically inferring schema of tables output by CLI to make it easy to filter. If you have suggestions on  how to improve this, let me know", "url": "https://github.com/apache/hudi/pull/1341#discussion_r382195220", "createdAt": "2020-02-20T18:59:49Z", "author": {"login": "satishkotha"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempTableUtil.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.utils;\n+\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public class TempTableUtil {\n+  private static final Logger LOG = LogManager.getLogger(TempTableUtil.class);\n+\n+  private JavaSparkContext jsc;\n+  private SQLContext sqlContext;\n+\n+  public TempTableUtil(String appName) {\n+    try {\n+      SparkConf sparkConf = new SparkConf().setAppName(appName)\n+              .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").setMaster(\"local[8]\");\n+      jsc = new JavaSparkContext(sparkConf);\n+      jsc.setLogLevel(\"ERROR\");\n+\n+      sqlContext = new SQLContext(jsc);\n+    } catch (Throwable ex) {\n+      // log full stack trace and rethrow. Without this its difficult to debug failures, if any\n+      LOG.error(\"unable to initialize spark context \", ex);\n+      throw new HoodieException(ex);\n+    }\n+  }\n+\n+  public void write(String tableName, List<String> headers, List<List<Comparable>> rows) {\n+    try {\n+      if (headers.isEmpty() || rows.isEmpty()) {\n+        return;\n+      }\n+\n+      if (rows.stream().filter(row -> row.size() != headers.size()).count() > 0) {\n+        throw new HoodieException(\"Invalid row, does not match headers \" + headers.size() + \" \" + rows.size());\n+      }\n+\n+      // replace all whitespaces in headers to make it easy to write sql queries\n+      List<String> headersNoSpaces = headers.stream().map(title -> title.replaceAll(\"\\\\s+\",\"\"))\n+              .collect(Collectors.toList());\n+\n+      // generate schema for table\n+      StructType structType = new StructType();\n+      for (int i = 0; i < headersNoSpaces.size(); i++) {\n+        // try guessing data type from column data.\n+        DataType headerDataType = getDataType(rows.get(0).get(i));\n+        structType = structType.add(DataTypes.createStructField(headersNoSpaces.get(i), headerDataType, true));\n+      }\n+      List<Row> records = rows.stream().map(row -> RowFactory.create(row.toArray(new Comparable[row.size()])))\n+              .collect(Collectors.toList());\n+      Dataset<Row> dataset = this.sqlContext.createDataFrame(records, structType);\n+      dataset.createOrReplaceTempView(tableName);\n+      System.out.println(\"Wrote table view: \" + tableName);\n+    } catch (Throwable ex) {\n+      // log full stack trace and rethrow. Without this its difficult to debug failures, if any\n+      LOG.error(\"unable to write \", ex);\n+      throw new HoodieException(ex);\n+    }\n+  }\n+\n+  public void runQuery(String sqlText) {\n+    try {\n+      this.sqlContext.sql(sqlText).show(Integer.MAX_VALUE, false);\n+    } catch (Throwable ex) {\n+      // log full stack trace and rethrow. Without this its difficult to debug failures, if any\n+      LOG.error(\"unable to read \", ex);\n+      throw new HoodieException(ex);\n+    }\n+  }\n+\n+  public void deleteTable(String tableName) {\n+    try {\n+      sqlContext.sql(\"DROP TABLE IF EXISTS \" + tableName);\n+    } catch (Throwable ex) {\n+      // log full stack trace and rethrow. Without this its difficult to debug failures, if any\n+      LOG.error(\"unable to initialize spark context \", ex);\n+      throw new HoodieException(ex);\n+    }\n+  }\n+\n+  private DataType getDataType(Comparable comparable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDk5MzM0Ng=="}, "originalCommit": null, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM3NTI2MzIxOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQwMDoxMDo1N1rOFt0aMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQwNzowMTowOFrOFt6mcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU4ODkxMg==", "bodyText": "This is not a real hive table. So maybe reword this for clarity. Also, export means something persistent while this is a temporary view.\nName of in-memory view to cache results.", "url": "https://github.com/apache/hudi/pull/1341#discussion_r383588912", "createdAt": "2020-02-25T00:10:57Z", "author": {"login": "prashantwason"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java", "diffHunk": "@@ -145,13 +148,16 @@ private String printCommitsWithMetadata(HoodieDefaultTimeline timeline,\n             .addTableHeaderField(\"Total Rollback Blocks\").addTableHeaderField(\"Total Log Records\")\n             .addTableHeaderField(\"Total Updated Records Compacted\").addTableHeaderField(\"Total Write Bytes\");\n \n-    return HoodiePrintHelper.print(header, new HashMap<>(), sortByField, descending, limit, headerOnly, rows);\n+    return HoodiePrintHelper.print(header, new HashMap<>(), sortByField, descending,\n+            limit, headerOnly, rows, tempTableName);\n   }\n \n   @CliCommand(value = \"commits show\", help = \"Show the commits\")\n   public String showCommits(\n       @CliOption(key = {\"includeExtraMetadata\"}, help = \"Include extra metadata\",\n           unspecifiedDefaultValue = \"false\") final boolean includeExtraMetadata,\n+      @CliOption(key = {\"exportToTableName\"}, mandatory = false, help = \"hive table name to export\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzY5MDM1NA==", "bodyText": "Changed names to view instead of table. Initial idea was to create actual tables with all the metadata and register them. so thats where name came from. But, i think exporting to external table requires lot more work with the way CLI is setup. Just doing views in CLI for now. I can work on writing a tool outside CLI to export metadata to another table.", "url": "https://github.com/apache/hudi/pull/1341#discussion_r383690354", "createdAt": "2020-02-25T07:01:08Z", "author": {"login": "satishkotha"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/CommitsCommand.java", "diffHunk": "@@ -145,13 +148,16 @@ private String printCommitsWithMetadata(HoodieDefaultTimeline timeline,\n             .addTableHeaderField(\"Total Rollback Blocks\").addTableHeaderField(\"Total Log Records\")\n             .addTableHeaderField(\"Total Updated Records Compacted\").addTableHeaderField(\"Total Write Bytes\");\n \n-    return HoodiePrintHelper.print(header, new HashMap<>(), sortByField, descending, limit, headerOnly, rows);\n+    return HoodiePrintHelper.print(header, new HashMap<>(), sortByField, descending,\n+            limit, headerOnly, rows, tempTableName);\n   }\n \n   @CliCommand(value = \"commits show\", help = \"Show the commits\")\n   public String showCommits(\n       @CliOption(key = {\"includeExtraMetadata\"}, help = \"Include extra metadata\",\n           unspecifiedDefaultValue = \"false\") final boolean includeExtraMetadata,\n+      @CliOption(key = {\"exportToTableName\"}, mandatory = false, help = \"hive table name to export\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzU4ODkxMg=="}, "originalCommit": null, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MjQ2MTMwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempViewProvider.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxOTozMDoxNFrOFu5FyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQwNjo0MToxOVrOFwU5Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDcxNDE4NA==", "bodyText": "Can we provide 2 different API's :\n\ncreateTable\nwriteToTable", "url": "https://github.com/apache/hudi/pull/1341#discussion_r384714184", "createdAt": "2020-02-26T19:30:14Z", "author": {"login": "n3nash"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempViewProvider.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.utils;\n+\n+import java.util.List;\n+\n+public interface TempViewProvider {\n+  void write(String tableName, List<String> headers, List<List<Comparable>> rows);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjIxODMzOQ==", "bodyText": "this is slightly difficult to do because schema is required for both create and write. I just renamed write to createAndWrite to make this easy to use.  Do you have strong preference for having separate calls?", "url": "https://github.com/apache/hudi/pull/1341#discussion_r386218339", "createdAt": "2020-03-02T06:41:19Z", "author": {"login": "satishkotha"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/TempViewProvider.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.utils;\n+\n+import java.util.List;\n+\n+public interface TempViewProvider {\n+  void write(String tableName, List<String> headers, List<List<Comparable>> rows);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDcxNDE4NA=="}, "originalCommit": null, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MjQ2NDg2OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/TempViewCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxOTozMToxNlrOFu5H5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wMlQwNjo0MToyNlrOFwU5fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDcxNDcyNg==", "bodyText": "Just keep a constant :\nString DUMMY_STRING = \"\";\nand then return that constant..", "url": "https://github.com/apache/hudi/pull/1341#discussion_r384714726", "createdAt": "2020-02-26T19:31:16Z", "author": {"login": "n3nash"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/TempViewCommand.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.cli.HoodieCLI;\n+\n+import org.springframework.shell.core.CommandMarker;\n+import org.springframework.shell.core.annotation.CliCommand;\n+import org.springframework.shell.core.annotation.CliOption;\n+import org.springframework.stereotype.Component;\n+\n+import java.io.IOException;\n+\n+/**\n+ * CLI command to query/delete temp views.\n+ */\n+@Component\n+public class TempViewCommand implements CommandMarker {\n+\n+  @CliCommand(value = \"temp_query\", help = \"query against created temp view\")\n+  public String query(\n+          @CliOption(key = {\"sql\"}, mandatory = true, help = \"select query to run against view\") final String sql)\n+          throws IOException {\n+\n+    HoodieCLI.getTempViewProvider().runQuery(sql);\n+    return \"\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjIxODM2NA==", "bodyText": "Fixed", "url": "https://github.com/apache/hudi/pull/1341#discussion_r386218364", "createdAt": "2020-03-02T06:41:26Z", "author": {"login": "satishkotha"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/TempViewCommand.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.cli.HoodieCLI;\n+\n+import org.springframework.shell.core.CommandMarker;\n+import org.springframework.shell.core.annotation.CliCommand;\n+import org.springframework.shell.core.annotation.CliOption;\n+import org.springframework.stereotype.Component;\n+\n+import java.io.IOException;\n+\n+/**\n+ * CLI command to query/delete temp views.\n+ */\n+@Component\n+public class TempViewCommand implements CommandMarker {\n+\n+  @CliCommand(value = \"temp_query\", help = \"query against created temp view\")\n+  public String query(\n+          @CliOption(key = {\"sql\"}, mandatory = true, help = \"select query to run against view\") final String sql)\n+          throws IOException {\n+\n+    HoodieCLI.getTempViewProvider().runQuery(sql);\n+    return \"\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDcxNDcyNg=="}, "originalCommit": null, "originalPosition": 42}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4838, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}