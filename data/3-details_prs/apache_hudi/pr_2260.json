{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIyOTk4MDM4", "number": 2260, "title": "[HUDI-1381] Schedule compaction based on time elapsed", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nSchedule compaction based on time elapsed\nBrief change log\nGH : #2229\nIt would be helpful to introduce configuration to schedule compaction based on time elapsed since last scheduled compaction.\nVerify this pull request\nThis pull request is already covered by existing tests, such as (please describe tests).\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-11-18T08:05:50Z", "url": "https://github.com/apache/hudi/pull/2260", "merged": true, "mergeCommit": {"oid": "9431aabfab47a3b679bd6ccfa8b5fa584260fc9e"}, "closed": true, "closedAt": "2021-02-17T15:44:53Z", "author": {"login": "Karl-WangSK"}, "timelineItems": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABddvvWpABqjQwMTEyMjI0MDA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABd6_QUOgFqTU5MjEwOTA2Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4e81a88a2a81387be8ad97c496c227a3db7c2e12", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/4e81a88a2a81387be8ad97c496c227a3db7c2e12", "committedDate": "2020-11-18T14:48:17Z", "message": "Merge branch 'HUDI-1381' of github.com:Karl-WangSK/hudi into HUDI-1381"}, "afterCommit": {"oid": "679310a69fdb607d084bae429e03c528b5d60159", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/679310a69fdb607d084bae429e03c528b5d60159", "committedDate": "2020-11-18T08:01:45Z", "message": "update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwMzc2NjU4", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-540376658", "createdAt": "2020-11-28T13:49:35Z", "commit": {"oid": "6dac8043125b788234a2184f423ecc9d21c5757a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOFQxMzo0OTozNVrOH7ZSIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yOFQxMzo1MjozMVrOH7ZTNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjA0MjI3NA==", "bodyText": "Missing doc about initialTime .", "url": "https://github.com/apache/hudi/pull/2260#discussion_r532042274", "createdAt": "2020-11-28T13:49:35Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -315,7 +315,8 @@ public HoodieActiveTimeline getActiveTimeline() {\n    */\n   public abstract Option<HoodieCompactionPlan> scheduleCompaction(HoodieEngineContext context,\n                                                                   String instantTime,\n-                                                                  Option<Map<String, String>> extraMetadata);\n+                                                                  Option<Map<String, String>> extraMetadata,\n+                                                                  String initialTime);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6dac8043125b788234a2184f423ecc9d21c5757a"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjA0MjU1MQ==", "bodyText": "break the second with to a new line?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r532042551", "createdAt": "2020-11-28T13:52:31Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -39,17 +39,18 @@\n \n public class TestInlineCompaction extends CompactionTestBase {\n \n-  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits) {\n+  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime) {\n     return getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits).build())\n+            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6dac8043125b788234a2184f423ecc9d21c5757a"}, "originalPosition": 9}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0750f24485e4b04fa7ca4b7acfcbb5e932d148ae", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/0750f24485e4b04fa7ca4b7acfcbb5e932d148ae", "committedDate": "2020-12-17T10:08:10Z", "message": "update"}, "afterCommit": {"oid": "6dac8043125b788234a2184f423ecc9d21c5757a", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/6dac8043125b788234a2184f423ecc9d21c5757a", "committedDate": "2020-11-18T15:28:28Z", "message": "support flink"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4NzQ2NTY2", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-558746566", "createdAt": "2020-12-25T03:36:01Z", "commit": {"oid": "eb7f81b491b81f16984d75b8969db168066d21ac"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNVQwMzozNjowMVrOILXjdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNVQwNDoyMDo1OFrOILX54A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5MTE1OQ==", "bodyText": "How about mark this initialTime as lastCompaction time, then we can gei it from timeline.\nthen, when we get a new commit, we can check the interval between these two timestamps to decide whether execute compact or not.\nin this way:\n\nthere is no need to update it additionally\nIt would be more accurate.(Time elapsed since the last compact)", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548791159", "createdAt": "2020-12-25T03:36:01Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -129,6 +130,7 @@ public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig\n     this.metrics = new HoodieMetrics(config, config.getTableName());\n     this.rollbackPending = rollbackPending;\n     this.index = createIndex(writeConfig);\n+    this.initialTime = HoodieActiveTimeline.createNewInstantTime();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb7f81b491b81f16984d75b8969db168066d21ac"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5MTc5Mw==", "bodyText": "Do you mean one day?  might be too long, who about an hour", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548791793", "createdAt": "2020-12-25T03:41:01Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -109,6 +110,7 @@\n   private static final String DEFAULT_INLINE_COMPACT = \"false\";\n   private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n+  private static final String DEFAULT_INLINE_COMPACT_ELAPSED_TIME = String.valueOf(60 * 60 * 24);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb7f81b491b81f16984d75b8969db168066d21ac"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5Njc3MQ==", "bodyText": "how about triggering a compaction when any one of the conditions is met?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548796771", "createdAt": "2020-12-25T04:19:44Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -65,10 +66,12 @@ protected HoodieCompactionPlan scheduleCompaction() {\n \n     int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n         .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n+    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+                    && timeCompaction(instantTime, initialTime, deltaCommitsSinceLastCompaction)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb7f81b491b81f16984d75b8969db168066d21ac"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODc5Njg5Ng==", "bodyText": "The timestamp is of yyyyMMddHHmmss format, we can not simply sum them by +", "url": "https://github.com/apache/hudi/pull/2260#discussion_r548796896", "createdAt": "2020-12-25T04:20:58Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -85,4 +88,15 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     }\n   }\n \n+  protected boolean timeCompaction(String instantTime, String initialTime, int deltaCommitsSinceLastCompaction) {\n+    if (Long.parseLong(initialTime) + config.getInlineCompactDeltaElapsedTimeMax() > Long.parseLong(instantTime)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb7f81b491b81f16984d75b8969db168066d21ac"}, "originalPosition": 32}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e1bcc125d6474311ec1e05fef908ef542477447e", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/e1bcc125d6474311ec1e05fef908ef542477447e", "committedDate": "2020-12-26T11:49:37Z", "message": "update"}, "afterCommit": {"oid": "5fcea7d755bf8c8619b0ff0684c18a2f25ced2da", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/5fcea7d755bf8c8619b0ff0684c18a2f25ced2da", "committedDate": "2020-12-26T11:57:35Z", "message": "update"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5fcea7d755bf8c8619b0ff0684c18a2f25ced2da", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/5fcea7d755bf8c8619b0ff0684c18a2f25ced2da", "committedDate": "2020-12-26T11:57:35Z", "message": "update"}, "afterCommit": {"oid": "87a7489330e2247931a9071828363c0413080cf6", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/87a7489330e2247931a9071828363c0413080cf6", "committedDate": "2020-12-26T12:04:42Z", "message": "update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzUxNTM4", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-559351538", "createdAt": "2020-12-29T01:44:51Z", "commit": {"oid": "3639b90ea46dc77ba307bcbb2a23500f256f1992"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTo0NDo1MVrOIMFMgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTo0NDo1MVrOIMFMgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzODk0NQ==", "bodyText": "we'd better throw an 'HoodieCompactionException' here", "url": "https://github.com/apache/hudi/pull/2260#discussion_r549538945", "createdAt": "2020-12-29T01:44:51Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -90,4 +99,13 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     }\n   }\n \n+  public Long parseTs(String time) {\n+    Long timestamp = null;\n+    try {\n+      timestamp = HoodieActiveTimeline.COMMIT_FORMATTER.parse(time).getTime() / 1000;\n+    } catch (ParseException e) {\n+      e.printStackTrace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3639b90ea46dc77ba307bcbb2a23500f256f1992"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzUzNzM0", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-559353734", "createdAt": "2020-12-29T02:00:48Z", "commit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyNjY1MTc4", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-562665178", "createdAt": "2021-01-06T12:14:09Z", "commit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxMjoxNDowOVrOIO8ztg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxMjoyNDozMlrOIO9VoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU0NzI1NA==", "bodyText": "Can we define a variable to make the code more readable?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552547254", "createdAt": "2021-01-06T12:14:09Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,23 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+                    && parseTs(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseTs(instantTime)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU0ODcxOQ==", "bodyText": "IMO, we can use String.format(...) to make the log message more readable, right?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552548719", "createdAt": "2021-01-06T12:15:45Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,23 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+                    && parseTs(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseTs(instantTime)) {\n       LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n           + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n+          + config.getInlineCompactDeltaCommitMax() + \". Or \" + config.getInlineCompactDeltaElapsedTimeMax()\n+              + \"ms elapsed time need since last compaction \" + lastCompactionTs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1MTQwOA==", "bodyText": "Please give it a better name?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552551408", "createdAt": "2021-01-06T12:18:51Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -90,4 +99,13 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     }\n   }\n \n+  public Long parseTs(String time) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1MzAwMg==", "bodyText": "Can we break the second withXXX  into a new line?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552553002", "createdAt": "2021-01-06T12:20:45Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -39,21 +38,23 @@\n \n public class TestInlineCompaction extends CompactionTestBase {\n \n-  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits) {\n+  private HoodieWriteConfig getConfigForInlineCompaction(int maxDeltaCommits, int maxDeltaTime) {\n     return getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits).build())\n+            .withInlineCompaction(true).withMaxNumDeltaCommitsBeforeCompaction(maxDeltaCommits)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1NTQ0NA==", "bodyText": "Replacing For to BasedOn or Via sounds better?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552555444", "createdAt": "2021-01-06T12:23:53Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -62,9 +63,9 @@ public void testCompactionIsNotScheduledEarly() throws Exception {\n   }\n \n   @Test\n-  public void testSuccessfulCompaction() throws Exception {\n+  public void testSuccessfulCompactionForNumCommits() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1NTY1OQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552555659", "createdAt": "2021-01-06T12:24:09Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +86,94 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionForTime() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjU1NTkzNw==", "bodyText": "whitespace between 5,10", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552555937", "createdAt": "2021-01-06T12:24:32Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +86,94 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionForTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5,10);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b637294d15f9886e745c09bc9fcc78a9d998294"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyNzkxODY0", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-562791864", "createdAt": "2021-01-06T15:18:48Z", "commit": {"oid": "ee958deef5982da76b0ddb5683880e4599e03857"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxNToxODo0OFrOIPGjQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxNToxODo0OFrOIPGjQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjcwNjg4MQ==", "bodyText": "Want to make sure: if we need to match the two conditions at the same time? Or they are two different choices? Can users only choose one of them?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r552706881", "createdAt": "2021-01-06T15:18:48Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,23 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee958deef5982da76b0ddb5683880e4599e03857"}, "originalPosition": 41}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "04a88c17be5ca80bd19b3594827cc9d523f6ffdb", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/04a88c17be5ca80bd19b3594827cc9d523f6ffdb", "committedDate": "2021-01-07T02:01:11Z", "message": "update"}, "afterCommit": {"oid": "ee958deef5982da76b0ddb5683880e4599e03857", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/ee958deef5982da76b0ddb5683880e4599e03857", "committedDate": "2021-01-06T13:19:18Z", "message": "update"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ee958deef5982da76b0ddb5683880e4599e03857", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/ee958deef5982da76b0ddb5683880e4599e03857", "committedDate": "2021-01-06T13:19:18Z", "message": "update"}, "afterCommit": {"oid": "b593f1062931a4d017ae8bd7dd42e47a8873a39f", "author": {"user": {"login": "wangxianghu", "name": "wangxianghu"}}, "url": "https://github.com/apache/hudi/commit/b593f1062931a4d017ae8bd7dd42e47a8873a39f", "committedDate": "2021-01-06T15:07:24Z", "message": "[MINOR] Rename unit test package of hudi-spark3 from scala to java (#2411)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200", "committedDate": "2021-01-07T06:27:07Z", "message": "update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzOTU1Nzg4", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-563955788", "createdAt": "2021-01-08T02:04:41Z", "commit": {"oid": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowNDo0MVrOIQDHEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjowNDo0MVrOIQDHEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTA5MQ==", "bodyText": "Since we have added flags for num style compaction and time elapsed style compaction. maybe we should check the flags first, to make sure at least one of them is enabled. if not, make compact with commits num as default(with a warn log).\nbesides, we got 4 conditions here:\n\ncompact with commit num only;\ncompact with time elapsed only;\ncompact when both commit num and time elapsed meet requirements\uff1b\ncompact when one of them is met\n\nWDYT @Karl-WangSK  cc @yanghua", "url": "https://github.com/apache/hudi/pull/2260#discussion_r553699091", "createdAt": "2021-01-08T02:04:41Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,32 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean numCommitEnabled = config.getInlineCompactDeltaNumCommitEnabled();\n+    boolean timeEnabled = config.getInlineCompactDeltaElapsedEnabled();\n+    boolean compactable;\n+    if (numCommitEnabled && !timeEnabled) {\n+      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction;\n+    } else if (!numCommitEnabled && timeEnabled) {\n+      compactable = parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n+    } else {\n+      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+          && parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200"}, "originalPosition": 51}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d087f09d4da2b2339413799f4eb13a091faacb9a", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/d087f09d4da2b2339413799f4eb13a091faacb9a", "committedDate": "2021-01-10T06:53:22Z", "message": "update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY3MDUxMzc1", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-567051375", "createdAt": "2021-01-13T09:48:28Z", "commit": {"oid": "d087f09d4da2b2339413799f4eb13a091faacb9a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xM1QwOTo0ODoyOVrOISnaqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xM1QwOTo0ODoyOVrOISnaqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjM5MTA4MA==", "bodyText": "how about extract all these logic to one method needCompact(Table table, CompactType compactType ), and init proper vars when need.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r556391080", "createdAt": "2021-01-13T09:48:29Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,34 +63,64 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = getCompactType(deltaCommitsSinceLastCompaction, lastCompactionTs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d087f09d4da2b2339413799f4eb13a091faacb9a"}, "originalPosition": 42}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/7d0453e72c7f9136e16ffaf928e09906917f745f", "committedDate": "2021-01-14T03:32:57Z", "message": "update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwMjEwNzI2", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-570210726", "createdAt": "2021-01-18T06:59:09Z", "commit": {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwNjo1OTowOVrOIVcBAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQwNzowOToxMFrOIVcOmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDAxNw==", "bodyText": "Please revert this change.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350017", "createdAt": "2021-01-18T06:59:09Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -18,6 +18,7 @@\n \n package org.apache.hudi.config;\n \n+import org.apache.hadoop.hbase.io.compress.Compression;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDc3Mw==", "bodyText": "please revert this change.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350773", "createdAt": "2021-01-18T07:01:16Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -468,7 +477,7 @@ public String getClusteringExecutionStrategyClass() {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MDc5OA==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559350798", "createdAt": "2021-01-18T07:01:22Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -484,7 +493,7 @@ public long getClusteringTargetFileMaxBytes() {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MTc1NA==", "bodyText": "if we call this function. it means we turn ASYNC_COMPACT_ENABLE_OPT_KEY on. Do we still need to check first?\n\nThe switch of ASYNC_COMPACT_ENABLE_OPT_KEY  must be the pre-condition.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559351754", "createdAt": "2021-01-18T07:04:15Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -60,17 +63,32 @@ protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n+      deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean numCommitEnabled = config.getInlineCompactDeltaNumCommitEnabled();\n+    boolean timeEnabled = config.getInlineCompactDeltaElapsedEnabled();\n+    boolean compactable;\n+    if (numCommitEnabled && !timeEnabled) {\n+      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction;\n+    } else if (!numCommitEnabled && timeEnabled) {\n+      compactable = parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n+    } else {\n+      compactable = config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction\n+          && parseToTimestamp(lastCompactionTs) + config.getInlineCompactDeltaElapsedTimeMax() > parseToTimestamp(instantTime);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTA5MQ=="}, "originalCommit": {"oid": "ddf8fcaf8f6e551fbec41c4e6740dc7e4d925200"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzAxMw==", "bodyText": "We need to describe why it caused compaction, not only some runtime information. e.g. add the log into the case statement?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559353013", "createdAt": "2021-01-18T07:07:50Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,90 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTM1MzQ5OQ==", "bodyText": "We may not tell why does not trigger compaction. It means nothing happened, right.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559353499", "createdAt": "2021-01-18T07:09:10Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,90 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1;\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            || parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = config.getInlineCompactDeltaCommitMax() <= threshold._1\n+            && parseToTimestamp(threshold._2) + config.getInlineCompactDeltaElapsedTimeMax() <= parseToTimestamp(instantTime);\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n+              compactType.name(), threshold._1, threshold._2));\n+    } else {\n+      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d0453e72c7f9136e16ffaf928e09906917f745f"}, "originalPosition": 117}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fbd87b4915f52d3010edeebe66b966855e1abfdc", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/fbd87b4915f52d3010edeebe66b966855e1abfdc", "committedDate": "2021-01-18T07:34:35Z", "message": "update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/96e596a8c14dc2a45606a5363e39b87474747b5c", "committedDate": "2021-01-18T09:56:46Z", "message": "add log in each case"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwNDA2MjYx", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-570406261", "createdAt": "2021-01-18T11:32:35Z", "commit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQxMTozMjozNVrOIVlSMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQxMTo1MDozNVrOIVl5pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwMTg3Mg==", "bodyText": "Still exists?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559501872", "createdAt": "2021-01-18T11:32:35Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -468,7 +477,7 @@ public String getClusteringExecutionStrategyClass() {\n   public long getClusteringMaxBytesInGroup() {\n     return Long.parseLong(props.getProperty(HoodieClusteringConfig.CLUSTERING_MAX_BYTES_PER_GROUP));\n   }\n-  \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwMTk3MQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559501971", "createdAt": "2021-01-18T11:32:49Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -484,7 +493,7 @@ public long getClusteringTargetFileMaxBytes() {\n   public int getTargetPartitionsForClustering() {\n     return Integer.parseInt(props.getProperty(HoodieClusteringConfig.CLUSTERING_TARGET_PARTITIONS));\n   }\n-  \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwNjEzOQ==", "bodyText": "COMMIT_NUM and NUM  do not keep consistent. What about NUM or COMMITS?\nIMO, CompactType may make users confused. what about CompactionTriggerStrategy Or CompactionScheduleStrategy", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559506139", "createdAt": "2021-01-18T11:40:13Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactType.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.compact;\n+\n+public enum  CompactType {\n+    COMMIT_NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwNzI3Nw==", "bodyText": "Revert this change, please.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559507277", "createdAt": "2021-01-18T11:42:18Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUwODczOQ==", "bodyText": "IMO, getLastDeltaCommitInfo sounds better? Correct me, if it's not good for you.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559508739", "createdAt": "2021-01-18T11:44:58Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUxMDY3Ng==", "bodyText": "The commit number is larger than xxx, trigger compaction scheduler. sounds better?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559510676", "createdAt": "2021-01-18T11:48:17Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTUxMTk3NQ==", "bodyText": "Timestamp is not clear, it means second or mills or something else?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r559511975", "createdAt": "2021-01-18T11:50:35Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,98 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactType());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> checkCompact(CompactType compactType) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n-        .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+            .filterCompletedInstants().lastInstant();\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n-\n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+    if (compactType != CompactType.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n     }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n-    HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n-    try {\n-      SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n-      Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n-          .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n-          .collect(Collectors.toSet());\n-      // exclude files in pending clustering from compaction.\n-      fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n-      return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+  public boolean needCompact(CompactType compactType) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = checkCompact(compactType);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    switch (compactType) {\n+      case COMMIT_NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s\", inlineCompactDeltaCommitMax));\n+        break;\n+      case TIME_ELAPSED:\n+        compactable = parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when elapsed_time >=%ss\", inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      case NUM_OR_TIME:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1\n+            || parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s or elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n+                inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      case NUM_AND_TIME:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1\n+            && parseToTimestamp(threshold._2) + inlineCompactDeltaElapsedTimeMax <= parseToTimestamp(instantTime);\n+        LOG.info(String.format(\"Trigger compaction when commit_num >=%s and elapsed_time >=%ss\", inlineCompactDeltaCommitMax,\n+                inlineCompactDeltaElapsedTimeMax));\n+        break;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactType());\n+    }\n \n-    } catch (IOException e) {\n-      throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+    if (compactable) {\n+      LOG.info(String.format(\"Scheduling compaction: %s. Delta commits found: %s times, and last compaction time is %s.\",\n+              compactType.name(), threshold._1, threshold._2));\n+    } else {\n+      LOG.info(String.format(\"Not scheduling compaction as only %s delta commits was found since last compaction %s.\"\n+                      + \"Waiting for %s,or %sms elapsed time need since last compaction %s.\", threshold._1,\n+              threshold._2, config.getInlineCompactDeltaCommitMax(), config.getInlineCompactDeltaElapsedTimeMax(), threshold._2));\n     }\n+    return compactable;\n   }\n \n+  public Long parseToTimestamp(String time) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e596a8c14dc2a45606a5363e39b87474747b5c"}, "originalPosition": 132}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a74ea8120c56e5b40de3491d0a4d61a93981011d", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/a74ea8120c56e5b40de3491d0a4d61a93981011d", "committedDate": "2021-01-18T16:32:35Z", "message": "update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4259478b945ea0440e0c28c56dc6290ee2df4442", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/4259478b945ea0440e0c28c56dc6290ee2df4442", "committedDate": "2021-01-18T16:44:59Z", "message": "Update HoodieWriteConfig.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c3e12ef34026b9d5b89d3d537a37040046abae3", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/3c3e12ef34026b9d5b89d3d537a37040046abae3", "committedDate": "2021-01-18T17:02:14Z", "message": "update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a318c91498d1899107f886b73c3404cd5f50258f", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/a318c91498d1899107f886b73c3404cd5f50258f", "committedDate": "2021-01-18T17:02:33Z", "message": "Merge branch 'HUDI-1381' of github.com:Karl-WangSK/hudi into HUDI-1381"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9a1a63910825d68b9a7e99cd9dd9995a4312448", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/b9a1a63910825d68b9a7e99cd9dd9995a4312448", "committedDate": "2021-01-19T01:49:36Z", "message": "update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d720b4e939dab49e8493dbce073c7ee859b03594", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/d720b4e939dab49e8493dbce073c7ee859b03594", "committedDate": "2021-01-19T01:53:41Z", "message": "update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcxMTM4NDc4", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-571138478", "createdAt": "2021-01-19T11:10:51Z", "commit": {"oid": "d720b4e939dab49e8493dbce073c7ee859b03594"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQxMToxMDo1MVrOIWJtUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQxMToxMDo1MVrOIWJtUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA5ODY0MA==", "bodyText": "how about renaming it to hoodie.compact.inline.max.delta.seconds, it seems more readable cc @yanghua", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560098640", "createdAt": "2021-01-19T11:10:51Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.time\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d720b4e939dab49e8493dbce073c7ee859b03594"}, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/48f7392acd2021a8cbfc3a39e71189a9564e2c4f", "committedDate": "2021-01-19T14:38:31Z", "message": "Update HoodieCompactionConfig.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcxOTYxOTcz", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-571961973", "createdAt": "2021-01-20T08:17:21Z", "commit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwODoxNzoyMlrOIWxxxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwODo0MjoyOFrOIWytbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc1NTE0Mw==", "bodyText": "Can we unify the constant name to INLINE_COMPACT_TIME_DELTA_SECONDS_PROP  so that we can align with INLINE_COMPACT_NUM_DELTA_COMMITS_PROP  and withMaxDeltaTimeBeforeCompaction", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560755143", "createdAt": "2021-01-20T08:17:22Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_ELAPSED_TIME_PROP = \"hoodie.compact.inline.max.delta.seconds\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc1NTIwNQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560755205", "createdAt": "2021-01-20T08:17:30Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -109,6 +112,8 @@\n   private static final String DEFAULT_INLINE_COMPACT = \"false\";\n   private static final String DEFAULT_INCREMENTAL_CLEANER = \"true\";\n   private static final String DEFAULT_INLINE_COMPACT_NUM_DELTA_COMMITS = \"5\";\n+  private static final String DEFAULT_INLINE_COMPACT_ELAPSED_TIME = String.valueOf(60 * 60);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MDMwMA==", "bodyText": "Do we also need to judge NUM_OR_TIME ?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560760300", "createdAt": "2021-01-20T08:26:05Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MDc5Ng==", "bodyText": "return -> get", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560760796", "createdAt": "2021-01-20T08:26:59Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2MzAyNg==", "bodyText": "Actually, it's not a threshold, right? it's real value. The below two are threshold values if you want to define.  threshold is immutable.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560763026", "createdAt": "2021-01-20T08:30:27Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2Mzk5MA==", "bodyText": "IMO, we do not need all the else statement, right? It's too normal if we do not match the compaction strategy.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560763990", "createdAt": "2021-01-20T08:32:06Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2NTQ4OQ==", "bodyText": "Since you have defined a compactable , let's use break here and return it in the end.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560765489", "createdAt": "2021-01-20T08:34:35Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n+              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2NjI3MQ==", "bodyText": "compact type -> compaction trigger strategy.", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560766271", "createdAt": "2021-01-20T08:35:47Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,112 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);\n+  }\n \n-    int deltaCommitsSinceLastCompaction = table.getActiveTimeline().getDeltaCommitTimeline()\n-        .findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n-    if (config.getInlineCompactDeltaCommitMax() > deltaCommitsSinceLastCompaction) {\n-      LOG.info(\"Not scheduling compaction as only \" + deltaCommitsSinceLastCompaction\n-          + \" delta commits was found since last compaction \" + lastCompactionTs + \". Waiting for \"\n-          + config.getInlineCompactDeltaCommitMax());\n-      return new HoodieCompactionPlan();\n+  public boolean needCompact(CompactionTriggerStrategy compactionTriggerStrategy) {\n+    boolean compactable;\n+    // return deltaCommitsSinceLastCompaction and lastCompactionTs\n+    Tuple2<Integer, String> threshold = getLastDeltaCommitInfo(compactionTriggerStrategy);\n+    int inlineCompactDeltaCommitMax = config.getInlineCompactDeltaCommitMax();\n+    int inlineCompactDeltaElapsedTimeMax = config.getInlineCompactDeltaElapsedTimeMax();\n+    long elapsedTime;\n+    switch (compactionTriggerStrategy) {\n+      case NUM:\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s, trigger compaction scheduler.\", inlineCompactDeltaCommitMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits needed since last compaction %s.\"\n+              + \"But only %s delta commits found.\", inlineCompactDeltaCommitMax, threshold._2, threshold._1));\n+        }\n+        return compactable;\n+      case TIME_ELAPSED:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The elapsed time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s elapsed time needed since last compaction %s.\"\n+              + \"But only %ss elapsed time found\", inlineCompactDeltaElapsedTimeMax, threshold._2, elapsedTime));\n+        }\n+        return compactable;\n+      case NUM_OR_TIME:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 || inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s or elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits or %ss elapsed time needed since last compaction %s.\"\n+                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n+      case NUM_AND_TIME:\n+        elapsedTime = parsedToSeconds(instantTime) - parsedToSeconds(threshold._2);\n+        compactable = inlineCompactDeltaCommitMax <= threshold._1 && inlineCompactDeltaElapsedTimeMax <= elapsedTime;\n+        if (compactable) {\n+          LOG.info(String.format(\"The delta commits >= %s and elapsed_time >=%ss, trigger compaction scheduler.\", inlineCompactDeltaCommitMax,\n+              inlineCompactDeltaElapsedTimeMax));\n+        } else {\n+          LOG.info(String.format(\"Not scheduling compaction because %s delta commits and %ss elapsed time needed since last compaction %s.\"\n+                  + \"But only %s delta commits and %ss elapsed time found\", inlineCompactDeltaCommitMax, inlineCompactDeltaElapsedTimeMax, threshold._2,\n+              threshold._1, elapsedTime));\n+        }\n+        return compactable;\n+      default:\n+        throw new HoodieCompactionException(\"Unsupported compact type: \" + config.getInlineCompactTriggerStrategy());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2ODgyNw==", "bodyText": "We should avoid use sleep, it will add the CI time. Can we fetch the relevant status with a loop?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560768827", "createdAt": "2021-01-20T08:39:55Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTQzNA==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769434", "createdAt": "2021-01-20T08:40:47Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTYyMw==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769623", "createdAt": "2021-01-20T08:41:06Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTg0NQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769845", "createdAt": "2021-01-20T08:41:27Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTkzMQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560769931", "createdAt": "2021-01-20T08:41:37Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java", "diffHunk": "@@ -85,32 +88,185 @@ public void testSuccessfulCompaction() throws Exception {\n   }\n \n   @Test\n-  public void testCompactionRetryOnFailure() throws Exception {\n+  public void testSuccessfulCompactionBasedOnTime() throws Exception {\n+    // Given: make one commit\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(5, 10, CompactionTriggerStrategy.TIME_ELAPSED);\n+\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+      List<HoodieRecord> records = dataGen.generateInserts(instantTime, 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, Arrays.asList(instantTime), records, cfg, true, new ArrayList<>());\n+\n+      // after 10s, that will trigger compaction\n+      Thread.sleep(10000);\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 100), writeClient, metaClient, cfg, false);\n+\n+      // Then: ensure the file slices are compacted as per policy\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      assertEquals(HoodieTimeline.COMMIT_ACTION, metaClient.getActiveTimeline().lastInstant().get().getAction());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Then: trigger the compaction because reach 3 commits.\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction because reach the time elapsed\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testSuccessfulCompactionBasedOnNumAndTime() throws Exception {\n+    // Given: make three commits\n+    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_AND_TIME);\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      List<String> instants = IntStream.range(0, 3).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+\n+      // Then: ensure no compaction is executedm since there are only 3 delta commits\n+      assertEquals(3, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+      Thread.sleep(20000);\n+      // 4th commit, that will trigger compaction\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n+      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n+\n+      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      assertEquals(5, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    }\n+  }\n+\n+  @Test\n+  public void testCompactionRetryOnFailureBasedOnNumCommits() throws Exception {\n+    // Given: two commits, schedule compaction and its failed/in-flight\n+    HoodieWriteConfig cfg = getConfigBuilder(false)\n+        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n+            .withInlineCompaction(false)\n+            .withMaxNumDeltaCommitsBeforeCompaction(1).build())\n+        .build();\n+    List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n+    String instantTime2;\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n+      List<HoodieRecord> records = dataGen.generateInserts(instants.get(0), 100);\n+      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n+      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n+      // Schedule compaction instant2, make it in-flight (simulates inline compaction failing)\n+      instantTime2 = HoodieActiveTimeline.createNewInstantTime();\n+      scheduleCompaction(instantTime2, writeClient, cfg);\n+      moveCompactionFromRequestedToInflight(instantTime2, cfg);\n+    }\n+\n+    // When: a third commit happens\n+    HoodieWriteConfig inlineCfg = getConfigForInlineCompaction(2, 60, CompactionTriggerStrategy.NUM);\n+    String instantTime3 = HoodieActiveTimeline.createNewInstantTime();\n+    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(inlineCfg)) {\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+      createNextDeltaCommit(instantTime3, dataGen.generateUpdates(instantTime3, 100), writeClient, metaClient, inlineCfg, false);\n+    }\n+\n+    // Then: 1 delta commit is done, the failed compaction is retried\n+    metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n+    assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n+    assertEquals(instantTime2, metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants().firstInstant().get().getTimestamp());\n+  }\n+\n+  @Test\n+  public void testCompactionRetryOnFailureBasedOnTime() throws Exception {\n     // Given: two commits, schedule compaction and its failed/in-flight\n     HoodieWriteConfig cfg = getConfigBuilder(false)\n         .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n-            .withInlineCompaction(false).withMaxNumDeltaCommitsBeforeCompaction(1).build())\n+            .withInlineCompaction(false)\n+            .withMaxDeltaTimeBeforeCompaction(5)\n+            .withInlineCompactionTriggerStrategy(CompactionTriggerStrategy.TIME_ELAPSED).build())\n         .build();\n-    List<String> instants = CollectionUtils.createImmutableList(\"000\", \"001\");\n+    String instantTime;\n+    List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n     try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n       List<HoodieRecord> records = dataGen.generateInserts(instants.get(0), 100);\n       HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n       runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n-      // Schedule compaction 002, make it in-flight (simulates inline compaction failing)\n-      scheduleCompaction(\"002\", writeClient, cfg);\n-      moveCompactionFromRequestedToInflight(\"002\", cfg);\n+      // Schedule compaction instantTime, make it in-flight (simulates inline compaction failing)\n+      Thread.sleep(10000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc3MDQxNQ==", "bodyText": "Useless exception class?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r560770415", "createdAt": "2021-01-20T08:42:28Z", "author": {"login": "yanghua"}, "path": "hudi-common/src/main/java/org/apache/hudi/exception/HoodieCompactException.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.exception;\n+\n+public class HoodieCompactException extends HoodieException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "48f7392acd2021a8cbfc3a39e71189a9564e2c4f"}, "originalPosition": 21}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/c2a695a7fc90389ed68bedbd0677bea8820e47a0", "committedDate": "2021-01-20T14:44:25Z", "message": "update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTgyNzgyMDQy", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-582782042", "createdAt": "2021-02-03T20:40:50Z", "commit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QyMDo0MDo1MFrOIfVwFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wM1QyMToxMjoyM1rOIfW9ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTczMzE0MA==", "bodyText": "please add comments/java docs explaining what this controls", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569733140", "createdAt": "2021-02-03T20:40:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -46,6 +47,8 @@\n   public static final String INLINE_COMPACT_PROP = \"hoodie.compact.inline\";\n   // Run a compaction every N delta commits\n   public static final String INLINE_COMPACT_NUM_DELTA_COMMITS_PROP = \"hoodie.compact.inline.max.delta.commits\";\n+  public static final String INLINE_COMPACT_TIME_DELTA_SECONDS_PROP = \"hoodie.compact.inline.max.delta.seconds\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0MjM1MA==", "bodyText": "rename to NUM_COMMITS and add a line of description for each?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569742350", "createdAt": "2021-02-03T20:56:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/CompactionTriggerStrategy.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.compact;\n+\n+public enum CompactionTriggerStrategy {\n+    NUM, TIME_ELAPSED, NUM_AND_TIME, NUM_OR_TIME", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NDI5Ng==", "bodyText": "can this call the method above or otherwise and reduce the code duplication?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569744296", "createdAt": "2021-02-03T20:59:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java", "diffHunk": "@@ -90,6 +90,20 @@ public static String createNewInstantTime() {\n     });\n   }\n \n+  /**\n+   * Returns next instant time that adds milliseconds in the {@link #COMMIT_FORMATTER} format.\n+   * Ensures each instant time is atleast 1 second apart since we create instant times at second granularity\n+   */\n+  public static String createNewInstantTime(long milliseconds) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NTQ3Ng==", "bodyText": "IIUC this block is just moved, no changes to code here within the if block?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569745476", "createdAt": "2021-02-03T21:01:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc0NjA1NA==", "bodyText": "can we use Pair instead of Tuple?", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569746054", "createdAt": "2021-02-03T21:02:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfter(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      } else {\n+        deltaCommitsSinceLastCompaction = deltaCommits.findInstantsAfterOrEquals(lastCompactionTs, Integer.MAX_VALUE).countInstants();\n+      }\n+    }\n+    return new Tuple2(deltaCommitsSinceLastCompaction, lastCompactionTs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc1MTk2Mg==", "bodyText": "I understand this is how it was. but overloading lastCompactionTs with the first delta commit and reusing this again is hard to grok. Can we atleast rename lastCompactionTs -> latestInstantTs or something more generic", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569751962", "createdAt": "2021-02-03T21:10:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTc1Mjk1NQ==", "bodyText": "can we always compute deltaCommitsSinceLastCompaction regardless of strategy. it should be a cheap in-memory operation. then we can merge these two blocks back together", "url": "https://github.com/apache/hudi/pull/2260#discussion_r569752955", "createdAt": "2021-02-03T21:12:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/SparkScheduleCompactionActionExecutor.java", "diffHunk": "@@ -58,36 +62,97 @@ public SparkScheduleCompactionActionExecutor(HoodieEngineContext context,\n   @Override\n   protected HoodieCompactionPlan scheduleCompaction() {\n     LOG.info(\"Checking if compaction needs to be run on \" + config.getBasePath());\n+    // judge if we need to compact according to num delta commits and time elapsed\n+    boolean compactable = needCompact(config.getInlineCompactTriggerStrategy());\n+    if (compactable) {\n+      LOG.info(\"Generating compaction plan for merge on read table \" + config.getBasePath());\n+      HoodieSparkMergeOnReadTableCompactor compactor = new HoodieSparkMergeOnReadTableCompactor();\n+      try {\n+        SyncableFileSystemView fileSystemView = (SyncableFileSystemView) table.getSliceView();\n+        Set<HoodieFileGroupId> fgInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+            .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+            .collect(Collectors.toSet());\n+        // exclude files in pending clustering from compaction.\n+        fgInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getLeft).collect(Collectors.toSet()));\n+        return compactor.generateCompactionPlan(context, table, config, instantTime, fgInPendingCompactionAndClustering);\n+      } catch (IOException e) {\n+        throw new HoodieCompactionException(\"Could not schedule compaction \" + config.getBasePath(), e);\n+      }\n+    }\n+\n+    return new HoodieCompactionPlan();\n+  }\n+\n+  public Tuple2<Integer, String> getLastDeltaCommitInfo(CompactionTriggerStrategy compactionTriggerStrategy) {\n     Option<HoodieInstant> lastCompaction = table.getActiveTimeline().getCommitTimeline()\n         .filterCompletedInstants().lastInstant();\n-    String lastCompactionTs = \"0\";\n+    HoodieTimeline deltaCommits = table.getActiveTimeline().getDeltaCommitTimeline();\n+\n+    String lastCompactionTs;\n+    int deltaCommitsSinceLastCompaction = 0;\n     if (lastCompaction.isPresent()) {\n       lastCompactionTs = lastCompaction.get().getTimestamp();\n+    } else {\n+      lastCompactionTs = deltaCommits.firstInstant().get().getTimestamp();\n     }\n+    if (compactionTriggerStrategy != CompactionTriggerStrategy.TIME_ELAPSED) {\n+      if (lastCompaction.isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c2a695a7fc90389ed68bedbd0677bea8820e47a0"}, "originalPosition": 60}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/1ffe0f6b0f59991fcec6e8d99ca98da4d62760c5", "committedDate": "2021-02-05T07:40:40Z", "message": "update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b34c416c83f5d651cbf9d10fe35c0047d69fc964", "author": {"user": {"login": "Karl-WangSK", "name": "Karl_Wang"}}, "url": "https://github.com/apache/hudi/commit/b34c416c83f5d651cbf9d10fe35c0047d69fc964", "committedDate": "2021-02-05T08:28:56Z", "message": "update"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTkyMTA5MDY3", "url": "https://github.com/apache/hudi/pull/2260#pullrequestreview-592109067", "createdAt": "2021-02-17T11:46:41Z", "commit": {"oid": "b34c416c83f5d651cbf9d10fe35c0047d69fc964"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4322, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}