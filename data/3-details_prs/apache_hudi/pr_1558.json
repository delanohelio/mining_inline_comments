{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA4NTE3NDg0", "number": 1558, "title": "[HUDI-796] Add deduping logic for upserts case", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nAdded deduping logic for upserts case.\nBrief change log\n\nAdded logic in DedupeSparkJob for upserts case\nMade dryRun and useCommitTimeForDedupe parameters configurable in RepairsCommand.\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-04-24T12:19:10Z", "url": "https://github.com/apache/hudi/pull/1558", "merged": true, "mergeCommit": {"oid": "73e5b4c7bbd1306a6a2686f6e8d85a2c871ac7ff"}, "closed": true, "closedAt": "2020-09-18T11:37:52Z", "author": {"login": "pratyakshsharma"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcawekqgH2gAyNDA4NTE3NDg0OjQxMGIyYmRiYWEyNWEyYTRlMzk5OWE1MDY2ODZlMzI5OWNmMmE0NTc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdKDw7RAFqTQ5MTM3NDEzNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "410b2bdbaa25a2a4e3999a506686e3299cf2a457", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/410b2bdbaa25a2a4e3999a506686e3299cf2a457", "committedDate": "2020-04-24T12:16:57Z", "message": "[HUDI-796]: added deduping logic for upserts case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/5e1e5f4ec01e5cd69211283304a0351173787136", "committedDate": "2020-04-24T12:21:17Z", "message": "[HUDI-796]: small fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxNDcyODcw", "url": "https://github.com/apache/hudi/pull/1558#pullrequestreview-401472870", "createdAt": "2020-04-28T03:29:36Z", "commit": {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMzoyOTozNlrOGNA7tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNDoxODozOFrOGNB1SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI5OTk1Ng==", "bodyText": "Let us modify help string of dryrun, statements are inaccurate :)", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416299956", "createdAt": "2020-04-28T03:29:36Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,11 +64,15 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          mandatory = true) final String sparkPropertiesPath,\n+      @CliOption(key = {\"useCommitTimeForDedupe\"}, help = \"Set it to true if duplicates have never been updated\",\n+        unspecifiedDefaultValue = \"true\") final boolean useCommitTimeForDedupe,\n+      @CliOption(key = {\"dryrun\"}, help = \"Should we actually add or just print what would be done\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNDY5Ng==", "bodyText": "It's better not use break here, rows.init also can get the rows will be delete.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416314696", "createdAt": "2020-04-28T04:18:38Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -103,24 +105,51 @@ class DedupeSparkJob(basePath: String,\n     // Mark all files except the one with latest commits for deletion\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n+\n+      if (useCommitTimeForDedupe) {\n+        /*\n+        This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+         */\n+        var maxCommit = -1L\n+\n+        rows.foreach(r => {\n+          val c = r(3).asInstanceOf[String].toLong\n+          if (c > maxCommit)\n+            maxCommit = c\n+        })\n+        rows.foreach(r => {\n+          val c = r(3).asInstanceOf[String].toLong\n+          if (c != maxCommit) {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n           }\n-          fileToDeleteKeyMap(f).add(key)\n+        })\n+      } else {\n+        /*\n+        This corresponds to the case where duplicates have been updated at least once.\n+        Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+         */\n+        val size = rows.size - 1\n+        var i = 0\n+        val loop = new Breaks\n+        loop.breakable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136"}, "originalPosition": 67}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a6af53c4f06838071ffb85303f844659cec9f6c", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/6a6af53c4f06838071ffb85303f844659cec9f6c", "committedDate": "2020-04-28T09:24:05Z", "message": "[HUDI-796]: addressed code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "417f5a14346745930bbec44145ffc35050e43c45", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/417f5a14346745930bbec44145ffc35050e43c45", "committedDate": "2020-04-28T09:29:14Z", "message": "[HUDI-796]: removed unused imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cfb1735e62265722c9aeddd38f078b06eebc7755", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/cfb1735e62265722c9aeddd38f078b06eebc7755", "committedDate": "2020-05-15T11:09:55Z", "message": "[HUDI-796]: addressed code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f285cb85d51b078bc269ff35ef0ccf2534c8f049", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/f285cb85d51b078bc269ff35ef0ccf2534c8f049", "committedDate": "2020-05-15T12:46:25Z", "message": "[HUDI-796]: in sync with master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4abe3a28bd02d91c4425ee8dace6d05e9118a5f", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/e4abe3a28bd02d91c4425ee8dace6d05e9118a5f", "committedDate": "2020-05-15T17:49:05Z", "message": "[HUDI-796]: added test cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fde5a159a9f7fc39e895b48bcea8426da21639a", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/9fde5a159a9f7fc39e895b48bcea8426da21639a", "committedDate": "2020-05-16T10:09:29Z", "message": "[HUDI-796]: small fix for upsert dedupe"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "838382a5e52719e0e9ef190d375dc7d6c91e5248", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/838382a5e52719e0e9ef190d375dc7d6c91e5248", "committedDate": "2020-05-16T19:33:04Z", "message": "[HUDI-796]: commented a test case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/5dcdd4100ec792f38c1017bf29dd63f3074e8aa3", "committedDate": "2020-05-19T13:23:40Z", "message": "[HUDI-796]: fixed test case"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE1MzkyODM3", "url": "https://github.com/apache/hudi/pull/1558#pullrequestreview-415392837", "createdAt": "2020-05-20T14:25:51Z", "commit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDoyNTo1M1rOGYOe-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDo1NjozM1rOGYP9sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ==", "bodyText": "Can use DeDupeType.withName(\"insertType\") instead?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428056315", "createdAt": "2020-05-20T14:25:53Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -263,13 +265,26 @@ private static int compact(JavaSparkContext jsc, String basePath, String tableNa\n   }\n \n   private static int deduplicatePartitionPath(JavaSparkContext jsc, String duplicatedPartitionPath,\n-      String repairedOutputPath, String basePath, String dryRun) {\n+      String repairedOutputPath, String basePath, boolean dryRun, String dedupeType) {\n     DedupeSparkJob job = new DedupeSparkJob(basePath, duplicatedPartitionPath, repairedOutputPath, new SQLContext(jsc),\n-        FSUtils.getFs(basePath, jsc.hadoopConfiguration()));\n-    job.fixDuplicates(Boolean.parseBoolean(dryRun));\n+        FSUtils.getFs(basePath, jsc.hadoopConfiguration()), getDedupeType(dedupeType));\n+    job.fixDuplicates(dryRun);\n     return 0;\n   }\n \n+  private static Enumeration.Value getDedupeType(String type) {\n+    switch (type) {\n+      case \"insertType\":\n+        return DeDupeType.insertType();\n+      case \"updateType\":\n+        return DeDupeType.updateType();\n+      case \"upsertType\":\n+        return DeDupeType.upsertType();\n+      default:\n+        throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MTUyNQ==", "bodyText": "It's better to show the three types in help string and have a type check at first line of command.", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428071525", "createdAt": "2020-05-20T14:45:26Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,7 +77,9 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Check DeDupeType.scala for valid values\",\n+          unspecifiedDefaultValue = \"insertType\") final String dedupeType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3ODk1Ng==", "bodyText": "Can we make it all uppercase to keep the format uniform\nhttps://github.com/apache/incubator-hudi/blob/74ecc27e920c70fa4598d8e5a696954203a5b127/hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java#L30-L34", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428078956", "createdAt": "2020-05-20T14:54:37Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DeDupeType.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli\n+\n+object DeDupeType extends Enumeration {\n+\n+  type dedupeType = Value\n+\n+  val insertType = Value(\"insertType\")\n+  val updateType = Value(\"updateType\")\n+  val upsertType = Value(\"upsertType\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA4MDU2MQ==", "bodyText": "Can we use $ to get value? like:\nhttps://github.com/apache/incubator-hudi/blob/74ecc27e920c70fa4598d8e5a696954203a5b127/hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala#L144", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428080561", "createdAt": "2020-05-20T14:56:33Z", "author": {"login": "hddong"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.updateType =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.insertType =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            }\n+          })\n+\n+        case DeDupeType.upsertType =>\n+          /*\n+          This corresponds to the case where duplicates got created as a result of inserts as well as updates,\n+          i.e few duplicate records have been updated, while others were never updated.\n+           */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          val rowsWithMaxCommit = new ListBuffer[Row]()\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            } else {\n+              rowsWithMaxCommit += r\n+            }\n+          })\n+\n+          rowsWithMaxCommit.toList.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+\n+        case _ => throw new IllegalArgumentException(\"Please provide valid type for deduping!\")\n+      }\n     })\n+    LOG.debug(\"fileToDeleteKeyMap size : \" + fileToDeleteKeyMap.size + \", map: \" + fileToDeleteKeyMap)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3"}, "originalPosition": 129}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a63c700e6630c0931570453fdeff6c857265301b", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/a63c700e6630c0931570453fdeff6c857265301b", "committedDate": "2020-05-22T13:39:44Z", "message": "[HUDI-796]: addressed code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72f850be6e98f7b20c0658642fbffeafd6a98db5", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/72f850be6e98f7b20c0658642fbffeafd6a98db5", "committedDate": "2020-05-22T14:26:28Z", "message": "[HUDI-796]: small change in SparkMain"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/8abfd9941b11e10de715930fee32f26829b11f75", "committedDate": "2020-06-27T19:31:46Z", "message": "Merge branch 'master' of https://github.com/apache/incubator-hudi into hudi-796"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5OTE0NjQ3", "url": "https://github.com/apache/hudi/pull/1558#pullrequestreview-439914647", "createdAt": "2020-06-30T11:16:30Z", "commit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMToxNjozMVrOGq3pWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNToxMzoxOFrOGrBRJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTA4MQ==", "bodyText": "Can we get the representation of the dedupeType by the DeDupeType enum?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447605081", "createdAt": "2020-06-30T11:16:31Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,16 +77,21 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(\"insert_type\") && !dedupeType.equals(\"update_type\") && !dedupeType.equals(\"upsert_type\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTcyMw==", "bodyText": "Can we append this parameter into the list of the parameters?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447605723", "createdAt": "2020-06-30T11:17:45Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,16 +77,21 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(\"insert_type\") && !dedupeType.equals(\"update_type\") && !dedupeType.equals(\"upsert_type\")) {\n+      throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n     if (StringUtils.isNullOrEmpty(sparkPropertiesPath)) {\n       sparkPropertiesPath =\n           Utils.getDefaultPropertiesFile(JavaConverters.mapAsScalaMapConverter(System.getenv()).asScala());\n     }\n \n     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);\n     sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,\n-        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(),\n+        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(), dedupeType,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNjEwMA==", "bodyText": "ditto, move to the last of the method's parameter list?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447606100", "createdAt": "2020-06-30T11:18:26Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -75,8 +76,8 @@ public static void main(String[] args) throws Exception {\n         returnCode = rollback(jsc, args[3], args[4]);\n         break;\n       case DEDUPLICATE:\n-        assert (args.length == 7);\n-        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], args[6]);\n+        assert (args.length == 8);\n+        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], Boolean.parseBoolean(args[7]), args[6]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNzYzMA==", "bodyText": "split via a new empty line?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447607630", "createdAt": "2020-06-30T11:21:15Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1ODA2Mg==", "bodyText": "split via empty line", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447758062", "createdAt": "2020-06-30T15:07:01Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.INSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1ODc0OA==", "bodyText": "why do you add new line here?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447758748", "createdAt": "2020-06-30T15:07:57Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.INSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            }\n+          })\n+\n+        case DeDupeType.UPSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created as a result of inserts as well as updates,\n+          i.e few duplicate records have been updated, while others were never updated.\n+           */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          val rowsWithMaxCommit = new ListBuffer[Row]()\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            } else {\n+              rowsWithMaxCommit += r\n+            }\n+          })\n+\n+          rowsWithMaxCommit.toList.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+\n+        case _ => throw new IllegalArgumentException(\"Please provide valid type for deduping!\")\n+      }\n     })\n+    LOG.debug(s\"fileToDeleteKeyMap size: ${fileToDeleteKeyMap.size}, map: $fileToDeleteKeyMap\")\n     fileToDeleteKeyMap\n   }\n \n \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1OTgyMA==", "bodyText": "Can you explain this change?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447759820", "createdAt": "2020-06-30T15:09:25Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -152,7 +209,7 @@ class DedupeSparkJob(basePath: String,\n       val newFilePath = new Path(s\"$repairOutputPath/${fileNameToPathMap(fileName).getName}\")\n       LOG.info(\" Skipping and writing new file for : \" + fileName)\n       SparkHelpers.skipKeysAndWriteNewFile(instantTime, fs, badFilePath, newFilePath, dupeFixPlan(fileName))\n-      fs.delete(badFilePath, false)\n+      fs.delete(badFilePath, true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MjcyNg==", "bodyText": "Can we rename the exists testDeduplicate  to testDeduplicateWithDefault or testDeduplicateWithInserts?", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447762726", "createdAt": "2020-06-30T15:13:18Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -145,6 +166,60 @@ public void testDeduplicate() throws IOException {\n     assertEquals(200, result.count());\n   }\n \n+  @Test\n+  public void testDeduplicateWithUpdates() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8abfd9941b11e10de715930fee32f26829b11f75"}, "originalPosition": 57}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2e34c5d4304524574322c3a0d6909dc37f398f5", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/e2e34c5d4304524574322c3a0d6909dc37f398f5", "committedDate": "2020-07-01T18:34:05Z", "message": "[HUDI-796]: code review changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "59eb45ce254e1952a03ee8023604c13c96082862", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/59eb45ce254e1952a03ee8023604c13c96082862", "committedDate": "2020-07-11T15:12:21Z", "message": "[HUDI-796]: fixed failing test cases"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2OTI4ODE3", "url": "https://github.com/apache/hudi/pull/1558#pullrequestreview-446928817", "createdAt": "2020-07-13T00:59:13Z", "commit": {"oid": "59eb45ce254e1952a03ee8023604c13c96082862"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwMDo1OToxM1rOGwYj6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwMDo1OToxM1rOGwYj6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzM4NzI0Mw==", "bodyText": "Can we change it to be:\n      if (!DeDupeType.values().contains(DeDupeType.withName(dedupeType))) {}", "url": "https://github.com/apache/hudi/pull/1558#discussion_r453387243", "createdAt": "2020-07-13T00:59:13Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,8 +78,14 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(DeDupeType.INSERT_TYPE().toString()) && !dedupeType.equals(DeDupeType.UPDATE_TYPE().toString())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "59eb45ce254e1952a03ee8023604c13c96082862"}, "originalPosition": 17}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aae238a473d24695d07dd4dfa13fe1d8464e3d10", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/aae238a473d24695d07dd4dfa13fe1d8464e3d10", "committedDate": "2020-09-16T19:50:24Z", "message": "[HUDI-796]: In sync with master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "092ccc8673b8bfc471acb5e2a6b95a16fece7a4f", "author": {"user": {"login": "pratyakshsharma", "name": "Pratyaksh Sharma"}}, "url": "https://github.com/apache/hudi/commit/092ccc8673b8bfc471acb5e2a6b95a16fece7a4f", "committedDate": "2020-09-17T07:31:49Z", "message": "[HUDI-796]: fixed error"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkxMzc0MTM0", "url": "https://github.com/apache/hudi/pull/1558#pullrequestreview-491374134", "createdAt": "2020-09-18T11:19:38Z", "commit": {"oid": "092ccc8673b8bfc471acb5e2a6b95a16fece7a4f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3012, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}