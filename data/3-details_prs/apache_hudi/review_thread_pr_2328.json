{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM2NTkxOTY2", "number": 2328, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjozNzoyMFrOFGBWlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxODoyMjoxN1rOFIrCKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxODU3OTQyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjozNzoyMFrOIG0_JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjozNzoyMFrOIG0_JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDAzMDUwMQ==", "bodyText": "all other configs are named as \"bulkinsert\". can we fix this one too instead of \"bulk_insert\".", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544030501", "createdAt": "2020-12-16T06:37:20Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -70,6 +70,7 @@\n   public static final String INSERT_PARALLELISM = \"hoodie.insert.shuffle.parallelism\";\n   public static final String BULKINSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\";\n   public static final String BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = \"hoodie.bulkinsert.user.defined.partitioner.class\";\n+  public static final String BULKINSERT_INPUT_DATA_SCHEMA_DDL = \"hoodie.bulk_insert.schema.ddl\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxODY0MTU2OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjo0OToxNVrOIG1lwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjo0OToxNVrOIG1lwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0MDM4NA==", "bodyText": "do you think we can create \"BaseDefaultSource\" and move SparkSession, Configuration and  getters there. So that spark 3 and spark2 defaultSource can extend from it.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544040384", "createdAt": "2020-12-16T06:49:15Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+import java.util.Map;\n+\n+/**\n+ * DataSource V2 implementation for managing internal write logic. Only called internally.\n+ * This class is only compatible with datasource V2 API in Spark 3.\n+ */\n+public class DefaultSource implements TableProvider {\n+\n+  private SparkSession sparkSession = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxODY4MTE5OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjo1NzowOFrOIG1-KQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNzowNTo0M1rOIG2bhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NjYzMw==", "bodyText": "I assume this class is almost exact replica of HoodieDataSourceInternalWriter", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544046633", "createdAt": "2020-12-16T06:57:08Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.connector.write.BatchWrite;\n+import org.apache.spark.sql.connector.write.PhysicalWriteInfo;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implementation of {@link BatchWrite} for datasource \"hudi.spark3.internal\" to be used in datasource implementation\n+ * of bulk insert.\n+ */\n+public class HoodieDataSourceInternalBatchWriter implements BatchWrite {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1NDE0OA==", "bodyText": "why named it as \"...Writer\" even though interface is called Write. i.e. BatchWrite.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544054148", "createdAt": "2020-12-16T07:05:43Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.connector.write.BatchWrite;\n+import org.apache.spark.sql.connector.write.PhysicalWriteInfo;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implementation of {@link BatchWrite} for datasource \"hudi.spark3.internal\" to be used in datasource implementation\n+ * of bulk insert.\n+ */\n+public class HoodieDataSourceInternalBatchWriter implements BatchWrite {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NjYzMw=="}, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxODY4NzAyOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriterFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjo1ODoxN1rOIG2B0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMzo0OToxM1rOIJpjYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NzU3MQ==", "bodyText": "I assume this is exact replica of HoodieBulkInsertDataInternalWriterFactory in spark2 datasource.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544047571", "createdAt": "2020-12-16T06:58:17Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriterFactory.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * Factory to assist in instantiating {@link HoodieBulkInsertDataInternalWriter}.\n+ */\n+public class HoodieBulkInsertDataInternalWriterFactory implements DataWriterFactory {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk4ODg5OA==", "bodyText": "I assume this is exact replica of HoodieBulkInsertDataInternalWriterFactory in spark2 datasource.\n\nThe only difference is the import class path.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r546988898", "createdAt": "2020-12-21T23:49:13Z", "author": {"login": "zhedoubushishi"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriterFactory.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * Factory to assist in instantiating {@link HoodieBulkInsertDataInternalWriter}.\n+ */\n+public class HoodieBulkInsertDataInternalWriterFactory implements DataWriterFactory {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NzU3MQ=="}, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxODY5Mjk5OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjo1OToyMlrOIG2FtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwNjozMDo1OFrOIKVY1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0ODU2NA==", "bodyText": "is there any changes in this class compared to HoodieBulkInsertDataInternalWriter in spark 2 datasource?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544048564", "createdAt": "2020-12-16T06:59:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+/**\n+ * Hoodie's Implementation of {@link DataWriter<InternalRow>}. This is used in data source \"hudi.spark3.internal\" implementation for bulk insert.\n+ */\n+public class HoodieBulkInsertDataInternalWriter implements DataWriter<InternalRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzcwNzA5Mw==", "bodyText": "The only difference is:\nimport org.apache.spark.sql.sources.v2.writer.DataWriter;\nimport org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n\nimport org.apache.spark.sql.connector.write.DataWriter;\nimport org.apache.spark.sql.connector.write.WriterCommitMessage;\n\nI created a class named HoodieBulkInsertDataInternalWriterHelper to avoid duplicated code.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r547707093", "createdAt": "2020-12-23T06:30:58Z", "author": {"login": "zhedoubushishi"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+/**\n+ * Hoodie's Implementation of {@link DataWriter<InternalRow>}. This is used in data source \"hudi.spark3.internal\" implementation for bulk insert.\n+ */\n+public class HoodieBulkInsertDataInternalWriter implements DataWriter<InternalRow> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0ODU2NA=="}, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxODcxNjAzOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNzowMzo1NlrOIG2U3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNzowMzo1NlrOIG2U3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1MjQ0NA==", "bodyText": "java docs.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544052444", "createdAt": "2020-12-16T07:03:56Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.SupportsWrite;\n+import org.apache.spark.sql.connector.catalog.TableCapability;\n+import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxODcyODU5OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriterBuilder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNzowNjoxMlrOIG2cwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNzowNjoxMlrOIG2cwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1NDQ2NA==", "bodyText": "why suffixed it as \"WriterBuilder\" while interface is called \"WriteBuilder\"", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544054464", "createdAt": "2020-12-16T07:06:12Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriterBuilder.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.write.BatchWrite;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * Implementation of {@link WriteBuilder} for datasource \"hudi.spark3.internal\" to be used in datasource implementation\n+ * of bulk insert.\n+ */\n+public class HoodieDataSourceInternalBatchWriterBuilder implements WriteBuilder {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjE2MjU2OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxNzozNzoyNlrOIKr_bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxNzozNzoyNlrOIKr_bQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3NzQyMQ==", "bodyText": "does it makes sense to print Arrays.toString(writeStatuses.toArray()) ?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548077421", "createdAt": "2020-12-23T17:37:26Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+\n+import java.util.List;\n+\n+/**\n+ * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n+ */\n+public class BaseHoodieWriterCommitMessage {\n+\n+  private List<HoodieInternalWriteStatus> writeStatuses;\n+\n+  public BaseHoodieWriterCommitMessage(List<HoodieInternalWriteStatus> writeStatuses) {\n+    this.writeStatuses = writeStatuses;\n+  }\n+\n+  public List<HoodieInternalWriteStatus> getWriteStatuses() {\n+    return writeStatuses;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"HoodieWriterCommitMessage{\" + \"writeStatuses=\" + writeStatuses + '}';", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjE2Njc0OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieBulkInsertDataInternalWriterHelper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxNzozODoxOFrOIKsCPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwMToyODoxOFrOIK8JNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3ODE0Mw==", "bodyText": "my bad. since this is not public class per se, we can remove \"Hoodie\" prefix. btw, can you help me understand why this needs to be public? package private should work right?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548078143", "createdAt": "2020-12-23T17:38:18Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieBulkInsertDataInternalWriterHelper.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+/**\n+ * Helper class for HoodieBulkInsertDataInternalWriter used by Spark datasource v2.\n+ */\n+public class HoodieBulkInsertDataInternalWriterHelper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM0MjA3MQ==", "bodyText": "Sure will remove \"Hoodie\" prefix.\nThis class is under hudi-spark-common package, and it's called in HoodieBulkInsertDataInternalWriter which is located in hudi-spark2 & hudi-spark3 package. So it needs to be public.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548342071", "createdAt": "2020-12-24T01:28:18Z", "author": {"login": "zhedoubushishi"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieBulkInsertDataInternalWriterHelper.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+/**\n+ * Helper class for HoodieBulkInsertDataInternalWriter used by Spark datasource v2.\n+ */\n+public class HoodieBulkInsertDataInternalWriterHelper {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3ODE0Mw=="}, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjIxMTA2OnYy", "diffSide": "LEFT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxNzo0NzoyMVrOIKseGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwMzo0NzozMlrOIK904g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA4NTI3Mw==", "bodyText": "2 cents. feel free to take a call. Should we explicitly check only if spark version  is 2 or 3 we will proceed, if not, will throw an exception. just in case, in future, when we have spark 4, we don't need to make any code fixes.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548085273", "createdAt": "2020-12-23T17:47:21Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -130,9 +130,6 @@ private[hudi] object HoodieSparkSqlWriter {\n       // scalastyle:off\n       if (parameters(ENABLE_ROW_WRITER_OPT_KEY).toBoolean &&\n         operation == WriteOperationType.BULK_INSERT) {\n-        if (!SPARK_VERSION.startsWith(\"2.\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM0MDczMg==", "bodyText": "Yea I think this depends on whether Spark 4 will change the datasource API api again, if it keeps the same datasource v2 api, then even for Spark 4 we can still use our exist code, right?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548340732", "createdAt": "2020-12-24T01:21:52Z", "author": {"login": "zhedoubushishi"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -130,9 +130,6 @@ private[hudi] object HoodieSparkSqlWriter {\n       // scalastyle:off\n       if (parameters(ENABLE_ROW_WRITER_OPT_KEY).toBoolean &&\n         operation == WriteOperationType.BULK_INSERT) {\n-        if (!SPARK_VERSION.startsWith(\"2.\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA4NTI3Mw=="}, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM2OTYzNA==", "bodyText": "but unless we test it out, we can't let users try it out and then raise bugs if it doesn't work IMO.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548369634", "createdAt": "2020-12-24T03:47:32Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -130,9 +130,6 @@ private[hudi] object HoodieSparkSqlWriter {\n       // scalastyle:off\n       if (parameters(ENABLE_ROW_WRITER_OPT_KEY).toBoolean &&\n         operation == WriteOperationType.BULK_INSERT) {\n-        if (!SPARK_VERSION.startsWith(\"2.\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA4NTI3Mw=="}, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjM0MzQ4OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxODoxMzo1OVrOIKty4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwMToyMDoxMlrOIK8C0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODEwNjk3OQ==", "bodyText": "help me understand. do we need two separate derived classes for each datasource. Why can't we just use this class directly in both datasources?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548106979", "createdAt": "2020-12-23T18:13:59Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+\n+import java.util.List;\n+\n+/**\n+ * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n+ */\n+public class BaseHoodieWriterCommitMessage {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM0MDQzMw==", "bodyText": "Because Spark 3 relocate this class: WriterCommitMessage.\nIn Spark 2, it's:\nimport org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n\nIn Spark 3, it's:\nimport org.apache.spark.sql.connector.write.WriterCommitMessage;\n\nThat's why we cannot use a single class to do this.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548340433", "createdAt": "2020-12-24T01:20:12Z", "author": {"login": "zhedoubushishi"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+\n+import java.util.List;\n+\n+/**\n+ * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n+ */\n+public class BaseHoodieWriterCommitMessage {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODEwNjk3OQ=="}, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjM2MzExOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxODoxODoxM1rOIKt_xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQwNzoxMjoyOVrOILBLTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMDI3OA==", "bodyText": "can we test both classes in one test class only? may be parametrized?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548110278", "createdAt": "2020-12-23T18:18:13Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -18,63 +18,32 @@\n \n package org.apache.hudi.internal;\n \n-import org.apache.hudi.client.HoodieInternalWriteStatus;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.table.HoodieSparkTable;\n import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.testutils.HoodieClientTestHarness;\n \n-import org.apache.spark.package$;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.jupiter.api.AfterEach;\n-import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n import java.util.ArrayList;\n import java.util.List;\n-import java.util.Random;\n \n import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getInternalRowWithError;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n-import static org.junit.jupiter.api.Assertions.assertFalse;\n-import static org.junit.jupiter.api.Assertions.assertNotNull;\n-import static org.junit.jupiter.api.Assertions.assertNull;\n-import static org.junit.jupiter.api.Assertions.assertTrue;\n import static org.junit.jupiter.api.Assertions.fail;\n-import static org.junit.jupiter.api.Assumptions.assumeTrue;\n \n /**\n  * Unit tests {@link HoodieBulkInsertDataInternalWriter}.\n  */\n-public class TestHoodieBulkInsertDataInternalWriter extends HoodieClientTestHarness {\n-\n-  private static final Random RANDOM = new Random();\n-\n-  @BeforeEach\n-  public void setUp() throws Exception {\n-    // this test is only compatible with spark 2\n-    assumeTrue(package$.MODULE$.SPARK_VERSION().startsWith(\"2.\"));\n-    initSparkContexts(\"TestHoodieBulkInsertDataInternalWriter\");\n-    initPath();\n-    initFileSystem();\n-    initTestDataGenerator();\n-    initMetaClient();\n-  }\n-\n-  @AfterEach\n-  public void tearDown() throws Exception {\n-    cleanupResources();\n-  }\n+public class TestHoodieBulkInsertDataInternalWriter extends", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQyNDUyNA==", "bodyText": "Currently one class mainly tests HoodieBulkInsertDataInternalWriter, one class mainly tests HoodieDataSourceInternalBatchWrite. I am not sure if we should merge them into one. But definitely we can do this.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548424524", "createdAt": "2020-12-24T07:12:29Z", "author": {"login": "zhedoubushishi"}, "path": "hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -18,63 +18,32 @@\n \n package org.apache.hudi.internal;\n \n-import org.apache.hudi.client.HoodieInternalWriteStatus;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.table.HoodieSparkTable;\n import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.testutils.HoodieClientTestHarness;\n \n-import org.apache.spark.package$;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.jupiter.api.AfterEach;\n-import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n import java.util.ArrayList;\n import java.util.List;\n-import java.util.Random;\n \n import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getInternalRowWithError;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n-import static org.junit.jupiter.api.Assertions.assertFalse;\n-import static org.junit.jupiter.api.Assertions.assertNotNull;\n-import static org.junit.jupiter.api.Assertions.assertNull;\n-import static org.junit.jupiter.api.Assertions.assertTrue;\n import static org.junit.jupiter.api.Assertions.fail;\n-import static org.junit.jupiter.api.Assumptions.assumeTrue;\n \n /**\n  * Unit tests {@link HoodieBulkInsertDataInternalWriter}.\n  */\n-public class TestHoodieBulkInsertDataInternalWriter extends HoodieClientTestHarness {\n-\n-  private static final Random RANDOM = new Random();\n-\n-  @BeforeEach\n-  public void setUp() throws Exception {\n-    // this test is only compatible with spark 2\n-    assumeTrue(package$.MODULE$.SPARK_VERSION().startsWith(\"2.\"));\n-    initSparkContexts(\"TestHoodieBulkInsertDataInternalWriter\");\n-    initPath();\n-    initFileSystem();\n-    initTestDataGenerator();\n-    initMetaClient();\n-  }\n-\n-  @AfterEach\n-  public void tearDown() throws Exception {\n-    cleanupResources();\n-  }\n+public class TestHoodieBulkInsertDataInternalWriter extends", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMDI3OA=="}, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjM3NTY1OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxODoyMToxMlrOIKuH4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQxMjo1Mjo0OVrOILHImg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ==", "bodyText": "again I see some opportunity for code reuse here. Do you think we can avoid duplicating the test code across two test classes. Except for few lines, rest could be made common.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548112355", "createdAt": "2020-12-23T18:21:12Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieDataSourceInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+\n+/**\n+ * Unit tests {@link HoodieDataSourceInternalBatchWrite}.\n+ */\n+public class TestHoodieDataSourceInternalBatchWrite extends\n+    HoodieDataSourceInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataSourceWriter() throws Exception {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    String instantTime = \"001\";\n+    // init writer\n+    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n+        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n+    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n+\n+    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQyNzU4Mw==", "bodyText": "Few more comments. thanks for code reuse. now most of our core logic is in one place for both datasources.\nbtw, can you add tests to HoodieSparkSqlWriterSuite as well.\nYou can check out previous tests here.\n\nThanks for the quick review.\nI am not very sure about what kind of test do you want to add here.\nThe test test(\"test bulk insert dataset with datasource impl\") in HoodieSparkSqlWriterSuite can also be used for testing this pr. You can do this through mvn test xxx -Pspark3.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548427583", "createdAt": "2020-12-24T07:24:25Z", "author": {"login": "zhedoubushishi"}, "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieDataSourceInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+\n+/**\n+ * Unit tests {@link HoodieDataSourceInternalBatchWrite}.\n+ */\n+public class TestHoodieDataSourceInternalBatchWrite extends\n+    HoodieDataSourceInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataSourceWriter() throws Exception {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    String instantTime = \"001\";\n+    // init writer\n+    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n+        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n+    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n+\n+    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ=="}, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODQzMjAzMg==", "bodyText": "Yea I can try to reduce the duplicated code between TestHoodieBulkInsertDataInternalWriter and TestHoodieDataSourceInternalBatchWrite.\nThe code of org.apache.hudi.internal.TestHoodieBulkInsertDataInternalWriter is almost the same as  org.apache.hudi.spark3.internal.TestHoodieBulkInsertDataInternalWriter. However it's kind of tricky to avoid this. Because one HoodieBulkInsertDataInternalWriter implements  org.apache.spark.sql.connector.write.DataWriter which is only compatible with Spark 3 but another HoodieBulkInsertDataInternalWriter implements org.apache.spark.sql.sources.v2.writer.DataWriter which is only compatible with Spark 2.\nReflection is one way to solve this problem.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548432032", "createdAt": "2020-12-24T07:40:23Z", "author": {"login": "zhedoubushishi"}, "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieDataSourceInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+\n+/**\n+ * Unit tests {@link HoodieDataSourceInternalBatchWrite}.\n+ */\n+public class TestHoodieDataSourceInternalBatchWrite extends\n+    HoodieDataSourceInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataSourceWriter() throws Exception {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    String instantTime = \"001\";\n+    // init writer\n+    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n+        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n+    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n+\n+    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ=="}, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODUyMjEzOA==", "bodyText": "if we can solve it using Reflection, it might be okay for a test.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548522138", "createdAt": "2020-12-24T12:52:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieDataSourceInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+\n+/**\n+ * Unit tests {@link HoodieDataSourceInternalBatchWrite}.\n+ */\n+public class TestHoodieDataSourceInternalBatchWrite extends\n+    HoodieDataSourceInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataSourceWriter() throws Exception {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    String instantTime = \"001\";\n+    // init writer\n+    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n+        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n+    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n+\n+    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ=="}, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0NjM3OTkyOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxODoyMjoxN1rOIKuKag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxODoyMjoxN1rOIKuKag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMzAwMg==", "bodyText": "same comment as other tests.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548113002", "createdAt": "2020-12-23T18:22:17Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieBulkInsertDataInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getInternalRowWithError;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+/**\n+ * Unit tests {@link HoodieBulkInsertDataInternalWriter}.\n+ */\n+public class TestHoodieBulkInsertDataInternalWriter extends\n+    HoodieBulkInsertDataInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataInternalWriter() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 50}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4010, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}