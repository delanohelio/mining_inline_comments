{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI1MzQ1NTg3", "number": 1687, "title": " [HUDI-684] Introduced abstraction for writing and reading different types of base file formats.", "bodyText": "What is the purpose of the pull request\nThis PR creates a cleaner interface for HUDI to integrate different type of base file formats (ORC, HFILE, etc).\nBrief change log\nThe type of the base file format is chosen during the initial creation of HUDI dataset. The name of the base file format is written to the hoodie.properties file and is available to reset of the modules via HoodieTableConfig.\nWriter side: HoodieStorageWriter\nReader side: HoodieStorageReader\nInputFormat side: HoodieInputFormat and HoodieRealtimeInputFormat\nTo test the interface, I have also implemented support for HFile base file format.\nTODO:\n\nHUDI-960 and HUDI-961 are being implemented to abstract the DataBlock in the Log files. Currently the MOR table log blocks are still being saved as Avro formatted.\n\nVerify this pull request\nAll existing tests should work and pass.\nHoodie Table tests have been parameterized to run with various base file formats.\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-05-29T21:40:18Z", "url": "https://github.com/apache/hudi/pull/1687", "merged": true, "mergeCommit": {"oid": "2603cfb33e272632d7f36a53e1b13fe86dbb8627"}, "closed": true, "closedAt": "2020-06-26T06:46:56Z", "author": {"login": "prashantwason"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcnhCjVgFqTQyMzE2Njg0Mw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcu9dZZgFqTQzODA0NzA5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTY2ODQz", "url": "https://github.com/apache/hudi/pull/1687#pullrequestreview-423166843", "createdAt": "2020-06-03T01:51:47Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMTo1MTo0OFrOGeJeBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMzozNTo0OFrOGeK7Kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2NTYwNg==", "bodyText": "Can we push the sorting to the spark shuffle machinery? repartitionAndSortWithinPartitions(). It cheap and practically free", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434265606", "createdAt": "2020-06-03T01:51:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -133,6 +136,12 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n       // Create the writer for writing the new version file\n       storageWriter =\n           HoodieStorageWriterFactory.getStorageWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n+\n+      if (hoodieTable.requireSortedRecords()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2Nzc2NA==", "bodyText": "If we push sorting to spark, then all the iterators fed to create/merge/append handle will sort records by records key.. for merge handle we can do a simple merge sort style sort merge (instead of hash merge). We just assert that both existing files and incoming records are sorted", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434267764", "createdAt": "2020-06-03T02:01:09Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -214,6 +223,36 @@ private boolean writeRecord(HoodieRecord<T> hoodieRecord, Option<IndexedRecord>\n    */\n   public void write(GenericRecord oldRecord) {\n     String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+\n+    if (hoodieTable.requireSortedRecords()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3MzEwNg==", "bodyText": "The advantage is more modularity as well as low memory overhead to merge..\nNote that we don\u2019t do this for data today because we may not necessarily want the data Sorted by key", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434273106", "createdAt": "2020-06-03T02:24:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -214,6 +223,36 @@ private boolean writeRecord(HoodieRecord<T> hoodieRecord, Option<IndexedRecord>\n    */\n   public void write(GenericRecord oldRecord) {\n     String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+\n+    if (hoodieTable.requireSortedRecords()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2Nzc2NA=="}, "originalCommit": null, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3MzkzNQ==", "bodyText": "I did not review this block too closely. Let\u2019s settle on the high level approach first", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434273935", "createdAt": "2020-06-03T02:28:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -214,6 +223,36 @@ private boolean writeRecord(HoodieRecord<T> hoodieRecord, Option<IndexedRecord>\n    */\n   public void write(GenericRecord oldRecord) {\n     String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+\n+    if (hoodieTable.requireSortedRecords()) {\n+      // To maintain overall sorted order across updates and inserts, write any new inserts whose keys are less than\n+      // the oldRecord's key.\n+      while (!newRecordKeysSorted.isEmpty() && newRecordKeysSorted.peek().compareTo(key) <= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NDY1MQ==", "bodyText": "HFilev itself contains bloom filters right.. why do we need them outside?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434274651", "createdAt": "2020-06-03T02:31:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieStorageWriterFactory.java", "diffHunk": "@@ -66,4 +67,21 @@\n \n     return new HoodieParquetWriter<>(instantTime, path, parquetConfig, schema, sparkTaskContextSupplier);\n   }\n+\n+  private static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> newHFileStorageWriter(\n+      String instantTime, Path path, HoodieWriteConfig config, Schema schema, HoodieTable hoodieTable,\n+      SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+\n+    BloomFilter filter = createBloomFilter(config);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NTE1OQ==", "bodyText": "StorageWriter is not a great name really.. let\u2019s rename consistently to HoodieFileWriter/HoodieFileWriterFactory?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434275159", "createdAt": "2020-06-03T02:33:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieStorageWriterFactory.java", "diffHunk": "@@ -34,29 +35,29 @@\n \n import java.io.IOException;\n \n-import static org.apache.hudi.common.model.HoodieFileFormat.HOODIE_LOG;\n import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;\n+import static org.apache.hudi.common.model.HoodieFileFormat.HFILE;\n \n public class HoodieStorageWriterFactory {\n \n   public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(\n       String instantTime, Path path, HoodieTable<T> hoodieTable, HoodieWriteConfig config, Schema schema,\n       SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n-    final String name = path.getName();\n-    final String extension = FSUtils.isLogFile(path) ? HOODIE_LOG.getFileExtension() : FSUtils.getFileExtension(name);\n+    final HoodieTableConfig tableConfig = hoodieTable.getMetaClient().getTableConfig();\n+    final String extension = FSUtils.isLogFile(path) ? tableConfig.getLogFileFormat().getFileExtension() : tableConfig.getBaseFileFormat().getFileExtension();\n     if (PARQUET.getFileExtension().equals(extension)) {\n       return newParquetStorageWriter(instantTime, path, config, schema, hoodieTable, sparkTaskContextSupplier);\n     }\n+    if (HFILE.getFileExtension().equals(extension)) {\n+      return newHFileStorageWriter(instantTime, path, config, schema, hoodieTable, sparkTaskContextSupplier);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4Mjg4Nw==", "bodyText": "Assume this actually pushed the projection predicates down??", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434282887", "createdAt": "2020-06-03T03:07:09Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionExecutor.java", "diffHunk": "@@ -89,11 +87,12 @@ public CommitActionExecutor(JavaSparkContext jsc,\n       throw new HoodieUpsertException(\n           \"Error in finding the old file path at commit \" + instantTime + \" for fileId: \" + fileId);\n     } else {\n-      AvroReadSupport.setAvroReadSchema(table.getHadoopConf(), upsertHandle.getWriterSchema());\n       BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n-      try (ParquetReader<IndexedRecord> reader =\n-          AvroParquetReader.<IndexedRecord>builder(upsertHandle.getOldFilePath()).withConf(table.getHadoopConf()).build()) {\n-        wrapper = new SparkBoundedInMemoryExecutor(config, new ParquetReaderIterator(reader),\n+      try {\n+        HoodieStorageReader<IndexedRecord> storageReader =\n+            HoodieStorageReaderFactory.getStorageReader(table.getHadoopConf(), upsertHandle.getOldFilePath());\n+        wrapper =\n+            new SparkBoundedInMemoryExecutor(config, storageReader.getRecordIterator(upsertHandle.getWriterSchema()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MzYwNw==", "bodyText": "Have you tried to fish out all such occurrences in this pr. This is great!", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434283607", "createdAt": "2020-06-03T03:10:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/RollbackHelper.java", "diffHunk": "@@ -71,8 +71,9 @@ public RollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config\n    */\n   public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<RollbackRequest> rollbackRequests) {\n \n+    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n     SerializablePathFilter filter = (path) -> {\n-      if (path.toString().contains(\".parquet\")) {\n+      if (path.toString().contains(basefileExtension)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDMzMQ==", "bodyText": "hudi-common has Scala now? Why do we need this", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434284331", "createdAt": "2020-06-03T03:13:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/pom.xml", "diffHunk": "@@ -78,6 +92,38 @@\n           </imports>\n         </configuration>\n       </plugin>\n+      <plugin>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDQ3OQ==", "bodyText": "Please pull this into a variable in parent pom", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434284479", "createdAt": "2020-06-03T03:14:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/pom.xml", "diffHunk": "@@ -201,7 +247,26 @@\n       <groupId>org.apache.hbase</groupId>\n       <artifactId>hbase-server</artifactId>\n       <version>${hbase.version}</version>\n-      <scope>test</scope>\n+      <scope>provided</scope>\n+    </dependency>\n+\n+    <!-- Spark -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-sql_${scala.binary.version}</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+\n+    <!-- spark-avro -->\n+    <dependency>\n+      <groupId>com.databricks</groupId>\n+      <artifactId>spark-avro_${scala.binary.version}</artifactId>\n+      <version>4.0.0</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDU5Mw==", "bodyText": "Why do we need this? hudi-common having spark is an anti pattern", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434284593", "createdAt": "2020-06-03T03:14:43Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/pom.xml", "diffHunk": "@@ -201,7 +247,26 @@\n       <groupId>org.apache.hbase</groupId>\n       <artifactId>hbase-server</artifactId>\n       <version>${hbase.version}</version>\n-      <scope>test</scope>\n+      <scope>provided</scope>\n+    </dependency>\n+\n+    <!-- Spark -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-sql_${scala.binary.version}</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NTM4NA==", "bodyText": "Let\u2019s rename HoodieStorageReader to HoodieFileReader (also the factory)", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434285384", "createdAt": "2020-06-03T03:17:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import scala.Tuple2;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieStorageReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NTYzMQ==", "bodyText": "Use of raw type", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434285631", "createdAt": "2020-06-03T03:19:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import scala.Tuple2;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieStorageReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  @Override\n+  public String[] readMinMaxRecordKeys() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      return new String[] { new String(fileInfo.get(KEY_MIN_RECORD.getBytes())),\n+          new String(fileInfo.get(KEY_MAX_RECORD.getBytes()))};\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read min/max record key out of file information block correctly from path\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Schema getSchema() {\n+    try {\n+      Map<byte[], byte[]> fileInfo = reader.loadFileInfo();\n+      return new Schema.Parser().parse(new String(fileInfo.get(KEY_SCHEMA.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read schema of file from path\", e);\n+    }\n+\n+  }\n+\n+  @Override\n+  public BloomFilter readBloomFilter() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      ByteBuffer serializedFilter = reader.getMetaBlock(KEY_BLOOM_FILTER_META_BLOCK, false);\n+      byte[] filterBytes = new byte[serializedFilter.remaining()];\n+      serializedFilter.get(filterBytes); // read the bytes that were written\n+      return BloomFilterFactory.fromString(new String(filterBytes),\n+          new String(fileInfo.get(KEY_BLOOM_FILTER_TYPE_CODE.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read bloom filter from \" + path, e);\n+    }\n+  }\n+\n+  @Override\n+  public Set<String> filterRowKeys(Set candidateRowKeys) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjE1MA==", "bodyText": "Introducing scala into hudi-common is a no-go.. there are non spark query bundles built off this, which will all need different scala  version artifacts", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434286150", "createdAt": "2020-06-03T03:21:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/scala/com/databricks/spark/avro/HoodieAvroSchemaConversion.scala", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.databricks.spark.avro\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.GenericRecord\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.types.DataType\n+\n+/**\n+ * This helper class is required since SchemaConverters.createConverterToSQL is currently private.\n+ */\n+object HoodieAvroSchemaConversion {\n+  def createConverterToSQL(avroSchema: Schema, sparkSchema: DataType): (GenericRecord) => Row =", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjM5NA==", "bodyText": "Why do we need this?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434286394", "createdAt": "2020-06-03T03:22:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieHFileInputFormat.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+\n+import java.io.IOException;\n+\n+/**\n+ * HoodieInputFormat for HUDI datasets which store data in HFile base file format.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjkzMQ==", "bodyText": "This code is being actively refactored by @garyli1019  and @bhasudha  as well. Gary\u2019s pr will land soon I believe..", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434286931", "createdAt": "2020-06-03T03:25:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java", "diffHunk": "@@ -18,339 +18,14 @@\n \n package org.apache.hudi.hadoop;\n \n-import org.apache.hadoop.conf.Configurable;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n-import org.apache.hadoop.io.ArrayWritable;\n-import org.apache.hadoop.io.NullWritable;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapred.RecordReader;\n-import org.apache.hadoop.mapred.Reporter;\n-import org.apache.hadoop.mapreduce.Job;\n-import org.apache.hudi.common.model.HoodieBaseFile;\n-import org.apache.hudi.common.model.HoodieCommitMetadata;\n-import org.apache.hudi.common.model.HoodiePartitionMetadata;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieTimeline;\n-import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n-import org.apache.hudi.common.table.view.TableFileSystemView.BaseFileOnlyView;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.StringUtils;\n-import org.apache.hudi.exception.HoodieIOException;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.stream.Collectors;\n \n /**\n- * HoodieInputFormat which understands the Hoodie File Structure and filters files based on the Hoodie Mode. If paths\n- * that does not correspond to a hoodie table then they are passed in as is (as what FileInputFormat.listStatus()\n- * would do). The JobConf could have paths from multipe Hoodie/Non-Hoodie tables\n+ * HoodieInputFormat for HUDI datasets which store data in Parquet base file format.\n  */\n @UseFileSplitsFromInputFormat\n-public class HoodieParquetInputFormat extends MapredParquetInputFormat implements Configurable {\n-\n-  private static final Logger LOG = LogManager.getLogger(HoodieParquetInputFormat.class);\n-\n-  protected Configuration conf;\n-\n-  @Override\n-  public FileStatus[] listStatus(JobConf job) throws IOException {\n-    // Segregate inputPaths[] to incremental, snapshot and non hoodie paths\n-    List<String> incrementalTables = HoodieHiveUtil.getIncrementalTableNames(Job.getInstance(job));\n-    InputPathHandler inputPathHandler = new InputPathHandler(conf, getInputPaths(job), incrementalTables);\n-    List<FileStatus> returns = new ArrayList<>();\n-\n-    Map<String, HoodieTableMetaClient> tableMetaClientMap = inputPathHandler.getTableMetaClientMap();\n-    // process incremental pulls first\n-    for (String table : incrementalTables) {\n-      HoodieTableMetaClient metaClient = tableMetaClientMap.get(table);\n-      if (metaClient == null) {\n-        /* This can happen when the INCREMENTAL mode is set for a table but there were no InputPaths\n-         * in the jobConf\n-         */\n-        continue;\n-      }\n-      List<Path> inputPaths = inputPathHandler.getGroupedIncrementalPaths().get(metaClient);\n-      List<FileStatus> result = listStatusForIncrementalMode(job, metaClient, inputPaths);\n-      if (result != null) {\n-        returns.addAll(result);\n-      }\n-    }\n-\n-    // process non hoodie Paths next.\n-    List<Path> nonHoodiePaths = inputPathHandler.getNonHoodieInputPaths();\n-    if (nonHoodiePaths.size() > 0) {\n-      setInputPaths(job, nonHoodiePaths.toArray(new Path[nonHoodiePaths.size()]));\n-      FileStatus[] fileStatuses = super.listStatus(job);\n-      returns.addAll(Arrays.asList(fileStatuses));\n-    }\n-\n-    // process snapshot queries next.\n-    List<Path> snapshotPaths = inputPathHandler.getSnapshotPaths();\n-    if (snapshotPaths.size() > 0) {\n-      setInputPaths(job, snapshotPaths.toArray(new Path[snapshotPaths.size()]));\n-      FileStatus[] fileStatuses = super.listStatus(job);\n-      Map<HoodieTableMetaClient, List<FileStatus>> groupedFileStatus =\n-          groupFileStatusForSnapshotPaths(fileStatuses, tableMetaClientMap.values());\n-      LOG.info(\"Found a total of \" + groupedFileStatus.size() + \" groups\");\n-      for (Map.Entry<HoodieTableMetaClient, List<FileStatus>> entry : groupedFileStatus.entrySet()) {\n-        List<FileStatus> result = filterFileStatusForSnapshotMode(entry.getKey(), entry.getValue());\n-        if (result != null) {\n-          returns.addAll(result);\n-        }\n-      }\n-    }\n-    return returns.toArray(new FileStatus[returns.size()]);\n-  }\n-\n-  /**\n-   * Filter any specific instants that we do not want to process.\n-   * example timeline:\n-   *\n-   * t0 -> create bucket1.parquet\n-   * t1 -> create and append updates bucket1.log\n-   * t2 -> request compaction\n-   * t3 -> create bucket2.parquet\n-   *\n-   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n-   *\n-   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n-   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n-   */\n-  protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n-    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n-    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline.filterPendingCompactionTimeline().firstInstant();\n-    if (pendingCompactionInstant.isPresent()) {\n-      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline.findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n-      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n-          - instantsTimeline.getCommitsTimeline().countInstants();\n-      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n-              + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n-\n-      return instantsTimeline;\n-    } else {\n-      return timeline;\n-    }\n-  }\n-\n-  /**\n-   * Achieves listStatus functionality for an incrementally queried table. Instead of listing all\n-   * partitions and then filtering based on the commits of interest, this logic first extracts the\n-   * partitions touched by the desired commits and then lists only those partitions.\n-   */\n-  private List<FileStatus> listStatusForIncrementalMode(\n-      JobConf job, HoodieTableMetaClient tableMetaClient, List<Path> inputPaths) throws IOException {\n-    String tableName = tableMetaClient.getTableConfig().getTableName();\n-    Job jobContext = Job.getInstance(job);\n-    HoodieDefaultTimeline baseTimeline;\n-    if (HoodieHiveUtil.stopAtCompaction(jobContext, tableName)) {\n-      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n-    } else {\n-      baseTimeline = tableMetaClient.getActiveTimeline();\n-    }\n-\n-    HoodieTimeline timeline = baseTimeline.getCommitsTimeline().filterCompletedInstants();\n-    String lastIncrementalTs = HoodieHiveUtil.readStartCommitTime(jobContext, tableName);\n-    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n-    Integer maxCommits = HoodieHiveUtil.readMaxCommits(jobContext, tableName);\n-    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n-    List<HoodieInstant> commitsToCheck = timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n-        .getInstants().collect(Collectors.toList());\n-    // Extract partitions touched by the commitsToCheck\n-    Set<String> partitionsToList = new HashSet<>();\n-    for (HoodieInstant commit : commitsToCheck) {\n-      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n-          HoodieCommitMetadata.class);\n-      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n-    }\n-    if (partitionsToList.isEmpty()) {\n-      return null;\n-    }\n-    String incrementalInputPaths = partitionsToList.stream()\n-        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n-        .filter(s -> {\n-          /*\n-           * Ensure to return only results from the original input path that has incremental changes\n-           * This check is needed for the following corner case -  When the caller invokes\n-           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n-           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n-           * accidentally return all incremental changes for the entire table in every listStatus()\n-           * call. This will create redundant splits. Instead we only want to return the incremental\n-           * changes (if so any) in that batch of input paths.\n-           *\n-           * NOTE on Hive queries that are executed using Fetch task:\n-           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n-           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n-           * disable fetch tasks using the hive session property for incremental queries:\n-           * `set hive.fetch.task.conversion=none;`\n-           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n-           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n-           * those partitions.\n-           */\n-          for (Path path : inputPaths) {\n-            if (path.toString().contains(s)) {\n-              return true;\n-            }\n-          }\n-          return false;\n-        })\n-        .collect(Collectors.joining(\",\"));\n-    if (StringUtils.isNullOrEmpty(incrementalInputPaths)) {\n-      return null;\n-    }\n-    // Mutate the JobConf to set the input paths to only partitions touched by incremental pull.\n-    setInputPaths(job, incrementalInputPaths);\n-    FileStatus[] fileStatuses = super.listStatus(job);\n-    BaseFileOnlyView roView = new HoodieTableFileSystemView(tableMetaClient, timeline, fileStatuses);\n-    List<String> commitsList = commitsToCheck.stream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFilesInRange(commitsList).collect(Collectors.toList());\n-    List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n-      LOG.debug(\"Processing incremental hoodie file - \" + filteredFile.getPath());\n-      filteredFile = checkFileStatus(filteredFile);\n-      returns.add(filteredFile.getFileStatus());\n-    }\n-    LOG.info(\"Total paths to process after hoodie incremental filter \" + filteredFiles.size());\n-    return returns;\n-  }\n-\n-  /**\n-   * Takes in a list of filesStatus and a list of table metadatas. Groups the files status list\n-   * based on given table metadata.\n-   * @param fileStatuses\n-   * @param metaClientList\n-   * @return\n-   * @throws IOException\n-   */\n-  private Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatusForSnapshotPaths(\n-      FileStatus[] fileStatuses, Collection<HoodieTableMetaClient> metaClientList) {\n-    // This assumes the paths for different tables are grouped together\n-    Map<HoodieTableMetaClient, List<FileStatus>> grouped = new HashMap<>();\n-    HoodieTableMetaClient metadata = null;\n-    for (FileStatus status : fileStatuses) {\n-      Path inputPath = status.getPath();\n-      if (!inputPath.getName().endsWith(\".parquet\")) {\n-        //FIXME(vc): skip non parquet files for now. This wont be needed once log file name start\n-        // with \".\"\n-        continue;\n-      }\n-      if ((metadata == null) || (!inputPath.toString().contains(metadata.getBasePath()))) {\n-        for (HoodieTableMetaClient metaClient : metaClientList) {\n-          if (inputPath.toString().contains(metaClient.getBasePath())) {\n-            metadata = metaClient;\n-            if (!grouped.containsKey(metadata)) {\n-              grouped.put(metadata, new ArrayList<>());\n-            }\n-            break;\n-          }\n-        }\n-      }\n-      grouped.get(metadata).add(status);\n-    }\n-    return grouped;\n-  }\n-\n-  /**\n-   * Filters data files for a snapshot queried table.\n-   */\n-  private List<FileStatus> filterFileStatusForSnapshotMode(\n-      HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n-    }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n-    List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Processing latest hoodie file - \" + filteredFile.getPath());\n-      }\n-      filteredFile = checkFileStatus(filteredFile);\n-      returns.add(filteredFile.getFileStatus());\n-    }\n-    return returns;\n-  }\n-\n-  /**\n-   * Checks the file status for a race condition which can set the file size to 0. 1. HiveInputFormat does\n-   * super.listStatus() and gets back a FileStatus[] 2. Then it creates the HoodieTableMetaClient for the paths listed.\n-   * 3. Generation of splits looks at FileStatus size to create splits, which skips this file\n-   */\n-  private HoodieBaseFile checkFileStatus(HoodieBaseFile dataFile) {\n-    Path dataPath = dataFile.getFileStatus().getPath();\n-    try {\n-      if (dataFile.getFileSize() == 0) {\n-        FileSystem fs = dataPath.getFileSystem(conf);\n-        LOG.info(\"Refreshing file status \" + dataFile.getPath());\n-        return new HoodieBaseFile(fs.getFileStatus(dataPath));\n-      }\n-      return dataFile;\n-    } catch (IOException e) {\n-      throw new HoodieIOException(\"Could not get FileStatus on path \" + dataPath);\n-    }\n-  }\n-\n-  public void setConf(Configuration conf) {\n-    this.conf = conf;\n-  }\n-\n-  @Override\n-  public Configuration getConf() {\n-    return conf;\n-  }\n-\n-  @Override\n-  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf job,\n-      final Reporter reporter) throws IOException {\n-    // TODO enable automatic predicate pushdown after fixing issues\n-    // FileSplit fileSplit = (FileSplit) split;\n-    // HoodieTableMetadata metadata = getTableMetadata(fileSplit.getPath().getParent());\n-    // String tableName = metadata.getTableName();\n-    // String mode = HoodieHiveUtil.readMode(job, tableName);\n-\n-    // if (HoodieHiveUtil.INCREMENTAL_SCAN_MODE.equals(mode)) {\n-    // FilterPredicate predicate = constructHoodiePredicate(job, tableName, split);\n-    // LOG.info(\"Setting parquet predicate push down as \" + predicate);\n-    // ParquetInputFormat.setFilterPredicate(job, predicate);\n-    // clearOutExistingPredicate(job);\n-    // }\n-    return super.getRecordReader(split, job, reporter);\n-  }\n-\n-  /**\n-   * Read the table metadata from a data path. This assumes certain hierarchy of files which should be changed once a\n-   * better way is figured out to pass in the hoodie meta directory\n-   */\n-  protected static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {\n-    int levels = HoodieHiveUtil.DEFAULT_LEVELS_TO_BASEPATH;\n-    if (HoodiePartitionMetadata.hasPartitionMetadata(fs, dataPath)) {\n-      HoodiePartitionMetadata metadata = new HoodiePartitionMetadata(fs, dataPath);\n-      metadata.readFromFS();\n-      levels = metadata.getPartitionDepth();\n-    }\n-    Path baseDir = HoodieHiveUtil.getNthParent(dataPath, levels);\n-    LOG.info(\"Reading hoodie metadata from path \" + baseDir.toString());\n-    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());\n+public class HoodieParquetInputFormat extends HoodieInputFormat {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 339}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzI1NA==", "bodyText": "Is this change strictly necessary for this pr", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434287254", "createdAt": "2020-06-03T03:26:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java", "diffHunk": "@@ -872,7 +873,7 @@ public void createPool(JobConf conf, PathFilter... filters) {\n         job.set(\"hudi.hive.realtime\", \"true\");\n         InputSplit[] splits;\n         if (hoodieFilter) {\n-          HoodieParquetInputFormat input = new HoodieParquetRealtimeInputFormat();\n+          HoodieParquetRealtimeInputFormat input = new HoodieParquetRealtimeInputFormat();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzQ2Ng==", "bodyText": "This is a behavior change.. ccc @n3nash  to confirm if this is ok", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434287466", "createdAt": "2020-06-03T03:27:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java", "diffHunk": "@@ -872,7 +873,7 @@ public void createPool(JobConf conf, PathFilter... filters) {\n         job.set(\"hudi.hive.realtime\", \"true\");\n         InputSplit[] splits;\n         if (hoodieFilter) {\n-          HoodieParquetInputFormat input = new HoodieParquetRealtimeInputFormat();\n+          HoodieParquetRealtimeInputFormat input = new HoodieParquetRealtimeInputFormat();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzI1NA=="}, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzgyNg==", "bodyText": "Same thing here. This code is already changing momentarily", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434287826", "createdAt": "2020-06-03T03:28:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java", "diffHunk": "@@ -18,250 +18,23 @@\n \n package org.apache.hudi.hadoop.realtime;\n \n-import org.apache.hudi.common.fs.FSUtils;\n-import org.apache.hudi.common.model.FileSlice;\n-import org.apache.hudi.common.model.HoodieLogFile;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieTimeline;\n-import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n-import org.apache.hudi.common.util.CollectionUtils;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.exception.HoodieException;\n-import org.apache.hudi.exception.HoodieIOException;\n-import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n import org.apache.hudi.hadoop.UseFileSplitsFromInputFormat;\n \n-import org.apache.hadoop.conf.Configurable;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n-import org.apache.hadoop.io.ArrayWritable;\n-import org.apache.hadoop.io.NullWritable;\n-import org.apache.hadoop.mapred.FileSplit;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapred.RecordReader;\n-import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n import org.apache.hudi.hadoop.UseRecordReaderFromInputFormat;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n-import java.util.stream.Stream;\n \n /**\n- * Input Format, that provides a real-time view of data in a Hoodie table.\n+ * HoodieRealtimeInputFormat for HUDI datasets which store data in Parquet base file format.\n  */\n @UseRecordReaderFromInputFormat\n @UseFileSplitsFromInputFormat\n-public class HoodieParquetRealtimeInputFormat extends HoodieParquetInputFormat implements Configurable {\n-\n-  private static final Logger LOG = LogManager.getLogger(HoodieParquetRealtimeInputFormat.class);\n-\n-  // These positions have to be deterministic across all tables\n-  public static final int HOODIE_COMMIT_TIME_COL_POS = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4OTA3Mg==", "bodyText": "AFAIK hive will check in places whether the registered input format is instanceof ParquetInputFormat.. for applying certain optimizations. It\u2019s probably better to not change the class hierarchy, but try to abstract by using more modular classes", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434289072", "createdAt": "2020-06-03T03:34:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java", "diffHunk": "@@ -18,339 +18,14 @@\n \n package org.apache.hudi.hadoop;\n \n-import org.apache.hadoop.conf.Configurable;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n-import org.apache.hadoop.io.ArrayWritable;\n-import org.apache.hadoop.io.NullWritable;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapred.RecordReader;\n-import org.apache.hadoop.mapred.Reporter;\n-import org.apache.hadoop.mapreduce.Job;\n-import org.apache.hudi.common.model.HoodieBaseFile;\n-import org.apache.hudi.common.model.HoodieCommitMetadata;\n-import org.apache.hudi.common.model.HoodiePartitionMetadata;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieTimeline;\n-import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n-import org.apache.hudi.common.table.view.TableFileSystemView.BaseFileOnlyView;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.StringUtils;\n-import org.apache.hudi.exception.HoodieIOException;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.stream.Collectors;\n \n /**\n- * HoodieInputFormat which understands the Hoodie File Structure and filters files based on the Hoodie Mode. If paths\n- * that does not correspond to a hoodie table then they are passed in as is (as what FileInputFormat.listStatus()\n- * would do). The JobConf could have paths from multipe Hoodie/Non-Hoodie tables\n+ * HoodieInputFormat for HUDI datasets which store data in Parquet base file format.\n  */\n @UseFileSplitsFromInputFormat\n-public class HoodieParquetInputFormat extends MapredParquetInputFormat implements Configurable {\n-\n-  private static final Logger LOG = LogManager.getLogger(HoodieParquetInputFormat.class);\n-\n-  protected Configuration conf;\n-\n-  @Override\n-  public FileStatus[] listStatus(JobConf job) throws IOException {\n-    // Segregate inputPaths[] to incremental, snapshot and non hoodie paths\n-    List<String> incrementalTables = HoodieHiveUtil.getIncrementalTableNames(Job.getInstance(job));\n-    InputPathHandler inputPathHandler = new InputPathHandler(conf, getInputPaths(job), incrementalTables);\n-    List<FileStatus> returns = new ArrayList<>();\n-\n-    Map<String, HoodieTableMetaClient> tableMetaClientMap = inputPathHandler.getTableMetaClientMap();\n-    // process incremental pulls first\n-    for (String table : incrementalTables) {\n-      HoodieTableMetaClient metaClient = tableMetaClientMap.get(table);\n-      if (metaClient == null) {\n-        /* This can happen when the INCREMENTAL mode is set for a table but there were no InputPaths\n-         * in the jobConf\n-         */\n-        continue;\n-      }\n-      List<Path> inputPaths = inputPathHandler.getGroupedIncrementalPaths().get(metaClient);\n-      List<FileStatus> result = listStatusForIncrementalMode(job, metaClient, inputPaths);\n-      if (result != null) {\n-        returns.addAll(result);\n-      }\n-    }\n-\n-    // process non hoodie Paths next.\n-    List<Path> nonHoodiePaths = inputPathHandler.getNonHoodieInputPaths();\n-    if (nonHoodiePaths.size() > 0) {\n-      setInputPaths(job, nonHoodiePaths.toArray(new Path[nonHoodiePaths.size()]));\n-      FileStatus[] fileStatuses = super.listStatus(job);\n-      returns.addAll(Arrays.asList(fileStatuses));\n-    }\n-\n-    // process snapshot queries next.\n-    List<Path> snapshotPaths = inputPathHandler.getSnapshotPaths();\n-    if (snapshotPaths.size() > 0) {\n-      setInputPaths(job, snapshotPaths.toArray(new Path[snapshotPaths.size()]));\n-      FileStatus[] fileStatuses = super.listStatus(job);\n-      Map<HoodieTableMetaClient, List<FileStatus>> groupedFileStatus =\n-          groupFileStatusForSnapshotPaths(fileStatuses, tableMetaClientMap.values());\n-      LOG.info(\"Found a total of \" + groupedFileStatus.size() + \" groups\");\n-      for (Map.Entry<HoodieTableMetaClient, List<FileStatus>> entry : groupedFileStatus.entrySet()) {\n-        List<FileStatus> result = filterFileStatusForSnapshotMode(entry.getKey(), entry.getValue());\n-        if (result != null) {\n-          returns.addAll(result);\n-        }\n-      }\n-    }\n-    return returns.toArray(new FileStatus[returns.size()]);\n-  }\n-\n-  /**\n-   * Filter any specific instants that we do not want to process.\n-   * example timeline:\n-   *\n-   * t0 -> create bucket1.parquet\n-   * t1 -> create and append updates bucket1.log\n-   * t2 -> request compaction\n-   * t3 -> create bucket2.parquet\n-   *\n-   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n-   *\n-   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n-   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n-   */\n-  protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n-    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n-    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline.filterPendingCompactionTimeline().firstInstant();\n-    if (pendingCompactionInstant.isPresent()) {\n-      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline.findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n-      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n-          - instantsTimeline.getCommitsTimeline().countInstants();\n-      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n-              + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n-\n-      return instantsTimeline;\n-    } else {\n-      return timeline;\n-    }\n-  }\n-\n-  /**\n-   * Achieves listStatus functionality for an incrementally queried table. Instead of listing all\n-   * partitions and then filtering based on the commits of interest, this logic first extracts the\n-   * partitions touched by the desired commits and then lists only those partitions.\n-   */\n-  private List<FileStatus> listStatusForIncrementalMode(\n-      JobConf job, HoodieTableMetaClient tableMetaClient, List<Path> inputPaths) throws IOException {\n-    String tableName = tableMetaClient.getTableConfig().getTableName();\n-    Job jobContext = Job.getInstance(job);\n-    HoodieDefaultTimeline baseTimeline;\n-    if (HoodieHiveUtil.stopAtCompaction(jobContext, tableName)) {\n-      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n-    } else {\n-      baseTimeline = tableMetaClient.getActiveTimeline();\n-    }\n-\n-    HoodieTimeline timeline = baseTimeline.getCommitsTimeline().filterCompletedInstants();\n-    String lastIncrementalTs = HoodieHiveUtil.readStartCommitTime(jobContext, tableName);\n-    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n-    Integer maxCommits = HoodieHiveUtil.readMaxCommits(jobContext, tableName);\n-    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n-    List<HoodieInstant> commitsToCheck = timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n-        .getInstants().collect(Collectors.toList());\n-    // Extract partitions touched by the commitsToCheck\n-    Set<String> partitionsToList = new HashSet<>();\n-    for (HoodieInstant commit : commitsToCheck) {\n-      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n-          HoodieCommitMetadata.class);\n-      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n-    }\n-    if (partitionsToList.isEmpty()) {\n-      return null;\n-    }\n-    String incrementalInputPaths = partitionsToList.stream()\n-        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n-        .filter(s -> {\n-          /*\n-           * Ensure to return only results from the original input path that has incremental changes\n-           * This check is needed for the following corner case -  When the caller invokes\n-           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n-           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n-           * accidentally return all incremental changes for the entire table in every listStatus()\n-           * call. This will create redundant splits. Instead we only want to return the incremental\n-           * changes (if so any) in that batch of input paths.\n-           *\n-           * NOTE on Hive queries that are executed using Fetch task:\n-           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n-           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n-           * disable fetch tasks using the hive session property for incremental queries:\n-           * `set hive.fetch.task.conversion=none;`\n-           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n-           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n-           * those partitions.\n-           */\n-          for (Path path : inputPaths) {\n-            if (path.toString().contains(s)) {\n-              return true;\n-            }\n-          }\n-          return false;\n-        })\n-        .collect(Collectors.joining(\",\"));\n-    if (StringUtils.isNullOrEmpty(incrementalInputPaths)) {\n-      return null;\n-    }\n-    // Mutate the JobConf to set the input paths to only partitions touched by incremental pull.\n-    setInputPaths(job, incrementalInputPaths);\n-    FileStatus[] fileStatuses = super.listStatus(job);\n-    BaseFileOnlyView roView = new HoodieTableFileSystemView(tableMetaClient, timeline, fileStatuses);\n-    List<String> commitsList = commitsToCheck.stream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFilesInRange(commitsList).collect(Collectors.toList());\n-    List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n-      LOG.debug(\"Processing incremental hoodie file - \" + filteredFile.getPath());\n-      filteredFile = checkFileStatus(filteredFile);\n-      returns.add(filteredFile.getFileStatus());\n-    }\n-    LOG.info(\"Total paths to process after hoodie incremental filter \" + filteredFiles.size());\n-    return returns;\n-  }\n-\n-  /**\n-   * Takes in a list of filesStatus and a list of table metadatas. Groups the files status list\n-   * based on given table metadata.\n-   * @param fileStatuses\n-   * @param metaClientList\n-   * @return\n-   * @throws IOException\n-   */\n-  private Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatusForSnapshotPaths(\n-      FileStatus[] fileStatuses, Collection<HoodieTableMetaClient> metaClientList) {\n-    // This assumes the paths for different tables are grouped together\n-    Map<HoodieTableMetaClient, List<FileStatus>> grouped = new HashMap<>();\n-    HoodieTableMetaClient metadata = null;\n-    for (FileStatus status : fileStatuses) {\n-      Path inputPath = status.getPath();\n-      if (!inputPath.getName().endsWith(\".parquet\")) {\n-        //FIXME(vc): skip non parquet files for now. This wont be needed once log file name start\n-        // with \".\"\n-        continue;\n-      }\n-      if ((metadata == null) || (!inputPath.toString().contains(metadata.getBasePath()))) {\n-        for (HoodieTableMetaClient metaClient : metaClientList) {\n-          if (inputPath.toString().contains(metaClient.getBasePath())) {\n-            metadata = metaClient;\n-            if (!grouped.containsKey(metadata)) {\n-              grouped.put(metadata, new ArrayList<>());\n-            }\n-            break;\n-          }\n-        }\n-      }\n-      grouped.get(metadata).add(status);\n-    }\n-    return grouped;\n-  }\n-\n-  /**\n-   * Filters data files for a snapshot queried table.\n-   */\n-  private List<FileStatus> filterFileStatusForSnapshotMode(\n-      HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n-    }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n-    List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Processing latest hoodie file - \" + filteredFile.getPath());\n-      }\n-      filteredFile = checkFileStatus(filteredFile);\n-      returns.add(filteredFile.getFileStatus());\n-    }\n-    return returns;\n-  }\n-\n-  /**\n-   * Checks the file status for a race condition which can set the file size to 0. 1. HiveInputFormat does\n-   * super.listStatus() and gets back a FileStatus[] 2. Then it creates the HoodieTableMetaClient for the paths listed.\n-   * 3. Generation of splits looks at FileStatus size to create splits, which skips this file\n-   */\n-  private HoodieBaseFile checkFileStatus(HoodieBaseFile dataFile) {\n-    Path dataPath = dataFile.getFileStatus().getPath();\n-    try {\n-      if (dataFile.getFileSize() == 0) {\n-        FileSystem fs = dataPath.getFileSystem(conf);\n-        LOG.info(\"Refreshing file status \" + dataFile.getPath());\n-        return new HoodieBaseFile(fs.getFileStatus(dataPath));\n-      }\n-      return dataFile;\n-    } catch (IOException e) {\n-      throw new HoodieIOException(\"Could not get FileStatus on path \" + dataPath);\n-    }\n-  }\n-\n-  public void setConf(Configuration conf) {\n-    this.conf = conf;\n-  }\n-\n-  @Override\n-  public Configuration getConf() {\n-    return conf;\n-  }\n-\n-  @Override\n-  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf job,\n-      final Reporter reporter) throws IOException {\n-    // TODO enable automatic predicate pushdown after fixing issues\n-    // FileSplit fileSplit = (FileSplit) split;\n-    // HoodieTableMetadata metadata = getTableMetadata(fileSplit.getPath().getParent());\n-    // String tableName = metadata.getTableName();\n-    // String mode = HoodieHiveUtil.readMode(job, tableName);\n-\n-    // if (HoodieHiveUtil.INCREMENTAL_SCAN_MODE.equals(mode)) {\n-    // FilterPredicate predicate = constructHoodiePredicate(job, tableName, split);\n-    // LOG.info(\"Setting parquet predicate push down as \" + predicate);\n-    // ParquetInputFormat.setFilterPredicate(job, predicate);\n-    // clearOutExistingPredicate(job);\n-    // }\n-    return super.getRecordReader(split, job, reporter);\n-  }\n-\n-  /**\n-   * Read the table metadata from a data path. This assumes certain hierarchy of files which should be changed once a\n-   * better way is figured out to pass in the hoodie meta directory\n-   */\n-  protected static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {\n-    int levels = HoodieHiveUtil.DEFAULT_LEVELS_TO_BASEPATH;\n-    if (HoodiePartitionMetadata.hasPartitionMetadata(fs, dataPath)) {\n-      HoodiePartitionMetadata metadata = new HoodiePartitionMetadata(fs, dataPath);\n-      metadata.readFromFS();\n-      levels = metadata.getPartitionDepth();\n-    }\n-    Path baseDir = HoodieHiveUtil.getNthParent(dataPath, levels);\n-    LOG.info(\"Reading hoodie metadata from path \" + baseDir.toString());\n-    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());\n+public class HoodieParquetInputFormat extends HoodieInputFormat {\n+  public HoodieParquetInputFormat() {\n+    super(new MapredParquetInputFormat());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 341}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4OTQ1MA==", "bodyText": "Why does checkstyle complain", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434289450", "createdAt": "2020-06-03T03:35:48Z", "author": {"login": "vinothchandar"}, "path": "style/checkstyle-suppressions.xml", "diffHunk": "@@ -26,4 +26,5 @@\n   <!-- Member Names expected to start with \"_\"  -->\n   <suppress checks=\"naming\" files=\"TestRecord.java\" lines=\"1-9999\"/>\n   <suppress checks=\"IllegalImport\" files=\"Option.java\" />\n+  <suppress checks=\"naming\" files=\"HoodieInputFormat.java\" lines=\"73\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxMDAyMzk4", "url": "https://github.com/apache/hudi/pull/1687#pullrequestreview-431002398", "createdAt": "2020-06-15T21:20:57Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMToyMDo1N1rOGkDLqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMjo0NToyNlrOGkFPAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ1NDA1OA==", "bodyText": "Is this only needed on the write side ?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440454058", "createdAt": "2020-06-15T21:20:57Z", "author": {"login": "bvaradar"}, "path": "hudi-client/pom.xml", "diffHunk": "@@ -185,6 +185,12 @@\n       <artifactId>hbase-client</artifactId>\n       <version>${hbase.version}</version>\n     </dependency>\n+    <dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2MTQ3Ng==", "bodyText": "Can we have HashBasedJoin and SortMergeJoin as first level abstractions which both taken in 2 streams of records to be merged ? We can then plugin the algorithm depending on storage preference.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440461476", "createdAt": "2020-06-15T21:37:42Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -133,6 +136,12 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n       // Create the writer for writing the new version file\n       storageWriter =\n           HoodieStorageWriterFactory.getStorageWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n+\n+      if (hoodieTable.requireSortedRecords()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2NTYwNg=="}, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTU1NQ==", "bodyText": "We have a family of things that needs to be constructed:\n\nStorageWriter\nStorageReader\nMergeAlgorithm\n\nIn this case, AbstractFactoryPattern would be useful  https://en.wikipedia.org/wiki/Abstract_factory_pattern#/media/File:Abstract_factory_UML.svg  pattern would be ideal for this case.\nThis is more or less similar to what has been done here. Currently, there is a separate Writer and Reader Factory. What do you think about creating an AbstractFactory and have one concrete implementation for Parquet and another for HFile  where all the above objects are constructed ?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440465555", "createdAt": "2020-06-15T21:47:13Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java", "diffHunk": "@@ -34,29 +34,28 @@\n \n import java.io.IOException;\n \n-import static org.apache.hudi.common.model.HoodieFileFormat.HOODIE_LOG;\n import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;\n+import static org.apache.hudi.common.model.HoodieFileFormat.HFILE;\n \n-public class HoodieStorageWriterFactory {\n+public class HoodieFileWriterFactory {\n \n-  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(\n+  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> getFileWriter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTc1Ng==", "bodyText": "Please see above for AbstractFactory suggestion.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440465756", "createdAt": "2020-06-15T21:47:45Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java", "diffHunk": "@@ -34,29 +34,28 @@\n \n import java.io.IOException;\n \n-import static org.apache.hudi.common.model.HoodieFileFormat.HOODIE_LOG;\n import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;\n+import static org.apache.hudi.common.model.HoodieFileFormat.HFILE;\n \n-public class HoodieStorageWriterFactory {\n+public class HoodieFileWriterFactory {\n \n-  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(\n+  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> getFileWriter(\n       String instantTime, Path path, HoodieTable<T> hoodieTable, HoodieWriteConfig config, Schema schema,\n       SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n-    final String name = path.getName();\n-    final String extension = FSUtils.isLogFile(path) ? HOODIE_LOG.getFileExtension() : FSUtils.getFileExtension(name);\n+    final String extension = FSUtils.getFileExtension(path.getName());\n     if (PARQUET.getFileExtension().equals(extension)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2ODI3MA==", "bodyText": "Looks like this is duplicated. Can you refactor to reuse this code.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440468270", "createdAt": "2020-06-15T21:53:43Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieHFileWriter.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.fs.HoodieWrapperFileSystem;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.io.Writable;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * HoodieHFileWriter writes IndexedRecords into an HFile. The record's key is used as the key and the\n+ * AVRO encoded record bytes are saved as the value.\n+ *\n+ * Limitations (compared to columnar formats like Parquet or ORC):\n+ *  1. Records should be added in order of keys\n+ *  2. There are no column stats\n+ */\n+public class HoodieHFileWriter<T extends HoodieRecordPayload, R extends IndexedRecord>\n+    implements HoodieFileWriter<R> {\n+  private static AtomicLong recordIndex = new AtomicLong(1);\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileWriter.class);\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  private final Path file;\n+  private HoodieHFileConfig hfileConfig;\n+  private final HoodieWrapperFileSystem fs;\n+  private final long maxFileSize;\n+  private final String instantTime;\n+  private final SparkTaskContextSupplier sparkTaskContextSupplier;\n+  private HFile.Writer writer;\n+  private String minRecordKey;\n+  private String maxRecordKey;\n+\n+  public HoodieHFileWriter(String instantTime, Path file, HoodieHFileConfig hfileConfig, Schema schema,\n+      SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+\n+    Configuration conf = registerFileSystem(file, hfileConfig.getHadoopConf());\n+    this.file = HoodieWrapperFileSystem.convertToHoodiePath(file, conf);\n+    this.fs = (HoodieWrapperFileSystem) this.file.getFileSystem(conf);\n+    this.hfileConfig = hfileConfig;\n+\n+    // We cannot accurately measure the snappy compressed output file size. We are choosing a\n+    // conservative 10%\n+    // TODO - compute this compression ratio dynamically by looking at the bytes written to the\n+    // stream and the actual file size reported by HDFS\n+    // this.maxFileSize = hfileConfig.getMaxFileSize()\n+    //    + Math.round(hfileConfig.getMaxFileSize() * hfileConfig.getCompressionRatio());\n+    this.maxFileSize = hfileConfig.getMaxFileSize();\n+    this.instantTime = instantTime;\n+    this.sparkTaskContextSupplier = sparkTaskContextSupplier;\n+\n+    HFileContext context = new HFileContextBuilder().withBlockSize(hfileConfig.getBlockSize())\n+          .withCompression(hfileConfig.getCompressionAlgorithm())\n+          .build();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    this.writer = HFile.getWriterFactory(conf, cacheConfig).withPath(this.fs, this.file).withFileContext(context).create();\n+\n+    writer.appendFileInfo(KEY_SCHEMA.getBytes(), schema.toString().getBytes());\n+  }\n+\n+  public static Configuration registerFileSystem(Path file, Configuration conf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3MTI5Nw==", "bodyText": "Instead of this, lets return merge algorithm to be employed.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440471297", "createdAt": "2020-06-15T22:01:14Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -562,4 +566,28 @@ public void validateInsertSchema() throws HoodieInsertException {\n       throw new HoodieInsertException(\"Failed insert schema compability check.\", e);\n     }\n   }\n+\n+  public HoodieFileFormat getBaseFileFormat() {\n+    return metaClient.getTableConfig().getBaseFileFormat();\n+  }\n+\n+  public HoodieFileFormat getLogFileFormat() {\n+    return metaClient.getTableConfig().getLogFileFormat();\n+  }\n+\n+  public HoodieLogBlockType getLogDataBlockFormat() {\n+    switch (getBaseFileFormat()) {\n+      case PARQUET:\n+        return HoodieLogBlockType.AVRO_DATA_BLOCK;\n+      case HFILE:\n+        return HoodieLogBlockType.HFILE_DATA_BLOCK;\n+      default:\n+        throw new HoodieException(\"Base file format \" + getBaseFileFormat()\n+            + \" does not have associated log block format\");\n+    }\n+  }\n+\n+  public boolean requireSortedRecords() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3NjA4Ng==", "bodyText": "This is one another shuffle which will cause performance overhead. We dont need universal order. We only need to order records that are getting written to a single partition.\nwe should just do https://spark.apache.org/docs/1.2.0/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html#repartitionAndSortWithinPartitions(org.apache.spark.Partitioner) when partitioning (See BaseCommitExecutor.java)", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440476086", "createdAt": "2020-06-15T22:13:09Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/WriteHelper.java", "diffHunk": "@@ -52,6 +52,10 @@\n       }\n       Duration indexLookupDuration = Duration.between(lookupBegin, Instant.now());\n \n+      if (table.requireSortedRecords()) {\n+        taggedRecords = taggedRecords.sortBy(r -> r.getRecordKey(), true, taggedRecords.getNumPartitions());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3ODc4Mg==", "bodyText": "We might eventually need such support when supporting spark datasource support on Hoodie HFile table. It would be better to move this logic to a Utils class to reuse.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440478782", "createdAt": "2020-06-15T22:20:18Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java", "diffHunk": "@@ -217,6 +239,51 @@ public static SparkConf getSparkConfForTest(String appName) {\n     }\n   }\n \n+  public static Dataset<Row> readHFile(JavaSparkContext jsc, SQLContext sqlContext, String[] paths) {\n+    // TODO: this should be ported to use HoodieStorageReader\n+    List<byte[]> valuesAsList = new LinkedList<>();\n+\n+    FileSystem fs = FSUtils.getFs(paths[0], jsc.hadoopConfiguration());\n+    CacheConfig cacheConfig = new CacheConfig(fs.getConf());\n+    Schema schema = null;\n+    for (String path : paths) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3OTIyMA==", "bodyText": "+1", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440479220", "createdAt": "2020-06-15T22:21:28Z", "author": {"login": "bvaradar"}, "path": "hudi-common/pom.xml", "diffHunk": "@@ -78,6 +92,38 @@\n           </imports>\n         </configuration>\n       </plugin>\n+      <plugin>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDMzMQ=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4MDI2Mg==", "bodyText": "Average Size ?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440480262", "createdAt": "2020-06-15T22:24:14Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * DataBlock contains a list of records serialized using formats compatible with the base file format.\n+ * For each base file format there is a corresponding DataBlock format.\n+ *\n+ * The Datablock contains:\n+ *   1. Data Block version\n+ *   2. Total number of records in the block\n+ *   3. Size of a record", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4MTA4MQ==", "bodyText": "Can you confirm there is no change to order and kinds of fields stored here. I want to make sure if we can read log files written by 0.5.x using this change.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440481081", "createdAt": "2020-06-15T22:26:37Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * DataBlock contains a list of records serialized using formats compatible with the base file format.\n+ * For each base file format there is a corresponding DataBlock format.\n+ *\n+ * The Datablock contains:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4NDU3Mg==", "bodyText": "+1", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440484572", "createdAt": "2020-06-15T22:36:17Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/scala/com/databricks/spark/avro/HoodieAvroSchemaConversion.scala", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.databricks.spark.avro\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.GenericRecord\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.types.DataType\n+\n+/**\n+ * This helper class is required since SchemaConverters.createConverterToSQL is currently private.\n+ */\n+object HoodieAvroSchemaConversion {\n+  def createConverterToSQL(avroSchema: Schema, sparkSchema: DataType): (GenericRecord) => Row =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjE1MA=="}, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4NzY4MQ==", "bodyText": "Can we add tons of tests (expand on something like TestHoodieParquetInputFormat) for HFIle. Also, I suspect when you put things end to  end like run a proper hive/presto query, you will run into  package version mismatches which is what I had to deal with when doing metadata bootstrap.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440487681", "createdAt": "2020-06-15T22:45:26Z", "author": {"login": "bvaradar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieHFileInputFormat.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.hadoop.utils.HoodieHiveUtils;\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * HoodieInputFormat for HUDI datasets which store data in HFile base file format.\n+ */\n+@UseFileSplitsFromInputFormat\n+public class HoodieHFileInputFormat extends FileInputFormat<NullWritable, ArrayWritable> implements Configurable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "b2a4c2e54b579e6c58ea9320f8b85625b577084e", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/b2a4c2e54b579e6c58ea9320f8b85625b577084e", "committedDate": "2020-06-17T06:50:47Z", "message": "[HUDI-684] Fixed merge conflict due to upstream changes.\n\nAdded extra unit tests for HFile Input format."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "75667356003fe23b5fc4df8b37bcfd8aa256c61e", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/75667356003fe23b5fc4df8b37bcfd8aa256c61e", "committedDate": "2020-06-18T21:13:44Z", "message": "[HUDI-684] Fixed integration tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0NTM2NjA5", "url": "https://github.com/apache/hudi/pull/1687#pullrequestreview-434536609", "createdAt": "2020-06-21T22:11:47Z", "commit": {"oid": "47e9f43b0054a8b16842fca22a2809c6a8ce5384"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQyMjoxMTo0N1rOGmuiAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQyMzo0NzowM1rOGmu-QA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTQ0Mg==", "bodyText": "might as well expose getBaseFileExtension() in HoodieTable.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443261442", "createdAt": "2020-06-21T22:11:47Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -115,7 +115,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       oldFilePath = new Path(config.getBasePath() + \"/\" + partitionPath + \"/\" + latestValidFilePath);\n       String relativePath = new Path((partitionPath.isEmpty() ? \"\" : partitionPath + \"/\")\n-          + FSUtils.makeDataFileName(instantTime, writeToken, fileId)).toString();\n+          + FSUtils.makeDataFileName(instantTime, writeToken, fileId,\n+              hoodieTable.getBaseFileFormat().getFileExtension())).toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47e9f43b0054a8b16842fca22a2809c6a8ce5384"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTY1OA==", "bodyText": "minor. Do you think we should name this getNewStorageReader(). Just to be cautious to avoid some caller using this more like a getter for Reader. This does not return a singleton, but creates a new reader everytime.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443261658", "createdAt": "2020-06-21T22:14:46Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieReadHandle.java", "diffHunk": "@@ -56,4 +61,9 @@ protected HoodieBaseFile getLatestDataFile() {\n     return hoodieTable.getBaseFileOnlyView()\n         .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n   }\n+\n+  protected HoodieFileReader getStorageReader() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47e9f43b0054a8b16842fca22a2809c6a8ce5384"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng==", "bodyText": "similar to HoodieReadHandle, how about we expose this writer instantiation in HoodieWriteHandle.\nHoodieFileWriter<IndexedRecord> getFileWriter(String instantTime, Path newFilePath){\n     return HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n}", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443262086", "createdAt": "2020-06-21T22:20:11Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -132,7 +133,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       // Create the writer for writing the new version file\n       storageWriter =\n-          HoodieStorageWriterFactory.getStorageWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n+          HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47e9f43b0054a8b16842fca22a2809c6a8ce5384"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NDAzMg==", "bodyText": "minor: similar to my suggestion in reader, try to see if we need to name this as getNewFileWriter()", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443264032", "createdAt": "2020-06-21T22:48:21Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -132,7 +133,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       // Create the writer for writing the new version file\n       storageWriter =\n-          HoodieStorageWriterFactory.getStorageWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n+          HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng=="}, "originalCommit": {"oid": "47e9f43b0054a8b16842fca22a2809c6a8ce5384"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NTUyNg==", "bodyText": "not sure if my way of thinking is right. correct me if I am wrong. we are making our file formats flexible where in either of base file or log file can be of any format. I do understand for index purposes both base file and log file will be HFile. but wondering if tagging this config to table is the right think to do or should we have two configs, one for base file and one for log file. What in case we want to experiment w/ Hfile format for log files and parquet as base files since we might get indexing capability for log files too.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443265526", "createdAt": "2020-06-21T23:09:52Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionExecutor.java", "diffHunk": "@@ -115,7 +115,11 @@ public CommitActionExecutor(JavaSparkContext jsc,\n   }\n \n   protected HoodieMergeHandle getUpdateHandle(String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordItr) {\n-    return new HoodieMergeHandle<>(config, instantTime, (HoodieTable<T>)table, recordItr, partitionPath, fileId, sparkTaskContextSupplier);\n+    if (table.requireSortedRecords()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47e9f43b0054a8b16842fca22a2809c6a8ce5384"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NzMxOQ==", "bodyText": "I did go thru the AbstractFactoryPattern. Is this what you are insinuating @bvaradar\nclass FileStorageFactoryProducer {\n\n\tFileStorageFactory getFileStorageFactory(Config config){\n\t\tif(config.getFileStorageType() == PARQUET) {\n\t\t\treturn new ParquetFileStorageFactory(....)\n\t\t} else if(config.getFileStorageType() == HFile) {\n\t\t\treturn new HFileFileStorageFactory(....)\n\t\t} else {\n\t\t   // throw exception\n\t\t}\n\t}  \n}\n\n\ninterface FileStorageFactory {\n\t\n\tpublic static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> getFileWriter(String instantTime, \nPath path, HoodieTable<T> hoodieTable, HoodieWriteConfig config, Schema schema, \nSparkTaskContextSupplier sparkTaskContextSupplier) throws IOException;\n\n        public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileReader<R> getFileReader(\nConfiguration conf, Path path) throws IOException;\n\n}\n\n\nclass ParquetFileStorageFactory implements FileStorageFactory {\n\t\n\t\t// exposes Writer and Reader for Parquet\n\n}\n\n\nclass HFileFileStorageFactory implements FileStorageFactory {\n\t\t// exposes Writer and Reader for HFile\n}", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443267319", "createdAt": "2020-06-21T23:31:04Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java", "diffHunk": "@@ -34,29 +34,28 @@\n \n import java.io.IOException;\n \n-import static org.apache.hudi.common.model.HoodieFileFormat.HOODIE_LOG;\n import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;\n+import static org.apache.hudi.common.model.HoodieFileFormat.HFILE;\n \n-public class HoodieStorageWriterFactory {\n+public class HoodieFileWriterFactory {\n \n-  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(\n+  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> getFileWriter(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTU1NQ=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2ODY3Mg==", "bodyText": "@vinothchandar :  Guess its taken care of. Here is the getRecordIterator for Parquet.\npublic Iterator<R> getRecordIterator(Schema schema) throws IOException {\n    AvroReadSupport.setAvroReadSchema(conf, schema);\n    ParquetReader<IndexedRecord> reader = AvroParquetReader.<IndexedRecord>builder(path).withConf(conf).build();\n    return new ParquetReaderIterator(reader);\n  }\n\nWe set the scheme before instantiating the reader.\nLet me know is my understanding is wrong.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443268672", "createdAt": "2020-06-21T23:47:03Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionExecutor.java", "diffHunk": "@@ -89,11 +87,12 @@ public CommitActionExecutor(JavaSparkContext jsc,\n       throw new HoodieUpsertException(\n           \"Error in finding the old file path at commit \" + instantTime + \" for fileId: \" + fileId);\n     } else {\n-      AvroReadSupport.setAvroReadSchema(table.getHadoopConf(), upsertHandle.getWriterSchema());\n       BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n-      try (ParquetReader<IndexedRecord> reader =\n-          AvroParquetReader.<IndexedRecord>builder(upsertHandle.getOldFilePath()).withConf(table.getHadoopConf()).build()) {\n-        wrapper = new SparkBoundedInMemoryExecutor(config, new ParquetReaderIterator(reader),\n+      try {\n+        HoodieStorageReader<IndexedRecord> storageReader =\n+            HoodieStorageReaderFactory.getStorageReader(table.getHadoopConf(), upsertHandle.getOldFilePath());\n+        wrapper =\n+            new SparkBoundedInMemoryExecutor(config, storageReader.getRecordIterator(upsertHandle.getWriterSchema()),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4Mjg4Nw=="}, "originalCommit": null, "originalPosition": 40}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/47e9f43b0054a8b16842fca22a2809c6a8ce5384", "committedDate": "2020-06-20T06:13:43Z", "message": "[HUDI-684] Added integration tests for HFile format.\n\nHive support is implemented with HoodieHFileInputFormat. SparkSql and Presto are not currently supported.\nAdded base file abstraction for HiveSync tool and HoodieDeltaStreamer."}, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "committedDate": "2020-06-22T23:33:11Z", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\nNotable changes:\n1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n4. HiveSyncTool accepts the base file format as a CLI parameter\n5. HoodieDeltaStreamer accepts the base file format as a CLI parameter"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MDczODQy", "url": "https://github.com/apache/hudi/pull/1687#pullrequestreview-436073842", "createdAt": "2020-06-23T19:05:15Z", "commit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxOTowNToxNVrOGn2vZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMTowMToxOFrOGn6f6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NDUxOA==", "bodyText": "Minor : Rename getFileReader => createFileReader ?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444444518", "createdAt": "2020-06-23T19:05:15Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieReadHandle.java", "diffHunk": "@@ -56,4 +61,9 @@ protected HoodieBaseFile getLatestDataFile() {\n     return hoodieTable.getBaseFileOnlyView()\n         .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n   }\n+\n+  protected HoodieFileReader getNewFileReader() throws IOException {\n+    return HoodieFileReaderFactory.getFileReader(hoodieTable.getHadoopConf(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NTU3Mg==", "bodyText": "Similar suggestion on rename getFileWriter => createFileWriter", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444445572", "createdAt": "2020-06-23T19:06:52Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -180,4 +183,10 @@ protected int getStageId() {\n   protected long getAttemptId() {\n     return sparkTaskContextSupplier.getAttemptIdSupplier().get();\n   }\n+\n+  protected HoodieFileWriter getNewFileWriter(String instantTime, Path path, HoodieTable<T> hoodieTable,\n+                                              HoodieWriteConfig config, Schema schema,\n+                                              SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+    return HoodieFileWriterFactory.getFileWriter(instantTime, path, hoodieTable, config, schema, sparkTaskContextSupplier);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzQwOQ==", "bodyText": "Can we move this to FSUtils class ?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444447409", "createdAt": "2020-06-23T19:10:14Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriter.java", "diffHunk": "@@ -33,4 +37,12 @@\n   void close() throws IOException;\n \n   void writeAvro(String key, R oldRecord) throws IOException;\n+\n+  static Configuration registerFileSystem(Path file, Configuration conf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ1ODYwMw==", "bodyText": "@nsivabalan : Yes, this is what I was referring to but it is not specific to Storage alone. Other parts like the merge algorithm that needs to be configured per storage-type is also created here.\n@prashantwason : Let's revisit this in subsequent PR where we parameterize merge algorithm based on storage types. It is ok to keep reader and writer factory separate.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444458603", "createdAt": "2020-06-23T19:31:41Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java", "diffHunk": "@@ -34,29 +34,28 @@\n \n import java.io.IOException;\n \n-import static org.apache.hudi.common.model.HoodieFileFormat.HOODIE_LOG;\n import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;\n+import static org.apache.hudi.common.model.HoodieFileFormat.HFILE;\n \n-public class HoodieStorageWriterFactory {\n+public class HoodieFileWriterFactory {\n \n-  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(\n+  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> getFileWriter(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTU1NQ=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ1OTI4Ng==", "bodyText": "thanks for cleaning this up", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444459286", "createdAt": "2020-06-23T19:33:00Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java", "diffHunk": "@@ -51,7 +49,6 @@\n   private final long maxFileSize;\n   private final HoodieAvroWriteSupport writeSupport;\n   private final String instantTime;\n-  private final Schema schema;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2MDk0MA==", "bodyText": "Is this done some where else now ?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444460940", "createdAt": "2020-06-23T19:36:19Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -150,11 +148,13 @@ public HoodieWriteMetadata compact(JavaSparkContext jsc, String compactionInstan\n       throw new HoodieUpsertException(\n           \"Error in finding the old file path at commit \" + instantTime + \" for fileId: \" + fileId);\n     } else {\n-      AvroReadSupport.setAvroReadSchema(getHadoopConf(), upsertHandle.getWriterSchema());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2NTg5OA==", "bodyText": "nvm, found it in getRecordIterator", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444465898", "createdAt": "2020-06-23T19:46:09Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -150,11 +148,13 @@ public HoodieWriteMetadata compact(JavaSparkContext jsc, String compactionInstan\n       throw new HoodieUpsertException(\n           \"Error in finding the old file path at commit \" + instantTime + \" for fileId: \" + fileId);\n     } else {\n-      AvroReadSupport.setAvroReadSchema(getHadoopConf(), upsertHandle.getWriterSchema());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2MDk0MA=="}, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3NDIzOQ==", "bodyText": "thanks for cleaning this up.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444474239", "createdAt": "2020-06-23T20:02:01Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -356,20 +356,7 @@ public MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompac\n    * @return\n    */\n   public MessageType readSchemaFromLogFile(Path path) throws IOException {\n-    FileSystem fs = metaClient.getRawFs();\n-    Reader reader = HoodieLogFormat.newReader(fs, new HoodieLogFile(path), null);\n-    HoodieAvroDataBlock lastBlock = null;\n-    while (reader.hasNext()) {\n-      HoodieLogBlock block = reader.next();\n-      if (block instanceof HoodieAvroDataBlock) {\n-        lastBlock = (HoodieAvroDataBlock) block;\n-      }\n-    }\n-    reader.close();\n-    if (lastBlock != null) {\n-      return new AvroSchemaConverter().convert(lastBlock.getSchema());\n-    }\n-    return null;\n+    return readSchemaFromLogFile(metaClient.getRawFs(), path);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NjgzOQ==", "bodyText": "Can we keep this in hudi-client ?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444486839", "createdAt": "2020-06-23T20:27:11Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetReaderIterator.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi.client.utils;\n+package org.apache.hudi.common.util;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NzYyNg==", "bodyText": "HFile => Avro", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444487626", "createdAt": "2020-06-23T20:28:45Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieAvroLogFormat.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.functional;\n+\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock.HoodieLogBlockType;\n+\n+/**\n+ * Tests HFile log format {@link HoodieHFileLogFormat}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA==", "bodyText": "Can we avoid this cleanup as this is not directly related to this PR and I am also making changes on these same methods.", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444498080", "createdAt": "2020-06-23T20:49:09Z", "author": {"login": "bvaradar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java", "diffHunk": "@@ -80,58 +77,6 @@ protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline tim\n     return timeline;\n   }\n \n-  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ==", "bodyText": "The output format (MapredParquetOutputFormat  in case of parquet) would have to change depending on storage type here. right ? We need to parameterize that as well", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444504285", "createdAt": "2020-06-23T20:57:50Z", "author": {"login": "bvaradar"}, "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -146,21 +146,22 @@ private void syncSchema(String tableName, boolean tableExists, boolean useRealTi\n     // Check and sync schema\n     if (!tableExists) {\n       LOG.info(\"Hive table \" + tableName + \" is not found. Creating it\");\n-      if (!useRealTimeInputFormat) {\n-        String inputFormatClassName = cfg.usePreApacheInputFormat ? com.uber.hoodie.hadoop.HoodieInputFormat.class.getName()\n-            : HoodieParquetInputFormat.class.getName();\n-        hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),\n-            ParquetHiveSerDe.class.getName());\n-      } else {\n-        // Custom serde will not work with ALTER TABLE REPLACE COLUMNS\n-        // https://github.com/apache/hive/blob/release-1.1.0/ql/src/java/org/apache/hadoop/hive\n-        // /ql/exec/DDLTask.java#L3488\n-        String inputFormatClassName =\n-            cfg.usePreApacheInputFormat ? com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat.class.getName()\n-                : HoodieParquetRealtimeInputFormat.class.getName();\n-        hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),\n-            ParquetHiveSerDe.class.getName());\n+      HoodieFileFormat baseFileFormat = HoodieFileFormat.valueOf(cfg.baseFileFormat.toUpperCase());\n+      String inputFormatClassName = HoodieInputFormatUtils.getInputFormatClassName(baseFileFormat, useRealTimeInputFormat,\n+          new Configuration());\n+\n+      if (baseFileFormat.equals(HoodieFileFormat.PARQUET) && cfg.usePreApacheInputFormat) {\n+        // Parquet input format had an InputFormat class visible under the old naming scheme.\n+        inputFormatClassName = useRealTimeInputFormat\n+            ? com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat.class.getName()\n+            : com.uber.hoodie.hadoop.HoodieInputFormat.class.getName();\n       }\n+\n+      // Custom serde will not work with ALTER TABLE REPLACE COLUMNS\n+      // https://github.com/apache/hive/blob/release-1.1.0/ql/src/java/org/apache/hadoop/hive\n+      // /ql/exec/DDLTask.java#L3488\n+      hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNjA4OA==", "bodyText": "Similar functionality needs to be done for Spark SQL Writer. See HoodieSparkSqlWriter.scala", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444506088", "createdAt": "2020-06-23T21:01:18Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -177,6 +177,9 @@ public Operation convert(String value) throws ParameterException {\n     @Parameter(names = {\"--table-type\"}, description = \"Type of table. COPY_ON_WRITE (or) MERGE_ON_READ\", required = true)\n     public String tableType;\n \n+    @Parameter(names = {\"--base-file-format\"}, description = \"File format for the base files. PARQUET (or) HFILE\", required = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "committedDate": "2020-06-22T23:33:11Z", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\nNotable changes:\n1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n4. HiveSyncTool accepts the base file format as a CLI parameter\n5. HoodieDeltaStreamer accepts the base file format as a CLI parameter"}, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MzgwNzM3", "url": "https://github.com/apache/hudi/pull/1687#pullrequestreview-436380737", "createdAt": "2020-06-24T07:11:34Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoxMTozNFrOGoFtfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoxNTo0M1rOGoF1Lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4OTc4OQ==", "bodyText": "@prashantwason : Can you confirm if all occurrences (including tests) are taken care of ?", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444689789", "createdAt": "2020-06-24T07:11:34Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/RollbackHelper.java", "diffHunk": "@@ -71,8 +71,9 @@ public RollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config\n    */\n   public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<RollbackRequest> rollbackRequests) {\n \n+    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n     SerializablePathFilter filter = (path) -> {\n-      if (path.toString().contains(\".parquet\")) {\n+      if (path.toString().contains(basefileExtension)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MzYwNw=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5MTc1OQ==", "bodyText": "Got it. hudi-hadoop-mr also needs HoodieFileReader abstraction for reading schema from file and this is reason why all these classes needs to be in hudi-common", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444691759", "createdAt": "2020-06-24T07:15:43Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetReaderIterator.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi.client.utils;\n+package org.apache.hudi.common.util;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NjgzOQ=="}, "originalCommit": {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ca70668690c33ddb5390b58c2f736210022538b", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/6ca70668690c33ddb5390b58c2f736210022538b", "committedDate": "2020-06-25T18:53:36Z", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\n    Notable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "6ca70668690c33ddb5390b58c2f736210022538b", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/6ca70668690c33ddb5390b58c2f736210022538b", "committedDate": "2020-06-25T18:53:36Z", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\n    Notable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MDQ3MDk2", "url": "https://github.com/apache/hudi/pull/1687#pullrequestreview-438047096", "createdAt": "2020-06-26T06:38:41Z", "commit": {"oid": "6ca70668690c33ddb5390b58c2f736210022538b"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNjozODo0MVrOGpVY_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwNjozODo0MVrOGpVY_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk5NTI2MQ==", "bodyText": "Created Jira to tack this https://issues.apache.org/jira/browse/HUDI-1055", "url": "https://github.com/apache/hudi/pull/1687#discussion_r445995261", "createdAt": "2020-06-26T06:38:41Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/RollbackHelper.java", "diffHunk": "@@ -71,8 +71,9 @@ public RollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config\n    */\n   public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<RollbackRequest> rollbackRequests) {\n \n+    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n     SerializablePathFilter filter = (path) -> {\n-      if (path.toString().contains(\".parquet\")) {\n+      if (path.toString().contains(basefileExtension)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MzYwNw=="}, "originalCommit": null, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3220, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}