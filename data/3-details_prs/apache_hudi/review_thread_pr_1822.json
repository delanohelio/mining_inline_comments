{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3NzYyMDkw", "number": 1822, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMzoyNzo0OVrOEOFgsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMzoyOTo1MlrOEOFiDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMjA1ODA4OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMzoyNzo0OVrOGxCpEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMzoyNzo0OVrOGxCpEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA3NjY5MA==", "bodyText": "ingest multiple topics at a single go ... ?", "url": "https://github.com/apache/hudi/pull/1822#discussion_r454076690", "createdAt": "2020-07-14T03:27:49Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -174,6 +174,42 @@ and then ingest it as follows.\n \n In some cases, you may want to migrate your existing table into Hudi beforehand. Please refer to [migration guide](/docs/migration_guide.html). \n \n+## MultiTableDeltaStreamer\n+\n+`HoodieMultiTableDeltaStreamer`, a wrapper on top of `HoodieDeltaStreamer`, enables one to ingest multiple tables at a go into hudi datasets. Currently it only supports sequential processing of tables to be ingested and COPY_ON_WRITE storage type. The command line options for `HoodieMultiTableDeltaStreamer` are pretty much similar to `HoodieDeltaStreamer` with the only exception that you are required to provide table wise configs in separate files in a dedicated config folder. The following command line options are introduced", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ff8abe5bdcd6e0ac807603388d3cf74c7d5c09a"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMjA2MTU3OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwMzoyOTo1MlrOGxCrGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwNjoyOToxMFrOG_9u_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA3NzIxMQ==", "bodyText": "do we specify a --target-table ?", "url": "https://github.com/apache/hudi/pull/1822#discussion_r454077211", "createdAt": "2020-07-14T03:29:52Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -174,6 +174,42 @@ and then ingest it as follows.\n \n In some cases, you may want to migrate your existing table into Hudi beforehand. Please refer to [migration guide](/docs/migration_guide.html). \n \n+## MultiTableDeltaStreamer\n+\n+`HoodieMultiTableDeltaStreamer`, a wrapper on top of `HoodieDeltaStreamer`, enables one to ingest multiple tables at a go into hudi datasets. Currently it only supports sequential processing of tables to be ingested and COPY_ON_WRITE storage type. The command line options for `HoodieMultiTableDeltaStreamer` are pretty much similar to `HoodieDeltaStreamer` with the only exception that you are required to provide table wise configs in separate files in a dedicated config folder. The following command line options are introduced\n+\n+```java\n+  * --config-folder\n+    the path to the folder which contains all the table wise config files\n+    --base-path-prefix\n+    this is added to enable users to create all the hudi datasets for related tables under one path in FS. The datasets are then created under the path - <base_path_prefix>/<database>/<table_to_be_ingested>. However you can override the paths for every table by setting the property hoodie.deltastreamer.ingestion.targetBasePath\n+```\n+\n+The following properties are needed to be set properly to ingest data using `HoodieMultiTableDeltaStreamer`. \n+\n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested\n+  comma separated names of tables to be ingested in the format <database>.<table>, for example db1.table1,db1.table2\n+hoodie.deltastreamer.ingestion.targetBasePath\n+  if you wish to ingest a particular table in a separate path, you can mention that path here\n+hoodie.deltastreamer.ingestion.<database>.<table>.configFile\n+  path to the config file in dedicated config folder which contains table overridden properties for the particular table to be ingested.\n+```\n+\n+Sample config files for table wise overridden properties can be found under `hudi-utilities/src/test/resources/delta-streamer-config`. The command to run `HoodieMultiTableDeltaStreamer` is also similar to how you run `HoodieDeltaStreamer`.\n+\n+```java\n+[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n+  --props file://${PWD}/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \\\n+  --config-folder file://tmp/hudi-ingestion-config \\\n+  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n+  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n+  --source-ordering-field impresssiontime \\\n+  --base-path-prefix file:\\/\\/\\/tmp/hudi-deltastreamer-op \\ \n+  --target-table uber.impressions \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ff8abe5bdcd6e0ac807603388d3cf74c7d5c09a"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTM4NjY3OQ==", "bodyText": "yes we have a option to specify --target-table in HoodieMultiTableDeltaStreamer, though it gets overwritten later through our code.", "url": "https://github.com/apache/hudi/pull/1822#discussion_r459386679", "createdAt": "2020-07-23T11:39:04Z", "author": {"login": "pratyakshsharma"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -174,6 +174,42 @@ and then ingest it as follows.\n \n In some cases, you may want to migrate your existing table into Hudi beforehand. Please refer to [migration guide](/docs/migration_guide.html). \n \n+## MultiTableDeltaStreamer\n+\n+`HoodieMultiTableDeltaStreamer`, a wrapper on top of `HoodieDeltaStreamer`, enables one to ingest multiple tables at a go into hudi datasets. Currently it only supports sequential processing of tables to be ingested and COPY_ON_WRITE storage type. The command line options for `HoodieMultiTableDeltaStreamer` are pretty much similar to `HoodieDeltaStreamer` with the only exception that you are required to provide table wise configs in separate files in a dedicated config folder. The following command line options are introduced\n+\n+```java\n+  * --config-folder\n+    the path to the folder which contains all the table wise config files\n+    --base-path-prefix\n+    this is added to enable users to create all the hudi datasets for related tables under one path in FS. The datasets are then created under the path - <base_path_prefix>/<database>/<table_to_be_ingested>. However you can override the paths for every table by setting the property hoodie.deltastreamer.ingestion.targetBasePath\n+```\n+\n+The following properties are needed to be set properly to ingest data using `HoodieMultiTableDeltaStreamer`. \n+\n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested\n+  comma separated names of tables to be ingested in the format <database>.<table>, for example db1.table1,db1.table2\n+hoodie.deltastreamer.ingestion.targetBasePath\n+  if you wish to ingest a particular table in a separate path, you can mention that path here\n+hoodie.deltastreamer.ingestion.<database>.<table>.configFile\n+  path to the config file in dedicated config folder which contains table overridden properties for the particular table to be ingested.\n+```\n+\n+Sample config files for table wise overridden properties can be found under `hudi-utilities/src/test/resources/delta-streamer-config`. The command to run `HoodieMultiTableDeltaStreamer` is also similar to how you run `HoodieDeltaStreamer`.\n+\n+```java\n+[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n+  --props file://${PWD}/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \\\n+  --config-folder file://tmp/hudi-ingestion-config \\\n+  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n+  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n+  --source-ordering-field impresssiontime \\\n+  --base-path-prefix file:\\/\\/\\/tmp/hudi-deltastreamer-op \\ \n+  --target-table uber.impressions \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA3NzIxMQ=="}, "originalCommit": {"oid": "4ff8abe5bdcd6e0ac807603388d3cf74c7d5c09a"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcyNDkyNw==", "bodyText": "@pratyakshsharma Is this a default target-table if its not overwritten by the individual table properties files ?", "url": "https://github.com/apache/hudi/pull/1822#discussion_r469724927", "createdAt": "2020-08-13T06:29:10Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -174,6 +174,42 @@ and then ingest it as follows.\n \n In some cases, you may want to migrate your existing table into Hudi beforehand. Please refer to [migration guide](/docs/migration_guide.html). \n \n+## MultiTableDeltaStreamer\n+\n+`HoodieMultiTableDeltaStreamer`, a wrapper on top of `HoodieDeltaStreamer`, enables one to ingest multiple tables at a go into hudi datasets. Currently it only supports sequential processing of tables to be ingested and COPY_ON_WRITE storage type. The command line options for `HoodieMultiTableDeltaStreamer` are pretty much similar to `HoodieDeltaStreamer` with the only exception that you are required to provide table wise configs in separate files in a dedicated config folder. The following command line options are introduced\n+\n+```java\n+  * --config-folder\n+    the path to the folder which contains all the table wise config files\n+    --base-path-prefix\n+    this is added to enable users to create all the hudi datasets for related tables under one path in FS. The datasets are then created under the path - <base_path_prefix>/<database>/<table_to_be_ingested>. However you can override the paths for every table by setting the property hoodie.deltastreamer.ingestion.targetBasePath\n+```\n+\n+The following properties are needed to be set properly to ingest data using `HoodieMultiTableDeltaStreamer`. \n+\n+```java\n+hoodie.deltastreamer.ingestion.tablesToBeIngested\n+  comma separated names of tables to be ingested in the format <database>.<table>, for example db1.table1,db1.table2\n+hoodie.deltastreamer.ingestion.targetBasePath\n+  if you wish to ingest a particular table in a separate path, you can mention that path here\n+hoodie.deltastreamer.ingestion.<database>.<table>.configFile\n+  path to the config file in dedicated config folder which contains table overridden properties for the particular table to be ingested.\n+```\n+\n+Sample config files for table wise overridden properties can be found under `hudi-utilities/src/test/resources/delta-streamer-config`. The command to run `HoodieMultiTableDeltaStreamer` is also similar to how you run `HoodieDeltaStreamer`.\n+\n+```java\n+[hoodie]$ spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieMultiTableDeltaStreamer `ls packaging/hudi-utilities-bundle/target/hudi-utilities-bundle-*.jar` \\\n+  --props file://${PWD}/hudi-utilities/src/test/resources/delta-streamer-config/kafka-source.properties \\\n+  --config-folder file://tmp/hudi-ingestion-config \\\n+  --schemaprovider-class org.apache.hudi.utilities.schema.SchemaRegistryProvider \\\n+  --source-class org.apache.hudi.utilities.sources.AvroKafkaSource \\\n+  --source-ordering-field impresssiontime \\\n+  --base-path-prefix file:\\/\\/\\/tmp/hudi-deltastreamer-op \\ \n+  --target-table uber.impressions \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA3NzIxMQ=="}, "originalCommit": {"oid": "4ff8abe5bdcd6e0ac807603388d3cf74c7d5c09a"}, "originalPosition": 36}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4557, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}