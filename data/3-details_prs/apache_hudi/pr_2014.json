{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcyMjE4MDM4", "number": 2014, "title": "[HUDI-1153] Spark DataSource and Streaming Write must fail when operation type is misconfigured", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nCurrently for Spark Streaming Write operation is being manually string compared on usage in most of the code, we also silently swallow illegal operation types by defaulting to upsert. This addresses these issues\nBrief change log\n\n[HUDI-1153] Spark DataSource and Streaming Write must fail when operation type is misconfigured\n\nVerify this pull request\nThis pull request is already covered by existing tests, such as (please describe tests).\nTestDataSourceUtils\n\ntestDoWriteOperationWithoutUserDefinedBulkInsertPartitioner\ntestDoWriteOperationWithNonExistUserDefinedBulkInsertPartitioner\ntestDoWriteOperationWithUserDefinedBulkInsertPartitioner\nIf all existing tests pass. This should be good to review\n\n\n Existing tests pass\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR - None\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA- Not a large task", "createdAt": "2020-08-24T02:07:54Z", "url": "https://github.com/apache/hudi/pull/2014", "merged": true, "mergeCommit": {"oid": "6537af26761ca785aaf4f8e9158909205875c1bf"}, "closed": true, "closedAt": "2020-09-04T16:08:31Z", "author": {"login": "sreeram26"}, "timelineItems": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdB47LggFqTQ3MzA4MDQwMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdFnexfAFqTQ4Mjc3ODcyNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMDgwNDAx", "url": "https://github.com/apache/hudi/pull/2014#pullrequestreview-473080401", "createdAt": "2020-08-24T02:10:29Z", "commit": {"oid": "7f81b53266a38e931650b31075e919f2a180d0e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwMjoxMDoyOVrOHFSaNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwMjoxMDoyOVrOHFSaNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwNjU1MQ==", "bodyText": "Not throwing an explicit error here, the only other value it can potentially have is Bootstrap based on the enum, the issue which exposed this issue would have thrown an exception on WriteOperationType.fromValue itself.\nCan change to throw a HoodieException, if the reviewer feels that is necessary", "url": "https://github.com/apache/hudi/pull/2014#discussion_r475306551", "createdAt": "2020-08-24T02:10:29Z", "author": {"login": "sreeram26"}, "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -248,15 +249,18 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n-      String instantTime, String operation) throws HoodieException {\n-    if (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) {\n+      String instantTime, WriteOperationType operation) throws HoodieException {\n+    if (operation == WriteOperationType.BULK_INSERT) {\n       Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n           createUserDefinedBulkInsertPartitioner(client.getConfig());\n       return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n-    } else if (operation.equals(DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())) {\n+    } else if (operation == WriteOperationType.INSERT) {\n       return client.insert(hoodieRecords, instantTime);\n     } else {\n       // default is upsert\n+      if (operation != WriteOperationType.UPSERT) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f81b53266a38e931650b31075e919f2a180d0e5"}, "originalPosition": 24}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7f81b53266a38e931650b31075e919f2a180d0e5", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/7f81b53266a38e931650b31075e919f2a180d0e5", "committedDate": "2020-08-24T02:02:21Z", "message": "[HUDI-1153] Spark DataSource and Streaming Write must fail when operation type is misconfigured"}, "afterCommit": {"oid": "24e0ccbf578a89be7caf7eaadbb3944993bba6b3", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/24e0ccbf578a89be7caf7eaadbb3944993bba6b3", "committedDate": "2020-08-24T02:10:52Z", "message": "[HUDI-1153] Spark DataSource and Streaming Write must fail when operation type is misconfigured"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1NjYwMjY5", "url": "https://github.com/apache/hudi/pull/2014#pullrequestreview-475660269", "createdAt": "2020-08-26T16:30:05Z", "commit": {"oid": "24e0ccbf578a89be7caf7eaadbb3944993bba6b3"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNjozMDowNVrOHHUJlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNjo0MDo0M1rOHHUkyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQzMjIxNQ==", "bodyText": "@sreeram26 : I think it is better to throw an exception in this case. We saw some cases where uses intended to use bulk insert but had a typo in the operation type.", "url": "https://github.com/apache/hudi/pull/2014#discussion_r477432215", "createdAt": "2020-08-26T16:30:05Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -248,15 +249,18 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n-      String instantTime, String operation) throws HoodieException {\n-    if (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) {\n+      String instantTime, WriteOperationType operation) throws HoodieException {\n+    if (operation == WriteOperationType.BULK_INSERT) {\n       Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner =\n           createUserDefinedBulkInsertPartitioner(client.getConfig());\n       return client.bulkInsert(hoodieRecords, instantTime, userDefinedBulkInsertPartitioner);\n-    } else if (operation.equals(DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())) {\n+    } else if (operation == WriteOperationType.INSERT) {\n       return client.insert(hoodieRecords, instantTime);\n     } else {\n       // default is upsert\n+      if (operation != WriteOperationType.UPSERT) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwNjU1MQ=="}, "originalCommit": {"oid": "7f81b53266a38e931650b31075e919f2a180d0e5"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQzNTM4OA==", "bodyText": "Also, since we have changed this to enum, can you also update DataSourceWriteOptions to reference this enum. It is good that both the enum values and those defined in DataSourceWriteOptions are consistent. So, we can do this without any backwards compatibility issues.\n  val OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n  val BULK_INSERT_OPERATION_OPT_VAL = WriteOperationType.BULK_INSERT.name\n  val INSERT_OPERATION_OPT_VAL = WriteOperationType.INSERT.name\n  val UPSERT_OPERATION_OPT_VAL = WriteOperationType.UPSERT.name\n  val DELETE_OPERATION_OPT_VAL = WriteOperationType.DELETE.name\n  val BOOTSTRAP_OPERATION_OPT_VAL = WriteOperationType.BOOTSTRAP.name\n  val DEFAULT_OPERATION_OPT_VAL = UPSERT_OPERATION_OPT_VAL", "url": "https://github.com/apache/hudi/pull/2014#discussion_r477435388", "createdAt": "2020-08-26T16:35:16Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -248,15 +249,18 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n-      String instantTime, String operation) throws HoodieException {\n-    if (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) {\n+      String instantTime, WriteOperationType operation) throws HoodieException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24e0ccbf578a89be7caf7eaadbb3944993bba6b3"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQzOTE3Ng==", "bodyText": "Suggestion : Instead of if-else, by using switch-case, we can take advantage of checkstyle to ensure we are not missing handling of any operation types .", "url": "https://github.com/apache/hudi/pull/2014#discussion_r477439176", "createdAt": "2020-08-26T16:40:43Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -248,15 +249,18 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doWriteOperation(HoodieWriteClient client, JavaRDD<HoodieRecord> hoodieRecords,\n-      String instantTime, String operation) throws HoodieException {\n-    if (operation.equals(DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL())) {\n+      String instantTime, WriteOperationType operation) throws HoodieException {\n+    if (operation == WriteOperationType.BULK_INSERT) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24e0ccbf578a89be7caf7eaadbb3944993bba6b3"}, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "44f251702c36078c7e86f8e8a701834c33ad3066", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/44f251702c36078c7e86f8e8a701834c33ad3066", "committedDate": "2020-08-27T17:25:26Z", "message": "Address Balaji comments"}, "afterCommit": {"oid": "3183b5ce0acc13603846b015102380a65df1bd19", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/3183b5ce0acc13603846b015102380a65df1bd19", "committedDate": "2020-08-27T17:30:04Z", "message": "Address Balaji comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "26a5845701526de33b751a51714d45ccfc2ac22c", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/26a5845701526de33b751a51714d45ccfc2ac22c", "committedDate": "2020-08-27T20:50:07Z", "message": "fixup"}, "afterCommit": {"oid": "764d5eedbb8b14874881885b874b99012c744b35", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/764d5eedbb8b14874881885b874b99012c744b35", "committedDate": "2020-08-27T20:53:05Z", "message": "Address Balaji comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a9ab7c678473fd67d4ba21b7f27b2eb29c9a5fc2", "author": {"user": {"login": "cdmikechen", "name": "Thinking Chen"}}, "url": "https://github.com/apache/hudi/commit/a9ab7c678473fd67d4ba21b7f27b2eb29c9a5fc2", "committedDate": "2020-08-31T00:28:22Z", "message": "[HUDI-1225] Fix: Avro Date logical type not handled correctly when converting to Spark Row (#2047)"}, "afterCommit": {"oid": "6692b6a99becf955c1154778aa83a48a7d9758f5", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/6692b6a99becf955c1154778aa83a48a7d9758f5", "committedDate": "2020-08-31T00:28:22Z", "message": "Move from name to value"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6692b6a99becf955c1154778aa83a48a7d9758f5", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/6692b6a99becf955c1154778aa83a48a7d9758f5", "committedDate": "2020-08-31T00:28:22Z", "message": "Move from name to value"}, "afterCommit": {"oid": "cc35f0db20e882b7c26d3296ec161dcb935e2ed7", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/cc35f0db20e882b7c26d3296ec161dcb935e2ed7", "committedDate": "2020-08-31T00:36:16Z", "message": "Move from name to value"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1ab8ee61f9893903a2d20d66ecd08fada532b362", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/1ab8ee61f9893903a2d20d66ecd08fada532b362", "committedDate": "2020-08-31T16:02:44Z", "message": "Add back value"}, "afterCommit": {"oid": "fa63b1edbc2409c00b41de016c4d486e93d35e77", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/fa63b1edbc2409c00b41de016c4d486e93d35e77", "committedDate": "2020-08-31T16:06:07Z", "message": "Add back value"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fa63b1edbc2409c00b41de016c4d486e93d35e77", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/fa63b1edbc2409c00b41de016c4d486e93d35e77", "committedDate": "2020-08-31T16:06:07Z", "message": "Add back value"}, "afterCommit": {"oid": "9e1c006597804a92d5cf5bbf7698ab4dcc75b3e5", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/9e1c006597804a92d5cf5bbf7698ab4dcc75b3e5", "committedDate": "2020-08-31T16:08:32Z", "message": "Add back value"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48fe545f43fe5f5c17febfaef444b868bc456014", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/48fe545f43fe5f5c17febfaef444b868bc456014", "committedDate": "2020-09-02T01:12:29Z", "message": "[HUDI-1153] Spark DataSource and Streaming Write must fail when operation type is misconfigured"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ce15085a16acc58519c2d0576dacc1ec5056a6a", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/0ce15085a16acc58519c2d0576dacc1ec5056a6a", "committedDate": "2020-09-02T01:12:29Z", "message": "Address Balaji comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a54d48a1981e38fef599ca9d40565af47b9bbc2", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/0a54d48a1981e38fef599ca9d40565af47b9bbc2", "committedDate": "2020-09-02T01:12:30Z", "message": "Move from name to value"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1871d6f5e520f4776d60394dc585e33dfa2ac7d7", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/1871d6f5e520f4776d60394dc585e33dfa2ac7d7", "committedDate": "2020-09-02T01:12:30Z", "message": "move to toString instead"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8fccb0ff2fbc4e374774f178474b04571626b97", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/f8fccb0ff2fbc4e374774f178474b04571626b97", "committedDate": "2020-09-02T01:12:30Z", "message": "Add back value"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20d18830fefcbc66c19e3c7484ea66f93f6d6974", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/20d18830fefcbc66c19e3c7484ea66f93f6d6974", "committedDate": "2020-09-02T01:12:30Z", "message": "fix linter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1dc0d0f5c94be653bfb02922add59e4f1819fc99", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/1dc0d0f5c94be653bfb02922add59e4f1819fc99", "committedDate": "2020-09-02T01:11:18Z", "message": "fix linter"}, "afterCommit": {"oid": "20d18830fefcbc66c19e3c7484ea66f93f6d6974", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/20d18830fefcbc66c19e3c7484ea66f93f6d6974", "committedDate": "2020-09-02T01:12:30Z", "message": "fix linter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "811fdc7b8c5352259259ccf36187be69f5c4ed10", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/811fdc7b8c5352259259ccf36187be69f5c4ed10", "committedDate": "2020-09-02T17:16:17Z", "message": "Removing toString() as the avro representative has it as string with a weird conversion logic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2f7564821b9ea026a460dd5f0693be2a44c19dd", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/e2f7564821b9ea026a460dd5f0693be2a44c19dd", "committedDate": "2020-09-03T00:31:30Z", "message": "Trigger build"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf330f47888857a03e0b26f20b75606aa0f69c85", "author": {"user": {"login": "sreeram26", "name": "Sreeram Ramji"}}, "url": "https://github.com/apache/hudi/commit/cf330f47888857a03e0b26f20b75606aa0f69c85", "committedDate": "2020-09-03T03:52:06Z", "message": "Trigger notification"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgyNzc4NzI1", "url": "https://github.com/apache/hudi/pull/2014#pullrequestreview-482778725", "createdAt": "2020-09-04T16:06:47Z", "commit": {"oid": "cf330f47888857a03e0b26f20b75606aa0f69c85"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4438, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}