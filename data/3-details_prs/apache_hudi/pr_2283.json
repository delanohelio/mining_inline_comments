{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI3ODkzOTI1", "number": 2283, "title": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table", "bodyText": "\u2026rk sql\nTips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nIf we update a hudi table twice more, we will get an incorrect query result by spark sql. There are many duplicate record in the results. This PR is trying to fix this issue.\nBrief change log\n\nSupport sync tableProperties & serdeProperties to Hive table  in HiveSyncTool.\nAdd new method in AbstractSyncHoodieClient to support sync table properties.\nAdd the extra table properties & serde properties need for spark datasource table in HoodieSparkSqlWriter\nwhen sync meta to hive.\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded HoodieSparkSqlWriterSuite to verify the change.\nAdded TestHiveSyncTool#testSyncWithProperties  to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-11-26T07:51:11Z", "url": "https://github.com/apache/hudi/pull/2283", "merged": true, "mergeCommit": {"oid": "aacb8be5213a64a3cc9ddd791e2321526517d044"}, "closed": true, "closedAt": "2021-04-20T21:21:38Z", "author": {"login": "pengzhiwei2018"}, "timelineItems": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdhnD0IABqjQwNTI1MTgzNTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABeO2q5BgBqjQ2MTY4MzQ0ODQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "767642cfca2c712a09521242bceb238fcf26e41b", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/767642cfca2c712a09521242bceb238fcf26e41b", "committedDate": "2020-11-26T07:44:59Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql"}, "afterCommit": {"oid": "2038dda8bfdd03c0e6e57f575a51773fe570c76b", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/2038dda8bfdd03c0e6e57f575a51773fe570c76b", "committedDate": "2020-11-30T15:26:42Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2038dda8bfdd03c0e6e57f575a51773fe570c76b", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/2038dda8bfdd03c0e6e57f575a51773fe570c76b", "committedDate": "2020-11-30T15:26:42Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql"}, "afterCommit": {"oid": "63e88099492dae817e428cc973d0d8c452dcb039", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/63e88099492dae817e428cc973d0d8c452dcb039", "committedDate": "2020-12-06T12:23:28Z", "message": "[HUDI-1415] refactor same code"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "63e88099492dae817e428cc973d0d8c452dcb039", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/63e88099492dae817e428cc973d0d8c452dcb039", "committedDate": "2020-12-06T12:23:28Z", "message": "[HUDI-1415] refactor same code"}, "afterCommit": {"oid": "0ff1f77a5b6dde711f2c2dad03d70a581e2532c2", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/0ff1f77a5b6dde711f2c2dad03d70a581e2532c2", "committedDate": "2020-12-06T12:26:45Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2ae4c6e5e2c0c4e553b21e469781cfa448f83d17", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/2ae4c6e5e2c0c4e553b21e469781cfa448f83d17", "committedDate": "2020-12-06T14:35:48Z", "message": "fix test case"}, "afterCommit": {"oid": "98e229343ed140f252d5058813855ce689030396", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/98e229343ed140f252d5058813855ce689030396", "committedDate": "2020-12-06T14:36:44Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5f1b7ecd8f7313713343a6a8b72ff16b229cb99d", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/5f1b7ecd8f7313713343a6a8b72ff16b229cb99d", "committedDate": "2020-12-07T07:52:49Z", "message": "fix read partitioned table exception"}, "afterCommit": {"oid": "a5e896a04d6fb4227874067ffd47073d0e23ab71", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/a5e896a04d6fb4227874067ffd47073d0e23ab71", "committedDate": "2020-12-07T07:53:30Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5324a6623c20c8e4c863228b612c30dd052e9c90", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/5324a6623c20c8e4c863228b612c30dd052e9c90", "committedDate": "2020-12-08T03:54:16Z", "message": "add log for test case"}, "afterCommit": {"oid": "c826eb838a35d4ea1bb21823b33c50e1c4a9d893", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/c826eb838a35d4ea1bb21823b33c50e1c4a9d893", "committedDate": "2020-12-10T03:25:33Z", "message": "add log for test case"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "732a2ae9094988f47ddf22b581311433df6479b7", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/732a2ae9094988f47ddf22b581311433df6479b7", "committedDate": "2020-12-18T12:17:19Z", "message": "add some log"}, "afterCommit": {"oid": "bc59a67f3d01dd17beb24592957ede089a5fa9a8", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/bc59a67f3d01dd17beb24592957ede089a5fa9a8", "committedDate": "2020-12-10T03:25:32Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bc59a67f3d01dd17beb24592957ede089a5fa9a8", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/bc59a67f3d01dd17beb24592957ede089a5fa9a8", "committedDate": "2020-12-10T03:25:32Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception"}, "afterCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/63cbf0148a033fe511d49b381691d126f78f8828", "committedDate": "2021-01-07T15:53:38Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTc5NzU2NDY0", "url": "https://github.com/apache/hudi/pull/2283#pullrequestreview-579756464", "createdAt": "2021-01-30T13:15:32Z", "commit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzoxNTozM1rOIc94FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzo1MTowOVrOIc-GkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NDgyMQ==", "bodyText": "I am new to these, hence might be wrong. but can you please clarify if this should be \"org.apache.hudi\" ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567244821", "createdAt": "2021-01-30T13:15:33Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NTM1OQ==", "bodyText": "can we rename to something like schemaParts or schemaSlices", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567245359", "createdAt": "2021-01-30T13:20:05Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NTQ1Mg==", "bodyText": "can you please add a comment on why we are doing this rather than setting it as just one param. Also, do link any references so that devs are aware of why we are doing and what all props we may need to set.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567245452", "createdAt": "2021-01-30T13:20:48Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> parts.size.toString\n+    )\n+    parts.zipWithIndex.foreach { case (part, index) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NjY4OA==", "bodyText": "are we just overriding every time and finally setting pathProp to final value ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567246688", "createdAt": "2021-01-30T13:34:31Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> parts.size.toString\n+    )\n+    parts.zipWithIndex.foreach { case (part, index) =>\n+      properties += s\"spark.sql.sources.schema.part.$index\" -> part\n+    }\n+    // add partition columns\n+    if (partitionSet.nonEmpty) {\n+      properties += \"spark.sql.sources.schema.numPartCols\" -> partitionSet.size.toString\n+      partitionSet.zipWithIndex.foreach { case (partCol, index) =>\n+        properties += s\"spark.sql.sources.schema.partCol.$index\" -> partCol\n+      }\n+    }\n+    var sqlPropertyText = ConfigUtils.configToString(properties)\n+    sqlPropertyText = if (parameters.containsKey(HIVE_TABLE_PROPERTIES)) {\n+      sqlPropertyText + \"\\n\" + parameters(HIVE_TABLE_PROPERTIES)\n+    } else {\n+      sqlPropertyText\n+    }\n+    parameters + (HIVE_TABLE_PROPERTIES -> sqlPropertyText)\n+  }\n+\n+  private def createSqlTableSerdeProperties(parameters: Map[String, String],\n+                                            basePath: String, pathDepth: Int): String = {\n+    assert(pathDepth >= 0, \"Path Depth must great or equal to 0\")\n+    var pathProp = s\"path=$basePath\"\n+    if (pathProp.endsWith(\"/\")) {\n+      pathProp = pathProp.substring(0, pathProp.length - 1)\n+    }\n+    for (_ <- 0 until pathDepth + 1) {\n+      pathProp = s\"$pathProp/*\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzAwNw==", "bodyText": "Ideally it would be nice to construct the expected output rather than hardcoding. Can we at least use structType to construct the schema parts in this expected output. would be good to avoid hardcoding it.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247007", "createdAt": "2021-01-30T13:37:13Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "diffHunk": "@@ -397,6 +401,46 @@ class HoodieSparkSqlWriterSuite extends FunSuite with Matchers {\n       }\n     })\n \n+  test(\"Test build sync config for spark sql\") {\n+    initSparkContext(\"test build sync config\")\n+    val addSqlTablePropertiesMethod =\n+        HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"addSqlTableProperties\",\n+          classOf[SQLConf], classOf[StructType], classOf[Map[_,_]])\n+    addSqlTablePropertiesMethod.setAccessible(true)\n+\n+    val schema = DataSourceTestUtils.getStructTypeExampleSchema\n+    val structType = AvroConversionUtils.convertAvroSchemaToStructType(schema)\n+    val basePath = \"/tmp/hoodie_test\"\n+    val params = Map(\n+      \"path\" -> basePath,\n+      DataSourceWriteOptions.TABLE_NAME_OPT_KEY -> \"test_hoodie\",\n+      DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"partition\"\n+    )\n+    val parameters = HoodieWriterUtils.parametersWithWriteDefaults(params)\n+    val newParams = addSqlTablePropertiesMethod.invoke(HoodieSparkSqlWriter,\n+      spark.sessionState.conf, structType, parameters)\n+      .asInstanceOf[Map[String, String]]\n+\n+    val buildSyncConfigMethod =\n+      HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"buildSyncConfig\", classOf[Path],\n+        classOf[Map[_,_]])\n+    buildSyncConfigMethod.setAccessible(true)\n+\n+    val hiveSyncConfig = buildSyncConfigMethod.invoke(HoodieSparkSqlWriter,\n+      new Path(basePath), newParams).asInstanceOf[HiveSyncConfig]\n+\n+    assertResult(\"spark.sql.sources.provider=hudi\\n\" +\n+      \"spark.sql.sources.schema.partCol.0=partition\\n\" +\n+      \"spark.sql.sources.schema.numParts=1\\n\" +\n+      \"spark.sql.sources.schema.numPartCols=1\\n\" +\n+      \"spark.sql.sources.schema.part.0=\" +\n+      \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"_row_key\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}},\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzAzMA==", "bodyText": "same comment as above.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247030", "createdAt": "2021-01-30T13:37:27Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "diffHunk": "@@ -397,6 +401,46 @@ class HoodieSparkSqlWriterSuite extends FunSuite with Matchers {\n       }\n     })\n \n+  test(\"Test build sync config for spark sql\") {\n+    initSparkContext(\"test build sync config\")\n+    val addSqlTablePropertiesMethod =\n+        HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"addSqlTableProperties\",\n+          classOf[SQLConf], classOf[StructType], classOf[Map[_,_]])\n+    addSqlTablePropertiesMethod.setAccessible(true)\n+\n+    val schema = DataSourceTestUtils.getStructTypeExampleSchema\n+    val structType = AvroConversionUtils.convertAvroSchemaToStructType(schema)\n+    val basePath = \"/tmp/hoodie_test\"\n+    val params = Map(\n+      \"path\" -> basePath,\n+      DataSourceWriteOptions.TABLE_NAME_OPT_KEY -> \"test_hoodie\",\n+      DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"partition\"\n+    )\n+    val parameters = HoodieWriterUtils.parametersWithWriteDefaults(params)\n+    val newParams = addSqlTablePropertiesMethod.invoke(HoodieSparkSqlWriter,\n+      spark.sessionState.conf, structType, parameters)\n+      .asInstanceOf[Map[String, String]]\n+\n+    val buildSyncConfigMethod =\n+      HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"buildSyncConfig\", classOf[Path],\n+        classOf[Map[_,_]])\n+    buildSyncConfigMethod.setAccessible(true)\n+\n+    val hiveSyncConfig = buildSyncConfigMethod.invoke(HoodieSparkSqlWriter,\n+      new Path(basePath), newParams).asInstanceOf[HiveSyncConfig]\n+\n+    assertResult(\"spark.sql.sources.provider=hudi\\n\" +\n+      \"spark.sql.sources.schema.partCol.0=partition\\n\" +\n+      \"spark.sql.sources.schema.numParts=1\\n\" +\n+      \"spark.sql.sources.schema.numPartCols=1\\n\" +\n+      \"spark.sql.sources.schema.part.0=\" +\n+      \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"_row_key\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}},\" +\n+      \"{\\\"name\\\":\\\"ts\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\" +\n+      \"{\\\"name\\\":\\\"partition\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}}]}\")(hiveSyncConfig.tableProperties)\n+\n+    assertResult(\"path=/tmp/hoodie_test/*/*\")(hiveSyncConfig.serdeProperties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzMyMQ==", "bodyText": "do you think we need to assert HIVE_TABLE_PROPERTIES as well ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247321", "createdAt": "2021-01-30T13:39:24Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "diffHunk": "@@ -397,6 +401,46 @@ class HoodieSparkSqlWriterSuite extends FunSuite with Matchers {\n       }\n     })\n \n+  test(\"Test build sync config for spark sql\") {\n+    initSparkContext(\"test build sync config\")\n+    val addSqlTablePropertiesMethod =\n+        HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"addSqlTableProperties\",\n+          classOf[SQLConf], classOf[StructType], classOf[Map[_,_]])\n+    addSqlTablePropertiesMethod.setAccessible(true)\n+\n+    val schema = DataSourceTestUtils.getStructTypeExampleSchema\n+    val structType = AvroConversionUtils.convertAvroSchemaToStructType(schema)\n+    val basePath = \"/tmp/hoodie_test\"\n+    val params = Map(\n+      \"path\" -> basePath,\n+      DataSourceWriteOptions.TABLE_NAME_OPT_KEY -> \"test_hoodie\",\n+      DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"partition\"\n+    )\n+    val parameters = HoodieWriterUtils.parametersWithWriteDefaults(params)\n+    val newParams = addSqlTablePropertiesMethod.invoke(HoodieSparkSqlWriter,\n+      spark.sessionState.conf, structType, parameters)\n+      .asInstanceOf[Map[String, String]]\n+\n+    val buildSyncConfigMethod =\n+      HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"buildSyncConfig\", classOf[Path],\n+        classOf[Map[_,_]])\n+    buildSyncConfigMethod.setAccessible(true)\n+\n+    val hiveSyncConfig = buildSyncConfigMethod.invoke(HoodieSparkSqlWriter,\n+      new Path(basePath), newParams).asInstanceOf[HiveSyncConfig]\n+\n+    assertResult(\"spark.sql.sources.provider=hudi\\n\" +\n+      \"spark.sql.sources.schema.partCol.0=partition\\n\" +\n+      \"spark.sql.sources.schema.numParts=1\\n\" +\n+      \"spark.sql.sources.schema.numPartCols=1\\n\" +\n+      \"spark.sql.sources.schema.part.0=\" +\n+      \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"_row_key\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}},\" +\n+      \"{\\\"name\\\":\\\"ts\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\" +\n+      \"{\\\"name\\\":\\\"partition\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}}]}\")(hiveSyncConfig.tableProperties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzU3MA==", "bodyText": "since this is abstract class and not every implementation will have some concrete override, can we make this empty here so that HoodieDLAClient does not need to do a no op override.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247570", "createdAt": "2021-01-30T13:42:11Z", "author": {"login": "nsivabalan"}, "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -75,6 +76,8 @@ public abstract void createTable(String tableName, MessageType storageSchema,\n \n   public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n \n+  public abstract void updateTableProperties(String tableName, Map<String, String> tableProperties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzY3NA==", "bodyText": "I understand it's not part of this diff. But wondering if you can add some java docs to this class in general. I realized we don't have any one. (at line 41 ish) .", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247674", "createdAt": "2021-01-30T13:43:29Z", "author": {"login": "nsivabalan"}, "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -63,7 +63,8 @@ public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning,\n   }\n \n   public abstract void createTable(String tableName, MessageType storageSchema,\n-                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+                                   String inputFormatClass, String outputFormatClass,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODM1NA==", "bodyText": "minor. \"Failed to update...\". remove extra \"get\"", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567248354", "createdAt": "2021-01-30T13:49:41Z", "author": {"login": "nsivabalan"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {\n+      return;\n+    }\n+    try {\n+      Table table = client.getTable(syncConfig.databaseName, tableName);\n+      for (Map.Entry<String, String> entry: tableProperties.entrySet()) {\n+        table.putToParameters(entry.getKey(), entry.getValue());\n+      }\n+      client.alter_table(syncConfig.databaseName, tableName, table);\n+    } catch (Exception e) {\n+      throw new HoodieHiveSyncException(\"Failed to get update table properties for table: \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODUyOA==", "bodyText": "Would be nice if you write a test for the actual problem you faced as per the title/desc. And ensure that the test fails w/o this patch and succeeds with this patch.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567248528", "createdAt": "2021-01-30T13:51:09Z", "author": {"login": "nsivabalan"}, "path": "hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -249,6 +255,54 @@ public void testBasicSync(boolean useJdbc, boolean useSchemaFromCommitMetadata)\n         \"The last commit that was sycned should be 100\");\n   }\n \n+  @ParameterizedTest\n+  @MethodSource({\"useJdbcAndSchemaFromCommitMetadata\"})\n+  public void testSyncWithProperties(boolean useJdbc, boolean useSchemaFromCommitMetadata) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 28}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/63cbf0148a033fe511d49b381691d126f78f8828", "committedDate": "2021-01-07T15:53:38Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception"}, "afterCommit": {"oid": "4b568770a4559f3b3c46694e45a13cf2673277d9", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/4b568770a4559f3b3c46694e45a13cf2673277d9", "committedDate": "2021-01-30T14:37:53Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ecff479df511acf9f4e28a55908ad751acd6513c", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/ecff479df511acf9f4e28a55908ad751acd6513c", "committedDate": "2021-02-08T15:54:10Z", "message": "fix some code review"}, "afterCommit": {"oid": "618ac88270a3e1745fade239ff5d742cec7bfe66", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/618ac88270a3e1745fade239ff5d742cec7bfe66", "committedDate": "2021-02-19T02:46:32Z", "message": "fix some code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "618ac88270a3e1745fade239ff5d742cec7bfe66", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/618ac88270a3e1745fade239ff5d742cec7bfe66", "committedDate": "2021-02-19T02:46:32Z", "message": "fix some code review"}, "afterCommit": {"oid": "c60a7b4b3d6457c73e88666bf7c9418b74c4c0f5", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/c60a7b4b3d6457c73e88666bf7c9418b74c4c0f5", "committedDate": "2021-02-19T03:20:42Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c60a7b4b3d6457c73e88666bf7c9418b74c4c0f5", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/c60a7b4b3d6457c73e88666bf7c9418b74c4c0f5", "committedDate": "2021-02-19T03:20:42Z", "message": "[HUDI-1415] Incorrect query result for hudi hive table when using spark sql\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review"}, "afterCommit": {"oid": "71363392e720bcc8d32c3c90c601b020f3d6366c", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/71363392e720bcc8d32c3c90c601b020f3d6366c", "committedDate": "2021-02-19T03:22:40Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "11831a3fdb77701b56fb25452952fb27e0fac059", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/11831a3fdb77701b56fb25452952fb27e0fac059", "committedDate": "2021-02-20T11:39:34Z", "message": "release after test finished"}, "afterCommit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/872519b8a0b0dfc883e09f25cf1c20d27c36caa7", "committedDate": "2021-02-20T11:40:45Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjA2NDIzODI2", "url": "https://github.com/apache/hudi/pull/2283#pullrequestreview-606423826", "createdAt": "2021-03-08T15:40:46Z", "commit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wOFQxNTo0MDo0NlrOIyNdwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wOFQxNTo0MDo0NlrOIyNdwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTUyMDMyMA==", "bodyText": "@pengzhiwei2018 hello, how spark sql judge to use datasource ? I see just set the table properties . But not the \"ROW FORMAT SERDE \"", "url": "https://github.com/apache/hudi/pull/2283#discussion_r589520320", "createdAt": "2021-03-08T15:40:46Z", "author": {"login": "lw309637554"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -378,11 +389,75 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.autoCreateDatabase = parameters.get(HIVE_AUTO_CREATE_DATABASE_OPT_KEY).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    // Convert the schema and partition info used by spark sql to hive table properties.\n+    // The following code refers to the spark code in\n+    // https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala\n+\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partitionCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrderedType = StructType(dataCols ++ partitionCols)\n+    val schemaParts = reOrderedType.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7"}, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjA2NDI0NjQy", "url": "https://github.com/apache/hudi/pull/2283#pullrequestreview-606424642", "createdAt": "2021-03-08T15:41:36Z", "commit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wOFQxNTo0MTozNlrOIyNgFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wOFQxNTo0MTozNlrOIyNgFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTUyMDkxNw==", "bodyText": "if we can persist this properties to metatable , not the hive table properties?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r589520917", "createdAt": "2021-03-08T15:41:36Z", "author": {"login": "lw309637554"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -378,11 +389,75 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.autoCreateDatabase = parameters.get(HIVE_AUTO_CREATE_DATABASE_OPT_KEY).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    // Convert the schema and partition info used by spark sql to hive table properties.\n+    // The following code refers to the spark code in\n+    // https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala\n+\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partitionCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrderedType = StructType(dataCols ++ partitionCols)\n+    val schemaParts = reOrderedType.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> schemaParts.size.toString", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7"}, "originalPosition": 107}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/872519b8a0b0dfc883e09f25cf1c20d27c36caa7", "committedDate": "2021-02-20T11:40:45Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished"}, "afterCommit": {"oid": "ba70819d3bfa1443c7c4b1f7b21ed89be76b76d6", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/ba70819d3bfa1443c7c4b1f7b21ed89be76b76d6", "committedDate": "2021-04-07T06:08:02Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6b9d6800c20e1d6f45340f1b52b370702a4f0743", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/6b9d6800c20e1d6f45340f1b52b370702a4f0743", "committedDate": "2021-04-08T12:07:37Z", "message": "fix test case"}, "afterCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/6343d09682dbcfb6e9716312889d876178d6349b", "committedDate": "2021-04-12T12:19:19Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM3MTc5MTg0", "url": "https://github.com/apache/hudi/pull/2283#pullrequestreview-637179184", "createdAt": "2021-04-15T22:32:38Z", "commit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMjozMjozOFrOJJ95hA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMzozNjo1OVrOJJ_2Qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzMTEwOA==", "bodyText": "Can be moved to metaSync or syncHive method.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614431108", "createdAt": "2021-04-15T22:32:38Z", "author": {"login": "umehrot2"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -306,7 +311,10 @@ private[hudi] object HoodieSparkSqlWriter {\n     } finally {\n       writeClient.close()\n     }\n-    val metaSyncSuccess = metaSync(parameters, basePath, jsc.hadoopConfiguration)\n+    val newParameters =\n+      addSqlTableProperties(sqlContext.sparkSession.sessionState.conf, df.schema, parameters)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzMTk3NQ==", "bodyText": "This modification seems unnecessary, as hadoopConf is not being used.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614431975", "createdAt": "2021-04-15T22:35:06Z", "author": {"login": "umehrot2"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -388,7 +399,8 @@ private[hudi] object HoodieSparkSqlWriter {\n     }\n   }\n \n-  private def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String]): Boolean = {\n+  private def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String],\n+                       hadoopConf: Configuration): Boolean = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzNTI4NA==", "bodyText": "Can you improve the javadoc ? It has missing properties and descriptions.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614435284", "createdAt": "2021-04-15T22:42:49Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzNTg0Mw==", "bodyText": "nit: tableProperties.isEmpty() ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614435843", "createdAt": "2021-04-15T22:44:20Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw==", "bodyText": "Can't we sync this while creating the table itself, like you are doing for serde properties ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614457527", "createdAt": "2021-04-15T23:20:55Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -164,7 +165,13 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n     LOG.info(\"Storage partitions scan complete. Found \" + writtenPartitionsSince.size());\n     // Sync the partitions if needed\n     syncPartitions(tableName, writtenPartitionsSince);\n-\n+    // Sync the table properties if need\n+    if (cfg.tableProperties != null) {\n+      Map<String, String> tableProperties = ConfigUtils.toMap(cfg.tableProperties);\n+      hoodieHiveClient.updateTableProperties(tableName, tableProperties);\n+      LOG.info(\"Sync table properties for \" + tableName + \", table properties is: \"\n+          + cfg.tableProperties);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MTMxNQ==", "bodyText": "Can you update the toString() in this class ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614461315", "createdAt": "2021-04-15T23:31:26Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java", "diffHunk": "@@ -88,6 +88,12 @@\n   @Parameter(names = {\"--verify-metadata-file-listing\"}, description = \"Verify file listing from Hudi's metadata against file system\")\n   public Boolean verifyMetadataFileListing = HoodieMetadataConfig.DEFAULT_METADATA_VALIDATE;\n \n+  @Parameter(names = {\"--table-properties\"}, description = \"Table properties to hive table\")\n+  public String tableProperties;\n+\n+  @Parameter(names = {\"--serde-properties\"}, description = \"Serde properties to hive table\")\n+  public String serdeProperties;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MzA0Mw==", "bodyText": "Lets introduce another additional boolean property hoodie.datasource.hive_sync.sync_as_datasource and put the feature behind it. We can use true by default, but atleast there would be a way to turn it off. This is going to change the way spark sql queries currently run with Hudi, and is a huge change.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614463043", "createdAt": "2021-04-15T23:36:59Z", "author": {"login": "umehrot2"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -353,6 +353,8 @@ object DataSourceWriteOptions {\n   val HIVE_IGNORE_EXCEPTIONS_OPT_KEY = \"hoodie.datasource.hive_sync.ignore_exceptions\"\n   val HIVE_SKIP_RO_SUFFIX = \"hoodie.datasource.hive_sync.skip_ro_suffix\"\n   val HIVE_SUPPORT_TIMESTAMP = \"hoodie.datasource.hive_sync.support_timestamp\"\n+  val HIVE_TABLE_PROPERTIES = \"hoodie.datasource.hive_sync.table_properties\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/6343d09682dbcfb6e9716312889d876178d6349b", "committedDate": "2021-04-12T12:19:19Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished"}, "afterCommit": {"oid": "33acbd66ba6204c3bc47a150d82b64337c3c5bd3", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/33acbd66ba6204c3bc47a150d82b64337c3c5bd3", "committedDate": "2021-04-19T02:37:47Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cc76eb9e5acc526e8926a82ade9f312b925ed71f", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/cc76eb9e5acc526e8926a82ade9f312b925ed71f", "committedDate": "2021-04-19T07:56:36Z", "message": "fix some comment"}, "afterCommit": {"oid": "1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "committedDate": "2021-04-19T07:57:07Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/1a92d2961ec179d6f0cbd9cbdd454c7b207262c5", "committedDate": "2021-04-19T07:57:07Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table\n\n[HUDI-1415] refactor same code\n\nfix test case\n\nfix read partitioned table exception\n\nfix some code review\n\nfix test case\n\nadd more comments\n\nfix kafka test leak\n\nrelease after test finished"}, "afterCommit": {"oid": "348cf1f629913dc193c17b3377b85351c2596ed9", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/348cf1f629913dc193c17b3377b85351c2596ed9", "committedDate": "2021-04-19T07:58:00Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "348cf1f629913dc193c17b3377b85351c2596ed9", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/348cf1f629913dc193c17b3377b85351c2596ed9", "committedDate": "2021-04-19T07:58:00Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table"}, "afterCommit": {"oid": "8bc3097f3683a62429b5e2d2b54833c87773fc5e", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/8bc3097f3683a62429b5e2d2b54833c87773fc5e", "committedDate": "2021-04-20T02:39:53Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6d2191215a1dd197cc9bebd0c6819e10fc97129b", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/6d2191215a1dd197cc9bebd0c6819e10fc97129b", "committedDate": "2021-04-20T02:44:52Z", "message": "add line"}, "afterCommit": {"oid": "173d21ee455b8ec1bca690d6aecc5c3396bbf7aa", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/173d21ee455b8ec1bca690d6aecc5c3396bbf7aa", "committedDate": "2021-04-20T02:45:16Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "77f1c51e4b696e2a346d78a98047717531359d5b", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/77f1c51e4b696e2a346d78a98047717531359d5b", "committedDate": "2021-04-20T02:57:52Z", "message": "remove dup code"}, "afterCommit": {"oid": "657bf34ca752c67ad4ae902d4458471e6e72601d", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/657bf34ca752c67ad4ae902d4458471e6e72601d", "committedDate": "2021-04-20T02:58:12Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "35b6fb4ab13972e7b2ea52b3ac3690ba75dd9ca1", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/35b6fb4ab13972e7b2ea52b3ac3690ba75dd9ca1", "committedDate": "2021-04-20T03:30:20Z", "message": "keep code consistent"}, "afterCommit": {"oid": "5b865ec35c296b1182b8a90ffbbbcd5a783aeacc", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/5b865ec35c296b1182b8a90ffbbbcd5a783aeacc", "committedDate": "2021-04-20T03:30:46Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "committedDate": "2021-04-20T05:04:54Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b8d85f60d967f12b09e6b3252c7ff25d43bdb7cd", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/b8d85f60d967f12b09e6b3252c7ff25d43bdb7cd", "committedDate": "2021-04-20T05:03:43Z", "message": "Fix code style"}, "afterCommit": {"oid": "fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "author": {"user": {"login": "pengzhiwei2018", "name": "pengzhiwei"}}, "url": "https://github.com/apache/hudi/commit/fa6198382bfaf4a1823064aeea36ea2e74b2b5ed", "committedDate": "2021-04-20T05:04:54Z", "message": "[HUDI-1415] Read Hoodie Table As Spark DataSource Table"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4369, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}