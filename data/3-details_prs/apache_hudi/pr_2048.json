{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc1MDA3ODM0", "number": 2048, "title": "[HUDI-1072] Introduce REPLACE top level action", "bodyText": "What is the purpose of the pull request\nAdd replace a top level action. Implement insert_overwrite operation on top of replace action\nBrief change log\n\nAll post commit actions work on top of HoodieCommitMetadata. Create HoodieReplaceMetadata to be subclass of HoodieCommitMetadata\nAdd insertOverwrite as new operation on HoodieWriteClient. insertOverwrite uses REPLACE action to mark all existing file groups as 'invalid'\nChange archival to delete replaced files before archiving REPLACE action metadata\n\nThere are two other things that needs to be addressed:\n\nEverywhere, we try to invoke 'getCommitsTimeline'/'filterCommits', we need to review and make sure caller can handle replace actions. OR create new methods and refactor all invocations of getCommitsTimeline call new methods.\nFileSystemView#getAllFileGroups (and other methods in view) by default excludes file groups that have been replaced. We have to make sure callers can handle file groups that existed before, but have been replaced. (Example: say, there is a CompactionPlan that includes file id f1. f1 is replaced at a later instant. If compaction is run after replace, it wont be able to find corresponding file group. We need to make sure compaction can make progress instead of throwing errors here.)\n\nVerify this pull request\nThis change added tests. Verified basic actions using quick start and docker setup. Added tests for insertOverwrite and FileSystemView changes. Need to add additional tests for backward compatibility. Also need to change integration tests to include insertOverwrite operations. Created followup tickets here https://issues.apache.org/jira/browse/HUDI-868\nThis is an example .hoodie folder from quick start setup:\n-rw-r--r--  1 satishkotha  wheel  1933 Aug 27 16:39 20200827163904.commit\n-rw-r--r--  1 satishkotha  wheel     0 Aug 27 16:39 20200827163904.commit.requested\n-rw-r--r--  1 satishkotha  wheel  1015 Aug 27 16:39 20200827163904.inflight\n-rw-r--r--  1 satishkotha  wheel  2610 Aug 27 16:39 20200827163927.replace\n-rw-r--r--  1 satishkotha  wheel  1024 Aug 27 16:39 20200827163927.replace.inflight\n-rw-r--r--  1 satishkotha  wheel     0 Aug 27 16:39 20200827163927.replace.requested\ndrwxr-xr-x  2 satishkotha  wheel    64 Aug 27 16:39 archived\n-rw-r--r--  1 satishkotha  wheel   235 Aug 27 16:39 hoodie.properties\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-08-27T23:48:41Z", "url": "https://github.com/apache/hudi/pull/2048", "merged": true, "mergeCommit": {"oid": "a99e93bed542c8ae30a641d1df616cc2cd5798e1"}, "closed": true, "closedAt": "2020-09-30T00:04:25Z", "author": {"login": "satishkotha"}, "timelineItems": {"totalCount": 39, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdDM-NUgBqjM3MDIyNjMzNDc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdNxSbwAFqTQ5OTAwNDQ5Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "04b5c12c47435da496065820c2cd42dd94362c20", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/04b5c12c47435da496065820c2cd42dd94362c20", "committedDate": "2020-08-27T23:41:02Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "d259d1f041753ff2b88c13870b03979734942363", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/d259d1f041753ff2b88c13870b03979734942363", "committedDate": "2020-08-28T04:05:31Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d259d1f041753ff2b88c13870b03979734942363", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/d259d1f041753ff2b88c13870b03979734942363", "committedDate": "2020-08-28T04:05:31Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "committedDate": "2020-08-28T05:24:10Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTExNDY1", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-478111465", "createdAt": "2020-08-29T08:40:01Z", "commit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwODo0MDowMVrOHJaFBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwOTo0Mzo0NlrOHJaZbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyNjUwMQ==", "bodyText": "As discussed, lets retain all the file-groups but perform filtering in the get APIs. THis would avoid correctness issues in filtering and also makes handling incremental file system view easier.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479626501", "createdAt": "2020-08-29T08:40:01Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -173,29 +180,59 @@ protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n     List<HoodieFileGroup> fileGroups = new ArrayList<>();\n     fileIdSet.forEach(pair -> {\n       String fileId = pair.getValue();\n-      HoodieFileGroup group = new HoodieFileGroup(pair.getKey(), fileId, timeline);\n-      if (baseFiles.containsKey(pair)) {\n-        baseFiles.get(pair).forEach(group::addBaseFile);\n-      }\n-      if (logFiles.containsKey(pair)) {\n-        logFiles.get(pair).forEach(group::addLogFile);\n-      }\n+      String partitionPath = pair.getKey();\n+      if (isExcludeFileGroup(partitionPath, fileId)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyOTcwOQ==", "bodyText": "Why are we not tracking dropped fileIds ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479629709", "createdAt": "2020-08-29T09:18:25Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceStat.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+/**\n+ * Statistics about a single Hoodie replace operation.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceStat extends HoodieWriteStat {\n+\n+  // records from the 'getFileId()' can be written to multiple new file groups. This list tracks all new fileIds\n+  private List<String> newFileIds;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDE0MQ==", "bodyText": "getAllExcludeFileGroups -> getReplacedFileGroups ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630141", "createdAt": "2020-08-29T09:23:53Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java", "diffHunk": "@@ -355,6 +357,18 @@ public RemoteHoodieTableFileSystemView(String server, int port, HoodieTableMetaC\n     }\n   }\n \n+  @Override\n+  public Stream<String> getAllExcludeFileGroups(final String partitionPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDIxMQ==", "bodyText": "ALL_EXCLUDE_FILEGROUPS_FOR_PARTITION_URL -> REPLACED_FILEGROUPS_FOR_PARTITION_URL\nLets use a single consistent name \"replaced\" instead of exclude everywhere.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630211", "createdAt": "2020-08-29T09:24:45Z", "author": {"login": "bvaradar"}, "path": "hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java", "diffHunk": "@@ -284,6 +284,13 @@ private void registerFileSlicesAPI() {\n       writeValueAsString(ctx, dtos);\n     }, true));\n \n+    app.get(RemoteHoodieTableFileSystemView.ALL_EXCLUDE_FILEGROUPS_FOR_PARTITION_URL, new ViewHandler(ctx -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMDMxMA==", "bodyText": "why is this needed ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479630310", "createdAt": "2020-08-29T09:25:46Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieCommonTestHarness.java", "diffHunk": "@@ -104,4 +104,8 @@ protected SyncableFileSystemView getFileSystemViewWithUnCommittedSlices(HoodieTa\n   protected HoodieTableType getTableType() {\n     return HoodieTableType.COPY_ON_WRITE;\n   }\n+\n+  protected boolean areTimeTravelTestsEnabled() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTExNQ==", "bodyText": "Are you planning to address this TODO as part of this PR ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479631115", "createdAt": "2020-08-29T09:36:10Z", "author": {"login": "bvaradar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java", "diffHunk": "@@ -102,7 +102,7 @@ public void commit(WriterCommitMessage[] messages) {\n             .flatMap(m -> m.getWriteStatuses().stream().map(m2 -> m2.getStat())).collect(Collectors.toList());\n \n     try {\n-      writeClient.commitStats(instantTime, writeStatList, Option.empty());\n+      writeClient.commitStats(instantTime, writeStatList, Option.empty(), HoodieTimeline.COMMIT_ACTION); //TODO get action type from HoodieWriterCommitMessage", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMTcyNQ==", "bodyText": "Rename to getReplacedFileGroups", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479631725", "createdAt": "2020-08-29T09:43:46Z", "author": {"login": "bvaradar"}, "path": "hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/handlers/FileSliceHandler.java", "diffHunk": "@@ -89,6 +89,10 @@ public FileSliceHandler(Configuration conf, FileSystemViewManager viewManager) t\n         .collect(Collectors.toList());\n   }\n \n+  public List<String> getExcludeFileGroups(String basePath, String partitionPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTE1MTkz", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-478115193", "createdAt": "2020-08-29T09:51:09Z", "commit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwOTo1MTowOVrOHJabtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwOTo1MTowOVrOHJabtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjMxMA==", "bodyText": "Please follow the same structure like the one we did for compaction.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479632310", "createdAt": "2020-08-29T09:51:09Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/SpillableMapBasedFileSystemView.java", "diffHunk": "@@ -77,6 +79,13 @@ public SpillableMapBasedFileSystemView(HoodieTableMetaClient metaClient, HoodieT\n     }\n   }\n \n+  @Override\n+  protected Map<String, Set<String>> createPartitionToExcludeFileGroups() {\n+    // TODO should we create another spillable directory under baseStoreDir?\n+    // the exclude file group is expected to be small, so use parent class in-memory representation\n+    return super.createPartitionToExcludeFileGroups();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 17}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTE1MjM0", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-478115234", "createdAt": "2020-08-29T09:52:03Z", "commit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwOTo1MjowNFrOHJab_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwOTo1MjowNFrOHJab_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzMjM4MQ==", "bodyText": "I think I have mentioned it somewhere else. Lets denote this feature with the consistent term \"Replace\" everywhere.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479632381", "createdAt": "2020-08-29T09:52:04Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/TableFileSystemView.java", "diffHunk": "@@ -148,6 +148,9 @@\n    */\n   Stream<HoodieFileGroup> getAllFileGroups(String partitionPath);\n \n+  Stream<String> getAllExcludeFileGroups(String partitionPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTE1NDA5", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-478115409", "createdAt": "2020-08-29T09:55:20Z", "commit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MTE2Nzc5", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-478116779", "createdAt": "2020-08-29T10:22:24Z", "commit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxMDoyMjoyNFrOHJalfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQxMDoyMjoyNFrOHJalfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYzNDgxMw==", "bodyText": "Instead of direct listStatus, can you use FileSystemViewAbstraction to get the file-group and then delete each files in it ? THis way, once consolidated metadata becomes available, you can take advantage of that. cc @prashantwason", "url": "https://github.com/apache/hudi/pull/2048#discussion_r479634813", "createdAt": "2020-08-29T10:22:24Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,44 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {\n+      try {\n+        HoodieCommitMetadata metadata = HoodieCommitMetadata.fromBytes(\n+            metaClient.getActiveTimeline().getInstantDetails(replaceInstant).get(),\n+            HoodieCommitMetadata.class);\n+\n+        metadata.getPartitionToReplaceStats().entrySet().stream().forEach(entry ->\n+            deleteFileGroups(entry.getKey(), entry.getValue().stream().map(e -> e.getFileId()).collect(Collectors.toSet()), instant)\n+        );\n+      } catch (IOException e) {\n+        throw new HoodieCommitException(\"Failed to archive because cannot delete replace files\", e);\n+      }\n+    });\n+  }\n+\n+  private void deleteFileGroups(String partitionPath, Set<String> fileIdsToDelete, HoodieInstant instant) {\n+    try {\n+      FileStatus[] statuses = metaClient.getFs().listStatus(FSUtils.getPartitionPath(metaClient.getBasePath(), partitionPath));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa"}, "originalPosition": 66}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/c864f503e6b5c671513c6e27eb8eb73802ddfaaa", "committedDate": "2020-08-28T05:24:10Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "1f5f737ee065b08979dca06d3153082806710057", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/1f5f737ee065b08979dca06d3153082806710057", "committedDate": "2020-08-31T07:33:49Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1f5f737ee065b08979dca06d3153082806710057", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/1f5f737ee065b08979dca06d3153082806710057", "committedDate": "2020-08-31T07:33:49Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "849d7880b8b3dd7b32abb8ab313f58b7793c604f", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/849d7880b8b3dd7b32abb8ab313f58b7793c604f", "committedDate": "2020-09-01T00:40:04Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "849d7880b8b3dd7b32abb8ab313f58b7793c604f", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/849d7880b8b3dd7b32abb8ab313f58b7793c604f", "committedDate": "2020-09-01T00:40:04Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "committedDate": "2020-09-01T00:47:33Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/aa5e4b663087e76d3b6d171633b1fd04bafa8cd2", "committedDate": "2020-09-01T00:47:33Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "28aad2ce30e77c11681b3eb8a8a021ae4af4f7bd", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/28aad2ce30e77c11681b3eb8a8a021ae4af4f7bd", "committedDate": "2020-09-01T17:15:17Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "28aad2ce30e77c11681b3eb8a8a021ae4af4f7bd", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/28aad2ce30e77c11681b3eb8a8a021ae4af4f7bd", "committedDate": "2020-09-01T17:15:17Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "f04fbc6bc375ec29271a449dd870cb2652f85626", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/f04fbc6bc375ec29271a449dd870cb2652f85626", "committedDate": "2020-09-01T20:22:50Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f04fbc6bc375ec29271a449dd870cb2652f85626", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/f04fbc6bc375ec29271a449dd870cb2652f85626", "committedDate": "2020-09-01T20:22:50Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "087909fe7d2000171a551be12c8ffa79b6ec6250", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/087909fe7d2000171a551be12c8ffa79b6ec6250", "committedDate": "2020-09-02T00:59:47Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "087909fe7d2000171a551be12c8ffa79b6ec6250", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/087909fe7d2000171a551be12c8ffa79b6ec6250", "committedDate": "2020-09-02T00:59:47Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "f5b19ed6194364a9125f7b7110ed49ae5090b66c", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/f5b19ed6194364a9125f7b7110ed49ae5090b66c", "committedDate": "2020-09-02T18:06:39Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f5b19ed6194364a9125f7b7110ed49ae5090b66c", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/f5b19ed6194364a9125f7b7110ed49ae5090b66c", "committedDate": "2020-09-02T18:06:39Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "6bd5bce92282f4131c0349083174e3e0b68a2f1d", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6bd5bce92282f4131c0349083174e3e0b68a2f1d", "committedDate": "2020-09-02T18:59:17Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6bd5bce92282f4131c0349083174e3e0b68a2f1d", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6bd5bce92282f4131c0349083174e3e0b68a2f1d", "committedDate": "2020-09-02T18:59:17Z", "message": "[HUDI-1072] Introduce REPLACE top level action"}, "afterCommit": {"oid": "00b151b7ed0b419146fabb9b85122903aaada57f", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/00b151b7ed0b419146fabb9b85122903aaada57f", "committedDate": "2020-09-04T19:41:30Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "00b151b7ed0b419146fabb9b85122903aaada57f", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/00b151b7ed0b419146fabb9b85122903aaada57f", "committedDate": "2020-09-04T19:41:30Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "6c9793056d885a74c17e3d0275d348d28684e63b", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6c9793056d885a74c17e3d0275d348d28684e63b", "committedDate": "2020-09-04T20:23:41Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6c9793056d885a74c17e3d0275d348d28684e63b", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6c9793056d885a74c17e3d0275d348d28684e63b", "committedDate": "2020-09-04T20:23:41Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "f7e55e9b2db8a3fd906c48af77c3759dca1f065d", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/f7e55e9b2db8a3fd906c48af77c3759dca1f065d", "committedDate": "2020-09-10T18:58:21Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1OTY5NTQ0", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-485969544", "createdAt": "2020-09-10T14:03:10Z", "commit": {"oid": "6c9793056d885a74c17e3d0275d348d28684e63b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNDowMzoxMFrOHP1ooA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNDowMzoxMFrOHP1ooA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjM2OTQ0MA==", "bodyText": "can't we just call commit(String, JavaRDD, Option.empty()) without having to implement this?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r486369440", "createdAt": "2020-09-10T14:03:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,55 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c9793056d885a74c17e3d0275d348d28684e63b"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzE1NzY2", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-487315766", "createdAt": "2020-09-13T15:14:50Z", "commit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "state": "COMMENTED", "comments": {"totalCount": 49, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xM1QxNToxNDo1MFrOHQ9LSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xM1QxODo1ODoyMFrOHQ-kcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTU3OA==", "bodyText": "rename: to just buildMetdata() , Commit is already  implicit from context.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541578", "createdAt": "2020-09-13T15:14:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,57 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, String commitActionType) {\n+    return commit(instantTime, writeStatuses, Option.empty(), commitActionType);\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType);\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n+      Option<Map<String, String>> extraMetadata, String commitActionType) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replaceStats = writeStatuses.filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, replaceStats);\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, List<HoodieWriteStat> replaceStats) {\n+    LOG.info(\"Committing \" + instantTime + \" action \" + commitActionType);\n     HoodieTableMetaClient metaClient = createMetaClient(false);\n-    String actionType = metaClient.getCommitActionType();\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n     HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-\n+    HoodieCommitMetadata metadata = CommitUtils.buildWriteActionMetadata(stats, replaceStats, extraMetadata, operationType, config.getSchema(), commitActionType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTY2Mw==", "bodyText": "note to self: lets see if we can simply the commit() overloaded methods.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541663", "createdAt": "2020-09-13T15:15:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -87,44 +88,57 @@ protected AbstractHoodieWriteClient(JavaSparkContext jsc, HoodieIndex index, Hoo\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n-    return commit(instantTime, writeStatuses, Option.empty());\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, Option.empty(), actionType);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, String commitActionType) {\n+    return commit(instantTime, writeStatuses, Option.empty(), commitActionType);\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType);\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MTg2NQ==", "bodyText": "rename: writeInputRecords()", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487541865", "createdAt": "2020-09-13T15:17:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -95,6 +93,13 @@ public HoodieWriteMetadata execute(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n       saveWorkloadProfileMetadataToInflight(profile, instantTime);\n     }\n \n+    JavaRDD<WriteStatus> writeStatusRDD = processInputRecords(inputRecordsRDD, profile);\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    updateIndexAndCommitIfNeeded(writeStatusRDD, result);\n+    return result;\n+  }\n+\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MjA3Mg==", "bodyText": "note to self: see if there is a way to avoid repeating this filtering here again", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542072", "createdAt": "2020-09-13T15:19:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MjI0MQ==", "bodyText": "rename: completeInstant(), its better to stay close to what the method is doing; using just one terminology", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542241", "createdAt": "2020-09-13T15:20:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n-  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> stats) {\n-    String actionType = table.getMetaClient().getCommitActionType();\n+  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n+    String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n-    HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n \n     result.setCommitted(true);\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-    result.setWriteStats(stats);\n+    result.setWriteStats(writeStats);\n+    result.setReplaceStats(replaceStats);\n \n     // Finalize write\n-    finalizeWrite(instantTime, stats, result);\n-\n-    // add in extra metadata\n-    if (extraMetadata.isPresent()) {\n-      extraMetadata.get().forEach(metadata::addMetadata);\n-    }\n-    metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, getSchemaToStoreInCommit());\n-    metadata.setOperationType(operationType);\n+    finalizeWrite(instantTime, writeStats, result);\n \n     try {\n-      activeTimeline.saveAsComplete(new HoodieInstant(true, actionType, instantTime),\n-          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n-      LOG.info(\"Committed \" + instantTime);\n+      HoodieCommitMetadata metadata = writeInstant(writeStats, replaceStats, extraMetadata);\n+      result.setCommitMetadata(Option.of(metadata));\n     } catch (IOException e) {\n       throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime,\n           e);\n     }\n-    result.setCommitMetadata(Option.of(metadata));\n+  }\n+\n+  private HoodieCommitMetadata writeInstant(List<HoodieWriteStat>  writeStats, List<HoodieWriteStat> replaceStats, Option<Map<String, String>> extraMetadata) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Mjc2OQ==", "bodyText": "nts: need to ensure the operation type  is properly set", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487542769", "createdAt": "2020-09-13T15:25:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/BaseCommitActionExecutor.java", "diffHunk": "@@ -204,41 +212,44 @@ protected void commitOnAutoCommit(HoodieWriteMetadata result) {\n   }\n \n   protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result) {\n-    commit(extraMetadata, result, result.getWriteStatuses().map(WriteStatus::getStat).collect());\n+    List<HoodieWriteStat> writeStats = result.getWriteStatuses().filter(w -> !w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    List<HoodieWriteStat> replacedStats = result.getWriteStatuses().filter(w -> w.isReplacedFileId()).map(WriteStatus::getStat).collect();\n+    commit(extraMetadata, result, writeStats, replacedStats);\n   }\n \n-  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> stats) {\n-    String actionType = table.getMetaClient().getCommitActionType();\n+  protected void commit(Option<Map<String, String>> extraMetadata, HoodieWriteMetadata result, List<HoodieWriteStat> writeStats, List<HoodieWriteStat> replaceStats) {\n+    String actionType = getCommitActionType();\n     LOG.info(\"Committing \" + instantTime + \", action Type \" + actionType);\n     // Create a Hoodie table which encapsulated the commits and files visible\n     HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n \n-    HoodieActiveTimeline activeTimeline = table.getActiveTimeline();\n-    HoodieCommitMetadata metadata = new HoodieCommitMetadata();\n \n     result.setCommitted(true);\n-    stats.forEach(stat -> metadata.addWriteStat(stat.getPartitionPath(), stat));\n-    result.setWriteStats(stats);\n+    result.setWriteStats(writeStats);\n+    result.setReplaceStats(replaceStats);\n \n     // Finalize write\n-    finalizeWrite(instantTime, stats, result);\n-\n-    // add in extra metadata\n-    if (extraMetadata.isPresent()) {\n-      extraMetadata.get().forEach(metadata::addMetadata);\n-    }\n-    metadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, getSchemaToStoreInCommit());\n-    metadata.setOperationType(operationType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzE5NA==", "bodyText": "I think at the HoodieTable level, the API has to be about replacing file groups and not insertOverwrite (which can be limited to the WriteClient level). This way clustering can also use the same method, to build on top.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487543194", "createdAt": "2020-09-13T15:29:36Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -213,6 +213,12 @@ public abstract HoodieWriteMetadata insertPrepped(JavaSparkContext jsc, String i\n   public abstract HoodieWriteMetadata bulkInsertPrepped(JavaSparkContext jsc, String instantTime,\n       JavaRDD<HoodieRecord<T>> preppedRecords,  Option<BulkInsertPartitioner> bulkInsertPartitioner);\n \n+  /**\n+   * Logically delete all existing records and Insert a batch of new records into Hoodie table at the supplied instantTime.\n+   */\n+  public abstract HoodieWriteMetadata insertOverwrite(JavaSparkContext jsc, String instantTime,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzMxNw==", "bodyText": "rename: deleteReplacedFileGroups() , to be consistent with our terminology", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487543317", "createdAt": "2020-09-13T15:30:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng==", "bodyText": "you probably dont want to fetch the fs object each time? Also lets delete in parallel, from the get go?  we will invariably need to do this, much like parallelizing cleaning.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487544876", "createdAt": "2020-09-13T15:45:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {\n+      fg.getAllRawFileSlices().forEach(fileSlice -> {\n+        fileSlice.getBaseFile().map(baseFile -> deletePath(baseFile.getFileStatus().getPath(), instant));\n+        fileSlice.getLogFiles().forEach(logFile -> deletePath(logFile.getPath(), instant));\n+      });\n+    });\n+  }\n+\n+  /**\n+   * Because we are creating new 'HoodieTable' and FileSystemView objects in this class constructor,\n+   * partition view may not be loaded correctly.\n+   * Reload all partitions modified by REPLACE action\n+   *\n+   * TODO find a better way to pass the FileSystemView to this class.\n+   */\n+  private void ensureReplacedPartitionsLoadedCorrectly(HoodieInstant instant, TableFileSystemView fileSystemView) {\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {\n+      try {\n+        HoodieReplaceCommitMetadata metadata = HoodieReplaceCommitMetadata.fromBytes(\n+            metaClient.getActiveTimeline().getInstantDetails(replaceInstant).get(),\n+            HoodieReplaceCommitMetadata.class);\n+\n+        metadata.getPartitionToReplaceStats().keySet().forEach(partition -> fileSystemView.getAllFileGroups(partition));\n+      } catch (IOException e) {\n+        throw new HoodieCommitException(\"Failed to archive because cannot delete replace files\", e);\n+      }\n+    });\n+  }\n+\n+  private boolean deletePath(Path path, HoodieInstant instant) {\n+    try {\n+      LOG.info(\"Deleting \" + path + \" before archiving \" + instant);\n+      metaClient.getFs().delete(path);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTAyMg==", "bodyText": "we need to do the deletion in parallel.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487545022", "createdAt": "2020-09-13T15:46:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTExNQ==", "bodyText": "probably a check that this is a replace instant as well?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487545115", "createdAt": "2020-09-13T15:47:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NzUwNQ==", "bodyText": "This seems like a check for whether the instant is a replacecommit or not. if the instant time is a completed instant and replacecommit type, then we must find the instant here, right?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487547505", "createdAt": "2020-09-13T16:10:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {\n+      fg.getAllRawFileSlices().forEach(fileSlice -> {\n+        fileSlice.getBaseFile().map(baseFile -> deletePath(baseFile.getFileStatus().getPath(), instant));\n+        fileSlice.getLogFiles().forEach(logFile -> deletePath(logFile.getPath(), instant));\n+      });\n+    });\n+  }\n+\n+  /**\n+   * Because we are creating new 'HoodieTable' and FileSystemView objects in this class constructor,\n+   * partition view may not be loaded correctly.\n+   * Reload all partitions modified by REPLACE action\n+   *\n+   * TODO find a better way to pass the FileSystemView to this class.\n+   */\n+  private void ensureReplacedPartitionsLoadedCorrectly(HoodieInstant instant, TableFileSystemView fileSystemView) {\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ==", "bodyText": "Do we need to ask the file system view for all the replace file groups? this must be in the metadata already right? As long as we can get the HoodieFileGroup objects corresponding to the filegroup ids in the metadata, we can go ahead? What I am suggesting in an alternative and subjectively cleaner replacement for ensureReplaced... above, which seems to make a dummy read to warm up the datastuctures. I prefer to let that happen naturally on its own as opposed to having this \"special\" call", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487547971", "createdAt": "2020-09-13T16:14:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODI4MA==", "bodyText": "you can just call startCommitWithTime(instantTime) from here?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487548280", "createdAt": "2020-09-13T16:17:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -576,7 +592,8 @@ public String startCommit() {\n       rollbackPendingCommits();\n     }\n     String instantTime = HoodieActiveTimeline.createNewInstantTime();\n-    startCommit(instantTime);\n+    HoodieTableMetaClient metaClient = createMetaClient(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0ODM1NQ==", "bodyText": "can we pass in the metaClient from caller. This seems to introduce additional creations, which all list .hoodie again", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487548355", "createdAt": "2020-09-13T16:18:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,15 +603,23 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, metaClient.getCommitActionType());\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  public void startCommitWithTime(String instantTime, String actionType) {\n     // NOTE : Need to ensure that rollback is done before a new commit is started\n     if (rollbackPending) {\n       // Only rollback inflight commit/delta-commits. Do not touch compaction commits\n       rollbackPendingCommits();\n     }\n-    startCommit(instantTime);\n+    startCommit(instantTime, actionType);\n   }\n \n-  private void startCommit(String instantTime) {\n+  private void startCommit(String instantTime, String actionType) {\n     LOG.info(\"Generate a new instant time \" + instantTime);\n     HoodieTableMetaClient metaClient = createMetaClient(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0OTc3Mw==", "bodyText": "typo: getReplaceStats", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487549773", "createdAt": "2020-09-13T16:32:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/HoodieWriteMetadata.java", "diffHunk": "@@ -94,6 +94,14 @@ public void setWriteStats(List<HoodieWriteStat> writeStats) {\n     this.writeStats = Option.of(writeStats);\n   }\n \n+  public Option<List<HoodieWriteStat>> getReplacetats() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MTQ2Nw==", "bodyText": "why limit to just base files. we may have log files without base files. i.e insert to log files code path", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487551467", "createdAt": "2020-09-13T16:49:36Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n+    // do necessary inserts into new file groups\n+    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    return writeStatuses.union(replaceStatuses);\n+  }\n+\n+  private JavaRDD<WriteStatus> getAllReplaceWriteStatus(WorkloadProfile profile) {\n+    JavaRDD<String> partitions = jsc.parallelize(new ArrayList<>(profile.getPartitionPaths()));\n+    JavaRDD<WriteStatus> replaceStatuses = partitions.flatMap(partition ->\n+        getAllExistingFileIds(partition).map(fileId -> getReplaceWriteStatus(partition, fileId)).iterator());\n+\n+    return replaceStatuses;\n+  }\n+\n+  private Stream<String> getAllExistingFileIds(String partitionPath) {\n+    // because new commit is not complete. it is safe to mark all base files as old files\n+    return table.getBaseFileOnlyView().getAllBaseFiles(partitionPath).map(baseFile -> baseFile.getFileId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MTY4MA==", "bodyText": "Seeing such large values in the metadata can be bit confusing. can we set it to -1 instead for now", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487551680", "createdAt": "2020-09-13T16:51:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);\n+    // do necessary inserts into new file groups\n+    JavaRDD<WriteStatus> writeStatuses = super.processInputRecords(inputRecordsRDD, profile);\n+    return writeStatuses.union(replaceStatuses);\n+  }\n+\n+  private JavaRDD<WriteStatus> getAllReplaceWriteStatus(WorkloadProfile profile) {\n+    JavaRDD<String> partitions = jsc.parallelize(new ArrayList<>(profile.getPartitionPaths()));\n+    JavaRDD<WriteStatus> replaceStatuses = partitions.flatMap(partition ->\n+        getAllExistingFileIds(partition).map(fileId -> getReplaceWriteStatus(partition, fileId)).iterator());\n+\n+    return replaceStatuses;\n+  }\n+\n+  private Stream<String> getAllExistingFileIds(String partitionPath) {\n+    // because new commit is not complete. it is safe to mark all base files as old files\n+    return table.getBaseFileOnlyView().getAllBaseFiles(partitionPath).map(baseFile -> baseFile.getFileId());\n+  }\n+\n+  private WriteStatus getReplaceWriteStatus(String partitionPath, String fileId) {\n+    // mark file as 'replaced' in metadata. the actual file will be removed later by cleaner to provide snapshot isolation\n+    WriteStatus status = new WriteStatus(false, 0.0);\n+    status.setReplacedFileId(true);\n+    status.setFileId(fileId);\n+    status.setTotalErrorRecords(0);\n+    status.setPartitionPath(partitionPath);\n+    HoodieWriteStat replaceStat = new HoodieWriteStat();\n+    status.setStat(replaceStat);\n+    replaceStat.setPartitionPath(partitionPath);\n+    replaceStat.setFileId(fileId);\n+    replaceStat.setPath(table.getBaseFileOnlyView().getLatestBaseFile(partitionPath, fileId).get().getPath());\n+    status.getStat().setNumDeletes(Integer.MAX_VALUE);//token to indicate all rows are deleted", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjE3MA==", "bodyText": "So, this creates a dependency on the workloadProfile for doing insert overwrite. While we always provide a WorkloadProfile for now, in the future we would like to remove this need for caching data in memory and building the profile.\nCan we try to reimplement this such that\n\nprocessInputRecords(..) just writes the new records and returns WriteStatus for the new file groups alone.\nDuring commit time, after we collect the WriteStatus, we can obtain the replaceStatuses based on the partitions that were actually written to during step above.\n\nThis also gives us a cleaner solution for avoiding the boolean flag we discussed. API is also consistent now, that writeClient.insertOverwrite() only returns the WriteStatus for the new file group IDs.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552170", "createdAt": "2020-09-13T16:56:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/InsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.stream.Stream;\n+\n+public class InsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends CommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(InsertOverwriteCommitActionExecutor.class);\n+\n+  private final JavaRDD<HoodieRecord<T>> inputRecordsRDD;\n+\n+  public InsertOverwriteCommitActionExecutor(JavaSparkContext jsc,\n+                                             HoodieWriteConfig config, HoodieTable table,\n+                                             String instantTime, JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n+    super(jsc, config, table, instantTime, WriteOperationType.INSERT_OVERWRITE);\n+    this.inputRecordsRDD = inputRecordsRDD;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata execute() {\n+    return WriteHelper.write(instantTime, inputRecordsRDD, jsc, (HoodieTable<T>) table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected Partitioner getPartitioner(WorkloadProfile profile) {\n+    return new InsertOverwritePartitioner<>(profile, jsc, table, config);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected JavaRDD<WriteStatus> processInputRecords(JavaRDD<HoodieRecord<T>> inputRecordsRDD, WorkloadProfile profile) {\n+    // get all existing fileIds to mark them as replaced\n+    JavaRDD<WriteStatus> replaceStatuses = getAllReplaceWriteStatus(profile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjMxOA==", "bodyText": "As an after thought, I also realize that if we just encoded the entire file group being replaced into the metadata, (as opposed to just encoding the file ids), we can simply delete the file groups without any interaction with tableFileSytemView at all. Probably a simpler solution even?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552318", "createdAt": "2020-09-13T16:57:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjYwOA==", "bodyText": "please keep test/naming to base and log files. and not leak parquet to the test? Also can you please see if this test can be authored by reusing existing helpers. Its often bit hard to read and reuse the exisiting helpers, but hte more one-offs we introduce, the worse this situation becomes.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552608", "createdAt": "2020-09-13T17:00:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -880,6 +880,89 @@ public void testDeletesWithDeleteApi() throws Exception {\n     testDeletes(client, updateBatch3.getRight(), 10, file1, \"007\", 140, keysSoFar);\n   }\n \n+  /**\n+   * Test scenario of writing more file groups than existing number of file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlingWithMoreRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(1000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing fewer file groups than existing number of file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlingWithFewerRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(3000, 1000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void testInsertOverwritePartitionHandlinWithSimilarNumberOfRecords() throws Exception {\n+    verifyInsertOverwritePartitionHandling(3000, 3000);\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records.\n+   *  2) Do write2 (insert overwrite) with 'batch2RecordsCount' number of records.\n+   *\n+   *  Verify that all records in step1 are overwritten\n+   */\n+  private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int batch2RecordsCount) throws Exception {\n+    final String testPartitionPath = \"americas\";\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000);\n+    HoodieWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\n+\n+    // Do Inserts\n+    String commitTime1 = \"001\";\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInserts(commitTime1, batch1RecordsCount);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batch1Buckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyParquetFileData(commitTime1, inserts1, statuses);\n+\n+    // Do Insert Overwrite\n+    String commitTime2 = \"002\";\n+    client.startCommitWithTime(commitTime2, HoodieTimeline.REPLACE_COMMIT_ACTION);\n+    List<HoodieRecord> inserts2 = dataGen.generateInserts(commitTime2, batch2RecordsCount);\n+    List<HoodieRecord> insertsAndUpdates2 = new ArrayList<>();\n+    insertsAndUpdates2.addAll(inserts2);\n+    JavaRDD<HoodieRecord> insertAndUpdatesRDD2 = jsc.parallelize(insertsAndUpdates2, 2);\n+    statuses = client.insertOverwrite(insertAndUpdatesRDD2, commitTime2).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> replacedBuckets = statuses.stream().filter(s -> s.isReplacedFileId())\n+        .map(s -> s.getFileId()).collect(Collectors.toSet());\n+    assertEquals(batch1Buckets, replacedBuckets);\n+    List<WriteStatus> newBuckets = statuses.stream().filter(s -> !(s.isReplacedFileId()))\n+        .collect(Collectors.toList());\n+    verifyParquetFileData(commitTime2, inserts2, newBuckets);\n+  }\n+\n+  /**\n+   * Verify data in parquet files matches expected records and commit time.\n+   */\n+  private void verifyParquetFileData(String commitTime, List<HoodieRecord> expectedRecords, List<WriteStatus> allStatus) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjY4NQ==", "bodyText": "rename: isReplaced", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552685", "createdAt": "2020-09-13T17:01:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/WriteStatus.java", "diffHunk": "@@ -52,6 +52,9 @@\n \n   private HoodieWriteStat stat = null;\n \n+  // if true, indicates the fileId in this WriteStatus is being replaced\n+  private boolean isReplacedFileId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Mjc4MQ==", "bodyText": "now that replace also is replacecommit. should we just leave the getCommitsAndCompactionTimeline() be?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552781", "createdAt": "2020-09-13T17:03:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1MjgyOA==", "bodyText": "nit: remove extra line?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487552828", "createdAt": "2020-09-13T17:03:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n+    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);\n   }\n \n   /**\n    * Adds the provided statuses into the file system view, and also caches it inside this object.\n    */\n   protected List<HoodieFileGroup> addFilesToView(FileStatus[] statuses) {\n     HoodieTimer timer = new HoodieTimer().startTimer();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NTEzOQ==", "bodyText": "please use HoodieTimer to time code segments.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487555139", "createdAt": "2020-09-13T17:27:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -196,6 +205,32 @@ protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n     return fileGroups;\n   }\n \n+  /**\n+   * Get replaced instant for each file group by looking at all commit instants.\n+   */\n+  private void resetFileGroupsReplaced(HoodieTimeline timeline) {\n+    Instant indexStartTime = Instant.now();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NjEzMA==", "bodyText": "I think its sufficient todo this reset in the init() method above, much like pendingCompaction and bootstreap handling. This method is simply used to refresh the timeline i.e the instants that are visible.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556130", "createdAt": "2020-09-13T17:36:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -110,14 +116,16 @@ protected void init(HoodieTableMetaClient metaClient, HoodieTimeline visibleActi\n    * @param visibleActiveTimeline Visible Active Timeline\n    */\n   protected void refreshTimeline(HoodieTimeline visibleActiveTimeline) {\n-    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getCommitsAndCompactionTimeline();\n+    this.visibleCommitsAndCompactionTimeline = visibleActiveTimeline.getWriteActionTimeline();\n+    resetFileGroupsReplaced(visibleCommitsAndCompactionTimeline);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NjYyNg==", "bodyText": "rename: getReplaceInstant()", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556626", "createdAt": "2020-09-13T17:41:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -727,6 +775,21 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract Stream<HoodieFileGroup> fetchAllStoredFileGroups();\n \n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for new file groups replaced.\n+   */\n+  protected abstract void addReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract Option<HoodieInstant> getReplacedInstant(final HoodieFileGroupId fileGroupId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Njg5OA==", "bodyText": "can you just call isFileGroupReplacedBeforeOrOn(fileGroup, max(instants)) ? without having to implement this again", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487556898", "createdAt": "2020-09-13T17:44:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -880,6 +957,30 @@ private FileSlice fetchMergedFileSlice(HoodieFileGroup fileGroup, FileSlice file\n         .fromJavaOptional(fetchLatestFileSlices(partitionPath).filter(fs -> fs.getFileId().equals(fileId)).findFirst());\n   }\n \n+  private boolean isFileGroupReplaced(HoodieFileGroup fileGroup) {\n+    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n+    return hoodieInstantOption.isPresent();\n+  }\n+\n+  private boolean isFileGroupReplacedBeforeAny(HoodieFileGroup fileGroup, List<String> instants) {\n+    Option<HoodieInstant> hoodieInstantOption = getReplacedInstant(fileGroup.getFileGroupId());\n+    if (!hoodieInstantOption.isPresent()) {\n+      return false;\n+    }\n+\n+    return HoodieTimeline.compareTimestamps(instants.stream().max(Comparator.naturalOrder()).get(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 270}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzA0OQ==", "bodyText": "deltacommit is not here. huh. file a \"code cleanup\" JIRA for later?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557049", "createdAt": "2020-09-13T17:45:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/ActionType.java", "diffHunk": "@@ -22,5 +22,5 @@\n  * The supported action types.\n  */\n public enum ActionType {\n-  commit, savepoint, compaction, clean, rollback\n+  commit, savepoint, compaction, clean, rollback, replacecommit", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzE5MA==", "bodyText": "please add a simple unit tests for this . testing for e.g that the schema is set, op type is set etc", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557190", "createdAt": "2020-09-13T17:46:52Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Helper class to generate compaction plan from FileGroup/FileSlice abstraction.\n+ */\n+public class CommitUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(CommitUtils.class);\n+\n+  public static HoodieCommitMetadata buildWriteActionMetadata(List<HoodieWriteStat> writeStats,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzM1Ng==", "bodyText": "should we just add at the end? not sure if it will be backwards compatible nicely otherwise.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557356", "createdAt": "2020-09-13T17:48:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieArchivedMetaEntry.avsc", "diffHunk": "@@ -36,6 +36,14 @@\n          ],\n          \"default\": null\n       },\n+      {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzM3Ng==", "bodyText": "nit: extra line", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557376", "createdAt": "2020-09-13T17:48:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieCommitMetadata.java", "diffHunk": "@@ -46,11 +46,12 @@\n   public static final String SCHEMA_KEY = \"schema\";\n   private static final Logger LOG = LogManager.getLogger(HoodieCommitMetadata.class);\n   protected Map<String, List<HoodieWriteStat>> partitionToWriteStats;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1NzQzMg==", "bodyText": "rename: getCompletedReplaceTimeline()  current naming gives the impression that its either completed or replacecommit", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557432", "createdAt": "2020-09-13T17:49:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java", "diffHunk": "@@ -113,6 +112,18 @@ public HoodieDefaultTimeline getCommitsAndCompactionTimeline() {\n     return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);\n   }\n \n+  @Override\n+  public HoodieDefaultTimeline getWriteActionTimeline() {\n+    Set<String> validActions = CollectionUtils.createSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION, REPLACE_COMMIT_ACTION);\n+    return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);\n+  }\n+\n+  @Override\n+  public HoodieTimeline getCompletedAndReplaceTimeline() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzc3Mw==", "bodyText": "for my own understanding, copying the fields from CommitMetadata is the only way to \"inherit\" the avro schema, I guess?\nalso, to be consistent. should we first place the base fields from commit metadata first, and add partitionToReplaceStats at the end?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557773", "createdAt": "2020-09-13T17:52:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieReplaceCommitMetadata.avsc", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieReplaceCommitMetadata\",\n+   \"fields\":[\n+      {\n+         \"name\":\"partitionToWriteStats\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzg5NA==", "bodyText": "these are the file groups being replaced? I thought we were going to just track the file ids?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487557894", "createdAt": "2020-09-13T17:54:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.PropertyAccessor;\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * All the metadata that gets stored along with a commit.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n+  private static final Logger LOG = LogManager.getLogger(HoodieReplaceCommitMetadata.class);\n+  protected Map<String, List<HoodieWriteStat>> partitionToReplaceStats;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODAwNA==", "bodyText": "if these are the file groups being replaced, then does this contain all the file slices (see my comment around deleting the replaced file groups in timeline archive log)", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558004", "createdAt": "2020-09-13T17:55:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.PropertyAccessor;\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * All the metadata that gets stored along with a commit.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n+  private static final Logger LOG = LogManager.getLogger(HoodieReplaceCommitMetadata.class);\n+  protected Map<String, List<HoodieWriteStat>> partitionToReplaceStats;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1Nzg5NA=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODA1NQ==", "bodyText": "rename: fgIdToReplaceInstants", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558055", "createdAt": "2020-09-13T17:55:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java", "diffHunk": "@@ -64,6 +64,11 @@\n    */\n   protected Map<HoodieFileGroupId, BootstrapBaseFileMapping> fgIdToBootstrapBaseFile;\n \n+  /**\n+   * Track replace time for replaced file groups.\n+   */\n+  protected Map<HoodieFileGroupId, HoodieInstant> fgIdToReplaceInstant;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODI3NQ==", "bodyText": "for tests, I suggest using the HoodieWritableTestTable etc instead of introducing new methods. Also please check other utilities to avoid writing a new method here", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558275", "createdAt": "2020-09-13T17:58:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java", "diffHunk": "@@ -366,6 +378,14 @@ public static void createCompactionRequestedFile(String basePath, String instant\n     createEmptyFile(basePath, commitFile, configuration);\n   }\n \n+  public static void createDataFile(String basePath, String partitionPath, String instantTime, String fileID, Configuration configuration)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODMzNQ==", "bodyText": "why is this change necessayr?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558335", "createdAt": "2020-09-13T17:59:21Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestDataGenerator.java", "diffHunk": "@@ -378,9 +398,9 @@ public static void createCompactionAuxiliaryMetadata(String basePath, HoodieInst\n         new Path(basePath + \"/\" + HoodieTableMetaClient.AUXILIARYFOLDER_NAME + \"/\" + instant.getFileName());\n     FileSystem fs = FSUtils.getFs(basePath, configuration);\n     try (FSDataOutputStream os = fs.create(commitFile, true)) {\n-      HoodieCompactionPlan workload = new HoodieCompactionPlan();\n+      HoodieCompactionPlan workload = HoodieCompactionPlan.newBuilder().setVersion(1).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODQ1MQ==", "bodyText": "there is nothing specific about replace in this method? should we move this to the test class itself. inline?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558451", "createdAt": "2020-09-13T18:00:24Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java", "diffHunk": "@@ -200,6 +199,14 @@ public static void createInflightCommitFiles(String basePath, String... instantT\n     }\n   }\n \n+  public static HoodieWriteStat createReplaceStat(final String partitionPath, final String fileId1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODUwNw==", "bodyText": "see earlier comment on whether we need this method.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558507", "createdAt": "2020-09-13T18:01:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java", "diffHunk": "@@ -133,6 +137,21 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+  /**\n+   * Timeline to just include commits (commit/deltacommit), replace and compaction actions.\n+   *\n+   * @return\n+   */\n+  HoodieDefaultTimeline getWriteActionTimeline();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw==", "bodyText": "@bvaradar if you can take a pass at these, that would be great", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558663", "createdAt": "2020-09-13T18:02:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java", "diffHunk": "@@ -371,6 +371,47 @@ void removeBootstrapBaseFileMapping(Stream<BootstrapBaseFileMapping> bootstrapBa\n         schemaHelper.getPrefixForSliceViewByPartitionFile(partitionPath, fileId)).map(Pair::getValue)).findFirst());\n   }\n \n+  @Override\n+  protected void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODkyNg==", "bodyText": "General question I have around incremental file system view and rocksDB like persistent file system view storage is whether we will keep this list updated. i.e when the archival/cleaning runs, how do we ensure the deleted replaced file groups are no longer tracked inside rocksdb.\nI guess the lines below, are doing a bulk delete and insert to achieve the same?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487558926", "createdAt": "2020-09-13T18:05:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java", "diffHunk": "@@ -371,6 +371,47 @@ void removeBootstrapBaseFileMapping(Stream<BootstrapBaseFileMapping> bootstrapBa\n         schemaHelper.getPrefixForSliceViewByPartitionFile(partitionPath, fileId)).map(Pair::getValue)).findFirst());\n   }\n \n+  @Override\n+  protected void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODY2Mw=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1OTI3NQ==", "bodyText": "better approach for these situations generally is to introduce a commitStats(.) that does not take the last argument and deal with it internally inside WriteClient.\nThis way the code will remain more readable, without having to reference replace stats in bulk_insert, which have nothing to do with each other. hope that makes sense", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487559275", "createdAt": "2020-09-13T18:09:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java", "diffHunk": "@@ -102,7 +104,9 @@ public void commit(WriterCommitMessage[] messages) {\n             .flatMap(m -> m.getWriteStatuses().stream().map(m2 -> m2.getStat())).collect(Collectors.toList());\n \n     try {\n-      writeClient.commitStats(instantTime, writeStatList, Option.empty());\n+      writeClient.commitStats(instantTime, writeStatList, Option.empty(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1OTQzNw==", "bodyText": "should we have a better way of getting the commit action type? I am bit concerned about creating new metaClient just for this.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487559437", "createdAt": "2020-09-13T18:10:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -111,6 +111,9 @@ private[hudi] object HoodieSparkSqlWriter {\n         tableConfig = tableMetaClient.getTableConfig\n       }\n \n+      val metaClient = new HoodieTableMetaClient(sparkContext.hadoopConfiguration, path.get)\n+      val commitActionType = DataSourceUtils.getCommitActionType(operation, metaClient)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1OTUyMQ==", "bodyText": "please remove the scala specific comment. its good to encapsulate like this anyway", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487559521", "createdAt": "2020-09-13T18:11:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -376,31 +379,35 @@ private[hudi] object HoodieSparkSqlWriter {\n     metaSyncSuccess\n   }\n \n+  /**\n+   * Scala says method cannot have more than 7 arguments. So group all table/action specific information into a case class.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2MzM4NA==", "bodyText": "To expand, I am wondering if we should just include replacecommit within getCommitsAndCompactionTimeline(). Most of its callers are around compaction/savepoint/restore etc. So we may not be seeing some cases here.\nThings work for now, since filterCompletedInstants() etc are including replace commit in the timeline when filtering for queries. Semantically, if replace is a commit level action that can add new data to the timeline, then we should just treat it like delta commit IMO.\nWould anything break if we did do that?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487563384", "createdAt": "2020-09-13T18:49:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieTimeline.java", "diffHunk": "@@ -133,6 +137,21 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+  /**\n+   * Timeline to just include commits (commit/deltacommit), replace and compaction actions.\n+   *\n+   * @return\n+   */\n+  HoodieDefaultTimeline getWriteActionTimeline();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU1ODUwNw=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2NDIxNQ==", "bodyText": "On second thoughts, I am okay leaving this as-is for now as well. and reeval when acutally implementing clustering", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487564215", "createdAt": "2020-09-13T18:56:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -213,6 +213,12 @@ public abstract HoodieWriteMetadata insertPrepped(JavaSparkContext jsc, String i\n   public abstract HoodieWriteMetadata bulkInsertPrepped(JavaSparkContext jsc, String instantTime,\n       JavaRDD<HoodieRecord<T>> preppedRecords,  Option<BulkInsertPartitioner> bulkInsertPartitioner);\n \n+  /**\n+   * Logically delete all existing records and Insert a batch of new records into Hoodie table at the supplied instantTime.\n+   */\n+  public abstract HoodieWriteMetadata insertOverwrite(JavaSparkContext jsc, String instantTime,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0MzE5NA=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU2NDQwMw==", "bodyText": "One more consideration, as I went through the remainder of the PR. if there was an pending compaction for the replaced file group, then the file group metadata we encode may miss new base files produced as a result of the compaction. This scenario needs to be thought thru.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r487564403", "createdAt": "2020-09-13T18:58:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 53}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/94b275dbd20ec82ebe568b47bb28447d92ab996f", "committedDate": "2020-09-10T20:23:30Z", "message": "Remove HoodieReplaceStat and add boolean in WriteStatus to track replaced/created files"}, "afterCommit": {"oid": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/4ac518737b4388ab4c510f517312072a4b3b6dc0", "committedDate": "2020-09-15T01:22:25Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4ac518737b4388ab4c510f517312072a4b3b6dc0", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/4ac518737b4388ab4c510f517312072a4b3b6dc0", "committedDate": "2020-09-15T01:22:25Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "6c6f370bd9800636aee60d02cb8ceaa3776a6715", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6c6f370bd9800636aee60d02cb8ceaa3776a6715", "committedDate": "2020-09-15T02:20:22Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6c6f370bd9800636aee60d02cb8ceaa3776a6715", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6c6f370bd9800636aee60d02cb8ceaa3776a6715", "committedDate": "2020-09-15T02:20:22Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/067c285f5dc762a3753ad4162d02957662daa1d9", "committedDate": "2020-09-15T21:35:09Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg5MzUwOTY3", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-489350967", "createdAt": "2020-09-16T07:34:37Z", "commit": {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNzozNDozOFrOHSj8hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwOTowODoxNlrOHSnhew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTIyNTM1MA==", "bodyText": "Is this needed ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489225350", "createdAt": "2020-09-16T07:34:38Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteResult.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Result of a write operation.\n+ */\n+public class HoodieWriteResult implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final long RANDOM_SEED = 9038412832L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0NTc1MQ==", "bodyText": "Only commit instants older than oldest pending compaction is allowed to be archived. But, if we encode the entire file-group in the replace metadata, we will have race conditions with pending compactions. So, I guess it is safer to figure out the file-group during the time of archiving when it is guaranteed pending compaction is done.\nRegarding the requirement for ensureReplacedPartitionsLoadedCorrectly, If you look at pending compaction handling in filesystem-view, pending compactions are eagerly loaded whenever we construct the filesystem view. This seems to be the case also for replace metadata. Then, why do we need to trigger loading from outside ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489245751", "createdAt": "2020-09-16T08:08:57Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0Nzk3MQ=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0NjY2Nw==", "bodyText": "+1", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489246667", "createdAt": "2020-09-16T08:10:24Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NTAyMg=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI0Nzk4Nw==", "bodyText": "This could become a performance issue when we are deleting lot of replaced files. HoodieTimelineArchiveLog.archive() method is taking JavaSparkContext. right ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489247987", "createdAt": "2020-09-16T08:12:37Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -301,6 +304,61 @@ private void deleteAnyLeftOverMarkerFiles(JavaSparkContext jsc, HoodieInstant in\n     }\n   }\n \n+  private void deleteReplacedFiles(HoodieInstant instant) {\n+    if (!instant.isCompleted()) {\n+      // only delete files for completed instants\n+      return;\n+    }\n+\n+    TableFileSystemView fileSystemView = this.table.getFileSystemView();\n+    ensureReplacedPartitionsLoadedCorrectly(instant, fileSystemView);\n+\n+    Stream<HoodieFileGroup> fileGroupsToDelete = fileSystemView\n+        .getReplacedFileGroupsBeforeOrOn(instant.getTimestamp());\n+\n+    fileGroupsToDelete.forEach(fg -> {\n+      fg.getAllRawFileSlices().forEach(fileSlice -> {\n+        fileSlice.getBaseFile().map(baseFile -> deletePath(baseFile.getFileStatus().getPath(), instant));\n+        fileSlice.getLogFiles().forEach(logFile -> deletePath(logFile.getPath(), instant));\n+      });\n+    });\n+  }\n+\n+  /**\n+   * Because we are creating new 'HoodieTable' and FileSystemView objects in this class constructor,\n+   * partition view may not be loaded correctly.\n+   * Reload all partitions modified by REPLACE action\n+   *\n+   * TODO find a better way to pass the FileSystemView to this class.\n+   */\n+  private void ensureReplacedPartitionsLoadedCorrectly(HoodieInstant instant, TableFileSystemView fileSystemView) {\n+    Option<HoodieInstant> replaceInstantOption = metaClient.getActiveTimeline().getCompletedAndReplaceTimeline()\n+        .filter(replaceInstant -> replaceInstant.getTimestamp().equals(instant.getTimestamp())).firstInstant();\n+\n+    replaceInstantOption.ifPresent(replaceInstant -> {\n+      try {\n+        HoodieReplaceCommitMetadata metadata = HoodieReplaceCommitMetadata.fromBytes(\n+            metaClient.getActiveTimeline().getInstantDetails(replaceInstant).get(),\n+            HoodieReplaceCommitMetadata.class);\n+\n+        metadata.getPartitionToReplaceStats().keySet().forEach(partition -> fileSystemView.getAllFileGroups(partition));\n+      } catch (IOException e) {\n+        throw new HoodieCommitException(\"Failed to archive because cannot delete replace files\", e);\n+      }\n+    });\n+  }\n+\n+  private boolean deletePath(Path path, HoodieInstant instant) {\n+    try {\n+      LOG.info(\"Deleting \" + path + \" before archiving \" + instant);\n+      metaClient.getFs().delete(path);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU0NDg3Ng=="}, "originalCommit": {"oid": "94b275dbd20ec82ebe568b47bb28447d92ab996f"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI1NDI5MA==", "bodyText": "Move this ReplaceHelper class ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489254290", "createdAt": "2020-09-16T08:23:07Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java", "diffHunk": "@@ -370,4 +430,17 @@ private IndexedRecord convertToAvroRecord(HoodieTimeline commitTimeline, HoodieI\n     avroMetaData.getExtraMetadata().put(HoodieRollingStatMetadata.ROLLING_STAT_METADATA_KEY, \"\");\n     return avroMetaData;\n   }\n+\n+  public static org.apache.hudi.avro.model.HoodieReplaceCommitMetadata convertReplaceCommitMetadata(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI2MDk1Ng==", "bodyText": "nit: Can you add @OverRide annotation ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489260956", "createdAt": "2020-09-16T08:33:39Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieReplaceCommitMetadata.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.model;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.PropertyAccessor;\n+import com.fasterxml.jackson.databind.DeserializationFeature;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * All the metadata that gets stored along with a commit.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class HoodieReplaceCommitMetadata extends HoodieCommitMetadata {\n+  private static final Logger LOG = LogManager.getLogger(HoodieReplaceCommitMetadata.class);\n+  protected Map<String, List<String>> partitionToReplaceFileIds;\n+\n+  // for ser/deser\n+  public HoodieReplaceCommitMetadata() {\n+    this(false);\n+  }\n+\n+  public HoodieReplaceCommitMetadata(boolean compacted) {\n+    super(compacted);\n+    partitionToReplaceFileIds = new HashMap<>();\n+  }\n+\n+  public void setPartitionToReplaceFileIds(Map<String, List<String>> partitionToReplaceFileIds) {\n+    this.partitionToReplaceFileIds = partitionToReplaceFileIds;\n+  }\n+\n+  public void addReplaceFileId(String partitionPath, String fileId) {\n+    if (!partitionToReplaceFileIds.containsKey(partitionPath)) {\n+      partitionToReplaceFileIds.put(partitionPath, new ArrayList<>());\n+    }\n+    partitionToReplaceFileIds.get(partitionPath).add(fileId);\n+  }\n+\n+  public List<String> getReplaceFileIds(String partitionPath) {\n+    return partitionToReplaceFileIds.get(partitionPath);\n+  }\n+\n+  public Map<String, List<String>> getPartitionToReplaceFileIds() {\n+    return partitionToReplaceFileIds;\n+  }\n+\n+  public String toJsonString() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw==", "bodyText": "@satishkotha : We need to handle case when we restore a .replace instant for incremental timeline file system view.\nAbout the implementation -\nIn addRestoreInstant(), we need to look at HoodieRestoreMetadata.instantsToRollback and for each instants which are .replace types, we need to remove the replace file-group mapping kept in the file-system view.  We would need a reverse mapping of instant to file-group-id and also a way to identify which of the entries in HoodieRestoreMetadata.instantsToRollback is replace metadata. Currently, we only store commit timestamps in HoodieRestoreMetadata.instantsToRollback.\nI think it would be useful if we add an additional field in HoodieRestoreCommitMetadata and HoodieRollbackCommitMetadata to store both the timestamp and commit-action-type and use it here.\nSince, we only read committed replace actions, rollback is fine though.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489278233", "createdAt": "2020-09-16T08:59:08Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java", "diffHunk": "@@ -251,6 +262,28 @@ private void addRollbackInstant(HoodieTimeline timeline, HoodieInstant instant)\n     LOG.info(\"Done Syncing rollback instant (\" + instant + \")\");\n   }\n \n+  /**\n+   * Add newly found REPLACE instant.\n+   *\n+   * @param timeline Hoodie Timeline\n+   * @param instant REPLACE Instant\n+   */\n+  private void addReplaceInstant(HoodieTimeline timeline, HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI4Mzk2Mw==", "bodyText": "Also, can you also add test cases for incremental file-system view for both addReplaceInstant and removeReplaceInstant in TestIncrementalFSViewSync ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r489283963", "createdAt": "2020-09-16T09:08:16Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/IncrementalTimelineSyncFileSystemView.java", "diffHunk": "@@ -251,6 +262,28 @@ private void addRollbackInstant(HoodieTimeline timeline, HoodieInstant instant)\n     LOG.info(\"Done Syncing rollback instant (\" + instant + \")\");\n   }\n \n+  /**\n+   * Add newly found REPLACE instant.\n+   *\n+   * @param timeline Hoodie Timeline\n+   * @param instant REPLACE Instant\n+   */\n+  private void addReplaceInstant(HoodieTimeline timeline, HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTI3ODIzMw=="}, "originalCommit": {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9"}, "originalPosition": 65}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "067c285f5dc762a3753ad4162d02957662daa1d9", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/067c285f5dc762a3753ad4162d02957662daa1d9", "committedDate": "2020-09-15T21:35:09Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "694656aa63d93a18fc9aa719b5b65bf2a082f455", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/694656aa63d93a18fc9aa719b5b65bf2a082f455", "committedDate": "2020-09-17T17:18:04Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "694656aa63d93a18fc9aa719b5b65bf2a082f455", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/694656aa63d93a18fc9aa719b5b65bf2a082f455", "committedDate": "2020-09-17T17:18:04Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "33a5a878d7a07f1797b33d32c336b33a66e4d727", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/33a5a878d7a07f1797b33d32c336b33a66e4d727", "committedDate": "2020-09-17T17:37:52Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "33a5a878d7a07f1797b33d32c336b33a66e4d727", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/33a5a878d7a07f1797b33d32c336b33a66e4d727", "committedDate": "2020-09-17T17:37:52Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "9de32eef15c6fff7296ea0683a5143dc954c0d90", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/9de32eef15c6fff7296ea0683a5143dc954c0d90", "committedDate": "2020-09-17T23:01:04Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9de32eef15c6fff7296ea0683a5143dc954c0d90", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/9de32eef15c6fff7296ea0683a5143dc954c0d90", "committedDate": "2020-09-17T23:01:04Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "a546978c03f71ee3beadd5ea7f65abb7635b407f", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/a546978c03f71ee3beadd5ea7f65abb7635b407f", "committedDate": "2020-09-18T17:08:50Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkzODA4MTA3", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-493808107", "createdAt": "2020-09-22T20:09:39Z", "commit": {"oid": "a546978c03f71ee3beadd5ea7f65abb7635b407f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDowOTozOVrOHWKi-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQyMDowOTozOVrOHWKi-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAwMzUxMw==", "bodyText": "@satishkotha  : As discussed, All the replace filtering needs to move to getXXX() apis as the fetch APIs are only responsible for fetching file slices/base-files from different types of storage.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493003513", "createdAt": "2020-09-22T20:09:39Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -738,7 +799,9 @@ private String formatPartitionKey(String partitionStr) {\n    * @param commitsToReturn Commits\n    */\n   Stream<FileSlice> fetchLatestFileSliceInRange(List<String> commitsToReturn) {\n-    return fetchAllStoredFileGroups().map(fileGroup -> fileGroup.getLatestFileSliceInRange(commitsToReturn))\n+    return fetchAllStoredFileGroups()\n+        .filter(fileGroup -> !isFileGroupReplacedBeforeAny(fileGroup, commitsToReturn))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a546978c03f71ee3beadd5ea7f65abb7635b407f"}, "originalPosition": 170}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a546978c03f71ee3beadd5ea7f65abb7635b407f", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/a546978c03f71ee3beadd5ea7f65abb7635b407f", "committedDate": "2020-09-18T17:08:50Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "1190b7218f7ea7f90f9808bf5d8155d9fac5fe33", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/1190b7218f7ea7f90f9808bf5d8155d9fac5fe33", "committedDate": "2020-09-23T18:43:57Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1190b7218f7ea7f90f9808bf5d8155d9fac5fe33", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/1190b7218f7ea7f90f9808bf5d8155d9fac5fe33", "committedDate": "2020-09-23T18:43:57Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "7f3de1b7355303d56aa8f50809cbb4afc256472e", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/7f3de1b7355303d56aa8f50809cbb4afc256472e", "committedDate": "2020-09-23T19:11:44Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7f3de1b7355303d56aa8f50809cbb4afc256472e", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/7f3de1b7355303d56aa8f50809cbb4afc256472e", "committedDate": "2020-09-23T19:11:44Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "committedDate": "2020-09-23T20:39:11Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTIyNjkz", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-495122693", "createdAt": "2020-09-23T23:41:52Z", "commit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMzo0MTo1M1rOHXEsnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMzowMTowNFrOHXH_Kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk1NjI1Mw==", "bodyText": "@satishkotha : Dont we need to use instant time when checking for replaced-file here ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493956253", "createdAt": "2020-09-23T23:41:53Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -425,10 +459,14 @@ protected HoodieBaseFile addBootstrapBaseFileIfPresent(HoodieFileGroupId fileGro\n       readLock.lock();\n       String partitionPath = formatPartitionKey(partitionStr);\n       ensurePartitionLoadedCorrectly(partitionPath);\n-      return fetchHoodieFileGroup(partitionPath, fileId).map(fileGroup -> fileGroup.getAllBaseFiles()\n-          .filter(baseFile -> HoodieTimeline.compareTimestamps(baseFile.getCommitTime(), HoodieTimeline.EQUALS,\n-              instantTime)).filter(df -> !isBaseFileDueToPendingCompaction(df)).findFirst().orElse(null))\n-          .map(df -> addBootstrapBaseFileIfPresent(new HoodieFileGroupId(partitionPath, fileId), df));\n+      if (isFileGroupReplaced(partitionPath, fileId)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk1NzU2MQ==", "bodyText": "same case here,, we need to use the maxInstantTime passed here instead of the timeline's maxInstant.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493957561", "createdAt": "2020-09-23T23:46:26Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -554,14 +608,16 @@ protected HoodieBaseFile addBootstrapBaseFileIfPresent(HoodieFileGroupId fileGro\n       readLock.lock();\n       String partition = formatPartitionKey(partitionStr);\n       ensurePartitionLoadedCorrectly(partition);\n-      return fetchAllStoredFileGroups(partition).map(fileGroup -> {\n-        Option<FileSlice> fileSlice = fileGroup.getLatestFileSliceBeforeOrOn(maxInstantTime);\n-        // if the file-group is under construction, pick the latest before compaction instant time.\n-        if (fileSlice.isPresent()) {\n-          fileSlice = Option.of(fetchMergedFileSlice(fileGroup, fileSlice.get()));\n-        }\n-        return fileSlice;\n-      }).filter(Option::isPresent).map(Option::get).map(this::addBootstrapBaseFileIfPresent);\n+      return fetchAllStoredFileGroups(partition)\n+          .filter(fg -> !isFileGroupReplaced(fg.getFileGroupId()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4NzE5Ng==", "bodyText": "Can we move this to separate avsc file and reference it here", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493987196", "createdAt": "2020-09-24T01:28:18Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/avro/HoodieRollbackMetadata.avsc", "diffHunk": "@@ -39,6 +39,18 @@\n         \"name\":\"version\",\n         \"type\":[\"int\", \"null\"],\n         \"default\": 1\n+     },\n+     /* overlaps with 'commitsRollback' field. Adding this to track action type for all the instants being rolled back. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4NzgyOA==", "bodyText": "Doc needs fixing.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493987828", "createdAt": "2020-09-24T01:30:57Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/CommitUtils.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieReplaceCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Helper class to generate compaction plan from FileGroup/FileSlice abstraction.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5MTg3Ng==", "bodyText": "rename removeReplacedFileIdsAtInstants ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493991876", "createdAt": "2020-09-24T01:47:31Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -727,6 +795,26 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract Stream<HoodieFileGroup> fetchAllStoredFileGroups();\n \n+  /**\n+   * Track instant time for file groups replaced.\n+   */\n+  protected abstract void resetReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Track instant time for new file groups replaced.\n+   */\n+  protected abstract void addReplacedFileGroups(final Map<HoodieFileGroupId, HoodieInstant> replacedFileGroups);\n+\n+  /**\n+   * Remove file groups that are replaced in any of the specified instants.\n+   */\n+  protected abstract void removeReplacedFileIds(Set<String> instants);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "originalPosition": 311}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5MzUzNg==", "bodyText": "this is no longer needed and can be removed ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r493993536", "createdAt": "2020-09-24T01:54:16Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -91,40 +92,47 @@ public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n   }\n \n   /**\n+   *\n    * Commit changes performed at the given instantTime marker.\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata) {\n-    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n-    return commitStats(instantTime, stats, extraMetadata);\n+                        Option<Map<String, String>> extraMetadata) {\n+    HoodieTableMetaClient metaClient = createMetaClient(false);\n+    String actionType = metaClient.getCommitActionType();\n+    return commit(instantTime, writeStatuses, extraMetadata, actionType, Collections.emptyMap());\n   }\n \n-  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {\n-    LOG.info(\"Committing \" + instantTime);\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n+      Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType) {\n+    return commitStats(instantTime, stats, extraMetadata, commitActionType, Collections.emptyMap());\n+  }\n+\n+  public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n+                             String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    LOG.info(\"Committing \" + instantTime + \" action \" + commitActionType);\n     HoodieTableMetaClient metaClient = createMetaClient(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwNzI4NQ==", "bodyText": "We are creating metaclient and loading timeline once here and in the function called in the next line. Can you make sure you create metaclient only once without loading timeline.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494007285", "createdAt": "2020-09-24T02:49:20Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,24 +602,39 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAxMDE1NA==", "bodyText": "Can you use pretty print mode (multi-lines) in the same way other section is presented.", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494010154", "createdAt": "2020-09-24T03:01:04Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/avro/HoodieRestoreMetadata.avsc", "diffHunk": "@@ -34,6 +34,8 @@\n         \"name\":\"version\",\n         \"type\":[\"int\", \"null\"],\n         \"default\": 1\n-     }\n+     },\n+     /* overlaps with 'instantsToRollback' field. Adding this to track action type for all the instants being rolled back. */\n+     {\"name\": \"restoreInstantInfo\", \"default\": null, \"type\": {\"type\": \"array\", \"default\": null, \"items\": [\"null\", \"HoodieInstantInfo\"]}}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5"}, "originalPosition": 7}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/980fed7b4ab7f8d595fe14fc2ffaa1a2eb0962f5", "committedDate": "2020-09-23T20:39:11Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/c281f2eb0d311c7b54ac850b831cea6d48a47502", "committedDate": "2020-09-25T03:27:35Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2MDkyNzAx", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-496092701", "createdAt": "2020-09-25T03:50:07Z", "commit": {"oid": "c281f2eb0d311c7b54ac850b831cea6d48a47502"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMzo1MDowN1rOHX0B-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMzo1MDowN1rOHX0B-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczMTc2OA==", "bodyText": "Does this have to be non-static ? Can it be moved to CommitUtils ?", "url": "https://github.com/apache/hudi/pull/2048#discussion_r494731768", "createdAt": "2020-09-25T03:50:07Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -586,24 +602,39 @@ public String startCommit() {\n    * @param instantTime Instant time to be generated\n    */\n   public void startCommitWithTime(String instantTime) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, metaClient.getCommitActionType(), metaClient);\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  public void startCommitWithTime(String instantTime, String actionType) {\n+    HoodieTableMetaClient metaClient = createMetaClient(true);\n+    startCommitWithTime(instantTime, actionType, metaClient);\n+  }\n+\n+  /**\n+   * Completes a new commit time for a write operation (insert/update/delete) with specified action.\n+   */\n+  private void startCommitWithTime(String instantTime, String actionType, HoodieTableMetaClient metaClient) {\n     // NOTE : Need to ensure that rollback is done before a new commit is started\n     if (rollbackPending) {\n       // Only rollback inflight commit/delta-commits. Do not touch compaction commits\n       rollbackPendingCommits();\n     }\n-    startCommit(instantTime);\n+    startCommit(instantTime, actionType, metaClient);\n   }\n \n-  private void startCommit(String instantTime) {\n-    LOG.info(\"Generate a new instant time \" + instantTime);\n-    HoodieTableMetaClient metaClient = createMetaClient(true);\n+  private void startCommit(String instantTime, String actionType, HoodieTableMetaClient metaClient) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c281f2eb0d311c7b54ac850b831cea6d48a47502"}, "originalPosition": 64}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c281f2eb0d311c7b54ac850b831cea6d48a47502", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/c281f2eb0d311c7b54ac850b831cea6d48a47502", "committedDate": "2020-09-25T03:27:35Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "committedDate": "2020-09-28T17:23:44Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "committedDate": "2020-09-28T20:11:48Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/d8b2a3e0ad84ae91372b5fcc61e60dedfe5cb036", "committedDate": "2020-09-28T17:23:44Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}, "afterCommit": {"oid": "aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9", "committedDate": "2020-09-28T20:11:48Z", "message": "[HUDI-1072] Introduce REPLACE top level action. Implement insert_overwrite operation on top of replace action"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk5MDA0NDkz", "url": "https://github.com/apache/hudi/pull/2048#pullrequestreview-499004493", "createdAt": "2020-09-30T00:03:44Z", "commit": {"oid": "aeea5b6c9d6c2c8a5e3e3ad85a31630fbc2507a9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4472, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}