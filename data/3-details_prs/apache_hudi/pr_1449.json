{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkzOTg1OTYw", "number": 1449, "title": "[HUDI-698]Add unit test for CleansCommand", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nAdd unit test for CleansCommand in hudi-cli module\nBrief change log\n\nAdd unit test for CleansCommand\n\nVerify this pull request\n\nAdd unit test for CleansCommand.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-03-26T06:41:49Z", "url": "https://github.com/apache/hudi/pull/1449", "merged": true, "mergeCommit": {"oid": "644c1cc8bd1965271c4433edccb17aa8fda5f403"}, "closed": true, "closedAt": "2020-04-14T09:54:47Z", "author": {"login": "hddong"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcSddKQgFqTM4MzQzNTY0NQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcXgciCAFqTM5Mjc3NTk1NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgzNDM1NjQ1", "url": "https://github.com/apache/hudi/pull/1449#pullrequestreview-383435645", "createdAt": "2020-03-29T17:34:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOVQxNzozNDozOVrOF9Tmzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOVQxNzozNDozOVrOF9Tmzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyODY4Ng==", "bodyText": "use standard java for this - instead of Parquet.Strings.join()", "url": "https://github.com/apache/hudi/pull/1449#discussion_r399828686", "createdAt": "2020-03-29T17:34:39Z", "author": {"login": "smarthi"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.parquet.Strings;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Iterator;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    String localPropsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\").getPath();\n+    propsFilePath = \"/tmp/clean.properties\";\n+    initDFS();\n+    jsc.hadoopConfiguration().addResource(dfs.getConf());\n+    HoodieCLI.conf = dfs.getConf();\n+\n+    dfs.mkdir(new Path(\"/tmp\"), FsPermission.getDefault());\n+    dfs.copyFromLocalFile(new Path(localPropsFilePath), new Path(\"/tmp\"));\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // First, run clean\n+    dfs.create(new Path(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+\n+    SparkMain.clean(jsc, HoodieCLI.basePath, \"local\", propsFilePath, \"2G\", new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().get();\n+    TableHeader header =\n+        new TableHeader().addTableHeaderField(\"CleanTime\").addTableHeaderField(\"EarliestCommandRetained\")\n+            .addTableHeaderField(\"Total Files Deleted\").addTableHeaderField(\"Total Time Taken\");\n+    List<Comparable[]> rows = new ArrayList<>();\n+\n+    // EarliestCommandRetained should be 102, since hoodie.cleaner.commits.retained=2\n+    // Total Time Taken should be -1, since hoodie.metrics.on is false by default\n+    rows.add(new Comparable[]{clean.getTimestamp(), \"102\", \"0\", \"-1\"});\n+\n+    String expected = HoodiePrintHelper.print(header, new HashMap<>(), \"\", false, -1, false, rows);\n+    assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for cleans run.\n+   */\n+  @Test\n+  public void testRunClean() throws IOException, InterruptedException, URISyntaxException {\n+    // First, there should none of clean instant.\n+    assertEquals(0, metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    // Create partition metadata\n+    dfs.create(new Path(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+    dfs.create(new Path(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+\n+    SparkEnvCommand.env.put(\"SPARK_MASTER\", \"local\");\n+\n+    List<String> configs = new ArrayList();\n+    Iterator<Map.Entry<String, String>> iterator = dfs.getConf().iterator();\n+    while (iterator.hasNext()) {\n+      Map.Entry<String, String> e = iterator.next();\n+      configs.add(e.getKey() + \"=\" + e.getValue());\n+    }\n+    String hadoopConf = Strings.join(configs, \" \");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 149}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NjcwMjAx", "url": "https://github.com/apache/hudi/pull/1449#pullrequestreview-389670201", "createdAt": "2020-04-08T05:55:39Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwNTo1NTozOVrOGCf-3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwNTo1NTozOVrOGCf-3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTI3NDMzMg==", "bodyText": "Please make sure we do not use STDOUT in the test case.", "url": "https://github.com/apache/hudi/pull/1449#discussion_r405274332", "createdAt": "2020-04-08T05:55:39Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -56,8 +58,16 @@ public void init() throws IOException {\n \n     String tableName = \"test_table\";\n     tablePath = basePath + File.separator + tableName;\n-    propsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\").getPath();\n-\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();\n+    if (propsFilePath == null) {\n+      System.out.println(\"-------------------\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxOTE0MzA5", "url": "https://github.com/apache/hudi/pull/1449#pullrequestreview-391914309", "createdAt": "2020-04-13T01:19:15Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QwMToxOToxNVrOGEaSkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QwMToyNDo1MVrOGEaVig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODIyNg==", "bodyText": "getResource may be nullable. Here, we may need to check null firstly.", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278226", "createdAt": "2020-04-13T01:19:15Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODQzOQ==", "bodyText": "HoodieTableType.COPY_ON_WRITE.name is better?", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278439", "createdAt": "2020-04-13T01:20:55Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODc3OQ==", "bodyText": "We may need to check if the value is present here?", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278779", "createdAt": "2020-04-13T01:23:27Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // First, run clean\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath, new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODkxOA==", "bodyText": "No need to box this value, could be -1L.", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278918", "createdAt": "2020-04-13T01:24:18Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // First, run clean\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath, new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().get();\n+    TableHeader header =\n+        new TableHeader().addTableHeaderField(\"CleanTime\").addTableHeaderField(\"EarliestCommandRetained\")\n+            .addTableHeaderField(\"Total Files Deleted\").addTableHeaderField(\"Total Time Taken\");\n+    List<Comparable[]> rows = new ArrayList<>();\n+\n+    // EarliestCommandRetained should be 102, since hoodie.cleaner.commits.retained=2\n+    // Total Time Taken need read from metadata\n+    rows.add(new Comparable[]{clean.getTimestamp(), \"102\", \"0\", getLatestCleanTimeTakenInMillis().toString()});\n+\n+    String expected = HoodiePrintHelper.print(header, new HashMap<>(), \"\", false, -1, false, rows);\n+    assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for show partitions of a clean instant.\n+   */\n+  @Test\n+  public void testShowCleanPartitions() throws IOException {\n+    // First, run clean with two partition\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath, new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().get();\n+\n+    CommandResult cr = getShell().executeCommand(\"clean showpartitions --clean \" + clean.getTimestamp());\n+    assertTrue(cr.isSuccess());\n+\n+    TableHeader header = new TableHeader().addTableHeaderField(\"Partition Path\").addTableHeaderField(\"Cleaning policy\")\n+        .addTableHeaderField(\"Total Files Successfully Deleted\").addTableHeaderField(\"Total Failed Deletions\");\n+\n+    // There should be two partition path\n+    List<Comparable[]> rows = new ArrayList<>();\n+    rows.add(new Comparable[]{HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH,\n+        HoodieCleaningPolicy.KEEP_LATEST_COMMITS, \"0\", \"0\"});\n+    rows.add(new Comparable[]{HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        HoodieCleaningPolicy.KEEP_LATEST_COMMITS, \"0\", \"0\"});\n+\n+    String expected = HoodiePrintHelper.print(header, new HashMap<>(), \"\", false, -1, false, rows);\n+    assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Get time taken of latest instant.\n+   */\n+  private Long getLatestCleanTimeTakenInMillis() throws IOException {\n+    HoodieActiveTimeline activeTimeline = HoodieCLI.getTableMetaClient().getActiveTimeline();\n+    HoodieTimeline timeline = activeTimeline.getCleanerTimeline().filterCompletedInstants();\n+    HoodieInstant clean = timeline.getReverseOrderedInstants().findFirst().orElse(null);\n+    if (clean != null) {\n+      HoodieCleanMetadata cleanMetadata =\n+          TimelineMetadataUtils.deserializeHoodieCleanMetadata(timeline.getInstantDetails(clean).get());\n+      return cleanMetadata.getTimeTakenInMillis();\n+    }\n+    return new Long(-1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODk4Ng==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278986", "createdAt": "2020-04-13T01:24:51Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCleansCommand.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ITTestCleansCommand extends AbstractShellIntegrationTest {\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\").getPath();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/5d301c5b96520bb929ccee6657df512a0edb530f", "committedDate": "2020-04-13T15:48:10Z", "message": "Add test for cleanCommand"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/5d301c5b96520bb929ccee6657df512a0edb530f", "committedDate": "2020-04-13T15:48:10Z", "message": "Add test for cleanCommand"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNTcxMzg1", "url": "https://github.com/apache/hudi/pull/1449#pullrequestreview-392571385", "createdAt": "2020-04-14T02:59:14Z", "commit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMjo1OToxNFrOGE8ZfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMzowODowMFrOGE8iyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNzA1Mw==", "bodyText": "Are you sure we must introduce this phase in hudi-cli module?", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407837053", "createdAt": "2020-04-14T02:59:14Z", "author": {"login": "yanghua"}, "path": "hudi-cli/pom.xml", "diffHunk": "@@ -122,6 +122,31 @@\n           <includeTestSourceDirectory>false</includeTestSourceDirectory>\n         </configuration>\n       </plugin>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-failsafe-plugin</artifactId>\n+        <version>2.22.0</version>\n+        <configuration>\n+          <includes>\n+            <include>**/ITT*.java</include>\n+          </includes>\n+        </configuration>\n+        <executions>\n+          <execution>\n+            <phase>integration-test</phase>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzODgwMQ==", "bodyText": "add a blank before the number 1?", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407838801", "createdAt": "2020-04-14T03:05:41Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCleansCommand.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ITTestCleansCommand extends AbstractShellIntegrationTest {\n+  private String tablePath;\n+  private URL propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\");\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+  }\n+\n+  /**\n+   * Test case for cleans run.\n+   */\n+  @Test\n+  public void testRunClean() throws IOException {\n+    // First, there should none of clean instant.\n+    assertEquals(0, metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    // Check properties file exists.\n+    assertNotNull(\"Not found properties file\", propsFilePath);\n+\n+    // Create partition metadata\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans run --sparkMaster local --propsFilePath \" + propsFilePath.toString());\n+    assertTrue(cr.isSuccess());\n+\n+    // After run clean, there should have 1 clean instant\n+    assertEquals(\"Loaded 1 clean and the count should match\",1,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzODk2MA==", "bodyText": "add an empty line before this line?", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407838960", "createdAt": "2020-04-14T03:06:08Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/resources/clean.properties", "diffHunk": "@@ -0,0 +1,19 @@\n+###\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+###\n+hoodie.cleaner.incremental.mode=true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzOTIxMw==", "bodyText": "Can we extract the spark and Hadoop version number into a variable for upgrading purposes?", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407839213", "createdAt": "2020-04-14T03:07:08Z", "author": {"login": "yanghua"}, "path": ".travis.yml", "diffHunk": "@@ -39,3 +39,9 @@ script:\n   - scripts/run_travis_tests.sh $TEST_SUITE\n after_success:\n   - bash <(curl -s https://codecov.io/bash)\n+before_script:\n+  - echo \"=====[ Download spark]=====\"\n+  - wget http://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz -O /tmp/spark-2.4.4.tgz", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzOTQzMg==", "bodyText": "Download spark -> Downloading Apache Spark?", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407839432", "createdAt": "2020-04-14T03:08:00Z", "author": {"login": "yanghua"}, "path": ".travis.yml", "diffHunk": "@@ -39,3 +39,9 @@ script:\n   - scripts/run_travis_tests.sh $TEST_SUITE\n after_success:\n   - bash <(curl -s https://codecov.io/bash)\n+before_script:\n+  - echo \"=====[ Download spark]=====\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNTc0ODYx", "url": "https://github.com/apache/hudi/pull/1449#pullrequestreview-392574861", "createdAt": "2020-04-14T03:11:20Z", "commit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMzoxMToyMFrOGE8mEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMzoxMToyMFrOGE8mEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg0MDI3NQ==", "bodyText": "It can be assertNotNull(...)", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407840275", "createdAt": "2020-04-14T03:11:20Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private URL propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\");\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // Check properties file exists.\n+    assertNotNull(\"Not found properties file\", propsFilePath);\n+\n+    // First, run clean\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath.getPath(), new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().orElse(null);\n+    assertTrue(clean != null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f"}, "originalPosition": 114}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b14b63b14de667ecc0ff88c5d149682da15fab6c", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/b14b63b14de667ecc0ff88c5d149682da15fab6c", "committedDate": "2020-04-14T08:09:36Z", "message": "add version for download"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNzc1OTU1", "url": "https://github.com/apache/hudi/pull/1449#pullrequestreview-392775955", "createdAt": "2020-04-14T09:54:28Z", "commit": {"oid": "b14b63b14de667ecc0ff88c5d149682da15fab6c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3252, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}