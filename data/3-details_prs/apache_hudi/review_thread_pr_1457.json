{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk0OTE1MDgx", "number": 1457, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNToyNTo1MlrODsuhYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNjoyMToxNFrODwRO0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjI2MTQ0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNToyNTo1MlrOF9vRcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMjoxNToyOVrOF9-PAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MTk3MA==", "bodyText": "using the Handle classes at this level, breaks the layering.. Please refactor into a common helper.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400281970", "createdAt": "2020-03-30T15:25:52Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzEwNw==", "bodyText": "Moved to HoodieAvroUtils", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527107", "createdAt": "2020-03-30T22:15:29Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MTk3MA=="}, "originalCommit": null, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjI2NTM4OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNToyNjo0NlrOF9vT_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMjoxNTozNVrOF9-PQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MjYyMQ==", "bodyText": "this whole block can be pulled into a private method like validateSchema() and reused? without being inlined into upsertRecordsInternal()", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400282621", "createdAt": "2020-03-30T15:26:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzE2OA==", "bodyText": "done.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527168", "createdAt": "2020-03-30T22:15:35Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MjYyMQ=="}, "originalCommit": null, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjI3MzQ1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNToyODoyOFrOF9vZTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMjoxNTo0NVrOF9-PkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4Mzk4Mw==", "bodyText": "can this and the previous line be combined into one\nSchema tableSchema = schemaUtil.getLatestSchema()\n\nI don't think any other code cares about savedParquetSchema", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400283983", "createdAt": "2020-03-30T15:28:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzI0OQ==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527249", "createdAt": "2020-03-30T22:15:45Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4Mzk4Mw=="}, "originalCommit": null, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjI3NjQyOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNToyOTowN1rOF9vbMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMjoxNTo1M1rOF9-P1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NDQ2Ng==", "bodyText": "lets have this as a single ERROR statement?", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400284466", "createdAt": "2020-03-30T15:29:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzMxNg==", "bodyText": "The same msg is also used to raise the exception a few lines below. So I did not duplicate the string and created a local variable.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527316", "createdAt": "2020-03-30T22:15:53Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NDQ2Ng=="}, "originalCommit": null, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjI4MTI1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTozMDowNFrOF9veKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMjoxNjowMFrOF9-P_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTIyNg==", "bodyText": "can we handle this in code nicely above before try, instead of the catch block as an error case?", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400285226", "createdAt": "2020-03-30T15:30:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);\n+          LOG.warn(\"WriterSchema: \" + writerSchema);\n+          LOG.warn(\"Table latest schema: \" + savedSchema);\n+          throw new HoodieUpsertException(msg);\n+        }\n+      } catch (Exception e) {\n+        // If this is the first insert into the table then schema will not be present\n+        if (hoodieTable.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().countInstants() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzM1OQ==", "bodyText": "Reading the schema may fail due to IO / timeout errors too which will raise their exceptions.\nHaving this code here:\n\nremoves code duplication (as we need to throw HoodieUpsertException here)\nHandles the rare case gracefully - when reading schema fails due to IO errors but there was no schema.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527359", "createdAt": "2020-03-30T22:16:00Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);\n+          LOG.warn(\"WriterSchema: \" + writerSchema);\n+          LOG.warn(\"Table latest schema: \" + savedSchema);\n+          throw new HoodieUpsertException(msg);\n+        }\n+      } catch (Exception e) {\n+        // If this is the first insert into the table then schema will not be present\n+        if (hoodieTable.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().countInstants() > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTIyNg=="}, "originalCommit": null, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjI4NTMwOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTozMDo1N1rOF9vguA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMjoxNjowNlrOF9-QHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTg4MA==", "bodyText": "so this is recaught below? IMO this method should just throw exception from one place..", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400285880", "createdAt": "2020-03-30T15:30:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);\n+          LOG.warn(\"WriterSchema: \" + writerSchema);\n+          LOG.warn(\"Table latest schema: \" + savedSchema);\n+          throw new HoodieUpsertException(msg);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzM5MQ==", "bodyText": "Refactored.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527391", "createdAt": "2020-03-30T22:16:06Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);\n+          LOG.warn(\"WriterSchema: \" + writerSchema);\n+          LOG.warn(\"Table latest schema: \" + savedSchema);\n+          throw new HoodieUpsertException(msg);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTg4MA=="}, "originalCommit": null, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjI4ODM5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTozMTo0MlrOF9viyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMjoxNjoxM1rOF9-QRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NjQwOQ==", "bodyText": "rename to hoodie.avro.schema.validate ?", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400286409", "createdAt": "2020-03-30T15:31:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -52,6 +52,8 @@\n   private static final String TIMELINE_LAYOUT_VERSION = \"hoodie.timeline.layout.version\";\n   private static final String BASE_PATH_PROP = \"hoodie.base.path\";\n   private static final String AVRO_SCHEMA = \"hoodie.avro.schema\";\n+  private static final String SCHEMA_CHECK = \"hoodie.schema.check\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzQyOA==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527428", "createdAt": "2020-03-30T22:16:13Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -52,6 +52,8 @@\n   private static final String TIMELINE_LAYOUT_VERSION = \"hoodie.timeline.layout.version\";\n   private static final String BASE_PATH_PROP = \"hoodie.base.path\";\n   private static final String AVRO_SCHEMA = \"hoodie.avro.schema\";\n+  private static final String SCHEMA_CHECK = \"hoodie.schema.check\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NjQwOQ=="}, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjMxNTE4OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTozNzoyNlrOF9vzxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMDoxMDo0NFrOGGJxog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MDc1OQ==", "bodyText": "this needs to be attributed in LICENSE and NOTICE ..  Can we document all the changes? I am wondering if we can just wrap or extend the org.apache.avro.SchemaCompatibility class instead of copying in full..\ncc @bvaradar as well..", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400290759", "createdAt": "2020-03-30T15:37:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzQ4MQ==", "bodyText": "The functions that are changed in org.apache.avro.SchemaCompatibility are all private.\nI can reduce the size of this file by referring to classes from org.apache.avro.SchemaCompatibility but I think starting with a copy makes it easier to see a diff.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527481", "createdAt": "2020-03-30T22:16:20Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MDc1OQ=="}, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEwNDgwMg==", "bodyText": "Not required any longer as I have removed the copied file. Please see the latest diff.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r409104802", "createdAt": "2020-04-15T20:10:44Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MDc1OQ=="}, "originalCommit": null, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjMyMDQyOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTozODoyOVrOF9v3Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQyMjoxNjoyNVrOF9-QrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MTU5NA==", "bodyText": "lets move this to org.apache.hudi.common.avro", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400291594", "createdAt": "2020-03-30T15:38:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzUzMg==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527532", "createdAt": "2020-03-30T22:16:25Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MTU5NA=="}, "originalCommit": null, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjMyODU1OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaUtil.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNTo0MDoxMFrOF9v8FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMDoxMTowMlrOGGJyPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA==", "bodyText": "let's rename to TableSchemaReader and move to org.apache.hudi.common.table ? (I am trying to move us out of the habit of piling up more util classes)", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400292884", "createdAt": "2020-03-30T15:40:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaUtil.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Utilities to read Schema from data files and log files.\n+ */\n+public class SchemaUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMyNTMyMg==", "bodyText": "Edit: TableSchemaResolver is a better name.. ?  since it does more complex than things than just reading\nand btw let's add some unit tests to both these classes?", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400325322", "createdAt": "2020-03-30T16:25:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaUtil.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Utilities to read Schema from data files and log files.\n+ */\n+public class SchemaUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA=="}, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzYwMA==", "bodyText": "Done. Unit test TBD.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527600", "createdAt": "2020-03-30T22:16:31Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaUtil.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Utilities to read Schema from data files and log files.\n+ */\n+public class SchemaUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA=="}, "originalCommit": null, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEwNDk1Ng==", "bodyText": "Unit tests implemented.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r409104956", "createdAt": "2020-04-15T20:11:02Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaUtil.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Utilities to read Schema from data files and log files.\n+ */\n+public class SchemaUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA=="}, "originalCommit": null, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5Mjg4MTU5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoxNzo1OFrOF_V_DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQyMjo0OTozN1rOGEHs3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDgxMg==", "bodyText": "nit : devolved/evolved", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401964812", "createdAt": "2020-04-01T23:17:58Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3MzY2MA==", "bodyText": "I mean devolved here as insert with a devolved schema (with missing field) should not be allowed.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406973660", "createdAt": "2020-04-10T22:49:37Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDgxMg=="}, "originalCommit": null, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5Mjg4MTk2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoxODowOVrOF_V_Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQyMjo1MDowMVrOGEHtMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDg2Mw==", "bodyText": "same", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401964863", "createdAt": "2020-04-01T23:18:09Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed\n+    HoodieWriteConfig hoodieDevolvedWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+    client = getHoodieWriteClient(hoodieDevolvedWriteConfig, false);\n+    final List<HoodieRecord> failedRecords = dataGenDevolved.generateInserts(\"004\", numRecords);\n+    try {\n+      // We cannot use insertBatch directly here because we want to insert records\n+      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3Mzc0Nw==", "bodyText": "I mean devolved here as updates with a devolved schema (with missing field) should not be allowed.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406973747", "createdAt": "2020-04-10T22:50:01Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed\n+    HoodieWriteConfig hoodieDevolvedWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+    client = getHoodieWriteClient(hoodieDevolvedWriteConfig, false);\n+    final List<HoodieRecord> failedRecords = dataGenDevolved.generateInserts(\"004\", numRecords);\n+    try {\n+      // We cannot use insertBatch directly here because we want to insert records\n+      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDg2Mw=="}, "originalCommit": null, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5Mjg4NzQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoyMDo0NFrOF_WCgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQyMjo1MTo0MVrOGEHuaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NTY5OA==", "bodyText": "If you mean to say \"older schema\" or \"shorter schema\" or \"retracted schema\" - not sure if devolved is a work so I'm not sure are you evolving or retracting", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401965698", "createdAt": "2020-04-01T23:20:44Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed\n+    HoodieWriteConfig hoodieDevolvedWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+    client = getHoodieWriteClient(hoodieDevolvedWriteConfig, false);\n+    final List<HoodieRecord> failedRecords = dataGenDevolved.generateInserts(\"004\", numRecords);\n+    try {\n+      // We cannot use insertBatch directly here because we want to insert records\n+      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.\n+      writeBatch(client, \"005\", \"004\", Option.empty(), \"003\", numRecords,\n+          (String s, Integer a) -> failedRecords, HoodieWriteClient::insert, false, 0, 0, 0);\n+      fail(\"Insert with devolved scheme should fail\");\n+    } catch (HoodieInsertException ex) {\n+      // no new commit\n+      checkLatestDeltaCommit(\"004\");\n+      checkReadRecords(\"000\", numRecords);\n+      client.rollback(\"005\");\n+    }\n+\n+    // Update with devolved schema is also not allowed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3NDA1OQ==", "bodyText": "I had the same confusion with what word to use here. Devolved plays well with evolved and seems to be a valid word.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406974059", "createdAt": "2020-04-10T22:51:41Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed\n+    HoodieWriteConfig hoodieDevolvedWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+    client = getHoodieWriteClient(hoodieDevolvedWriteConfig, false);\n+    final List<HoodieRecord> failedRecords = dataGenDevolved.generateInserts(\"004\", numRecords);\n+    try {\n+      // We cannot use insertBatch directly here because we want to insert records\n+      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.\n+      writeBatch(client, \"005\", \"004\", Option.empty(), \"003\", numRecords,\n+          (String s, Integer a) -> failedRecords, HoodieWriteClient::insert, false, 0, 0, 0);\n+      fail(\"Insert with devolved scheme should fail\");\n+    } catch (HoodieInsertException ex) {\n+      // no new commit\n+      checkLatestDeltaCommit(\"004\");\n+      checkReadRecords(\"000\", numRecords);\n+      client.rollback(\"005\");\n+    }\n+\n+    // Update with devolved schema is also not allowed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NTY5OA=="}, "originalCommit": null, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ5Mjg5NDUwOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/avro/SchemaCompatibility.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMVQyMzoyNDoxMVrOF_WG0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQyMzowMzoyNVrOGEH3_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NjgwMA==", "bodyText": "@prashantwason Can you please mark the lines/methods you have changed with may be MOD for future references ? Also, can you add 1-2 lines as to why that change is needed", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401966800", "createdAt": "2020-04-01T23:24:11Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/avro/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.avro;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.\n+ *\n+ * HUDI requires a Schema to be specified in HoodieWriteConfig and is used by the HoodieWriteClient to\n+ * create the records. The schema is also saved in the data files (parquet format) and log files (avro format).\n+ * Since a schema is required each time new data is ingested into a HUDI dataset, schema can be evolved over time.\n+ *\n+ * HUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by\n+ * checking that the data written using the old schema can be read using the new schema.\n+ *\n+ * New Schema is compatible only if:\n+ * 1. There is no change in schema\n+ * 2. A field has been added and it has a default value specified\n+ *\n+ * New Schema is incompatible if:\n+ * 1. A field has been deleted\n+ * 2. A field has been renamed (treated as delete + add)\n+ * 3. A field's type has changed to be incompatible with the older type\n+ */\n+public class SchemaCompatibility {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3NjUxMA==", "bodyText": "Done. I have added the limitation with org.apache.avro.SchemaCompatibility in the file as well as in the HUDI-741", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406976510", "createdAt": "2020-04-10T23:03:25Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/avro/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.avro;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.\n+ *\n+ * HUDI requires a Schema to be specified in HoodieWriteConfig and is used by the HoodieWriteClient to\n+ * create the records. The schema is also saved in the data files (parquet format) and log files (avro format).\n+ * Since a schema is required each time new data is ingested into a HUDI dataset, schema can be evolved over time.\n+ *\n+ * HUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by\n+ * checking that the data written using the old schema can be read using the new schema.\n+ *\n+ * New Schema is compatible only if:\n+ * 1. There is no change in schema\n+ * 2. A field has been added and it has a default value specified\n+ *\n+ * New Schema is incompatible if:\n+ * 1. A field has been deleted\n+ * 2. A field has been renamed (treated as delete + add)\n+ * 3. A field's type has changed to be incompatible with the older type\n+ */\n+public class SchemaCompatibility {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NjgwMA=="}, "originalCommit": null, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxOTMzNzY2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNTo0NzoxNVrOGDKZGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNTo0NzoxNVrOGDKZGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2OTE3Ng==", "bodyText": "there is a pending PR to move this code into the new table.action package.. FYI.. this may need to move there as well", "url": "https://github.com/apache/hudi/pull/1457#discussion_r405969176", "createdAt": "2020-04-09T05:47:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -487,6 +492,58 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n     return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, instantTime);\n   }\n \n+  /**\n+   * Ensure that the current writerSchema is compatible with the latest schema of this dataset.\n+   *\n+   * When inserting/updating data, we read records using the schema saved in the data/log files\n+   * and convert them to the GenericRecords with writerSchema. Hence, we need to ensure that\n+   * this conversion can take place without errors.\n+   *\n+   * @param hoodieTable The Hoodie Table\n+   * @param isUpsert If this is a check during upserts\n+   * @throws HoodieUpsertException If schema check fails during upserts\n+   * @throws HoodieInsertException If schema check fails during inserts\n+   */\n+  private void validateSchema(HoodieTable<T> hoodieTable, final boolean isUpsert)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxOTQwNTYwOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNjoyMToxNFrOGDLB0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQyMzoxMjoxOVrOGEIA8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk3OTYwMQ==", "bodyText": "Since we log schema now in commit metadata, can't we just try to read it first. If it is not there, then we can try reading from parquet file.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r405979601", "createdAt": "2020-04-09T06:21:14Z", "author": {"login": "bvaradar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.avro.SchemaCompatibility;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Helper class to read schema from data files and log files and to convert it between different formats.\n+ */\n+public class TableSchemaResolver {\n+\n+  private static final Logger LOG = LogManager.getLogger(TableSchemaResolver.class);\n+  private HoodieTableMetaClient metaClient;\n+\n+  public TableSchemaResolver(HoodieTableMetaClient metaClient) {\n+    this.metaClient = metaClient;\n+  }\n+\n+  /**\n+   * Gets the schema for a hoodie table. Depending on the type of table, read from any file written in the latest\n+   * commit. We will assume that the schema has not changed within a single atomic write.\n+   *\n+   * @return Parquet schema for this table\n+   * @throws Exception\n+   */\n+  public MessageType getDataSchema() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3ODgwMg==", "bodyText": "This simplifies the code. I have updated to use the schema from commit metadata.\nFor MOR tables, the compaction operation also leads to a commit (.commit extension) which saves HoodieCommitMetadata but without the SCHEMA. I guess this is a miss and not as per design. I have fixed this as I test both types of tables.", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406978802", "createdAt": "2020-04-10T23:12:19Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.avro.SchemaCompatibility;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Helper class to read schema from data files and log files and to convert it between different formats.\n+ */\n+public class TableSchemaResolver {\n+\n+  private static final Logger LOG = LogManager.getLogger(TableSchemaResolver.class);\n+  private HoodieTableMetaClient metaClient;\n+\n+  public TableSchemaResolver(HoodieTableMetaClient metaClient) {\n+    this.metaClient = metaClient;\n+  }\n+\n+  /**\n+   * Gets the schema for a hoodie table. Depending on the type of table, read from any file written in the latest\n+   * commit. We will assume that the schema has not changed within a single atomic write.\n+   *\n+   * @return Parquet schema for this table\n+   * @throws Exception\n+   */\n+  public MessageType getDataSchema() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk3OTYwMQ=="}, "originalCommit": null, "originalPosition": 67}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4721, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}