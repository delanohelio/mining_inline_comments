{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNDQ1ODYw", "number": 2342, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMDo0NTowN1rOFG5fyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwNjoxMTo0NVrOFICWpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzc3ODAxOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMDo0NTowN1rOIIIGEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMToyMToxMlrOIIPX6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5MjE0NA==", "bodyText": "I would prefer to still have an interface for HoodieTableMetadata and return that here. We can have the abstract class internal to the actual implementation?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r545392144", "createdAt": "2020-12-17T20:45:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -635,9 +635,9 @@ public boolean requireSortedRecords() {\n     return getBaseFileFormat() == HoodieFileFormat.HFILE;\n   }\n \n-  public HoodieTableMetadata metadata() {\n+  public AbstractHoodieTableMetadata metadata() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56717d03adf0391ed581fc3042eaff3a9bf733fb"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMTQwMQ==", "bodyText": "I maintained this interface in revision after rebasing and then built AbstractHoodieTableMetadata on top of this.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r545511401", "createdAt": "2020-12-18T01:21:12Z", "author": {"login": "rmpifer"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -635,9 +635,9 @@ public boolean requireSortedRecords() {\n     return getBaseFileFormat() == HoodieFileFormat.HFILE;\n   }\n \n-  public HoodieTableMetadata metadata() {\n+  public AbstractHoodieTableMetadata metadata() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5MjE0NA=="}, "originalCommit": {"oid": "56717d03adf0391ed581fc3042eaff3a9bf733fb"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzc5MjA3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataTimelineUtil.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMDo0ODo1OVrOIIIOGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMToyMToxNVrOIIPX_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5NDIwMw==", "bodyText": "I think we can simply call this HoodieTableMetadataUtils ?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r545394203", "createdAt": "2020-12-17T20:48:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataTimelineUtil.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.metadata.AbstractHoodieTableMetadata.NON_PARTITIONED_NAME;\n+\n+/**\n+ * A utility to convert timeline information to metadata table records.\n+ */\n+public class HoodieTableMetadataTimelineUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56717d03adf0391ed581fc3042eaff3a9bf733fb"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMTQyMA==", "bodyText": "Updated this", "url": "https://github.com/apache/hudi/pull/2342#discussion_r545511420", "createdAt": "2020-12-18T01:21:15Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataTimelineUtil.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.metadata.AbstractHoodieTableMetadata.NON_PARTITIONED_NAME;\n+\n+/**\n+ * A utility to convert timeline information to metadata table records.\n+ */\n+public class HoodieTableMetadataTimelineUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM5NDIwMw=="}, "originalCommit": {"oid": "56717d03adf0391ed581fc3042eaff3a9bf733fb"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIwNjQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjoxNjoyNFrOIJAFKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDozMjo1NVrOIKNLYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMwOTQxNw==", "bodyText": "I think we can keep the factory method in the interface?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546309417", "createdAt": "2020-12-20T02:16:24Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -266,7 +256,7 @@ private void initialize(JavaSparkContext jsc, HoodieTableMetaClient datasetMetaC\n   }\n \n   private void initTableMetadata() {\n-    this.metadata = new HoodieBackedTableMetadata(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),\n+    this.metadata = (HoodieBackedTableMetadata) AbstractHoodieTableMetadata.create(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MjU3OQ==", "bodyText": "Along with Prashant's comment we can just create HoodieBackedTableMetadata since HoodieBackedTableMetadaWriter should only be associated with this implementation", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547572579", "createdAt": "2020-12-23T00:32:55Z", "author": {"login": "rmpifer"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -266,7 +256,7 @@ private void initialize(JavaSparkContext jsc, HoodieTableMetaClient datasetMetaC\n   }\n \n   private void initTableMetadata() {\n-    this.metadata = new HoodieBackedTableMetadata(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),\n+    this.metadata = (HoodieBackedTableMetadata) AbstractHoodieTableMetadata.create(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMwOTQxNw=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIwOTUxOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjoyMDo0MFrOIJAGcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwODo1NzowMlrOIJzbGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMwOTc0Nw==", "bodyText": "won't metadata be synced in postCommit() of the last upsert above. wondering how this is actually not in sync", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546309747", "createdAt": "2020-12-20T02:20:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "diffHunk": "@@ -619,32 +618,97 @@ public void testMetadataMetrics() throws Exception {\n     }\n   }\n \n+  //@ParameterizedTest\n+  //@EnumSource(HoodieTableType.class)\n+  public void testMetadataOutOfSync(HoodieTableType tableType) throws Exception {\n+    init(HoodieTableType.COPY_ON_WRITE);\n+\n+    HoodieWriteClient unsyncedClient = new HoodieWriteClient<>(jsc, getWriteConfig(true, true));\n+\n+    // Enable metadata so table is initialized\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, true))) {\n+      // Perform Bulk Insert\n+      String newCommitTime = \"001\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+    }\n+\n+    // Perform commit operations with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // Perform Insert\n+      String newCommitTime = \"002\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Perform Upsert\n+      newCommitTime = \"003\";\n+      client.startCommitWithTime(newCommitTime);\n+      records = dataGen.generateUniqueUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Compaction\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"004\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzMDExOQ==", "bodyText": "I disabled metadata in the write client used for these commit operations so it would not be synced. This way we could simulate timeline and metadata being out of sync.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546930119", "createdAt": "2020-12-21T21:08:11Z", "author": {"login": "rmpifer"}, "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "diffHunk": "@@ -619,32 +618,97 @@ public void testMetadataMetrics() throws Exception {\n     }\n   }\n \n+  //@ParameterizedTest\n+  //@EnumSource(HoodieTableType.class)\n+  public void testMetadataOutOfSync(HoodieTableType tableType) throws Exception {\n+    init(HoodieTableType.COPY_ON_WRITE);\n+\n+    HoodieWriteClient unsyncedClient = new HoodieWriteClient<>(jsc, getWriteConfig(true, true));\n+\n+    // Enable metadata so table is initialized\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, true))) {\n+      // Perform Bulk Insert\n+      String newCommitTime = \"001\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+    }\n+\n+    // Perform commit operations with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // Perform Insert\n+      String newCommitTime = \"002\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Perform Upsert\n+      newCommitTime = \"003\";\n+      client.startCommitWithTime(newCommitTime);\n+      records = dataGen.generateUniqueUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Compaction\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"004\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMwOTc0Nw=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE1MDYxOA==", "bodyText": "Got it. Makes sense", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547150618", "createdAt": "2020-12-22T08:57:02Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "diffHunk": "@@ -619,32 +618,97 @@ public void testMetadataMetrics() throws Exception {\n     }\n   }\n \n+  //@ParameterizedTest\n+  //@EnumSource(HoodieTableType.class)\n+  public void testMetadataOutOfSync(HoodieTableType tableType) throws Exception {\n+    init(HoodieTableType.COPY_ON_WRITE);\n+\n+    HoodieWriteClient unsyncedClient = new HoodieWriteClient<>(jsc, getWriteConfig(true, true));\n+\n+    // Enable metadata so table is initialized\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, true))) {\n+      // Perform Bulk Insert\n+      String newCommitTime = \"001\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+    }\n+\n+    // Perform commit operations with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // Perform Insert\n+      String newCommitTime = \"002\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Perform Upsert\n+      newCommitTime = \"003\";\n+      client.startCommitWithTime(newCommitTime);\n+      records = dataGen.generateUniqueUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Compaction\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"004\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMwOTc0Nw=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIxNjU4OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjozMToxNlrOIJAJeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMTowODoyMlrOIJl-Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMDUyMQ==", "bodyText": "just tableMetadata?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546310521", "createdAt": "2020-12-20T02:31:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "diffHunk": "@@ -619,32 +618,97 @@ public void testMetadataMetrics() throws Exception {\n     }\n   }\n \n+  //@ParameterizedTest\n+  //@EnumSource(HoodieTableType.class)\n+  public void testMetadataOutOfSync(HoodieTableType tableType) throws Exception {\n+    init(HoodieTableType.COPY_ON_WRITE);\n+\n+    HoodieWriteClient unsyncedClient = new HoodieWriteClient<>(jsc, getWriteConfig(true, true));\n+\n+    // Enable metadata so table is initialized\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, true))) {\n+      // Perform Bulk Insert\n+      String newCommitTime = \"001\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+    }\n+\n+    // Perform commit operations with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // Perform Insert\n+      String newCommitTime = \"002\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Perform Upsert\n+      newCommitTime = \"003\";\n+      client.startCommitWithTime(newCommitTime);\n+      records = dataGen.generateUniqueUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Compaction\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"004\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+\n+    // Perform clean operation with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // One more commit needed to trigger clean so upsert and compact\n+      String newCommitTime = \"005\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"006\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+\n+      // Clean\n+      newCommitTime = \"007\";\n+      client.clean(newCommitTime);\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+\n+    // Perform restore with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      client.restoreToInstant(\"004\");\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+  }\n+\n   /**\n    * Validate the metadata tables contents to ensure it matches what is on the file system.\n    *\n    * @throws IOException\n    */\n   private void validateMetadata(HoodieWriteClient client) throws IOException {\n     HoodieWriteConfig config = client.getConfig();\n-    HoodieBackedTableMetadataWriter metadataWriter = metadataWriter(client);\n-    assertNotNull(metadataWriter, \"MetadataWriter should have been initialized\");\n+\n+    HoodieBackedTableMetadata metadataReader = metadata(client);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzMDE4Mg==", "bodyText": "Updated", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546930182", "createdAt": "2020-12-21T21:08:22Z", "author": {"login": "rmpifer"}, "path": "hudi-client/src/test/java/org/apache/hudi/metadata/TestHoodieBackedMetadata.java", "diffHunk": "@@ -619,32 +618,97 @@ public void testMetadataMetrics() throws Exception {\n     }\n   }\n \n+  //@ParameterizedTest\n+  //@EnumSource(HoodieTableType.class)\n+  public void testMetadataOutOfSync(HoodieTableType tableType) throws Exception {\n+    init(HoodieTableType.COPY_ON_WRITE);\n+\n+    HoodieWriteClient unsyncedClient = new HoodieWriteClient<>(jsc, getWriteConfig(true, true));\n+\n+    // Enable metadata so table is initialized\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, true))) {\n+      // Perform Bulk Insert\n+      String newCommitTime = \"001\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+    }\n+\n+    // Perform commit operations with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // Perform Insert\n+      String newCommitTime = \"002\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n+      client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Perform Upsert\n+      newCommitTime = \"003\";\n+      client.startCommitWithTime(newCommitTime);\n+      records = dataGen.generateUniqueUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      // Compaction\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"004\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+\n+    // Perform clean operation with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      // One more commit needed to trigger clean so upsert and compact\n+      String newCommitTime = \"005\";\n+      client.startCommitWithTime(newCommitTime);\n+      List<HoodieRecord> records = dataGen.generateUpdates(newCommitTime, 20);\n+      client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n+\n+      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n+        newCommitTime = \"006\";\n+        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n+        client.compact(newCommitTime);\n+      }\n+\n+      // Clean\n+      newCommitTime = \"007\";\n+      client.clean(newCommitTime);\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+\n+    // Perform restore with metadata disabled\n+    try (HoodieWriteClient client = new HoodieWriteClient<>(jsc, getWriteConfig(true, false))) {\n+      client.restoreToInstant(\"004\");\n+    }\n+\n+    assertFalse(metadata(unsyncedClient).isInSync());\n+    validateMetadata(unsyncedClient);\n+  }\n+\n   /**\n    * Validate the metadata tables contents to ensure it matches what is on the file system.\n    *\n    * @throws IOException\n    */\n   private void validateMetadata(HoodieWriteClient client) throws IOException {\n     HoodieWriteConfig config = client.getConfig();\n-    HoodieBackedTableMetadataWriter metadataWriter = metadataWriter(client);\n-    assertNotNull(metadataWriter, \"MetadataWriter should have been initialized\");\n+\n+    HoodieBackedTableMetadata metadataReader = metadata(client);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMDUyMQ=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIyMjc0OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjozOToxOVrOIJAMFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwOTowMDozMlrOIJzh_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTE4OQ==", "bodyText": "Given the only subclass of AbstractHoodieTableMetadata is this class and the abstract class already assumes the existence of the metadata table, I am not sure if the splitting is needed per se.\nI was under the impression that we would like to implement the timeline merging as another subclass. If we stick with the new scanner approach (which I actually have grown to like actually), we no longer need the abstract class right?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311189", "createdAt": "2020-12-20T02:39:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -66,24 +58,13 @@\n  * If the metadata table does not exist, RPC calls are used to retrieve file listings from the file system.\n  * No updates are applied to the table and it is not synced.\n  */\n-public class HoodieBackedTableMetadata implements HoodieTableMetadata {\n+public class HoodieBackedTableMetadata extends AbstractHoodieTableMetadata {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzODUyMA==", "bodyText": "There still needs to be a place where the instant scanner result is merged with the metadata table result. I'm not sure I'm able to see how we would like this logic at scanner level. Since this should be common behavior regardless of metadata storage this is why I thought to create this", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546938520", "createdAt": "2020-12-21T21:29:39Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -66,24 +58,13 @@\n  * If the metadata table does not exist, RPC calls are used to retrieve file listings from the file system.\n  * No updates are applied to the table and it is not synced.\n  */\n-public class HoodieBackedTableMetadata implements HoodieTableMetadata {\n+public class HoodieBackedTableMetadata extends AbstractHoodieTableMetadata {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTE4OQ=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE1MjM4MA==", "bodyText": "okay that also makes sense. Personally, I would not have done the split, until a second kind of metadata impl shows up. but udit added the FileSytemBackedMetadata, it does not need the merging per se.. Still we can keep it like this for now.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547152380", "createdAt": "2020-12-22T09:00:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -66,24 +58,13 @@\n  * If the metadata table does not exist, RPC calls are used to retrieve file listings from the file system.\n  * No updates are applied to the table and it is not synced.\n  */\n-public class HoodieBackedTableMetadata implements HoodieTableMetadata {\n+public class HoodieBackedTableMetadata extends AbstractHoodieTableMetadata {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTE4OQ=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIyMzA0OnYy", "diffSide": "LEFT", "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjozOTo0NlrOIJAMPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMToyNjo1N1rOIJmaig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTIzMA==", "bodyText": "I assume all of this code, is just verbatim moved up to the base class ?", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311230", "createdAt": "2020-12-20T02:39:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -117,172 +93,15 @@ public HoodieBackedTableMetadata(Configuration conf, String datasetBasePath, Str\n     } else {\n       LOG.info(\"Metadata table is disabled.\");\n     }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzNzQ4Mg==", "bodyText": "Correct", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546937482", "createdAt": "2020-12-21T21:26:57Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -117,172 +93,15 @@ public HoodieBackedTableMetadata(Configuration conf, String datasetBasePath, Str\n     } else {\n       LOG.info(\"Metadata table is disabled.\");\n     }\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTIzMA=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIyNTcxOnYy", "diffSide": "LEFT", "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjo0NDozMFrOIJANeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjo0NDozMFrOIJANeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTU0NQ==", "bodyText": "left a comment around this scenario. there is one valid case here. good call out in the summary", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311545", "createdAt": "2020-12-20T02:44:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -569,114 +471,7 @@ public void update(HoodieRollbackMetadata rollbackMetadata, String instantTime)\n       return;\n     }\n \n-    Map<String, Map<String, Long>> partitionToAppendedFiles = new HashMap<>();\n-    Map<String, List<String>> partitionToDeletedFiles = new HashMap<>();\n-    processRollbackMetadata(rollbackMetadata, partitionToDeletedFiles, partitionToAppendedFiles);\n-    commitRollback(jsc, partitionToDeletedFiles, partitionToAppendedFiles, instantTime, \"Rollback\");\n-  }\n-\n-  /**\n-   * Extracts information about the deleted and append files from the {@code HoodieRollbackMetadata}.\n-   *\n-   * During a rollback files may be deleted (COW, MOR) or rollback blocks be appended (MOR only) to files. This\n-   * function will extract this change file for each partition.\n-   *\n-   * @param rollbackMetadata {@code HoodieRollbackMetadata}\n-   * @param partitionToDeletedFiles The {@code Map} to fill with files deleted per partition.\n-   * @param partitionToAppendedFiles The {@code Map} to fill with files appended per partition and their sizes.\n-   */\n-  private void processRollbackMetadata(HoodieRollbackMetadata rollbackMetadata,\n-                                       Map<String, List<String>> partitionToDeletedFiles,\n-                                       Map<String, Map<String, Long>> partitionToAppendedFiles) {\n-    rollbackMetadata.getPartitionMetadata().values().forEach(pm -> {\n-      final String partition = pm.getPartitionPath();\n-\n-      if (!pm.getSuccessDeleteFiles().isEmpty()) {\n-        if (!partitionToDeletedFiles.containsKey(partition)) {\n-          partitionToDeletedFiles.put(partition, new ArrayList<>());\n-        }\n-\n-        // Extract deleted file name from the absolute paths saved in getSuccessDeleteFiles()\n-        List<String> deletedFiles = pm.getSuccessDeleteFiles().stream().map(p -> new Path(p).getName())\n-            .collect(Collectors.toList());\n-        partitionToDeletedFiles.get(partition).addAll(deletedFiles);\n-      }\n-\n-      if (!pm.getAppendFiles().isEmpty()) {\n-        if (!partitionToAppendedFiles.containsKey(partition)) {\n-          partitionToAppendedFiles.put(partition, new HashMap<>());\n-        }\n-\n-        // Extract appended file name from the absolute paths saved in getAppendFiles()\n-        pm.getAppendFiles().forEach((path, size) -> {\n-          partitionToAppendedFiles.get(partition).merge(new Path(path).getName(), size, (oldSize, newSizeCopy) -> {\n-            return size + oldSize;\n-          });\n-        });\n-      }\n-    });\n-  }\n-\n-  /**\n-   * Create file delete records and commit.\n-   *\n-   * @param partitionToDeletedFiles {@code Map} of partitions and the deleted files\n-   * @param instantTime Timestamp at which the deletes took place\n-   * @param operation Type of the operation which caused the files to be deleted\n-   */\n-  private void commitRollback(JavaSparkContext jsc, Map<String, List<String>> partitionToDeletedFiles,\n-                              Map<String, Map<String, Long>> partitionToAppendedFiles, String instantTime,\n-                              String operation) {\n-    List<HoodieRecord> records = new LinkedList<>();\n-    int[] fileChangeCount = {0, 0}; // deletes, appends\n-\n-    partitionToDeletedFiles.forEach((partition, deletedFiles) -> {\n-      // Rollbacks deletes instants from timeline. The instant being rolled-back may not have been synced to the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 270}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIyNjYzOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjo0NTozMlrOIJAN2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMToyNzowOVrOIJma7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTY0Mw==", "bodyText": "assume most of this code is just from the writer class verbatim", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311643", "createdAt": "2020-12-20T02:45:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.metadata.HoodieTableMetadata.NON_PARTITIONED_NAME;\n+\n+/**\n+ * A utility to convert timeline information to metadata table records.\n+ */\n+public class HoodieTableMetadataUtil {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieTableMetadataUtil.class);\n+\n+  /**\n+   * Converts a timeline instant to metadata table records.\n+   *\n+   * @param datasetMetaClient The meta client associated with the timeline instant\n+   * @param instant to fetch and convert to metadata table records\n+   * @return a list of metadata table records\n+   * @throws IOException\n+   */\n+  public static Option<List<HoodieRecord>> convertInstantToMetaRecords(HoodieTableMetaClient datasetMetaClient, HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzNzU4MA==", "bodyText": "Correct other then the callout I mentioned", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546937580", "createdAt": "2020-12-21T21:27:09Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataUtil.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.CleanerUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.metadata.HoodieTableMetadata.NON_PARTITIONED_NAME;\n+\n+/**\n+ * A utility to convert timeline information to metadata table records.\n+ */\n+public class HoodieTableMetadataUtil {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieTableMetadataUtil.class);\n+\n+  /**\n+   * Converts a timeline instant to metadata table records.\n+   *\n+   * @param datasetMetaClient The meta client associated with the timeline instant\n+   * @param instant to fetch and convert to metadata table records\n+   * @return a list of metadata table records\n+   * @throws IOException\n+   */\n+  public static Option<List<HoodieRecord>> convertInstantToMetaRecords(HoodieTableMetaClient datasetMetaClient, HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTY0Mw=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIyODQxOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjo0NzoxMVrOIJAOmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMToyNjoyOFrOIJmZsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTgzNQ==", "bodyText": "should we pass in the filter? guess it does not matter, since we ll read the instants in full anyway", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546311835", "createdAt": "2020-12-20T02:47:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;\n+      }\n+    }\n+\n+    return metadataHoodieRecord;\n+  }\n+\n+  protected abstract Option<HoodieRecord<HoodieMetadataPayload>> getRecordByKeyFromMetadata(String key) throws IOException;\n+\n+  private void openTimelineScanner() throws IOException {\n+    if (timelineRecordScanner != null) {\n+      // Already opened\n+      return;\n+    }\n+\n+    HoodieTableMetaClient datasetMetaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+    List<HoodieInstant> unsyncedInstants = findInstantsToSync(datasetMetaClient);\n+    Schema schema = HoodieAvroUtils.addMetadataFields(HoodieMetadataRecord.getClassSchema());\n+    timelineRecordScanner =\n+            new HoodieMetadataMergedInstantRecordScanner(datasetMetaClient, unsyncedInstants, schema, MAX_MEMORY_SIZE_IN_BYTES, spillableMapDirectory, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzNzI2NQ==", "bodyText": "Filter would be beneficial if we are looking for a specific key. However case often when reading from metadata table is reading multiple keys so is beneficial to have everything in this case", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546937265", "createdAt": "2020-12-21T21:26:28Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;\n+      }\n+    }\n+\n+    return metadataHoodieRecord;\n+  }\n+\n+  protected abstract Option<HoodieRecord<HoodieMetadataPayload>> getRecordByKeyFromMetadata(String key) throws IOException;\n+\n+  private void openTimelineScanner() throws IOException {\n+    if (timelineRecordScanner != null) {\n+      // Already opened\n+      return;\n+    }\n+\n+    HoodieTableMetaClient datasetMetaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+    List<HoodieInstant> unsyncedInstants = findInstantsToSync(datasetMetaClient);\n+    Schema schema = HoodieAvroUtils.addMetadataFields(HoodieMetadataRecord.getClassSchema());\n+    timelineRecordScanner =\n+            new HoodieMetadataMergedInstantRecordScanner(datasetMetaClient, unsyncedInstants, schema, MAX_MEMORY_SIZE_IN_BYTES, spillableMapDirectory, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMTgzNQ=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 282}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNDIzMDQ1OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMFQwMjo0OTo0N1rOIJAPeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDozNTowNFrOIKNNbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMjA1OA==", "bodyText": "just return out of here, instead of reassigning to another important variable. may be easier to read.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546312058", "createdAt": "2020-12-20T02:49:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzNjk5Nw==", "bodyText": "I created separate variable for this so there is still one place of return", "url": "https://github.com/apache/hudi/pull/2342#discussion_r546936997", "createdAt": "2020-12-21T21:25:52Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMjA1OA=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE1NDU0Mg==", "bodyText": "Sorry, my core concern was that we are reassigning to a variable that holds the returned value from a key method call getRecordByKeyFromMetadata\nI was suggesting something like.\n    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n    // Retrieve record from unsynced timeline instants\n    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n    if (timelineHoodieRecord.isPresent()) {\n      if (metadataHoodieRecord.isPresent()) {\n        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n        return Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n      } else {\n        return timelineHoodieRecord;\n      }\n    }\n\n    return metadataHoodieRecord;\n\nif you feel strongly, we can keep as-is.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547154542", "createdAt": "2020-12-22T09:05:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMjA1OA=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MzEwMA==", "bodyText": "Sorry my comment was unclear. I created separate mergedRecord variable so I am not overriding metadataHoodieRecord", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547573100", "createdAt": "2020-12-23T00:35:04Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/AbstractHoodieTableMetadata.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieMetadataRecord;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.metrics.Registry;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.exception.HoodieMetadataException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Interface that supports querying various pieces of metadata about a hudi table.\n+ */\n+public abstract class AbstractHoodieTableMetadata implements HoodieTableMetadata {\n+\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieTableMetadata.class);\n+\n+  static final long MAX_MEMORY_SIZE_IN_BYTES = 1024 * 1024 * 1024;\n+  static final int BUFFER_SIZE = 10 * 1024 * 1024;\n+\n+  protected final SerializableConfiguration hadoopConf;\n+  protected final Option<HoodieMetadataMetrics> metrics;\n+  protected final String datasetBasePath;\n+\n+  // Directory used for Spillable Map when merging records\n+  final String spillableMapDirectory;\n+\n+  protected boolean enabled;\n+  private final boolean validateLookups;\n+  private final boolean assumeDatePartitioning;\n+\n+  private transient HoodieMetadataMergedInstantRecordScanner timelineRecordScanner;\n+\n+  protected AbstractHoodieTableMetadata(Configuration hadoopConf, String datasetBasePath, String spillableMapDirectory,\n+                                        boolean enabled, boolean validateLookups, boolean enableMetrics,\n+                                        boolean assumeDatePartitioning) {\n+    this.hadoopConf = new SerializableConfiguration(hadoopConf);\n+    this.datasetBasePath = datasetBasePath;\n+    this.spillableMapDirectory = spillableMapDirectory;\n+\n+    this.enabled = enabled;\n+    this.validateLookups = validateLookups;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+\n+    if (enableMetrics) {\n+      this.metrics = Option.of(new HoodieMetadataMetrics(Registry.getRegistry(\"HoodieMetadata\")));\n+    } else {\n+      this.metrics = Option.empty();\n+    }\n+  }\n+\n+  public static AbstractHoodieTableMetadata create(Configuration conf, String datasetBasePath, String spillableMapPath, boolean useFileListingFromMetadata,\n+                                                   boolean verifyListings, boolean enableMetrics, boolean shouldAssumeDatePartitioning) {\n+    return new HoodieBackedTableMetadata(conf, datasetBasePath, spillableMapPath, useFileListingFromMetadata, verifyListings,\n+        enableMetrics, shouldAssumeDatePartitioning);\n+  }\n+\n+\n+  /**\n+   * Return the list of files in a partition.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   * @param partitionPath The absolute path of the partition to list\n+   */\n+\n+  public FileStatus[] getAllFilesInPartition(Path partitionPath) throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllFilesInPartition(partitionPath);\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrive files in partition \" + partitionPath + \" from metadata\", e);\n+      }\n+    }\n+    return FSUtils.getFs(partitionPath.toString(), hadoopConf.get()).listStatus(partitionPath);\n+  }\n+\n+  /**\n+   * Return the list of partitions in the dataset.\n+   *\n+   * If the Metadata Table is enabled, the listing is retrieved from the stored metadata. Otherwise, the list of\n+   * partitions is retrieved directly from the underlying {@code FileSystem}.\n+   *\n+   * On any errors retrieving the listing from the metadata, defaults to using the file system listings.\n+   *\n+   */\n+  public List<String> getAllPartitionPaths() throws IOException {\n+    if (enabled) {\n+      try {\n+        return fetchAllPartitionPaths();\n+      } catch (Exception e) {\n+        LOG.error(\"Failed to retrieve list of partition from metadata\", e);\n+      }\n+    }\n+\n+    FileSystem fs = FSUtils.getFs(datasetBasePath, hadoopConf.get());\n+    return FSUtils.getAllPartitionPaths(fs, datasetBasePath, assumeDatePartitioning);\n+  }\n+\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  /**\n+   * Returns a list of all partitions.\n+   */\n+  protected List<String> fetchAllPartitionPaths() throws IOException {\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(RECORDKEY_PARTITION_LIST);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_PARTITIONS_STR, timer.endTimer()));\n+\n+    List<String> partitions = Collections.emptyList();\n+    if (hoodieRecord.isPresent()) {\n+      if (!hoodieRecord.get().getData().getDeletions().isEmpty()) {\n+        throw new HoodieMetadataException(\"Metadata partition list record is inconsistent: \"\n+                + hoodieRecord.get().getData());\n+      }\n+\n+      partitions = hoodieRecord.get().getData().getFilenames();\n+      // Partition-less tables have a single empty partition\n+      if (partitions.contains(NON_PARTITIONED_NAME)) {\n+        partitions.remove(NON_PARTITIONED_NAME);\n+        partitions.add(\"\");\n+      }\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      List<String> actualPartitions  = FSUtils.getAllPartitionPaths(metaClient.getFs(), datasetBasePath, false);\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_PARTITIONS_STR, timer.endTimer()));\n+\n+      Collections.sort(actualPartitions);\n+      Collections.sort(partitions);\n+      if (!actualPartitions.equals(partitions)) {\n+        LOG.error(\"Validation of metadata partition list failed. Lists do not match.\");\n+        LOG.error(\"Partitions from metadata: \" + Arrays.toString(partitions.toArray()));\n+        LOG.error(\"Partitions from file system: \" + Arrays.toString(actualPartitions.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      partitions = actualPartitions;\n+    }\n+\n+    LOG.info(\"Listed partitions from metadata: #partitions=\" + partitions.size());\n+    return partitions;\n+  }\n+\n+  /**\n+   * Return all the files from the partition.\n+   *\n+   * @param partitionPath The absolute path of the partition\n+   */\n+  private FileStatus[] fetchAllFilesInPartition(Path partitionPath) throws IOException {\n+    String partitionName = FSUtils.getRelativePartitionPath(new Path(datasetBasePath), partitionPath);\n+    if (partitionName.isEmpty()) {\n+      partitionName = NON_PARTITIONED_NAME;\n+    }\n+\n+    HoodieTimer timer = new HoodieTimer().startTimer();\n+    Option<HoodieRecord<HoodieMetadataPayload>> hoodieRecord = getMergedRecordByKey(partitionName);\n+    metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.LOOKUP_FILES_STR, timer.endTimer()));\n+\n+    FileStatus[] statuses = {};\n+    if (hoodieRecord.isPresent()) {\n+      statuses = hoodieRecord.get().getData().getFileStatuses(partitionPath);\n+    }\n+\n+    if (validateLookups) {\n+      // Validate the Metadata Table data by listing the partitions from the file system\n+      timer.startTimer();\n+\n+      // Ignore partition metadata file\n+      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf.get(), datasetBasePath);\n+      FileStatus[] directStatuses = metaClient.getFs().listStatus(partitionPath,\n+          p -> !p.getName().equals(HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+      metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_FILES_STR, timer.endTimer()));\n+\n+      List<String> directFilenames = Arrays.stream(directStatuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      List<String> metadataFilenames = Arrays.stream(statuses)\n+              .map(s -> s.getPath().getName()).sorted()\n+              .collect(Collectors.toList());\n+\n+      if (!metadataFilenames.equals(directFilenames)) {\n+        LOG.error(\"Validation of metadata file listing for partition \" + partitionName + \" failed.\");\n+        LOG.error(\"File list from metadata: \" + Arrays.toString(metadataFilenames.toArray()));\n+        LOG.error(\"File list from direct listing: \" + Arrays.toString(directFilenames.toArray()));\n+\n+        metrics.ifPresent(m -> m.updateMetrics(HoodieMetadataMetrics.VALIDATE_ERRORS_STR, 0));\n+      }\n+\n+      // Return the direct listing as it should be correct\n+      statuses = directStatuses;\n+    }\n+\n+    LOG.info(\"Listed file in partition from metadata: partition=\" + partitionName + \", #files=\" + statuses.length);\n+    return statuses;\n+  }\n+\n+  /**\n+   * Retrieve the merged {@code HoodieRecord} mapped to the given key.\n+   *\n+   * @param key The key of the record\n+   */\n+  private Option<HoodieRecord<HoodieMetadataPayload>> getMergedRecordByKey(String key) throws IOException {\n+    openTimelineScanner();\n+\n+    Option<HoodieRecord<HoodieMetadataPayload>> metadataHoodieRecord = getRecordByKeyFromMetadata(key);\n+\n+    // Retrieve record from unsynced timeline instants\n+    Option<HoodieRecord<HoodieMetadataPayload>> timelineHoodieRecord = timelineRecordScanner.getRecordByKey(key);\n+    if (timelineHoodieRecord.isPresent()) {\n+      if (metadataHoodieRecord.isPresent()) {\n+        HoodieRecordPayload mergedPayload = timelineHoodieRecord.get().getData().preCombine(metadataHoodieRecord.get().getData());\n+        metadataHoodieRecord = Option.of(new HoodieRecord(metadataHoodieRecord.get().getKey(), mergedPayload));\n+      } else {\n+        metadataHoodieRecord = timelineHoodieRecord;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjMxMjA1OA=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzOTU5OTAxOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedInstantRecordScanner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwNToxMDowNFrOIJukuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDozNzoxM1rOIKNPnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA3MTE2MQ==", "bodyText": "can be merged with the above function. I dont see it being called anywhere else.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547071161", "createdAt": "2020-12-22T05:10:04Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedInstantRecordScanner.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.DefaultSizeEstimator;\n+import org.apache.hudi.common.util.HoodieRecordSizeEstimator;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.ExternalSpillableMap;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+\n+/**\n+ * Provides functionality to convert timeline instants to table metadata records and then merge by key. Specify\n+ *  a filter to limit keys that are merged and stored in memory.\n+ */\n+public class HoodieMetadataMergedInstantRecordScanner {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieMetadataMergedInstantRecordScanner.class);\n+\n+  HoodieTableMetaClient metaClient;\n+  private List<HoodieInstant> instants;\n+  private Set<String> mergeKeyFilter;\n+  protected final ExternalSpillableMap<String, HoodieRecord<? extends HoodieRecordPayload>> records;\n+\n+  public HoodieMetadataMergedInstantRecordScanner(HoodieTableMetaClient metaClient, List<HoodieInstant> instants,\n+                                                  Schema readerSchema, Long maxMemorySizeInBytes,\n+                                                  String spillableMapBasePath, Set<String> mergeKeyFilter) throws IOException {\n+    this.metaClient = metaClient;\n+    this.instants = instants;\n+    this.mergeKeyFilter = mergeKeyFilter != null ? mergeKeyFilter : Collections.emptySet();\n+    this.records = new ExternalSpillableMap<>(maxMemorySizeInBytes, spillableMapBasePath, new DefaultSizeEstimator(),\n+            new HoodieRecordSizeEstimator(readerSchema));\n+\n+    scan();\n+  }\n+\n+  private void scan() {\n+    for (HoodieInstant instant : instants) {\n+      try {\n+        processInstant(instant);\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"Got exception when processing timeline instant %s\", instant.getTimestamp()), e);\n+        throw new HoodieException(String.format(\"Got exception when processing timeline instant %s\", instant.getTimestamp()), e);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts an instant to metadata table records and processes each record.\n+   *\n+   * @param instant\n+   * @throws IOException\n+   */\n+  private void processInstant(HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MzY2Mw==", "bodyText": "Updated", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547573663", "createdAt": "2020-12-23T00:37:13Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieMetadataMergedInstantRecordScanner.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.DefaultSizeEstimator;\n+import org.apache.hudi.common.util.HoodieRecordSizeEstimator;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.ExternalSpillableMap;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+\n+/**\n+ * Provides functionality to convert timeline instants to table metadata records and then merge by key. Specify\n+ *  a filter to limit keys that are merged and stored in memory.\n+ */\n+public class HoodieMetadataMergedInstantRecordScanner {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieMetadataMergedInstantRecordScanner.class);\n+\n+  HoodieTableMetaClient metaClient;\n+  private List<HoodieInstant> instants;\n+  private Set<String> mergeKeyFilter;\n+  protected final ExternalSpillableMap<String, HoodieRecord<? extends HoodieRecordPayload>> records;\n+\n+  public HoodieMetadataMergedInstantRecordScanner(HoodieTableMetaClient metaClient, List<HoodieInstant> instants,\n+                                                  Schema readerSchema, Long maxMemorySizeInBytes,\n+                                                  String spillableMapBasePath, Set<String> mergeKeyFilter) throws IOException {\n+    this.metaClient = metaClient;\n+    this.instants = instants;\n+    this.mergeKeyFilter = mergeKeyFilter != null ? mergeKeyFilter : Collections.emptySet();\n+    this.records = new ExternalSpillableMap<>(maxMemorySizeInBytes, spillableMapBasePath, new DefaultSizeEstimator(),\n+            new HoodieRecordSizeEstimator(readerSchema));\n+\n+    scan();\n+  }\n+\n+  private void scan() {\n+    for (HoodieInstant instant : instants) {\n+      try {\n+        processInstant(instant);\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"Got exception when processing timeline instant %s\", instant.getTimestamp()), e);\n+        throw new HoodieException(String.format(\"Got exception when processing timeline instant %s\", instant.getTimestamp()), e);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Converts an instant to metadata table records and processes each record.\n+   *\n+   * @param instant\n+   * @throws IOException\n+   */\n+  private void processInstant(HoodieInstant instant) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA3MTE2MQ=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzOTYzNTI5OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwNTozMDozMFrOIJu5LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDozNjoyMFrOIKNOuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA3NjM5Nw==", "bodyText": "I think there needs to be a corresponding close() for the timelineRecordScanner too.\nOnce we commit an update to the table, the unsynced instants have changed.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547076397", "createdAt": "2020-12-22T05:30:30Z", "author": {"login": "prashantwason"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -372,19 +191,6 @@ protected void closeReaders() {\n     logRecordScanner = null;\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MzQzMg==", "bodyText": "Thanks for the callout. I added this to closeReaders()", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547573432", "createdAt": "2020-12-23T00:36:20Z", "author": {"login": "rmpifer"}, "path": "hudi-common/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadata.java", "diffHunk": "@@ -372,19 +191,6 @@ protected void closeReaders() {\n     logRecordScanner = null;\n   }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA3NjM5Nw=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 263}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzOTcxNDk1OnYy", "diffSide": "LEFT", "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwNjoxMTo0NVrOIJvmUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDozMjo1OVrOIKNLbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA4Nzk1NA==", "bodyText": "The reader-write pairs in metadata are related. So this can be simplified to be simply new HoodieBackedTableMetadata.\nHoodieBackedTableMetadata cannot work with any other implementaton of HoodieTableMetadata.", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547087954", "createdAt": "2020-12-22T06:11:45Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -266,7 +256,7 @@ private void initialize(JavaSparkContext jsc, HoodieTableMetaClient datasetMetaC\n   }\n \n   private void initTableMetadata() {\n-    this.metadata = new HoodieBackedTableMetadata(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU3MjU5MQ==", "bodyText": "Agreed. Updated this", "url": "https://github.com/apache/hudi/pull/2342#discussion_r547572591", "createdAt": "2020-12-23T00:32:59Z", "author": {"login": "rmpifer"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieBackedTableMetadataWriter.java", "diffHunk": "@@ -266,7 +256,7 @@ private void initialize(JavaSparkContext jsc, HoodieTableMetaClient datasetMetaC\n   }\n \n   private void initTableMetadata() {\n-    this.metadata = new HoodieBackedTableMetadata(hadoopConf.get(), datasetWriteConfig.getBasePath(), datasetWriteConfig.getSpillableMapBasePath(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzA4Nzk1NA=="}, "originalCommit": {"oid": "78253ff7484ed50217fb4a2c21ba554e988a8c99"}, "originalPosition": 56}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4020, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}