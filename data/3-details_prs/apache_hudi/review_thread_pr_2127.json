{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0MjY5NTAw", "number": 2127, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNDo1MToyNVrOEqBYjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyNjo1MFrOEtNjcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDk4MzE5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNDo1MToyNVrOHcB9jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjozNjoyN1rOHcCdFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDMxOA==", "bodyText": "these 3 local vars could be removed", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154318", "createdAt": "2020-10-03T14:51:25Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjM5MA==", "bodyText": "thanks ,3 local vars will  use for prepareFirstRecordCommit method .i will move it to a method", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499162390", "createdAt": "2020-10-03T16:36:27Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDMxOA=="}, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDk4Mzk3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNDo1Mjo0NlrOHcB98g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjoyOTo1N1rOHcCbNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDQxOA==", "bodyText": "to avoid ambiguity, could we call this prepareFirstRecordCommit()?", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154418", "createdAt": "2020-10-03T14:52:46Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MTkwOA==", "bodyText": "thanks, make sense", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499161908", "createdAt": "2020-10-03T16:29:57Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDQxOA=="}, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDk4NTI5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNDo1NDowOVrOHcB-jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNjozNjozOFrOHcCdIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDU3NA==", "bodyText": "could we call them table and config instead of table2 and config2? since the method is split and they are the only vars left.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154574", "createdAt": "2020-10-03T14:54:09Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n \n     // Now try an update with an evolved schema\n     // Evolved schema does not have guarantee on preserving the original field ordering\n     final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+    final WriteStatus insertResult = prepareFirstCommitData(recordsStrs);\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n+    final HoodieSparkTable table2 = HoodieSparkTable.create(config2, context);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2MjQwMg==", "bodyText": "ok", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499162402", "createdAt": "2020-10-03T16:36:38Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n \n     // Now try an update with an evolved schema\n     // Evolved schema does not have guarantee on preserving the original field ordering\n     final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+    final WriteStatus insertResult = prepareFirstCommitData(recordsStrs);\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n+    final HoodieSparkTable table2 = HoodieSparkTable.create(config2, context);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDU3NA=="}, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDk4NjY0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNDo1NjoxMlrOHcB_Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxNzoyMzo1OFrOHcCq9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDczOA==", "bodyText": "seeing this is original code. i don't see why it needs to assert 1 equals to 1 returned eventually. could we simplify this by removing jsc.parallelize() and just retain the lambda block?", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499154738", "createdAt": "2020-10-03T14:56:12Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n \n     // Now try an update with an evolved schema\n     // Evolved schema does not have guarantee on preserving the original field ordering\n     final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+    final WriteStatus insertResult = prepareFirstCommitData(recordsStrs);\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n+    final HoodieSparkTable table2 = HoodieSparkTable.create(config2, context);\n     assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE2NTk0Mw==", "bodyText": "can not removing  jsc.parallelize() and just retain the lambda block.\nbecause HoodieMergeHandle need the param supplier.  supplier implement need\n\" return () -> TaskContext.get().stageId();\"\nso jsc.parallelize can init the TaskContext.   if we mock supplier, will bring  \"Task not serializable\".\nso i will remove assertEquals, but remain jsc.parallelize just like TestCopyOnWriteActionExecutor", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499165943", "createdAt": "2020-10-03T17:23:58Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,61 +71,62 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstCommitData(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n \n     // Now try an update with an evolved schema\n     // Evolved schema does not have guarantee on preserving the original field ordering\n     final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+    final WriteStatus insertResult = prepareFirstCommitData(recordsStrs);\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n+    final HoodieSparkTable table2 = HoodieSparkTable.create(config2, context);\n     assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1NDczOA=="}, "originalCommit": {"oid": "cec3a1656cc19746fc020dfcd8a14a0ae98ac01c"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjE1MjAyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDowNjoyNVrOHcLDDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDozODo0NVrOHcgIaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwMzE4Mg==", "bodyText": "I think we can merge these two catch blocks? also it's probably better to move the LOG.error to LOG.debug and only log record data when such debug/tracing is enabled? This might just flood the logs.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499303182", "createdAt": "2020-10-05T00:06:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -254,6 +253,10 @@ public void write(GenericRecord oldRecord) {\n         LOG.error(\"Failed to merge old record into new file for key \" + key + \" from old file \" + getOldFilePath()\n             + \" to new file \" + newFilePath, e);\n         throw new HoodieUpsertException(errMsg, e);\n+      } catch (RuntimeException e) {\n+        LOG.error(\"Summary is \" + e.getMessage() + \", detail is schema mismatch when rewriting old record \" + oldRecord + \" from file \" + getOldFilePath()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0ODYxNw==", "bodyText": "make sense .done", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499648617", "createdAt": "2020-10-05T14:38:45Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -254,6 +253,10 @@ public void write(GenericRecord oldRecord) {\n         LOG.error(\"Failed to merge old record into new file for key \" + key + \" from old file \" + getOldFilePath()\n             + \" to new file \" + newFilePath, e);\n         throw new HoodieUpsertException(errMsg, e);\n+      } catch (RuntimeException e) {\n+        LOG.error(\"Summary is \" + e.getMessage() + \", detail is schema mismatch when rewriting old record \" + oldRecord + \" from file \" + getOldFilePath()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwMzE4Mg=="}, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjE1ODUzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNTo0NFrOHcLGYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNTo0NFrOHcLGYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDAzNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                jsc.parallelize(Arrays.asList(1)).map(x -> {\n          \n          \n            \n                jsc.parallelize(Arrays.asList(1)).foreach(x -> {", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304034", "createdAt": "2020-10-05T00:15:44Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 272}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjE1ODY4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNTo1NFrOHcLGdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNTo1NFrOHcLGdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDA1NA==", "bodyText": "we can reuse some code in this file by pulling the common structure into a helper function ?", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304054", "createdAt": "2020-10-05T00:15:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjE1OTE5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNjozNVrOHcLGuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNjozNVrOHcLGuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDEyMA==", "bodyText": "Default sealed is false. we could skip unsealing.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304120", "createdAt": "2020-10-05T00:16:35Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 280}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjE1OTcxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNzoyNlrOHcLG8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwMDoxNzoyNlrOHcLG8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDE3OQ==", "bodyText": "instead of creating a new Configuration, would it be better with table.getHadoopConf()?", "url": "https://github.com/apache/hudi/pull/2127#discussion_r499304179", "createdAt": "2020-10-05T00:17:26Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,232 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n     String fileId = insertResult.getFileId();\n \n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n       // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n           + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n       List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n       assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n         Configuration conf = new Configuration();\n         AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n         List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n       }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n           + \"exampleEvolvedSchema.txt\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    String fileId = insertResult.getFileId();\n \n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertDoesNotThrow(() -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaChangeOrder.txt as column order change\");\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+      assertThrows(InvalidRecordException.class, () -> {\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+      }, \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\");\n+      mergeHandle.close();\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      assertThrows(HoodieUpsertException.class, () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+            updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+        Configuration conf = new Configuration();\n+        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n+            new Path(config.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+        for (GenericRecord rec : oldRecords) {\n+          mergeHandle.write(rec);\n+        }\n+        mergeHandle.close();\n+      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+          + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\");\n+      return 1;\n+    }).collect();\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    String fileId = insertResult.getFileId();\n+\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      // New content with values for the newly added field\n+      String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+      List<HoodieRecord> updateRecords = new ArrayList<>();\n+      RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+      HoodieRecord record =\n+          new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+      record.unseal();\n+      record.setCurrentLocation(new HoodieRecordLocation(\"101\", fileId));\n+      record.seal();\n+      updateRecords.add(record);\n+      HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config, \"101\", table,\n+          updateRecords.iterator(), record.getPartitionPath(), fileId, supplier);\n+      Configuration conf = new Configuration();\n+      AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8852471a7cdef76ed1db2a4dc95b8239119356ac"}, "originalPosition": 287}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1ODM2ODUwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoxMzoyOVrOHg3mvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoxMzoyOVrOHg3mvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyNzUxNw==", "bodyText": "a old -> an old", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504227517", "createdAt": "2020-10-13T20:13:29Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1ODM3Njg5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoxNjowM1rOHg3rsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoxNjowM1rOHg3rsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIyODc4NQ==", "bodyText": "generateMultiRecordsForExampleSchema -> generateMultipleRecordsForExampleSchema?\n@lw309637554 I would leave it upto you to decide though.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504228785", "createdAt": "2020-10-13T20:16:03Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1ODQwNzYxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMTowOFrOHg3_IA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMTozN1rOHg4BHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzMzc2MA==", "bodyText": "oldrecords -> old records", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504233760", "createdAt": "2020-10-13T20:21:08Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, InvalidRecordException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 224}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNDI2OQ==", "bodyText": "exampleEvolvedSchemaColumnRequire.txt ,because -> exampleEvolvedSchemaColumnRequire.txt, because", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504234269", "createdAt": "2020-10-13T20:21:37Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, InvalidRecordException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzMzc2MA=="}, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1ODQxNDkxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMjoyM1rOHg4D7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMjoyM1rOHg4D7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNDk4OQ==", "bodyText": "delete column ,Parquet/Avro -> delete column, Parquet/Avro", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504234989", "createdAt": "2020-10-13T20:22:23Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 209}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1ODQyMDExOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMzoxNFrOHg4HdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyMzoxNFrOHg4HdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzNTg5Mg==", "bodyText": "type ,org. -> type, org.", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504235892", "createdAt": "2020-10-13T20:23:14Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestUpdateSchemaEvolution.java", "diffHunk": "@@ -68,77 +71,158 @@ public void tearDown() throws IOException {\n     cleanupResources();\n   }\n \n-  @Test\n-  public void testSchemaEvolutionOnUpdate() throws Exception {\n+  private WriteStatus prepareFirstRecordCommit(List<String> recordsStrs) throws IOException {\n     // Create a bunch of records with a old version of schema\n     final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleSchema.txt\");\n     final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n-\n     final List<WriteStatus> statuses = jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n-      String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n-      String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n       List<HoodieRecord> insertRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n-      RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n-      RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n-      insertRecords\n-          .add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n-\n+      for (String recordStr : recordsStrs) {\n+        RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+        insertRecords\n+            .add(new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange));\n+      }\n       Map<String, HoodieRecord> insertRecordMap = insertRecords.stream()\n           .collect(Collectors.toMap(r -> r.getRecordKey(), Function.identity()));\n       HoodieCreateHandle createHandle =\n-          new HoodieCreateHandle(config, \"100\", table, rowChange1.getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n+          new HoodieCreateHandle(config, \"100\", table, insertRecords.get(0).getPartitionPath(), \"f1-0\", insertRecordMap, supplier);\n       createHandle.write();\n       return createHandle.close();\n     }).collect();\n \n     final Path commitFile = new Path(config.getBasePath() + \"/.hoodie/\" + HoodieTimeline.makeCommitFileName(\"100\"));\n     FSUtils.getFs(basePath, HoodieTestUtils.getDefaultHadoopConf()).create(commitFile);\n+    return statuses.get(0);\n+  }\n \n-    // Now try an update with an evolved schema\n-    // Evolved schema does not have guarantee on preserving the original field ordering\n-    final HoodieWriteConfig config2 = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n-    final WriteStatus insertResult = statuses.get(0);\n-    String fileId = insertResult.getFileId();\n-\n-    final HoodieSparkTable table2 = HoodieSparkTable.create(config, context);\n-    assertEquals(1, jsc.parallelize(Arrays.asList(1)).map(x -> {\n-      // New content with values for the newly added field\n-      String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n-          + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n-      List<HoodieRecord> updateRecords = new ArrayList<>();\n-      RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n-      HoodieRecord record1 =\n-          new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n-      record1.unseal();\n-      record1.setCurrentLocation(new HoodieRecordLocation(\"100\", fileId));\n-      record1.seal();\n-      updateRecords.add(record1);\n-\n-      assertDoesNotThrow(() -> {\n-        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(config2, \"101\", table2,\n-            updateRecords.iterator(), record1.getPartitionPath(), fileId, supplier);\n-        Configuration conf = new Configuration();\n-        AvroReadSupport.setAvroReadSchema(conf, mergeHandle.getWriterSchemaWithMetafields());\n-        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(conf,\n-            new Path(config2.getBasePath() + \"/\" + insertResult.getStat().getPath()));\n+  private List<String> generateMultiRecordsForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr1);\n+    recordsStrs.add(recordStr2);\n+    recordsStrs.add(recordStr3);\n+    return recordsStrs;\n+  }\n+\n+  private List<String> generateOneRecordForExampleSchema() {\n+    List<String> recordsStrs = new ArrayList<>();\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    recordsStrs.add(recordStr);\n+    return recordsStrs;\n+  }\n+\n+  private void assertSchemaEvolutionOnUpdateResult(WriteStatus insertResult, HoodieSparkTable updateTable,\n+                                                   List<HoodieRecord> updateRecords, String assertMsg, boolean isAssertThrow, Class expectedExceptionType) {\n+    jsc.parallelize(Arrays.asList(1)).map(x -> {\n+      Executable executable = () -> {\n+        HoodieMergeHandle mergeHandle = new HoodieMergeHandle(updateTable.getConfig(), \"101\", updateTable,\n+            updateRecords.iterator(), updateRecords.get(0).getPartitionPath(), insertResult.getFileId(), supplier);\n+        AvroReadSupport.setAvroReadSchema(updateTable.getHadoopConf(), mergeHandle.getWriterSchemaWithMetafields());\n+        List<GenericRecord> oldRecords = ParquetUtils.readAvroRecords(updateTable.getHadoopConf(),\n+            new Path(updateTable.getConfig().getBasePath() + \"/\" + insertResult.getStat().getPath()));\n         for (GenericRecord rec : oldRecords) {\n           mergeHandle.write(rec);\n         }\n         mergeHandle.close();\n-      }, \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n-          + \"exampleEvolvedSchema.txt\");\n-\n+      };\n+      if (isAssertThrow) {\n+        assertThrows(expectedExceptionType, executable, assertMsg);\n+      } else {\n+        assertDoesNotThrow(executable, assertMsg);\n+      }\n       return 1;\n-    }).collect().size());\n+    }).collect();\n+  }\n+\n+  private List<HoodieRecord> buildUpdateRecords(String recordStr, String insertFileId) throws IOException {\n+    List<HoodieRecord> updateRecords = new ArrayList<>();\n+    RawTripTestPayload rowChange = new RawTripTestPayload(recordStr);\n+    HoodieRecord record =\n+        new HoodieRecord(new HoodieKey(rowChange.getRowKey(), rowChange.getPartitionPath()), rowChange);\n+    record.setCurrentLocation(new HoodieRecordLocation(\"101\", insertFileId));\n+    record.seal();\n+    updateRecords.add(record);\n+    return updateRecords;\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithAddColumnHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchema.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    // New content with values for the newly added field\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchema.txt\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateSuccessWithChangeColumnOrder() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateMultiRecordsForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaChangeOrder.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"added_field\\\":1},\\\"number\\\":12\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaChangeOrder.txt as column order change\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, false, null);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithDeleteColumn() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaDeleteColumn.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when delete column ,Parquet/Avro schema mismatch: Avro field 'xxx' not found\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, InvalidRecordException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithAddColumnNotHaveDefault() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnRequire.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12,\\\"added_field\\\":1}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction could not read records written with exampleSchema.txt using the \"\n+        + \"exampleEvolvedSchemaColumnRequire.txt ,because oldrecords do not have required column added_field\";\n+    assertSchemaEvolutionOnUpdateResult(insertResult, table, updateRecords, assertMsg, true, HoodieUpsertException.class);\n+  }\n+\n+  @Test\n+  public void testSchemaEvolutionOnUpdateMisMatchWithChangeColumnType() throws Exception {\n+    final WriteStatus insertResult = prepareFirstRecordCommit(generateOneRecordForExampleSchema());\n+    // Now try an update with an evolved schema\n+    // Evolved schema does not have guarantee on preserving the original field ordering\n+    final HoodieWriteConfig config = makeHoodieClientConfig(\"/exampleEvolvedSchemaColumnType.txt\");\n+    final HoodieSparkTable table = HoodieSparkTable.create(config, context);\n+    String recordStr = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":\\\"12\\\"}\";\n+    List<HoodieRecord> updateRecords = buildUpdateRecords(recordStr, insertResult.getFileId());\n+    String assertMsg = \"UpdateFunction when change column type ,org.apache.parquet.avro.AvroConverters$FieldUTF8Converter\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 238}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1ODQzNDQwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/test/resources/exampleEvolvedSchemaDeleteColumn.txt", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyNjo1MFrOHg4QMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMDoyNjo1MFrOHg4QMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDIzODEyOQ==", "bodyText": "can we make the schema files as .avsc? @lw309637554", "url": "https://github.com/apache/hudi/pull/2127#discussion_r504238129", "createdAt": "2020-10-13T20:26:50Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-client/hudi-client-common/src/test/resources/exampleEvolvedSchemaDeleteColumn.txt", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d637855f337aad14e59ff944068a31aa2529a5e"}, "originalPosition": 18}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4310, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}