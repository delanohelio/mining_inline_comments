{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEyNDI2MDc0", "number": 1584, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QxOTozMjowM1rOD4zkdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QxOTo1MToyMFrOD4zsNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODkxNzY1OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QxOTozMjowM1rOGPuzIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QxOTozMjowM1rOGPuzIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE0ODU3OA==", "bodyText": "So this is the line where you mentioned the exception is thrown for you. Am I correct?", "url": "https://github.com/apache/hudi/pull/1584#discussion_r419148578", "createdAt": "2020-05-03T19:32:03Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java", "diffHunk": "@@ -64,19 +64,22 @@ public SourceFormatAdapter(Source source) {\n       }\n       case ROW: {\n         InputBatch<Dataset<Row>> r = ((RowSource) source).fetchNext(lastCkptStr, sourceLimit);\n-        return new InputBatch<>(Option.ofNullable(r.getBatch().map(\n-            rdd -> (\n-                (r.getSchemaProvider() instanceof FilebasedSchemaProvider)\n-                    // If the source schema is specified through Avro schema,\n-                    // pass in the schema for the Row-to-Avro conversion\n-                    // to avoid nullability mismatch between Avro schema and Row schema\n-                    ? AvroConversionUtils.createRdd(\n-                        rdd, r.getSchemaProvider().getSourceSchema(),\n-                        HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                    : AvroConversionUtils.createRdd(\n-                        rdd, HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                ))\n-            .orElse(null)), r.getCheckpointForNextBatch(), r.getSchemaProvider());\n+        if (r.getBatch().isPresent()) {\n+          return new InputBatch<>(r.getBatch().map(\n+              rdd -> (\n+                  (r.getSchemaProvider() instanceof FilebasedSchemaProvider)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODkyMTI4OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QxOTozNToyM1rOGPu05A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMDo0NzoyM1rOGSzLMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE0OTAyOA==", "bodyText": "I think since this method tries to fetch data in avro format, pre specifying a schema provider is mandatory. So even if you do not get any data, you should mention RowBasedSchemaProvider as the schema provider in the very beginning. If that is done, there is no need to do this change I believe. :)\nDo you face issues after pre specifying schema provider?\nPlease let me know your thoughts on this.", "url": "https://github.com/apache/hudi/pull/1584#discussion_r419149028", "createdAt": "2020-05-03T19:35:23Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java", "diffHunk": "@@ -64,19 +64,22 @@ public SourceFormatAdapter(Source source) {\n       }\n       case ROW: {\n         InputBatch<Dataset<Row>> r = ((RowSource) source).fetchNext(lastCkptStr, sourceLimit);\n-        return new InputBatch<>(Option.ofNullable(r.getBatch().map(\n-            rdd -> (\n-                (r.getSchemaProvider() instanceof FilebasedSchemaProvider)\n-                    // If the source schema is specified through Avro schema,\n-                    // pass in the schema for the Row-to-Avro conversion\n-                    // to avoid nullability mismatch between Avro schema and Row schema\n-                    ? AvroConversionUtils.createRdd(\n-                        rdd, r.getSchemaProvider().getSourceSchema(),\n-                        HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                    : AvroConversionUtils.createRdd(\n-                        rdd, HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                ))\n-            .orElse(null)), r.getCheckpointForNextBatch(), r.getSchemaProvider());\n+        if (r.getBatch().isPresent()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE1MDE1MQ==", "bodyText": "If I understand correctly, not specifying schema provider should be feasible in case of row based sources when you try to fetch the data in row format (i.e when using transformers).", "url": "https://github.com/apache/hudi/pull/1584#discussion_r419150151", "createdAt": "2020-05-03T19:44:05Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java", "diffHunk": "@@ -64,19 +64,22 @@ public SourceFormatAdapter(Source source) {\n       }\n       case ROW: {\n         InputBatch<Dataset<Row>> r = ((RowSource) source).fetchNext(lastCkptStr, sourceLimit);\n-        return new InputBatch<>(Option.ofNullable(r.getBatch().map(\n-            rdd -> (\n-                (r.getSchemaProvider() instanceof FilebasedSchemaProvider)\n-                    // If the source schema is specified through Avro schema,\n-                    // pass in the schema for the Row-to-Avro conversion\n-                    // to avoid nullability mismatch between Avro schema and Row schema\n-                    ? AvroConversionUtils.createRdd(\n-                        rdd, r.getSchemaProvider().getSourceSchema(),\n-                        HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                    : AvroConversionUtils.createRdd(\n-                        rdd, HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                ))\n-            .orElse(null)), r.getCheckpointForNextBatch(), r.getSchemaProvider());\n+        if (r.getBatch().isPresent()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE0OTAyOA=="}, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTkxOTMzNw==", "bodyText": "If I understand correctly, not specifying schema provider should be feasible in case of row based sources when you try to fetch the data in row format (i.e when using transformers).\n\nhi @pratyakshsharma I don't quite get this point. I'm referring tofetchNewDataInAvroFormat() being called when transformer is not set\nhttps://github.com/apache/incubator-hudi/blob/14d4fea8339913c0df8ea829036a45a187c55208/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java#L305-L306\nin case of ROW type source and no new data, r.getSchemaProvider() will throw an exception.\nAn example use case: reading a parquet source (ROW format) without setting either transformer or schemaprovider class, it works when new data keeps coming. But an exception will be thrown when no new data detected, calling r.getSchemaProvider() in this line below will throw and ask user to set schemaprovider.\nhttps://github.com/apache/incubator-hudi/blob/d0ee95ed16de6c3568f575169cb993b9c10ced3d/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java#L79\nIf in this example we still require users to set schema provider (like the built-in RowBasedSchemaProvider), then it conflicts with ROW source getting implicit schema. It seems like a usability issue.\nCould you confirm this understanding is accurate please?", "url": "https://github.com/apache/hudi/pull/1584#discussion_r419919337", "createdAt": "2020-05-05T07:41:09Z", "author": {"login": "xushiyan"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java", "diffHunk": "@@ -64,19 +64,22 @@ public SourceFormatAdapter(Source source) {\n       }\n       case ROW: {\n         InputBatch<Dataset<Row>> r = ((RowSource) source).fetchNext(lastCkptStr, sourceLimit);\n-        return new InputBatch<>(Option.ofNullable(r.getBatch().map(\n-            rdd -> (\n-                (r.getSchemaProvider() instanceof FilebasedSchemaProvider)\n-                    // If the source schema is specified through Avro schema,\n-                    // pass in the schema for the Row-to-Avro conversion\n-                    // to avoid nullability mismatch between Avro schema and Row schema\n-                    ? AvroConversionUtils.createRdd(\n-                        rdd, r.getSchemaProvider().getSourceSchema(),\n-                        HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                    : AvroConversionUtils.createRdd(\n-                        rdd, HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                ))\n-            .orElse(null)), r.getCheckpointForNextBatch(), r.getSchemaProvider());\n+        if (r.getBatch().isPresent()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE0OTAyOA=="}, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NjAwMg==", "bodyText": "I'm referring tofetchNewDataInAvroFormat() being called when transformer is not set in case of ROW type source and no new data\n\nYeah I got that. I was actually thinking on the following lines - Avro relies on schemas. So the original thinking behind writing the function fetchNewDataInAvroFormat() might have been that schema provider should be pre specified.\n\nit conflicts with ROW source getting implicit schema. It seems like a usability issue.\n\nBut the above point is valid. I guess you are correct :)", "url": "https://github.com/apache/hudi/pull/1584#discussion_r422366002", "createdAt": "2020-05-08T20:47:23Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/SourceFormatAdapter.java", "diffHunk": "@@ -64,19 +64,22 @@ public SourceFormatAdapter(Source source) {\n       }\n       case ROW: {\n         InputBatch<Dataset<Row>> r = ((RowSource) source).fetchNext(lastCkptStr, sourceLimit);\n-        return new InputBatch<>(Option.ofNullable(r.getBatch().map(\n-            rdd -> (\n-                (r.getSchemaProvider() instanceof FilebasedSchemaProvider)\n-                    // If the source schema is specified through Avro schema,\n-                    // pass in the schema for the Row-to-Avro conversion\n-                    // to avoid nullability mismatch between Avro schema and Row schema\n-                    ? AvroConversionUtils.createRdd(\n-                        rdd, r.getSchemaProvider().getSourceSchema(),\n-                        HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                    : AvroConversionUtils.createRdd(\n-                        rdd, HOODIE_RECORD_STRUCT_NAME, HOODIE_RECORD_NAMESPACE).toJavaRDD()\n-                ))\n-            .orElse(null)), r.getCheckpointForNextBatch(), r.getSchemaProvider());\n+        if (r.getBatch().isPresent()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE0OTAyOA=="}, "originalCommit": null, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODkzNjU4OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QxOTo1MDoyMFrOGPu8SQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMDo0OTo1OVrOGSzP4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE1MDkyMQ==", "bodyText": "when fetching in row format, this change should not be needed since RowBasedSchemaProvider is already getting initialised at the end.", "url": "https://github.com/apache/hudi/pull/1584#discussion_r419150921", "createdAt": "2020-05-03T19:50:20Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -276,6 +276,8 @@ private void refreshTimeline() throws IOException {\n       // to generic records for writing\n       InputBatch<Dataset<Row>> dataAndCheckpoint =\n           formatAdapter.fetchNewDataInRowFormat(resumeCheckpointStr, cfg.sourceLimit);\n+      SchemaProvider schemaProviderFromFetched = dataAndCheckpoint.getBatch().isPresent()\n+          ? dataAndCheckpoint.getSchemaProvider() : null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjM2NzIwMQ==", "bodyText": "@xushiyan Here this change should not be needed. WDYT?", "url": "https://github.com/apache/hudi/pull/1584#discussion_r422367201", "createdAt": "2020-05-08T20:49:59Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -276,6 +276,8 @@ private void refreshTimeline() throws IOException {\n       // to generic records for writing\n       InputBatch<Dataset<Row>> dataAndCheckpoint =\n           formatAdapter.fetchNewDataInRowFormat(resumeCheckpointStr, cfg.sourceLimit);\n+      SchemaProvider schemaProviderFromFetched = dataAndCheckpoint.getBatch().isPresent()\n+          ? dataAndCheckpoint.getSchemaProvider() : null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE1MDkyMQ=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwODkzNzUwOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QxOTo1MToyMFrOGPu8uA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wM1QxOTo1MToyMFrOGPu8uA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE1MTAzMg==", "bodyText": "please refer to my other comment on the changes in SourceFormatAdapter.", "url": "https://github.com/apache/hudi/pull/1584#discussion_r419151032", "createdAt": "2020-05-03T19:51:20Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -298,15 +300,15 @@ private void refreshTimeline() throws IOException {\n       // default to RowBasedSchemaProvider\n       schemaProvider = this.schemaProvider == null || this.schemaProvider.getTargetSchema() == null\n           ? transformed.map(r -> (SchemaProvider) new RowBasedSchemaProvider(r.schema())).orElse(\n-          dataAndCheckpoint.getSchemaProvider())\n+          schemaProviderFromFetched)\n           : this.schemaProvider;\n     } else {\n       // Pull the data from the source & prepare the write\n       InputBatch<JavaRDD<GenericRecord>> dataAndCheckpoint =\n           formatAdapter.fetchNewDataInAvroFormat(resumeCheckpointStr, cfg.sourceLimit);\n       avroRDDOptional = dataAndCheckpoint.getBatch();\n       checkpointStr = dataAndCheckpoint.getCheckpointForNextBatch();\n-      schemaProvider = dataAndCheckpoint.getSchemaProvider();\n+      schemaProvider = avroRDDOptional.isPresent() ? dataAndCheckpoint.getSchemaProvider() : null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4614, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}