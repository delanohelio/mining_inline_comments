{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg4MTgyMDA0", "number": 1405, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQxNToxNjowMlrODoDySw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwOTowOToxMVrODoosrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzMxNjU5OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQxNToxNjowM1rOF2aInw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQxNzo1NjowNlrOF2az7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU5NTYxNQ==", "bodyText": "\"hudi\" would be constant?", "url": "https://github.com/apache/hudi/pull/1405#discussion_r392595615", "createdAt": "2020-03-14T15:16:03Z", "author": {"login": "leesf"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -113,21 +126,7 @@ public int export(SparkSession spark, Config cfg) throws IOException {\n         return -1;\n       }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjYwNjcwMw==", "bodyText": "Sure. Will fix this once #1404 merged, with explicit format checking.", "url": "https://github.com/apache/hudi/pull/1405#discussion_r392606703", "createdAt": "2020-03-14T17:56:06Z", "author": {"login": "xushiyan"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -113,21 +126,7 @@ public int export(SparkSession spark, Config cfg) throws IOException {\n         return -1;\n       }\n       if (!cfg.outputFormat.equalsIgnoreCase(\"hudi\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU5NTYxNQ=="}, "originalCommit": null, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMzMxNzgxOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQxNToxODoxOFrOF2aJQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQxNzo1NjoxMFrOF2az9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU5NTc3Nw==", "bodyText": "Also need UT for user-defined Partitioner?", "url": "https://github.com/apache/hudi/pull/1405#discussion_r392595777", "createdAt": "2020-03-14T15:18:18Z", "author": {"login": "leesf"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -47,36 +49,47 @@\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.execution.datasources.DataSource;\n \n-import scala.Tuple2;\n-import scala.collection.JavaConversions;\n-\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n  *\n  * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n public class HoodieSnapshotExporter {\n+\n+  @FunctionalInterface\n+  public interface Partitioner {\n+\n+    DataFrameWriter<Row> partition(Dataset<Row> source);\n+\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjYwNjcxMQ==", "bodyText": "Sure. Will fix this once #1404 merged.", "url": "https://github.com/apache/hudi/pull/1405#discussion_r392606711", "createdAt": "2020-03-14T17:56:10Z", "author": {"login": "xushiyan"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -47,36 +49,47 @@\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.execution.datasources.DataSource;\n \n-import scala.Tuple2;\n-import scala.collection.JavaConversions;\n-\n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n  *\n  * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n public class HoodieSnapshotExporter {\n+\n+  @FunctionalInterface\n+  public interface Partitioner {\n+\n+    DataFrameWriter<Row> partition(Dataset<Row> source);\n+\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU5NTc3Nw=="}, "originalCommit": null, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzOTM2NDI4OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwOTowOToxMVrOF3TYkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QxODowNjozNlrOF3oGLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzUzMzU4Ng==", "bodyText": "should we document this that only support three formats?", "url": "https://github.com/apache/hudi/pull/1405#discussion_r393533586", "createdAt": "2020-03-17T09:09:11Z", "author": {"login": "leesf"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -45,41 +50,65 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.execution.datasources.DataSource;\n-\n-import scala.Tuple2;\n-import scala.collection.JavaConversions;\n \n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n  *\n  * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n public class HoodieSnapshotExporter {\n+\n+  @FunctionalInterface\n+  public interface Partitioner {\n+\n+    DataFrameWriter<Row> partition(Dataset<Row> source);\n+\n+  }\n+\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n+  public static class OutputFormatValidator implements IValueValidator<String> {\n+\n+    static final String HUDI = \"hudi\";\n+    static final List<String> FORMATS = ImmutableList.of(\"json\", \"parquet\", HUDI);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzg3Mjk0Mw==", "bodyText": "@leesf Sure add some explanation in the CLI help line", "url": "https://github.com/apache/hudi/pull/1405#discussion_r393872943", "createdAt": "2020-03-17T18:06:36Z", "author": {"login": "xushiyan"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieSnapshotExporter.java", "diffHunk": "@@ -45,41 +50,65 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.execution.datasources.DataSource;\n-\n-import scala.Tuple2;\n-import scala.collection.JavaConversions;\n \n import java.io.IOException;\n import java.io.Serializable;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n \n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+\n /**\n  * Export the latest records of Hudi dataset to a set of external files (e.g., plain parquet files).\n  *\n  * @experimental This export is an experimental tool. If you want to export hudi to hudi, please use HoodieSnapshotCopier.\n  */\n public class HoodieSnapshotExporter {\n+\n+  @FunctionalInterface\n+  public interface Partitioner {\n+\n+    DataFrameWriter<Row> partition(Dataset<Row> source);\n+\n+  }\n+\n   private static final Logger LOG = LogManager.getLogger(HoodieSnapshotExporter.class);\n \n+  public static class OutputFormatValidator implements IValueValidator<String> {\n+\n+    static final String HUDI = \"hudi\";\n+    static final List<String> FORMATS = ImmutableList.of(\"json\", \"parquet\", HUDI);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzUzMzU4Ng=="}, "originalCommit": null, "originalPosition": 75}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4916, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}