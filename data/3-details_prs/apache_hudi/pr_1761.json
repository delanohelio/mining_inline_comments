{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM4ODc1NDM2", "number": 1761, "title": "[MINOR] Add documentation for using multi-column table keys and for n\u2026", "bodyText": "\u2026ot partitioning tables\nTips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nThis pull request adds documentation for creating multi-column table keys and non-partitioned tables and for snapshot querying of partitioned tables.\nBrief change log\n(for example:)\n\nAdd documentation for using multi-column table keys and for non-partitioned tables\n\nVerify this pull request\nThis pull request is a trivial rework / code cleanup without any test coverage.\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-06-23T23:27:04Z", "url": "https://github.com/apache/hudi/pull/1761", "merged": true, "mergeCommit": {"oid": "a7a92815d1ce83e77d13771b2174c5ec7d8558d8"}, "closed": true, "closedAt": "2020-07-02T11:01:30Z", "author": {"login": "afeldman1"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcvBECJAFqTQzODIwMTA5Mg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcw8vqugFqTQ0MTU3MTkzMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjAxMDky", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438201092", "createdAt": "2020-06-26T10:54:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDo1NDo0OVrOGpcl0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDo1NDo0OVrOGpcl0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ==", "bodyText": "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY should be changed to KEYGENERATOR_CLASS_OPT_KEY? and also when specify no partitioning tables, would use NonpartitionedKeyGenerator", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446113235", "createdAt": "2020-06-26T10:54:49Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjAzNDgw", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438203480", "createdAt": "2020-06-26T10:59:16Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDo1OToxNlrOGpcs7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDo1OToxNlrOGpcs7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ==", "bodyText": "hi, why this cannot change across writes? would you please clarify? IIRU, It would be changed across writes.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446115055", "createdAt": "2020-06-26T10:59:16Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjA0NDAx", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438204401", "createdAt": "2020-06-26T11:01:03Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMTowMTowM1rOGpcvnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMTowMTowM1rOGpcvnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTc0MA==", "bodyText": "TimestampBasedKeyGenerator and GlobalDeleteKeyGenerator are also available.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446115740", "createdAt": "2020-06-26T11:01:03Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjEwMDQ4", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438210048", "createdAt": "2020-06-26T11:10:57Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMDo1N1rOGpc_gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMDo1N1rOGpc_gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTgxMQ==", "bodyText": "would be changed to format(\"hudi\")", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446119811", "createdAt": "2020-06-26T11:10:57Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n+\n+\n+Example:\n+Upsert a DataFrame, specifying the necessary field names for `recordKey => _row_key`, `partitionPath => partition`, and `precombineKey => timestamp`\n \n ```java\n inputDF.write()\n        .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjEwMDky", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438210092", "createdAt": "2020-06-26T11:11:03Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMTowM1rOGpc_ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMTowM1rOGpc_ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTg0Mw==", "bodyText": "would be changed to format(\"hudi\")", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446119843", "createdAt": "2020-06-26T11:11:03Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n+\n+\n+Example:\n+Upsert a DataFrame, specifying the necessary field names for `recordKey => _row_key`, `partitionPath => partition`, and `precombineKey => timestamp`\n \n ```java\n inputDF.write()\n        .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjEwNTMz", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438210533", "createdAt": "2020-06-26T11:11:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMTo0NVrOGpdA2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMTo0NVrOGpdA2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMDE1NQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446120155", "createdAt": "2020-06-26T11:11:45Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjEyNjU4", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438212658", "createdAt": "2020-06-26T11:15:32Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxNTozMlrOGpdHWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxNTozMlrOGpdHWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA==", "bodyText": "when partition path is ${tablePath}/a/b/c, the load would be changed to tablePath + \"/*/*/*/*\" accordingly.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446121818", "createdAt": "2020-06-26T11:15:32Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")\n+     .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL())\n+     .load(tablePath + \"/*\") //Include \"/*\" at the end of the path if the table is partitioned", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MjE1NTk5", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438215599", "createdAt": "2020-06-26T11:20:32Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NTcwNjAx", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438570601", "createdAt": "2020-06-26T20:18:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQyMDoxODo0MFrOGpti0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQyMDoxODo0MFrOGpti0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM5MDk5Mw==", "bodyText": "Add clarity on current limitation\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            `classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n          \n          \n            \n            `classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName` (Non-partitioned tables can currently only have a single key column, [HUDI-1053](https://issues.apache.org/jira/browse/HUDI-1053)), `classOf[ComplexKeyGenerator].getName`", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446390993", "createdAt": "2020-06-26T20:18:40Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4NjgxODEw", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-438681810", "createdAt": "2020-06-27T08:05:11Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5MTM3MDEx", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-439137011", "createdAt": "2020-06-29T13:22:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMzoyMjozNVrOGqQr3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMzoyMjozNVrOGqQr3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA==", "bodyText": "hi, again the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY is not needed when not syncing to hive.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446966748", "createdAt": "2020-06-29T13:22:35Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5MTM4NTc1", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-439138575", "createdAt": "2020-06-29T13:24:19Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMzoyNDoxOVrOGqQwtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMzoyNDoxOVrOGqQwtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Nzk5MA==", "bodyText": "same to this section.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446967990", "createdAt": "2020-06-29T13:24:19Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to. Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark `SaveMode.Append` mode.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName` (Non-partitioned tables can currently only have a single key column, [HUDI-1053](https://issues.apache.org/jira/browse/HUDI-1053)), `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6768a13c9c1b0cce5caed5e2966c1b48d695ed63", "author": {"user": {"login": "afeldman1", "name": "Adam"}}, "url": "https://github.com/apache/hudi/commit/6768a13c9c1b0cce5caed5e2966c1b48d695ed63", "committedDate": "2020-06-29T23:28:12Z", "message": "[MINOR] Add documentation for using multi-column table keys and for not partitioning tables"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39cdafb67bd58bc1ca79d52a43fd00ade4262558", "author": {"user": {"login": "afeldman1", "name": "Adam"}}, "url": "https://github.com/apache/hudi/commit/39cdafb67bd58bc1ca79d52a43fd00ade4262558", "committedDate": "2020-06-29T23:28:12Z", "message": "Changes based on PR comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "39cdafb67bd58bc1ca79d52a43fd00ade4262558", "author": {"user": {"login": "afeldman1", "name": "Adam"}}, "url": "https://github.com/apache/hudi/commit/39cdafb67bd58bc1ca79d52a43fd00ade4262558", "committedDate": "2020-06-29T23:28:12Z", "message": "Changes based on PR comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd", "author": {"user": {"login": "afeldman1", "name": "Adam"}}, "url": "https://github.com/apache/hudi/commit/22b71470713c2e48f1b697f2682ea7d0b2e376cd", "committedDate": "2020-06-30T04:09:00Z", "message": "Fix snapshot query wildcard asterisks, clarify configs only required by hive, clarify deletion, and clean-up for consistency"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5ODk2OTg3", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-439896987", "createdAt": "2020-06-30T10:48:47Z", "commit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMDo0ODo0N1rOGq2zew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMDo0ODo0N1rOGq2zew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTI5MQ==", "bodyText": "hi, would we keep the partitionpath as it is?", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447591291", "createdAt": "2020-06-30T10:48:47Z", "author": {"login": "leesf"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -117,7 +117,7 @@ df.write.format(\"hudi\").\n   options(getQuickstartWriteConfigs).\n   option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n   option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partition_path\").", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5ODk5MTI2", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-439899126", "createdAt": "2020-06-30T10:52:00Z", "commit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMDo1MjowMFrOGq26Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMDo1MjowMFrOGq26Rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzAzMA==", "bodyText": "great.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447593030", "createdAt": "2020-06-30T10:52:00Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -220,9 +253,16 @@ For more info refer to [Delete support in Hudi](https://cwiki.apache.org/conflue\n \n  - **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. \n  This can be simply achieved by ensuring the appropriate fields are nullable in the table schema and simply upserting the table after setting these fields to null.\n- - **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the table. This can be achieved by issuing an upsert with a custom payload implementation\n- via either DataSource or DeltaStreamer which always returns Optional.Empty as the combined value. Hudi ships with a built-in `org.apache.hudi.EmptyHoodieRecordPayload` class that does exactly this.\n  \n+ - **Hard Deletes** : A stronger form of deletion is to physically remove any trace of the record from the table. This can be achieved in 3 different ways.\n+\n+   1) Using DataSource, set `OPERATION_OPT_KEY` to `DELETE_OPERATION_OPT_VAL`. This will remove all records in the DataSet being submitted.\n+   \n+   2) Using DataSource, set `PAYLOAD_CLASS_OPT_KEY` to `\"org.apache.hudi.EmptyHoodieRecordPayload\"`. This will remove all records in the DataSet being submitted. \n+   \n+   3) Using DataSource or DeltaStreamer, add a column named `_hoodie_is_deleted` to DataSet. The value of this column must be set to `true` for all records to be deleted and either `false` or left null for any records to be upserted.\n+    ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd"}, "originalPosition": 78}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4fa23993eee8423e948f755e015dcc48cdd44bd1", "author": {"user": {"login": "afeldman1", "name": "Adam"}}, "url": "https://github.com/apache/hudi/commit/4fa23993eee8423e948f755e015dcc48cdd44bd1", "committedDate": "2020-07-01T16:33:43Z", "message": "Standardize  to  and clarify wording"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxNTcxOTMw", "url": "https://github.com/apache/hudi/pull/1761#pullrequestreview-441571930", "createdAt": "2020-07-02T11:00:49Z", "commit": {"oid": "4fa23993eee8423e948f755e015dcc48cdd44bd1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2852, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}