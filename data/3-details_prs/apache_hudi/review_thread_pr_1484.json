{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk4MzY2ODYz", "number": 1484, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNFQwMDo0Nzo1OFrODuonLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOToxMDozNVrOD4dSSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwMjI2NDc5OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNFQwMDo0Nzo1OFrOGAtf0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQxNjo1ODowOVrOGTlCuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODYxMQ==", "bodyText": "Could we please avoid using this? Should not be too hard to roll our own..", "url": "https://github.com/apache/hudi/pull/1484#discussion_r403398611", "createdAt": "2020-04-04T00:47:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/*\n+ * Note: Based on RateLimiter implementation in Google/Guava.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzE4MzAzNQ==", "bodyText": "@vinothchandar Re implemented for most part and stripped unnecessary methods of RateLimiter. And removed Ticker class as well (that was from Google/Guava).", "url": "https://github.com/apache/hudi/pull/1484#discussion_r423183035", "createdAt": "2020-05-11T16:58:09Z", "author": {"login": "v3nkatesh"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/*\n+ * Note: Based on RateLimiter implementation in Google/Guava.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODYxMQ=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTQyNzc2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMjo1NToxMVrOGE4ELw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjoxNzowOVrOGPAU-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjA2Mw==", "bodyText": "Do you need this map as instance variable? Looks like there is one HBaseIndex object per client. We don't seem to be clearing entries from this map also.  So, over time, this map can get pretty large and can cause increased memory utilization? Please correct me if I'm misreading.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407766063", "createdAt": "2020-04-13T22:55:11Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private int maxPutsPerSec;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;\n+  Map<String, Integer> fileIdPartitionMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4NzE5Mg==", "bodyText": "Yeah we don't need a class variable for this. Moved it inside updateLocation() method.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418387192", "createdAt": "2020-05-01T02:17:09Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private int maxPutsPerSec;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;\n+  Map<String, Integer> fileIdPartitionMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjA2Mw=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTQzMDA0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMjo1NjoyNFrOGE4FpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjoxOTozMFrOGPAWxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjQzNg==", "bodyText": "I dont see how this is used. could you please add a comment for all these instance variables? It seems like they can be local variables specific to the operation being performed.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407766436", "createdAt": "2020-04-13T22:56:24Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private int maxPutsPerSec;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4NzY1Mg==", "bodyText": "Yeah can be removed actually, I am anyway recalculating this inside getBatchSize(). Also cleaned up other variables that are not used.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418387652", "createdAt": "2020-05-01T02:19:30Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private int maxPutsPerSec;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjQzNg=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTY5MDYxOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTowMzo1NlrOGE6eQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjozMDozNVrOGPAfKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNTUwNg==", "bodyText": "Can you help me understand this code? why do we need to force trigger here? Is this just to releaseQPSResources? releaseQPSResources seems to be doing nothing (at least default implementation, are there other implementations outside hoodie?). Is it really important to release here as opposed to doing it in 'close()' (earlier behavior)?", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407805506", "createdAt": "2020-04-14T01:03:56Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n+    JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n+        true);\n     // caching the index updated status RDD\n     writeStatusJavaRDD = writeStatusJavaRDD.persist(SparkConfigUtils.getWriteStatusStorageLevel(config.getProps()));\n+    // force trigger update location(hbase puts)\n+    writeStatusJavaRDD.count();\n+    this.hBaseIndexQPSResourceAllocator.releaseQPSResources();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM4OTgwMg==", "bodyText": "Correct, releaseQPSResources() has implementation outside of hoodie. This ensures that we release any resources acquired for HBase operations are released right after the spark stage is done (i.e. because of forced spark action)", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418389802", "createdAt": "2020-05-01T02:30:35Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n+    JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n+        true);\n     // caching the index updated status RDD\n     writeStatusJavaRDD = writeStatusJavaRDD.persist(SparkConfigUtils.getWriteStatusStorageLevel(config.getProps()));\n+    // force trigger update location(hbase puts)\n+    writeStatusJavaRDD.count();\n+    this.hBaseIndexQPSResourceAllocator.releaseQPSResources();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNTUwNg=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTcwNjc1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMToxMjo0NlrOGE6nuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjozNDozNlrOGPAiWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNzkyOQ==", "bodyText": "Not directly related to your change, so feel free to ignore this comment. But hBaseIndexQPSResourceAllocator  is instance variable. why is this again passed as argument. This seems like a consistent pattern in this class. Because we are also using exact same name for local variable, it masks instance variable and can become easily error prone if the two variables evolve to mean different things.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407807929", "createdAt": "2020-04-14T01:12:46Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n+    JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n+        true);\n     // caching the index updated status RDD\n     writeStatusJavaRDD = writeStatusJavaRDD.persist(SparkConfigUtils.getWriteStatusStorageLevel(config.getProps()));\n+    // force trigger update location(hbase puts)\n+    writeStatusJavaRDD.count();\n+    this.hBaseIndexQPSResourceAllocator.releaseQPSResources();\n     return writeStatusJavaRDD;\n   }\n \n-  private void setPutBatchSize(JavaRDD<WriteStatus> writeStatusRDD,\n-      HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator, final JavaSparkContext jsc) {\n+  private Option<Float> calculateQPSFraction(JavaRDD<WriteStatus> writeStatusRDD,\n+                                               HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MDYxOQ==", "bodyText": "Yeah I think it makes sense to refactor it here. Let me update other parts of this class where hBaseIndexQPSResourceAllocator is passed around.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418390619", "createdAt": "2020-05-01T02:34:36Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n+    JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n+        true);\n     // caching the index updated status RDD\n     writeStatusJavaRDD = writeStatusJavaRDD.persist(SparkConfigUtils.getWriteStatusStorageLevel(config.getProps()));\n+    // force trigger update location(hbase puts)\n+    writeStatusJavaRDD.count();\n+    this.hBaseIndexQPSResourceAllocator.releaseQPSResources();\n     return writeStatusJavaRDD;\n   }\n \n-  private void setPutBatchSize(JavaRDD<WriteStatus> writeStatusRDD,\n-      HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator, final JavaSparkContext jsc) {\n+  private Option<Float> calculateQPSFraction(JavaRDD<WriteStatus> writeStatusRDD,\n+                                               HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNzkyOQ=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTczOTU2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTozMDoyNFrOGE665Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjozNjo1NFrOGPAkIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxMjgzNw==", "bodyText": "Can you share the context on why we created HBaseIndexQPSResourceAllocator?  Do you think calls to RateLimiter#acquire can be made inside HBaseIndexQPSResourceAllocator#acquireQPSResources to simplify?", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407812837", "createdAt": "2020-04-14T01:30:24Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MTA3Mg==", "bodyText": "Synced offline to go over use-case outside of hoodie and why we need to rate limit here.\nJust to summarize, we need to rate limit here because the actual hbase operations are handled here. And HBaseIndexQPSResourceAllocator#acquireQPSResources is mostly meant to manage metadata like checking for available resources before an operation, releasing meta resources etc.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418391072", "createdAt": "2020-05-01T02:36:54Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxMjgzNw=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTc1NTQzOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTozOTowNVrOGE7EJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOTowNjowM1rOGPS16g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTIwNA==", "bodyText": "could you return the time spent waiting here? I think adding metrics on time taken is very important for debugging any potential performance issues. Also, would be useful to log if time taken is greater than some threshold (say, 300ms?)", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407815204", "createdAt": "2020-04-14T01:39:05Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/*\n+ * Note: Based on RateLimiter implementation in Google/Guava.\n+ *         - adopted from com.google.common.util.concurrent\n+ *           Copyright (C) 2012 The Guava Authors\n+ *           Home page: https://github.com/google/guava\n+ *           License: http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+@ThreadSafe\n+public abstract class RateLimiter {\n+  private final RateLimiter.SleepingTicker ticker;\n+  private final long offsetNanos;\n+  double storedPermits;\n+  double maxPermits;\n+  volatile double stableIntervalMicros;\n+  private final Object mutex;\n+  private long nextFreeTicketMicros;\n+\n+  public static RateLimiter create(double permitsPerSecond) {\n+    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond);\n+  }\n+\n+  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond) {\n+    RateLimiter rateLimiter = new RateLimiter.Bursty(ticker, 1.0D);\n+    rateLimiter.setRate(permitsPerSecond);\n+    return rateLimiter;\n+  }\n+\n+  public static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n+    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond, warmupPeriod, unit);\n+  }\n+\n+  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n+    RateLimiter rateLimiter = new RateLimiter.WarmingUp(ticker, warmupPeriod, unit);\n+    rateLimiter.setRate(permitsPerSecond);\n+    return rateLimiter;\n+  }\n+\n+  private RateLimiter(RateLimiter.SleepingTicker ticker) {\n+    this.mutex = new Object();\n+    this.nextFreeTicketMicros = 0L;\n+    this.ticker = ticker;\n+    this.offsetNanos = ticker.read();\n+  }\n+\n+  public final void setRate(double permitsPerSecond) {\n+    checkArgument(permitsPerSecond > 0.0D && !Double.isNaN(permitsPerSecond), \"rate must be positive\");\n+    Object var3 = this.mutex;\n+    synchronized (this.mutex) {\n+      this.resync(this.readSafeMicros());\n+      double stableIntervalMicros = (double)TimeUnit.SECONDS.toMicros(1L) / permitsPerSecond;\n+      this.stableIntervalMicros = stableIntervalMicros;\n+      this.doSetRate(permitsPerSecond, stableIntervalMicros);\n+    }\n+  }\n+\n+  abstract void doSetRate(double var1, double var3);\n+\n+  public final double getRate() {\n+    return (double)TimeUnit.SECONDS.toMicros(1L) / this.stableIntervalMicros;\n+  }\n+\n+  public void acquire() {\n+    this.acquire(1);\n+  }\n+\n+  public void acquire(int permits) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MDUzOA==", "bodyText": "any chance we can add these as metrics/logs?", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418690538", "createdAt": "2020-05-01T19:06:03Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/*\n+ * Note: Based on RateLimiter implementation in Google/Guava.\n+ *         - adopted from com.google.common.util.concurrent\n+ *           Copyright (C) 2012 The Guava Authors\n+ *           Home page: https://github.com/google/guava\n+ *           License: http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+@ThreadSafe\n+public abstract class RateLimiter {\n+  private final RateLimiter.SleepingTicker ticker;\n+  private final long offsetNanos;\n+  double storedPermits;\n+  double maxPermits;\n+  volatile double stableIntervalMicros;\n+  private final Object mutex;\n+  private long nextFreeTicketMicros;\n+\n+  public static RateLimiter create(double permitsPerSecond) {\n+    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond);\n+  }\n+\n+  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond) {\n+    RateLimiter rateLimiter = new RateLimiter.Bursty(ticker, 1.0D);\n+    rateLimiter.setRate(permitsPerSecond);\n+    return rateLimiter;\n+  }\n+\n+  public static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n+    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond, warmupPeriod, unit);\n+  }\n+\n+  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n+    RateLimiter rateLimiter = new RateLimiter.WarmingUp(ticker, warmupPeriod, unit);\n+    rateLimiter.setRate(permitsPerSecond);\n+    return rateLimiter;\n+  }\n+\n+  private RateLimiter(RateLimiter.SleepingTicker ticker) {\n+    this.mutex = new Object();\n+    this.nextFreeTicketMicros = 0L;\n+    this.ticker = ticker;\n+    this.offsetNanos = ticker.read();\n+  }\n+\n+  public final void setRate(double permitsPerSecond) {\n+    checkArgument(permitsPerSecond > 0.0D && !Double.isNaN(permitsPerSecond), \"rate must be positive\");\n+    Object var3 = this.mutex;\n+    synchronized (this.mutex) {\n+      this.resync(this.readSafeMicros());\n+      double stableIntervalMicros = (double)TimeUnit.SECONDS.toMicros(1L) / permitsPerSecond;\n+      this.stableIntervalMicros = stableIntervalMicros;\n+      this.doSetRate(permitsPerSecond, stableIntervalMicros);\n+    }\n+  }\n+\n+  abstract void doSetRate(double var1, double var3);\n+\n+  public final double getRate() {\n+    return (double)TimeUnit.SECONDS.toMicros(1L) / this.stableIntervalMicros;\n+  }\n+\n+  public void acquire() {\n+    this.acquire(1);\n+  }\n+\n+  public void acquire(int permits) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTIwNA=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMjExMjMwOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwNTowMToyNlrOGE-TPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjozNzo0M1rOGPAkpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2ODIyMw==", "bodyText": "Could you please make this a static class if its not using any instance variables of outer class", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407868223", "createdAt": "2020-04-14T05:01:26Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -498,4 +554,37 @@ public boolean isImplicitWithStorage() {\n   public void setHbaseConnection(Connection hbaseConnection) {\n     HBaseIndex.hbaseConnection = hbaseConnection;\n   }\n+\n+  /**\n+   * Partitions each WriteStatus with inserts into a unique single partition. WriteStatus without inserts will be\n+   * assigned to random partitions. This partitioner will be useful to utilize max parallelism with spark operations\n+   * that are based on inserts in each WriteStatus.\n+   */\n+  public class WriteStatusPartitioner extends Partitioner {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 331}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MTIwNA==", "bodyText": "done.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418391204", "createdAt": "2020-05-01T02:37:43Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -498,4 +554,37 @@ public boolean isImplicitWithStorage() {\n   public void setHbaseConnection(Connection hbaseConnection) {\n     HBaseIndex.hbaseConnection = hbaseConnection;\n   }\n+\n+  /**\n+   * Partitions each WriteStatus with inserts into a unique single partition. WriteStatus without inserts will be\n+   * assigned to random partitions. This partitioner will be useful to utilize max parallelism with spark operations\n+   * that are based on inserts in each WriteStatus.\n+   */\n+  public class WriteStatusPartitioner extends Partitioner {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2ODIyMw=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 331}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMjEyMTM5OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwNTowNjo0N1rOGE-YfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjozOToyNVrOGPAmCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2OTU2NQ==", "bodyText": "nit: looks like this is logged in the above method call too. so i think this can be removed.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407869565", "createdAt": "2020-04-14T05:06:47Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MTU2Mg==", "bodyText": "done.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418391562", "createdAt": "2020-05-01T02:39:25Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2OTU2NQ=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 191}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMjM2NTMwOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwNjo1NToxNVrOGFAmog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMjo0MTowNVrOGPAnLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzkwNTk1NA==", "bodyText": "Consider redoing this logic, because if this.numWriteStatusWithInserts == 0 , we still go through the process of generating fileIdPartitionMap which is not ideal.\nAlso, curious, if you did any performance measurements before and after this change. It is worth highlighting in release notes if this improvement is significant", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407905954", "createdAt": "2020-04-14T06:55:15Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5MTg1NQ==", "bodyText": "When this.numWriteStatusWithInserts == 0, fileIds will be empty. So fileIdPartitionMap will also be empty in this case.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418391855", "createdAt": "2020-05-01T02:41:05Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzkwNTk1NA=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNTAzMDM2OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxNzo1NzoyN1rOGFabIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMzowODozNFrOGPA8dA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMyODk5NA==", "bodyText": "nit: can we also move hTable.get(keys) inside this if?  do we need to invoke hTable.get if keys is empty?", "url": "https://github.com/apache/hudi/pull/1484#discussion_r408328994", "createdAt": "2020-04-14T17:57:27Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -252,8 +263,10 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n     };\n   }\n \n-  private Result[] doGet(HTable hTable, List<Get> keys) throws IOException {\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n+  private Result[] doGet(HTable hTable, List<Get> keys, RateLimiter limiter) throws IOException {\n+    if (keys.size() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODM5NzMwMA==", "bodyText": "Invoking hTable.get on empty keys is returning an empty Result array. But anyway, I changed it to explicitly return empty array now.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418397300", "createdAt": "2020-05-01T03:08:34Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -252,8 +263,10 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n     };\n   }\n \n-  private Result[] doGet(HTable hTable, List<Get> keys) throws IOException {\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n+  private Result[] doGet(HTable hTable, List<Get> keys, RateLimiter limiter) throws IOException {\n+    if (keys.size() > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMyODk5NA=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzNTEyODQzOnYy", "diffSide": "RIGHT", "path": "hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxODoyNDoyNFrOGFbZrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwMzozNjozMVrOGPBQVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM0NTAwNw==", "bodyText": "lot of code in this test seems like repetition from source code. consider refactoring this part into a library to reuse in tests if needed", "url": "https://github.com/apache/hudi/pull/1484#discussion_r408345007", "createdAt": "2020-04-14T18:24:24Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java", "diffHunk": "@@ -329,47 +332,140 @@ public void testPutBatchSizeCalculation() {\n     // All asserts cases below are derived out of the first\n     // example below, with change in one parameter at a time.\n \n-    int putBatchSize = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 100, 0.1f);\n-    // Expected batchSize is 8 because in that case, total request sent in one second is below\n-    // 8 (batchSize) * 200 (parallelism) * 10 (maxReqsInOneSecond) * 10 (numRegionServers) * 0.1 (qpsFraction)) => 16000\n-    // We assume requests get distributed to Region Servers uniformly, so each RS gets 1600 request\n-    // 1600 happens to be 10% of 16667 (maxQPSPerRegionServer) as expected.\n-    Assert.assertEquals(putBatchSize, 8);\n+    int putBatchSize = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 0.1f);\n+    // Total puts that can be sent  in 1 second = (10 * 16667 * 0.1) = 16,667\n+    // Total puts per batch will be (16,667 / parallelism) = 83.335, where 200 is the maxExecutors\n+    Assert.assertEquals(putBatchSize, 83);\n \n     // Number of Region Servers are halved, total requests sent in a second are also halved, so batchSize is also halved\n-    int putBatchSize2 = batchSizeCalculator.getBatchSize(5, 16667, 1200, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize2, 4);\n+    int putBatchSize2 = batchSizeCalculator.getBatchSize(5, 16667, 1200, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize2, 41);\n \n     // If the parallelism is halved, batchSize has to double\n-    int putBatchSize3 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 100, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize3, 16);\n+    int putBatchSize3 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 100, 0.1f);\n+    Assert.assertEquals(putBatchSize3, 166);\n \n     // If the parallelism is halved, batchSize has to double.\n     // This time parallelism is driven by numTasks rather than numExecutors\n-    int putBatchSize4 = batchSizeCalculator.getBatchSize(10, 16667, 100, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize4, 16);\n+    int putBatchSize4 = batchSizeCalculator.getBatchSize(10, 16667, 100, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize4, 166);\n \n     // If sleepTimeMs is halved, batchSize has to halve\n-    int putBatchSize5 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 100, 0.05f);\n-    Assert.assertEquals(putBatchSize5, 4);\n+    int putBatchSize5 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 0.05f);\n+    Assert.assertEquals(putBatchSize5, 41);\n \n     // If maxQPSPerRegionServer is doubled, batchSize also doubles\n-    int putBatchSize6 = batchSizeCalculator.getBatchSize(10, 33334, 1200, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize6, 16);\n+    int putBatchSize6 = batchSizeCalculator.getBatchSize(10, 33334, 1200, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize6, 166);\n   }\n \n   @Test\n   public void testsHBasePutAccessParallelism() {\n     HoodieWriteConfig config = getConfig();\n     HBaseIndex index = new HBaseIndex(config);\n     final JavaRDD<WriteStatus> writeStatusRDD = jsc.parallelize(\n-        Arrays.asList(getSampleWriteStatus(1, 2), getSampleWriteStatus(0, 3), getSampleWriteStatus(10, 0)), 10);\n+        Arrays.asList(\n+          getSampleWriteStatus(0, 2),\n+          getSampleWriteStatus(2, 3),\n+          getSampleWriteStatus(4, 3),\n+          getSampleWriteStatus(6, 3),\n+          getSampleWriteStatus(8, 0)),\n+        10);\n     final Tuple2<Long, Integer> tuple = index.getHBasePutAccessParallelism(writeStatusRDD);\n     final int hbasePutAccessParallelism = Integer.parseInt(tuple._2.toString());\n     final int hbaseNumPuts = Integer.parseInt(tuple._1.toString());\n     Assert.assertEquals(10, writeStatusRDD.getNumPartitions());\n-    Assert.assertEquals(2, hbasePutAccessParallelism);\n-    Assert.assertEquals(11, hbaseNumPuts);\n+    Assert.assertEquals(4, hbasePutAccessParallelism);\n+    Assert.assertEquals(20, hbaseNumPuts);\n+  }\n+\n+  @Test\n+  public void testsWriteStatusPartitioner() {\n+    HoodieWriteConfig config = getConfig();\n+    HBaseIndex index = new HBaseIndex(config);\n+    int parallelism = 4;\n+    final JavaRDD<WriteStatus> writeStatusRDD = jsc.parallelize(\n+        Arrays.asList(\n+          getSampleWriteStatusWithFileId(0, 2),\n+          getSampleWriteStatusWithFileId(2, 3),\n+          getSampleWriteStatusWithFileId(4, 3),\n+          getSampleWriteStatusWithFileId(0, 3),\n+          getSampleWriteStatusWithFileId(11, 0)), parallelism);\n+    int partitionIndex = 0;\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQwMjM5MA==", "bodyText": "Sure, I have moved this logic inside HBaseIndex to a separate method instead of a different class, let me know. Using this util method from tests.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418402390", "createdAt": "2020-05-01T03:36:31Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java", "diffHunk": "@@ -329,47 +332,140 @@ public void testPutBatchSizeCalculation() {\n     // All asserts cases below are derived out of the first\n     // example below, with change in one parameter at a time.\n \n-    int putBatchSize = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 100, 0.1f);\n-    // Expected batchSize is 8 because in that case, total request sent in one second is below\n-    // 8 (batchSize) * 200 (parallelism) * 10 (maxReqsInOneSecond) * 10 (numRegionServers) * 0.1 (qpsFraction)) => 16000\n-    // We assume requests get distributed to Region Servers uniformly, so each RS gets 1600 request\n-    // 1600 happens to be 10% of 16667 (maxQPSPerRegionServer) as expected.\n-    Assert.assertEquals(putBatchSize, 8);\n+    int putBatchSize = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 0.1f);\n+    // Total puts that can be sent  in 1 second = (10 * 16667 * 0.1) = 16,667\n+    // Total puts per batch will be (16,667 / parallelism) = 83.335, where 200 is the maxExecutors\n+    Assert.assertEquals(putBatchSize, 83);\n \n     // Number of Region Servers are halved, total requests sent in a second are also halved, so batchSize is also halved\n-    int putBatchSize2 = batchSizeCalculator.getBatchSize(5, 16667, 1200, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize2, 4);\n+    int putBatchSize2 = batchSizeCalculator.getBatchSize(5, 16667, 1200, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize2, 41);\n \n     // If the parallelism is halved, batchSize has to double\n-    int putBatchSize3 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 100, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize3, 16);\n+    int putBatchSize3 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 100, 0.1f);\n+    Assert.assertEquals(putBatchSize3, 166);\n \n     // If the parallelism is halved, batchSize has to double.\n     // This time parallelism is driven by numTasks rather than numExecutors\n-    int putBatchSize4 = batchSizeCalculator.getBatchSize(10, 16667, 100, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize4, 16);\n+    int putBatchSize4 = batchSizeCalculator.getBatchSize(10, 16667, 100, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize4, 166);\n \n     // If sleepTimeMs is halved, batchSize has to halve\n-    int putBatchSize5 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 100, 0.05f);\n-    Assert.assertEquals(putBatchSize5, 4);\n+    int putBatchSize5 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 0.05f);\n+    Assert.assertEquals(putBatchSize5, 41);\n \n     // If maxQPSPerRegionServer is doubled, batchSize also doubles\n-    int putBatchSize6 = batchSizeCalculator.getBatchSize(10, 33334, 1200, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize6, 16);\n+    int putBatchSize6 = batchSizeCalculator.getBatchSize(10, 33334, 1200, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize6, 166);\n   }\n \n   @Test\n   public void testsHBasePutAccessParallelism() {\n     HoodieWriteConfig config = getConfig();\n     HBaseIndex index = new HBaseIndex(config);\n     final JavaRDD<WriteStatus> writeStatusRDD = jsc.parallelize(\n-        Arrays.asList(getSampleWriteStatus(1, 2), getSampleWriteStatus(0, 3), getSampleWriteStatus(10, 0)), 10);\n+        Arrays.asList(\n+          getSampleWriteStatus(0, 2),\n+          getSampleWriteStatus(2, 3),\n+          getSampleWriteStatus(4, 3),\n+          getSampleWriteStatus(6, 3),\n+          getSampleWriteStatus(8, 0)),\n+        10);\n     final Tuple2<Long, Integer> tuple = index.getHBasePutAccessParallelism(writeStatusRDD);\n     final int hbasePutAccessParallelism = Integer.parseInt(tuple._2.toString());\n     final int hbaseNumPuts = Integer.parseInt(tuple._1.toString());\n     Assert.assertEquals(10, writeStatusRDD.getNumPartitions());\n-    Assert.assertEquals(2, hbasePutAccessParallelism);\n-    Assert.assertEquals(11, hbaseNumPuts);\n+    Assert.assertEquals(4, hbasePutAccessParallelism);\n+    Assert.assertEquals(20, hbaseNumPuts);\n+  }\n+\n+  @Test\n+  public void testsWriteStatusPartitioner() {\n+    HoodieWriteConfig config = getConfig();\n+    HBaseIndex index = new HBaseIndex(config);\n+    int parallelism = 4;\n+    final JavaRDD<WriteStatus> writeStatusRDD = jsc.parallelize(\n+        Arrays.asList(\n+          getSampleWriteStatusWithFileId(0, 2),\n+          getSampleWriteStatusWithFileId(2, 3),\n+          getSampleWriteStatusWithFileId(4, 3),\n+          getSampleWriteStatusWithFileId(0, 3),\n+          getSampleWriteStatusWithFileId(11, 0)), parallelism);\n+    int partitionIndex = 0;\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM0NTAwNw=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1ODExMjY4OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMzowMzozNFrOGIrLNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwMDoxMTozNVrOGVyRjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA==", "bodyText": "another question, what is the typical latency of these mutate operations? If time taken here combined with time taken to collect 'multiPutBatchSize' is > 1 second, then it seems like limiter would generate enough tokens for next run and would not wait at all.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r411749174", "createdAt": "2020-04-20T23:03:34Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQyNTU5MQ==", "bodyText": "Because of the synchronous nature, we will not acquire more permits until the operation is done even if it takes more than a second.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418425591", "createdAt": "2020-05-01T05:43:59Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5Njg4OQ==", "bodyText": "example to help clarify what i meant:\nlets say, mutator.mutate() + flush +clear takes 2 seconds as minimum. limiter.acquire would never wait because it generates mutations.size() tokens every second. So we would never wait. looks like this is expected and we dont see it as a problem. So I'm fine with it. (if possible, having metrics on per operation wait time would help us debug any potential issues)", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418696889", "createdAt": "2020-05-01T19:21:46Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ5Njk3Mw==", "bodyText": "I think we synced offline for this, but let me try to summarize.\nToken will be acquired only after previous batch is done, so it won't flood the system or over-utilize the cluster as planned. Though the side effect is hbase operation running slower than intended. Yes metrics on operation will be useful, will create follow up ticket.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r425496973", "createdAt": "2020-05-15T00:11:35Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA=="}, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 156}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTI2NjY0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOToxMDozNVrOGPS9Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwMDoxNjoyOVrOGVyWgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MjM5MQ==", "bodyText": "these two also seem like related to the operation being performed and not really need to be instance variables. If we can find a way to move them to local variables, that would make it cleaner.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418692391", "createdAt": "2020-05-01T19:10:35Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,14 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f603732e0bb3727170a0baf7ef9f60e2d8049f0"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ4OTczNQ==", "bodyText": "@v3nkatesh can you address or respond to this comment ?", "url": "https://github.com/apache/hudi/pull/1484#discussion_r425489735", "createdAt": "2020-05-14T23:45:52Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,14 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MjM5MQ=="}, "originalCommit": {"oid": "5f603732e0bb3727170a0baf7ef9f60e2d8049f0"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ5ODI0Mg==", "bodyText": "These 2 are actually used at multiple places inside the class, so did not refactor. It's possible to move them inside methods, but it will mean re-calculating.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r425498242", "createdAt": "2020-05-15T00:16:29Z", "author": {"login": "v3nkatesh"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,14 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MjM5MQ=="}, "originalCommit": {"oid": "5f603732e0bb3727170a0baf7ef9f60e2d8049f0"}, "originalPosition": 43}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4746, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}