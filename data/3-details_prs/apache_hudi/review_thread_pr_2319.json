{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM1NjE0MjY4", "number": 2319, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzoyNjoyNVrOFDefUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwNzoxODo0NVrOFD2Vpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MTg5NTg2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzoyNjoyNVrOIDJJiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMzoyNzowNFrOIDneeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE2NjUzOQ==", "bodyText": "@shenh062326 FYI, related to your Tuple2 refactoring PR \ud83d\ude09 .", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540166539", "createdAt": "2020-12-10T13:26:25Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -252,11 +252,9 @@ public boolean isImplicitWithStorage() {\n    * Make sure the parallelism is atleast the groupby parallelism for tagging location\n    */\n   JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(\n-      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n-      JavaPairRDD<String, String> partitionRecordKeyPairRDD, int shuffleParallelism, HoodieTable hoodieTable,\n+      JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c991a84ad90fcb7226ee60a0fa2c89d6ffa4f17"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY2MzQxOA==", "bodyText": "I am removing the scala dependency from hudl-client-common, it seems no relationship to with this PR, since it depends on scala.Tuple2 in hudi-spark-client.", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540663418", "createdAt": "2020-12-11T03:27:04Z", "author": {"login": "shenh062326"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -252,11 +252,9 @@ public boolean isImplicitWithStorage() {\n    * Make sure the parallelism is atleast the groupby parallelism for tagging location\n    */\n   JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(\n-      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n-      JavaPairRDD<String, String> partitionRecordKeyPairRDD, int shuffleParallelism, HoodieTable hoodieTable,\n+      JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE2NjUzOQ=="}, "originalCommit": {"oid": "0c991a84ad90fcb7226ee60a0fa2c89d6ffa4f17"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MjI3NzcyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNDo0NDowMlrOIDMrUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMjoyOTozM1rOIDmS6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDIyNDMzOQ==", "bodyText": "Maybe we can move fileGroupToComparisons  into this line or break the hoodieTable  to the next line. It would be better?", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540224339", "createdAt": "2020-12-10T14:44:02Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -252,11 +252,9 @@ public boolean isImplicitWithStorage() {\n    * Make sure the parallelism is atleast the groupby parallelism for tagging location\n    */\n   JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(\n-      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n-      JavaPairRDD<String, String> partitionRecordKeyPairRDD, int shuffleParallelism, HoodieTable hoodieTable,\n+      JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD,\n+      int shuffleParallelism, HoodieTable hoodieTable,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c991a84ad90fcb7226ee60a0fa2c89d6ffa4f17"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY0NDA3NQ==", "bodyText": "Thanks, agree ~", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540644075", "createdAt": "2020-12-11T02:29:33Z", "author": {"login": "danny0405"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -252,11 +252,9 @@ public boolean isImplicitWithStorage() {\n    * Make sure the parallelism is atleast the groupby parallelism for tagging location\n    */\n   JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(\n-      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n-      JavaPairRDD<String, String> partitionRecordKeyPairRDD, int shuffleParallelism, HoodieTable hoodieTable,\n+      JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD,\n+      int shuffleParallelism, HoodieTable hoodieTable,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDIyNDMzOQ=="}, "originalCommit": {"oid": "0c991a84ad90fcb7226ee60a0fa2c89d6ffa4f17"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTc5Nzg3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwNzoxNjo1MFrOIDsCNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwNzo0ODowMlrOIDs2SQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDczODEwMg==", "bodyText": "nit: wondering how checkstyle is happy with the indentation here. :)", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540738102", "createdAt": "2020-12-11T07:16:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -252,11 +252,10 @@ public boolean isImplicitWithStorage() {\n    * Make sure the parallelism is atleast the groupby parallelism for tagging location\n    */\n   JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(\n-      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n-      JavaPairRDD<String, String> partitionRecordKeyPairRDD, int shuffleParallelism, HoodieTable hoodieTable,\n+      JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD,\n+      int shuffleParallelism,\n+\t  HoodieTable hoodieTable,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "861fbe0d0fd396b5caaf81df510952103ba5070a"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc1MTQzMw==", "bodyText": "Thanks, i just notice that Hoodie use the whitespace for indentation.", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540751433", "createdAt": "2020-12-11T07:48:02Z", "author": {"login": "danny0405"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -252,11 +252,10 @@ public boolean isImplicitWithStorage() {\n    * Make sure the parallelism is atleast the groupby parallelism for tagging location\n    */\n   JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(\n-      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n-      JavaPairRDD<String, String> partitionRecordKeyPairRDD, int shuffleParallelism, HoodieTable hoodieTable,\n+      JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD,\n+      int shuffleParallelism,\n+\t  HoodieTable hoodieTable,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDczODEwMg=="}, "originalCommit": {"oid": "861fbe0d0fd396b5caaf81df510952103ba5070a"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NTgwMzI2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwNzoxODo0NVrOIDsFJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNjoxMzo0NlrOID_qtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDczODg1NA==", "bodyText": "do you actually see from the Spark UI that its not computed twice? I ask because, fileComparisonsRDD is not cached and thus even though this is declared only once, during runtime, Spark will lazily recompute fileComparisonsRDD once for each method that uses it.", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540738854", "createdAt": "2020-12-11T07:18:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -122,13 +122,15 @@ public SparkHoodieBloomIndex(HoodieWriteConfig config) {\n \n     // Step 3: Obtain a RDD, for each incoming record, that already exists, with the file id,\n     // that contains it.\n+    JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "861fbe0d0fd396b5caaf81df510952103ba5070a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc1MTE1OQ==", "bodyText": "I didn't check the Spark UI yet, just a simple analyze the process of data writing. For each batch of records to write, the SparkHoodieBloomIndex.lookupIndex was expected to be invoked once so the fileComparisonsRDD should only be evaluated only once, is there other invocation for SparkHoodieBloomIndex.lookupIndex ? Maybe i missed something.", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540751159", "createdAt": "2020-12-11T07:47:29Z", "author": {"login": "danny0405"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -122,13 +122,15 @@ public SparkHoodieBloomIndex(HoodieWriteConfig config) {\n \n     // Step 3: Obtain a RDD, for each incoming record, that already exists, with the file id,\n     // that contains it.\n+    JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDczODg1NA=="}, "originalCommit": {"oid": "861fbe0d0fd396b5caaf81df510952103ba5070a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc2NzMyMA==", "bodyText": "So Spark does lazy evaluation of an RDD. If the RDD is not persisted to disk/cache, it simply recomputes it. In this case, fileComparisonsRDD would be recomputed twice during runtime. the method explodeRecordRDDWithFileComparisons() will only be called once, but it does nothing in practice except \"define\" the RDD it returns.\nIn contrast, if you notice this line of code at the start of tagLocation\n// Step 0: cache the input record RDD\n    if (config.getBloomIndexUseCaching()) {\n      recordRDD.persist(SparkMemoryUtils.getBloomIndexInputStorageLevel(config.getProps()));\n    }\n\nThis caches the incoming recordRDD and thus however many times this RDD is used in the indexing DAG, it will not go to the previous stage. if we did not have the .persist() in here, then everytime an RDD derived off this recordRDD is needed for a Spark action, it will keep reading from source and compute the recordRDD again.\nApologies, if you knew all this already. :) and I am failing to see how this wont happen. but at-least you get my concern with this explanation.", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540767320", "createdAt": "2020-12-11T08:19:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -122,13 +122,15 @@ public SparkHoodieBloomIndex(HoodieWriteConfig config) {\n \n     // Step 3: Obtain a RDD, for each incoming record, that already exists, with the file id,\n     // that contains it.\n+    JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDczODg1NA=="}, "originalCommit": {"oid": "861fbe0d0fd396b5caaf81df510952103ba5070a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgwODIyNA==", "bodyText": "Thanks for the explanation, the recordRDD may be persisted, but the computation in explodeRecordRDDWithFileComparisons still need to do 2 times, right ? Sorry, i'm not that familiar with the RDD thing.", "url": "https://github.com/apache/hudi/pull/2319#discussion_r540808224", "createdAt": "2020-12-11T09:28:29Z", "author": {"login": "danny0405"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -122,13 +122,15 @@ public SparkHoodieBloomIndex(HoodieWriteConfig config) {\n \n     // Step 3: Obtain a RDD, for each incoming record, that already exists, with the file id,\n     // that contains it.\n+    JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDczODg1NA=="}, "originalCommit": {"oid": "861fbe0d0fd396b5caaf81df510952103ba5070a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTA1OTc2Ng==", "bodyText": "Correct. That's what i think will happen", "url": "https://github.com/apache/hudi/pull/2319#discussion_r541059766", "createdAt": "2020-12-11T16:13:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/bloom/SparkHoodieBloomIndex.java", "diffHunk": "@@ -122,13 +122,15 @@ public SparkHoodieBloomIndex(HoodieWriteConfig config) {\n \n     // Step 3: Obtain a RDD, for each incoming record, that already exists, with the file id,\n     // that contains it.\n+    JavaRDD<Tuple2<String, HoodieKey>> fileComparisonsRDD =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDczODg1NA=="}, "originalCommit": {"oid": "861fbe0d0fd396b5caaf81df510952103ba5070a"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3995, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}