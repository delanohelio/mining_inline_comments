{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ1NzA3NDc1", "number": 2382, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjowMTo0N1rOFY3W2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjozMToyMlrOFY4CkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxNjE3MTE0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-java-client/pom.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjowMTo0N1rOIjGLcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjowMTo0N1rOIjGLcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY3MjMwNQ==", "bodyText": "do not need this dependency?\n<dependency>\n      <groupId>${hive.groupid}</groupId>\n      <artifactId>hive-metastore</artifactId>\n      <version>${hive.version}</version>\n      <scope>test</scope>\n    </dependency>", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573672305", "createdAt": "2021-02-10T12:01:47Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-java-client/pom.xml", "diffHunk": "@@ -67,6 +67,14 @@\n             <scope>test</scope>\n         </dependency>\n \n+        <dependency>\n+            <groupId>${hive.groupid}</groupId>\n+            <artifactId>hive-exec</artifactId>\n+            <version>${hive.version}</version>\n+            <scope>test</scope>\n+            <classifier>${hive.exec.classifier}</classifier>\n+        </dependency>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2103800a5275d95283652072e3f9db288db6cfdb"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxNjE4OTMyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjowNjo0NFrOIjGWxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjowNjo0NFrOIjGWxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY3NTIwNA==", "bodyText": "spark -> java", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573675204", "createdAt": "2021-02-10T12:06:44Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaBulkInsertHelper.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.execution.JavaLazyInsertIterable;\n+import org.apache.hudi.execution.bulkinsert.JavaBulkInsertInternalPartitionerFactory;\n+import org.apache.hudi.io.CreateHandleFactory;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * A spark implementation of {@link AbstractBulkInsertHelper}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2103800a5275d95283652072e3f9db288db6cfdb"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxNjIwMjQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjoxMDowOFrOIjGetw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjoxMDowOFrOIjGetw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY3NzIzOQ==", "bodyText": "move to upper line?", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573677239", "createdAt": "2021-02-10T12:10:08Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class JavaInsertOverwriteCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseJavaCommitActionExecutor<T> {\n+\n+  private final List<HoodieRecord<T>> inputRecords;\n+\n+  public JavaInsertOverwriteCommitActionExecutor(HoodieEngineContext context,\n+                                                 HoodieWriteConfig config, HoodieTable table,\n+                                                 String instantTime, List<HoodieRecord<T>> inputRecords) {\n+    this(context, config, table, instantTime, inputRecords, WriteOperationType.INSERT_OVERWRITE);\n+  }\n+\n+  public JavaInsertOverwriteCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<HoodieRecord<T>> inputRecords,\n+                                                  WriteOperationType writeOperationType) {\n+    super(context, config, table, instantTime, writeOperationType);\n+    this.inputRecords = inputRecords;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<List<WriteStatus>> execute() {\n+    return JavaWriteHelper.newInstance().write(instantTime, inputRecords, context, table,\n+        config.shouldCombineBeforeInsert(), config.getInsertShuffleParallelism(), this, false);\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(List<WriteStatus> writeStatuses) {\n+    return context.mapToPair(\n+        writeStatuses.stream().map(status -> status.getStat().getPartitionPath()).distinct().collect(Collectors.toList()),\n+        partitionPath ->\n+            Pair.of(partitionPath, getAllExistingFileIds(partitionPath)),\n+        1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2103800a5275d95283652072e3f9db288db6cfdb"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxNjIxOTAzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjoxNDozNFrOIjGoxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjoxNDozNFrOIjGoxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY3OTgxNQ==", "bodyText": "setJobStatus is useless in java client, remove this?", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573679815", "createdAt": "2021-02-10T12:14:34Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaInsertOverwriteTableCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class JavaInsertOverwriteTableCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends JavaInsertOverwriteCommitActionExecutor<T> {\n+\n+  public JavaInsertOverwriteTableCommitActionExecutor(HoodieEngineContext context,\n+                                                      HoodieWriteConfig config, HoodieTable table,\n+                                                      String instantTime, List<HoodieRecord<T>> inputRecords) {\n+    super(context, config, table, instantTime, inputRecords, WriteOperationType.INSERT_OVERWRITE_TABLE);\n+  }\n+\n+  protected List<String> getAllExistingFileIds(String partitionPath) {\n+    return table.getSliceView().getLatestFileSlices(partitionPath)\n+        .map(fg -> fg.getFileId()).distinct().collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  protected Map<String, List<String>> getPartitionToReplacedFileIds(List<WriteStatus> writeStatuses) {\n+    Map<String, List<String>> partitionToExistingFileIds = new HashMap<>();\n+    List<String> partitionPaths = FSUtils.getAllPartitionPaths(context,\n+        table.getMetaClient().getBasePath(), config.useFileListingMetadata(),\n+        config.getFileListingMetadataVerify(), config.shouldAssumeDatePartitioning());\n+\n+    if (partitionPaths != null && partitionPaths.size() > 0) {\n+      context.setJobStatus(this.getClass().getSimpleName(), \"Getting ExistingFileIds of all partitions\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2103800a5275d95283652072e3f9db288db6cfdb"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxNjIzMzUyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/restore/JavaCopyOnWriteRestoreActionExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjoxODoxNFrOIjGxMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMlQxMTo0MDoxOFrOIkhRKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY4MTk2OQ==", "bodyText": "this code duplicates with SparkCopyOnWriteRestoreActionExcutor, would we do some refactor or file a jira ticket to track this?", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573681969", "createdAt": "2021-02-10T12:18:14Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/restore/JavaCopyOnWriteRestoreActionExecutor.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.restore;\n+\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieJavaEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.rollback.JavaCopyOnWriteRollbackActionExecutor;\n+\n+import java.util.List;\n+\n+public class JavaCopyOnWriteRestoreActionExecutor<T extends HoodieRecordPayload> extends\n+    BaseRestoreActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public JavaCopyOnWriteRestoreActionExecutor(HoodieJavaEngineContext context,\n+                                              HoodieWriteConfig config,\n+                                              HoodieTable table,\n+                                              String instantTime,\n+                                              String restoreInstantTime) {\n+    super(context, config, table, instantTime, restoreInstantTime);\n+  }\n+\n+  @Override\n+  protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback) {\n+    table.getMetaClient().reloadActiveTimeline();\n+    JavaCopyOnWriteRollbackActionExecutor rollbackActionExecutor = new JavaCopyOnWriteRollbackActionExecutor(\n+        context,\n+        config,\n+        table,\n+        HoodieActiveTimeline.createNewInstantTime(),\n+        instantToRollback,\n+        true,\n+        true,\n+        false);\n+    if (!instantToRollback.getAction().equals(HoodieTimeline.COMMIT_ACTION)\n+        && !instantToRollback.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION)) {\n+      throw new HoodieRollbackException(\"Unsupported action in rollback instant:\" + instantToRollback);\n+    }\n+    return rollbackActionExecutor.execute();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2103800a5275d95283652072e3f9db288db6cfdb"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3NTE2NDcxMw==", "bodyText": "Sure, I will add a jira ticket to track it.", "url": "https://github.com/apache/hudi/pull/2382#discussion_r575164713", "createdAt": "2021-02-12T11:40:18Z", "author": {"login": "shenh062326"}, "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/restore/JavaCopyOnWriteRestoreActionExecutor.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.restore;\n+\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieJavaEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.rollback.JavaCopyOnWriteRollbackActionExecutor;\n+\n+import java.util.List;\n+\n+public class JavaCopyOnWriteRestoreActionExecutor<T extends HoodieRecordPayload> extends\n+    BaseRestoreActionExecutor<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public JavaCopyOnWriteRestoreActionExecutor(HoodieJavaEngineContext context,\n+                                              HoodieWriteConfig config,\n+                                              HoodieTable table,\n+                                              String instantTime,\n+                                              String restoreInstantTime) {\n+    super(context, config, table, instantTime, restoreInstantTime);\n+  }\n+\n+  @Override\n+  protected HoodieRollbackMetadata rollbackInstant(HoodieInstant instantToRollback) {\n+    table.getMetaClient().reloadActiveTimeline();\n+    JavaCopyOnWriteRollbackActionExecutor rollbackActionExecutor = new JavaCopyOnWriteRollbackActionExecutor(\n+        context,\n+        config,\n+        table,\n+        HoodieActiveTimeline.createNewInstantTime(),\n+        instantToRollback,\n+        true,\n+        true,\n+        false);\n+    if (!instantToRollback.getAction().equals(HoodieTimeline.COMMIT_ACTION)\n+        && !instantToRollback.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION)) {\n+      throw new HoodieRollbackException(\"Unsupported action in rollback instant:\" + instantToRollback);\n+    }\n+    return rollbackActionExecutor.execute();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY4MTk2OQ=="}, "originalCommit": {"oid": "2103800a5275d95283652072e3f9db288db6cfdb"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxNjI0NTY5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/rollback/JavaListingBasedRollbackHelper.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjoyMTozMFrOIjG4lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjoyMTozMFrOIjG4lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY4Mzg2MA==", "bodyText": "move above", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573683860", "createdAt": "2021-02-10T12:21:30Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/rollback/JavaListingBasedRollbackHelper.java", "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.PathFilter;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Performs Rollback of Hoodie Tables.\n+ */\n+public class JavaListingBasedRollbackHelper implements Serializable {\n+\n+  private static final Logger LOG = LogManager.getLogger(JavaListingBasedRollbackHelper.class);\n+\n+  private final HoodieTableMetaClient metaClient;\n+  private final HoodieWriteConfig config;\n+\n+  public JavaListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config) {\n+    this.metaClient = metaClient;\n+    this.config = config;\n+  }\n+\n+  /**\n+   * Performs all rollback actions that we have collected in parallel.\n+   */\n+  public List<HoodieRollbackStat> performRollback(HoodieEngineContext context, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n+    Map<String, HoodieRollbackStat> partitionPathRollbackStatsPairs = maybeDeleteAndCollectStats(context, instantToRollback, rollbackRequests, true);\n+\n+    Map<String, List<Pair<String, HoodieRollbackStat>>> collect = partitionPathRollbackStatsPairs.entrySet()\n+        .stream()\n+        .map(x -> Pair.of(x.getKey(), x.getValue())).collect(Collectors.groupingBy(Pair::getLeft));\n+    return collect.values().stream()\n+        .map(pairs -> pairs.stream().map(Pair::getRight).reduce(RollbackUtils::mergeRollbackStat).orElse(null))\n+        .filter(Objects::nonNull)\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Collect all file info that needs to be rollbacked.\n+   */\n+  public List<HoodieRollbackStat> collectRollbackStats(HoodieEngineContext context, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n+    Map<String, HoodieRollbackStat> partitionPathRollbackStatsPairs = maybeDeleteAndCollectStats(context, instantToRollback, rollbackRequests, false);\n+    return new ArrayList<>(partitionPathRollbackStatsPairs.values());\n+  }\n+\n+  /**\n+   * May be delete interested files and collect stats or collect stats only.\n+   *\n+   * @param context           instance of {@link HoodieEngineContext} to use.\n+   * @param instantToRollback {@link HoodieInstant} of interest for which deletion or collect stats is requested.\n+   * @param rollbackRequests  List of {@link ListingBasedRollbackRequest} to be operated on.\n+   * @param doDelete          {@code true} if deletion has to be done. {@code false} if only stats are to be collected w/o performing any deletes.\n+   * @return stats collected with or w/o actual deletions.\n+   */\n+  Map<String, HoodieRollbackStat> maybeDeleteAndCollectStats(HoodieEngineContext context,\n+                                                             HoodieInstant instantToRollback,\n+                                                             List<ListingBasedRollbackRequest> rollbackRequests,\n+                                                             boolean doDelete) {\n+    return context.mapToPair(rollbackRequests, rollbackRequest -> {\n+      switch (rollbackRequest.getType()) {\n+        case DELETE_DATA_FILES_ONLY: {\n+          final Map<FileStatus, Boolean> filesToDeletedStatus = deleteBaseFiles(metaClient, config, instantToRollback.getTimestamp(),\n+              rollbackRequest.getPartitionPath(), doDelete);\n+          return new ImmutablePair<>(rollbackRequest.getPartitionPath(),\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withDeletedFileResults(filesToDeletedStatus).build());\n+        }\n+        case DELETE_DATA_AND_LOG_FILES: {\n+          final Map<FileStatus, Boolean> filesToDeletedStatus = deleteBaseAndLogFiles(metaClient, config, instantToRollback.getTimestamp(), rollbackRequest.getPartitionPath(), doDelete);\n+          return new ImmutablePair<>(rollbackRequest.getPartitionPath(),\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withDeletedFileResults(filesToDeletedStatus).build());\n+        }\n+        case APPEND_ROLLBACK_BLOCK: {\n+          HoodieLogFormat.Writer writer = null;\n+          try {\n+            writer = HoodieLogFormat.newWriterBuilder()\n+                .onParentPath(FSUtils.getPartitionPath(metaClient.getBasePath(), rollbackRequest.getPartitionPath()))\n+                .withFileId(rollbackRequest.getFileId().get())\n+                .overBaseCommit(rollbackRequest.getLatestBaseInstant().get()).withFs(metaClient.getFs())\n+                .withFileExtension(HoodieLogFile.DELTA_EXTENSION).build();\n+\n+            // generate metadata\n+            if (doDelete) {\n+              Map<HoodieLogBlock.HeaderMetadataType, String> header = generateHeader(instantToRollback.getTimestamp());\n+              // if update belongs to an existing log file\n+              writer.appendBlock(new HoodieCommandBlock(header));\n+            }\n+          } catch (IOException | InterruptedException io) {\n+            throw new HoodieRollbackException(\"Failed to rollback for instant \" + instantToRollback, io);\n+          } finally {\n+            try {\n+              if (writer != null) {\n+                writer.close();\n+              }\n+            } catch (IOException io) {\n+              throw new HoodieIOException(\"Error appending rollback block..\", io);\n+            }\n+          }\n+\n+          // This step is intentionally done after writer is closed. Guarantees that\n+          // getFileStatus would reflect correct stats and FileNotFoundException is not thrown in\n+          // cloud-storage : HUDI-168\n+          Map<FileStatus, Long> filesToNumBlocksRollback = Collections.singletonMap(\n+              metaClient.getFs().getFileStatus(Objects.requireNonNull(writer).getLogFile().getPath()),\n+              1L", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2103800a5275d95283652072e3f9db288db6cfdb"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzYxNjI4MzA1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjozMToyMlrOIjHPHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xMFQxMjozMToyMlrOIjHPHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzY4OTYzMA==", "bodyText": "remove this line?", "url": "https://github.com/apache/hudi/pull/2382#discussion_r573689630", "createdAt": "2021-02-10T12:31:22Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-java-client/src/test/java/org/apache/hudi/table/action/commit/TestJavaCopyOnWriteActionExecutor.java", "diffHunk": "@@ -0,0 +1,481 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.HoodieJavaWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.engine.EngineType;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.testutils.RawTripTestPayload;\n+import org.apache.hudi.common.testutils.Transformations;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.utils.HoodieHiveUtils;\n+import org.apache.hudi.io.HoodieCreateHandle;\n+import org.apache.hudi.table.HoodieJavaCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieJavaTable;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hudi.testutils.HoodieJavaClientTestBase;\n+import org.apache.hudi.testutils.MetadataMergeWriteStatus;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroReadSupport;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.testutils.HoodieTestTable.makeNewCommitTime;\n+import static org.apache.hudi.common.testutils.SchemaTestUtil.getSchemaFromResource;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class TestJavaCopyOnWriteActionExecutor extends HoodieJavaClientTestBase {\n+\n+  private static final Logger LOG = LogManager.getLogger(TestJavaCopyOnWriteActionExecutor.class);\n+  private static final Schema SCHEMA = getSchemaFromResource(TestJavaCopyOnWriteActionExecutor.class, \"/exampleSchema.avsc\");\n+\n+  @Test\n+  public void testMakeNewPath() {\n+    String fileName = UUID.randomUUID().toString();\n+    String partitionPath = \"2016/05/04\";\n+\n+    String instantTime = makeNewCommitTime();\n+    HoodieWriteConfig config = makeHoodieClientConfig();\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    HoodieTable table = HoodieJavaTable.create(config, context, metaClient);\n+\n+    Pair<Path, String> newPathWithWriteToken = Arrays.asList(1).stream().map(x -> {\n+      HoodieRecord record = mock(HoodieRecord.class);\n+      when(record.getPartitionPath()).thenReturn(partitionPath);\n+      String writeToken = FSUtils.makeWriteToken(context.getTaskContextSupplier().getPartitionIdSupplier().get(),\n+          context.getTaskContextSupplier().getStageIdSupplier().get(),\n+          context.getTaskContextSupplier().getAttemptIdSupplier().get());\n+      HoodieCreateHandle io = new HoodieCreateHandle(config, instantTime, table, partitionPath, fileName,\n+          context.getTaskContextSupplier());\n+      return Pair.of(io.makeNewPath(record.getPartitionPath()), writeToken);\n+    }).collect(Collectors.toList()).get(0);\n+\n+    assertEquals(newPathWithWriteToken.getKey().toString(), Paths.get(this.basePath, partitionPath,\n+        FSUtils.makeDataFileName(instantTime, newPathWithWriteToken.getRight(), fileName)).toString());\n+  }\n+\n+  private HoodieWriteConfig makeHoodieClientConfig() {\n+    return makeHoodieClientConfigBuilder().build();\n+  }\n+\n+  private HoodieWriteConfig.Builder makeHoodieClientConfigBuilder() {\n+    // Prepare the AvroParquetIO\n+    return HoodieWriteConfig.newBuilder()\n+        .withEngineType(EngineType.JAVA)\n+        .withPath(basePath)\n+        .withSchema(SCHEMA.toString());\n+  }\n+\n+  // TODO (weiy): Add testcases for crossing file writing.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2103800a5275d95283652072e3f9db288db6cfdb"}, "originalPosition": 120}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4073, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}