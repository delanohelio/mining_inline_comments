{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk4MzY2ODYz", "number": 1484, "title": "[HUDI-316] : Hbase qps repartition writestatus", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nThis pull request optimizes hbase index write operations.\nBrief change log\n\nReplaces Thread.sleep() with RateLimiter to reduce wait time during hbase puts operation\nRepartitions WriteStatus with new records to improve parallelism of hbase index operations\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded tests to TestHbaseIndex to verify the repartition optimization\nAlso verified the change by running a ob end to end\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-04-03T20:18:17Z", "url": "https://github.com/apache/hudi/pull/1484", "merged": true, "mergeCommit": {"oid": "59f995a3f5476e8171f24a32250da85e13d28daf"}, "closed": true, "closedAt": "2020-11-02T16:33:28Z", "author": {"login": "v3nkatesh"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcUKo4IAFqTM4NzY0NzgzOA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdYnONRAFqTUyMTc5NTUyNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NjQ3ODM4", "url": "https://github.com/apache/hudi/pull/1484#pullrequestreview-387647838", "createdAt": "2020-04-04T00:47:58Z", "commit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNFQwMDo0Nzo1OFrOGAtf0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNFQwMDo0Nzo1OFrOGAtf0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODYxMQ==", "bodyText": "Could we please avoid using this? Should not be too hard to roll our own..", "url": "https://github.com/apache/hudi/pull/1484#discussion_r403398611", "createdAt": "2020-04-04T00:47:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/*\n+ * Note: Based on RateLimiter implementation in Google/Guava.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNDkyOTk2", "url": "https://github.com/apache/hudi/pull/1484#pullrequestreview-392492996", "createdAt": "2020-04-13T22:55:11Z", "commit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMjo1NToxMVrOGE4ELw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxODoyNDoyNFrOGFbZrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjA2Mw==", "bodyText": "Do you need this map as instance variable? Looks like there is one HBaseIndex object per client. We don't seem to be clearing entries from this map also.  So, over time, this map can get pretty large and can cause increased memory utilization? Please correct me if I'm misreading.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407766063", "createdAt": "2020-04-13T22:55:11Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private int maxPutsPerSec;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;\n+  Map<String, Integer> fileIdPartitionMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc2NjQzNg==", "bodyText": "I dont see how this is used. could you please add a comment for all these instance variables? It seems like they can be local variables specific to the operation being performed.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407766436", "createdAt": "2020-04-13T22:56:24Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n   private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private int maxPutsPerSec;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNTUwNg==", "bodyText": "Can you help me understand this code? why do we need to force trigger here? Is this just to releaseQPSResources? releaseQPSResources seems to be doing nothing (at least default implementation, are there other implementations outside hoodie?). Is it really important to release here as opposed to doing it in 'close()' (earlier behavior)?", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407805506", "createdAt": "2020-04-14T01:03:56Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n+    JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n+        true);\n     // caching the index updated status RDD\n     writeStatusJavaRDD = writeStatusJavaRDD.persist(SparkConfigUtils.getWriteStatusStorageLevel(config.getProps()));\n+    // force trigger update location(hbase puts)\n+    writeStatusJavaRDD.count();\n+    this.hBaseIndexQPSResourceAllocator.releaseQPSResources();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgwNzkyOQ==", "bodyText": "Not directly related to your change, so feel free to ignore this comment. But hBaseIndexQPSResourceAllocator  is instance variable. why is this again passed as argument. This seems like a consistent pattern in this class. Because we are also using exact same name for local variable, it masks instance variable and can become easily error prone if the two variables evolve to mean different things.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407807929", "createdAt": "2020-04-14T01:12:46Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);\n+    JavaRDD<WriteStatus> writeStatusJavaRDD = partitionedRDD.mapPartitionsWithIndex(updateLocationFunction(),\n+        true);\n     // caching the index updated status RDD\n     writeStatusJavaRDD = writeStatusJavaRDD.persist(SparkConfigUtils.getWriteStatusStorageLevel(config.getProps()));\n+    // force trigger update location(hbase puts)\n+    writeStatusJavaRDD.count();\n+    this.hBaseIndexQPSResourceAllocator.releaseQPSResources();\n     return writeStatusJavaRDD;\n   }\n \n-  private void setPutBatchSize(JavaRDD<WriteStatus> writeStatusRDD,\n-      HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator, final JavaSparkContext jsc) {\n+  private Option<Float> calculateQPSFraction(JavaRDD<WriteStatus> writeStatusRDD,\n+                                               HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxMjgzNw==", "bodyText": "Can you share the context on why we created HBaseIndexQPSResourceAllocator?  Do you think calls to RateLimiter#acquire can be made inside HBaseIndexQPSResourceAllocator#acquireQPSResources to simplify?", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407812837", "createdAt": "2020-04-14T01:30:24Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,17 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTIwNA==", "bodyText": "could you return the time spent waiting here? I think adding metrics on time taken is very important for debugging any potential performance issues. Also, would be useful to log if time taken is greater than some threshold (say, 300ms?)", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407815204", "createdAt": "2020-04-14T01:39:05Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/RateLimiter.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.ThreadSafe;\n+\n+/*\n+ * Note: Based on RateLimiter implementation in Google/Guava.\n+ *         - adopted from com.google.common.util.concurrent\n+ *           Copyright (C) 2012 The Guava Authors\n+ *           Home page: https://github.com/google/guava\n+ *           License: http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+@ThreadSafe\n+public abstract class RateLimiter {\n+  private final RateLimiter.SleepingTicker ticker;\n+  private final long offsetNanos;\n+  double storedPermits;\n+  double maxPermits;\n+  volatile double stableIntervalMicros;\n+  private final Object mutex;\n+  private long nextFreeTicketMicros;\n+\n+  public static RateLimiter create(double permitsPerSecond) {\n+    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond);\n+  }\n+\n+  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond) {\n+    RateLimiter rateLimiter = new RateLimiter.Bursty(ticker, 1.0D);\n+    rateLimiter.setRate(permitsPerSecond);\n+    return rateLimiter;\n+  }\n+\n+  public static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n+    return create(RateLimiter.SleepingTicker.SYSTEM_TICKER, permitsPerSecond, warmupPeriod, unit);\n+  }\n+\n+  static RateLimiter create(RateLimiter.SleepingTicker ticker, double permitsPerSecond, long warmupPeriod, TimeUnit unit) {\n+    RateLimiter rateLimiter = new RateLimiter.WarmingUp(ticker, warmupPeriod, unit);\n+    rateLimiter.setRate(permitsPerSecond);\n+    return rateLimiter;\n+  }\n+\n+  private RateLimiter(RateLimiter.SleepingTicker ticker) {\n+    this.mutex = new Object();\n+    this.nextFreeTicketMicros = 0L;\n+    this.ticker = ticker;\n+    this.offsetNanos = ticker.read();\n+  }\n+\n+  public final void setRate(double permitsPerSecond) {\n+    checkArgument(permitsPerSecond > 0.0D && !Double.isNaN(permitsPerSecond), \"rate must be positive\");\n+    Object var3 = this.mutex;\n+    synchronized (this.mutex) {\n+      this.resync(this.readSafeMicros());\n+      double stableIntervalMicros = (double)TimeUnit.SECONDS.toMicros(1L) / permitsPerSecond;\n+      this.stableIntervalMicros = stableIntervalMicros;\n+      this.doSetRate(permitsPerSecond, stableIntervalMicros);\n+    }\n+  }\n+\n+  abstract void doSetRate(double var1, double var3);\n+\n+  public final double getRate() {\n+    return (double)TimeUnit.SECONDS.toMicros(1L) / this.stableIntervalMicros;\n+  }\n+\n+  public void acquire() {\n+    this.acquire(1);\n+  }\n+\n+  public void acquire(int permits) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2ODIyMw==", "bodyText": "Could you please make this a static class if its not using any instance variables of outer class", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407868223", "createdAt": "2020-04-14T05:01:26Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -498,4 +554,37 @@ public boolean isImplicitWithStorage() {\n   public void setHbaseConnection(Connection hbaseConnection) {\n     HBaseIndex.hbaseConnection = hbaseConnection;\n   }\n+\n+  /**\n+   * Partitions each WriteStatus with inserts into a unique single partition. WriteStatus without inserts will be\n+   * assigned to random partitions. This partitioner will be useful to utilize max parallelism with spark operations\n+   * that are based on inserts in each WriteStatus.\n+   */\n+  public class WriteStatusPartitioner extends Partitioner {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 331}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2OTU2NQ==", "bodyText": "nit: looks like this is logged in the above method call too. so i think this can be removed.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407869565", "createdAt": "2020-04-14T05:06:47Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());\n+    acquireQPSResourcesAndSetBatchSize(desiredQPSFraction, jsc);\n+    LOG.info(\"multiPutBatchSize before hbase puts: \" + this.multiPutBatchSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzkwNTk1NA==", "bodyText": "Consider redoing this logic, because if this.numWriteStatusWithInserts == 0 , we still go through the process of generating fileIdPartitionMap which is not ideal.\nAlso, curious, if you did any performance measurements before and after this change. It is worth highlighting in release notes if this improvement is significant", "url": "https://github.com/apache/hudi/pull/1484#discussion_r407905954", "createdAt": "2020-04-14T06:55:15Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n-  }\n-\n-  private static void sleepForTime(int sleepTimeMs) {\n-    try {\n-      Thread.sleep(sleepTimeMs);\n-    } catch (InterruptedException e) {\n-      LOG.error(\"Sleep interrupted during throttling\", e);\n-      throw new RuntimeException(e);\n-    }\n   }\n \n   @Override\n   public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n       HoodieTable<T> hoodieTable) {\n-    final HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-    setPutBatchSize(writeStatusRDD, hBaseIndexQPSResourceAllocator, jsc);\n-    LOG.info(\"multiPutBatchSize: before hbase puts\" + multiPutBatchSize);\n-    JavaRDD<WriteStatus> writeStatusJavaRDD = writeStatusRDD.mapPartitionsWithIndex(updateLocationFunction(), true);\n+    final Option<Float> desiredQPSFraction =  calculateQPSFraction(writeStatusRDD, hBaseIndexQPSResourceAllocator);\n+    // Map each fileId that has inserts to a unique partition Id. This will be used while\n+    // repartitioning RDD<WriteStatus>\n+    int partitionIndex = 0;\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)\n+                                   .map(w -> w.getFileId()).collect();\n+    for (final String fileId : fileIds) {\n+      this.fileIdPartitionMap.put(fileId, partitionIndex++);\n+    }\n+    JavaRDD<WriteStatus> partitionedRDD = this.numWriteStatusWithInserts == 0 ? writeStatusRDD :\n+                                          writeStatusRDD.mapToPair(w -> new Tuple2<>(w.getFileId(), w))\n+                                            .partitionBy(new WriteStatusPartitioner(this.fileIdPartitionMap,\n+                                              this.numWriteStatusWithInserts))\n+                                            .map(w -> w._2());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODMyODk5NA==", "bodyText": "nit: can we also move hTable.get(keys) inside this if?  do we need to invoke hTable.get if keys is empty?", "url": "https://github.com/apache/hudi/pull/1484#discussion_r408328994", "createdAt": "2020-04-14T17:57:27Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -252,8 +263,10 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n     };\n   }\n \n-  private Result[] doGet(HTable hTable, List<Get> keys) throws IOException {\n-    sleepForTime(SLEEP_TIME_MILLISECONDS);\n+  private Result[] doGet(HTable hTable, List<Get> keys, RateLimiter limiter) throws IOException {\n+    if (keys.size() > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM0NTAwNw==", "bodyText": "lot of code in this test seems like repetition from source code. consider refactoring this part into a library to reuse in tests if needed", "url": "https://github.com/apache/hudi/pull/1484#discussion_r408345007", "createdAt": "2020-04-14T18:24:24Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/test/java/org/apache/hudi/index/TestHbaseIndex.java", "diffHunk": "@@ -329,47 +332,140 @@ public void testPutBatchSizeCalculation() {\n     // All asserts cases below are derived out of the first\n     // example below, with change in one parameter at a time.\n \n-    int putBatchSize = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 100, 0.1f);\n-    // Expected batchSize is 8 because in that case, total request sent in one second is below\n-    // 8 (batchSize) * 200 (parallelism) * 10 (maxReqsInOneSecond) * 10 (numRegionServers) * 0.1 (qpsFraction)) => 16000\n-    // We assume requests get distributed to Region Servers uniformly, so each RS gets 1600 request\n-    // 1600 happens to be 10% of 16667 (maxQPSPerRegionServer) as expected.\n-    Assert.assertEquals(putBatchSize, 8);\n+    int putBatchSize = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 0.1f);\n+    // Total puts that can be sent  in 1 second = (10 * 16667 * 0.1) = 16,667\n+    // Total puts per batch will be (16,667 / parallelism) = 83.335, where 200 is the maxExecutors\n+    Assert.assertEquals(putBatchSize, 83);\n \n     // Number of Region Servers are halved, total requests sent in a second are also halved, so batchSize is also halved\n-    int putBatchSize2 = batchSizeCalculator.getBatchSize(5, 16667, 1200, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize2, 4);\n+    int putBatchSize2 = batchSizeCalculator.getBatchSize(5, 16667, 1200, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize2, 41);\n \n     // If the parallelism is halved, batchSize has to double\n-    int putBatchSize3 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 100, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize3, 16);\n+    int putBatchSize3 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 100, 0.1f);\n+    Assert.assertEquals(putBatchSize3, 166);\n \n     // If the parallelism is halved, batchSize has to double.\n     // This time parallelism is driven by numTasks rather than numExecutors\n-    int putBatchSize4 = batchSizeCalculator.getBatchSize(10, 16667, 100, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize4, 16);\n+    int putBatchSize4 = batchSizeCalculator.getBatchSize(10, 16667, 100, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize4, 166);\n \n     // If sleepTimeMs is halved, batchSize has to halve\n-    int putBatchSize5 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 100, 0.05f);\n-    Assert.assertEquals(putBatchSize5, 4);\n+    int putBatchSize5 = batchSizeCalculator.getBatchSize(10, 16667, 1200, 200, 0.05f);\n+    Assert.assertEquals(putBatchSize5, 41);\n \n     // If maxQPSPerRegionServer is doubled, batchSize also doubles\n-    int putBatchSize6 = batchSizeCalculator.getBatchSize(10, 33334, 1200, 200, 100, 0.1f);\n-    Assert.assertEquals(putBatchSize6, 16);\n+    int putBatchSize6 = batchSizeCalculator.getBatchSize(10, 33334, 1200, 200, 0.1f);\n+    Assert.assertEquals(putBatchSize6, 166);\n   }\n \n   @Test\n   public void testsHBasePutAccessParallelism() {\n     HoodieWriteConfig config = getConfig();\n     HBaseIndex index = new HBaseIndex(config);\n     final JavaRDD<WriteStatus> writeStatusRDD = jsc.parallelize(\n-        Arrays.asList(getSampleWriteStatus(1, 2), getSampleWriteStatus(0, 3), getSampleWriteStatus(10, 0)), 10);\n+        Arrays.asList(\n+          getSampleWriteStatus(0, 2),\n+          getSampleWriteStatus(2, 3),\n+          getSampleWriteStatus(4, 3),\n+          getSampleWriteStatus(6, 3),\n+          getSampleWriteStatus(8, 0)),\n+        10);\n     final Tuple2<Long, Integer> tuple = index.getHBasePutAccessParallelism(writeStatusRDD);\n     final int hbasePutAccessParallelism = Integer.parseInt(tuple._2.toString());\n     final int hbaseNumPuts = Integer.parseInt(tuple._1.toString());\n     Assert.assertEquals(10, writeStatusRDD.getNumPartitions());\n-    Assert.assertEquals(2, hbasePutAccessParallelism);\n-    Assert.assertEquals(11, hbaseNumPuts);\n+    Assert.assertEquals(4, hbasePutAccessParallelism);\n+    Assert.assertEquals(20, hbaseNumPuts);\n+  }\n+\n+  @Test\n+  public void testsWriteStatusPartitioner() {\n+    HoodieWriteConfig config = getConfig();\n+    HBaseIndex index = new HBaseIndex(config);\n+    int parallelism = 4;\n+    final JavaRDD<WriteStatus> writeStatusRDD = jsc.parallelize(\n+        Arrays.asList(\n+          getSampleWriteStatusWithFileId(0, 2),\n+          getSampleWriteStatusWithFileId(2, 3),\n+          getSampleWriteStatusWithFileId(4, 3),\n+          getSampleWriteStatusWithFileId(0, 3),\n+          getSampleWriteStatusWithFileId(11, 0)), parallelism);\n+    int partitionIndex = 0;\n+    final Map<String, Integer> fileIdPartitionMap = new HashMap<>();\n+\n+    final List<String> fileIds = writeStatusRDD.filter(w -> w.getStat().getNumInserts() > 0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2ODcxNTQ0", "url": "https://github.com/apache/hudi/pull/1484#pullrequestreview-396871544", "createdAt": "2020-04-20T23:03:34Z", "commit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMzowMzozNFrOGIrLNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQyMzowMzozNFrOGIrLNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc0OTE3NA==", "bodyText": "another question, what is the typical latency of these mutate operations? If time taken here combined with time taken to collect 'multiPutBatchSize' is > 1 second, then it seems like limiter would generate enough tokens for next run and would not wait at all.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r411749174", "createdAt": "2020-04-20T23:03:34Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -322,66 +347,94 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm\n   /**\n    * Helper method to facilitate performing mutations (including puts and deletes) in Hbase.\n    */\n-  private void doMutations(BufferedMutator mutator, List<Mutation> mutations) throws IOException {\n+  private void doMutations(BufferedMutator mutator, List<Mutation> mutations, RateLimiter limiter) throws IOException {\n     if (mutations.isEmpty()) {\n       return;\n     }\n+    // report number of operations to account per second with rate limiter.\n+    // If #limiter.getRate() operations are acquired within 1 second, ratelimiter will limit the rest of calls\n+    // for within that second\n+    limiter.acquire(mutations.size());\n     mutator.mutate(mutations);\n     mutator.flush();\n     mutations.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d10bc048bc9f5e705b309d843bc89c86c00ae64a"}, "originalPosition": 156}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0MzQ0OTE0", "url": "https://github.com/apache/hudi/pull/1484#pullrequestreview-404344914", "createdAt": "2020-05-01T19:10:35Z", "commit": {"oid": "5f603732e0bb3727170a0baf7ef9f60e2d8049f0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOToxMDozNVrOGPS9Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxOToxMDozNVrOGPS9Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY5MjM5MQ==", "bodyText": "these two also seem like related to the operation being performed and not really need to be instance variables. If we can find a way to move them to local variables, that would make it cleaner.", "url": "https://github.com/apache/hudi/pull/1484#discussion_r418692391", "createdAt": "2020-05-01T19:10:35Z", "author": {"login": "satishkotha"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java", "diffHunk": "@@ -83,13 +88,14 @@\n   private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n   private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n   private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n \n   private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n   private static Connection hbaseConnection = null;\n   private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n   private int maxQpsPerRegionServer;\n+  private long totalNumInserts;\n+  private int numWriteStatusWithInserts;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f603732e0bb3727170a0baf7ef9f60e2d8049f0"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyMjU5OTYz", "url": "https://github.com/apache/hudi/pull/1484#pullrequestreview-412259963", "createdAt": "2020-05-14T23:52:28Z", "commit": {"oid": "a1f54074cb2f40e141dd7340cc5b9bb78d59d3d5"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE1MjU3Njgw", "url": "https://github.com/apache/hudi/pull/1484#pullrequestreview-415257680", "createdAt": "2020-05-20T11:57:32Z", "commit": {"oid": "a1f54074cb2f40e141dd7340cc5b9bb78d59d3d5"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a1f54074cb2f40e141dd7340cc5b9bb78d59d3d5", "author": {"user": {"login": "v3nkatesh", "name": "Venkatesh Rudraraju"}}, "url": "https://github.com/apache/hudi/commit/a1f54074cb2f40e141dd7340cc5b9bb78d59d3d5", "committedDate": "2020-05-11T16:26:11Z", "message": "Refactor RateLimiter and remove Ticker class"}, "afterCommit": {"oid": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "author": {"user": {"login": "v3nkatesh", "name": "Venkatesh Rudraraju"}}, "url": "https://github.com/apache/hudi/commit/5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "committedDate": "2020-09-04T07:47:23Z", "message": "Refactor RateLimiter and remove Ticker class"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "author": {"user": {"login": "v3nkatesh", "name": "Venkatesh Rudraraju"}}, "url": "https://github.com/apache/hudi/commit/5cc01a1aeb4832b617343d5770f4d8a65eb62a73", "committedDate": "2020-09-04T07:47:23Z", "message": "Refactor RateLimiter and remove Ticker class"}, "afterCommit": {"oid": "6b62b8a72092fb63a0139b5fe3aac1e49a9bcf8e", "author": {"user": {"login": "v3nkatesh", "name": "Venkatesh Rudraraju"}}, "url": "https://github.com/apache/hudi/commit/6b62b8a72092fb63a0139b5fe3aac1e49a9bcf8e", "committedDate": "2020-10-29T07:28:49Z", "message": "Use RateLimiter instead of sleep. Repartition WriteStatus to optimize Hbase index writes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1bdf773ad1851843337674f9cc036926ef2d8ccb", "author": {"user": {"login": "v3nkatesh", "name": "Venkatesh Rudraraju"}}, "url": "https://github.com/apache/hudi/commit/1bdf773ad1851843337674f9cc036926ef2d8ccb", "committedDate": "2020-10-29T18:36:18Z", "message": "Use RateLimiter instead of sleep. Repartition WriteStatus to optimize Hbase index writes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6b62b8a72092fb63a0139b5fe3aac1e49a9bcf8e", "author": {"user": {"login": "v3nkatesh", "name": "Venkatesh Rudraraju"}}, "url": "https://github.com/apache/hudi/commit/6b62b8a72092fb63a0139b5fe3aac1e49a9bcf8e", "committedDate": "2020-10-29T07:28:49Z", "message": "Use RateLimiter instead of sleep. Repartition WriteStatus to optimize Hbase index writes"}, "afterCommit": {"oid": "1bdf773ad1851843337674f9cc036926ef2d8ccb", "author": {"user": {"login": "v3nkatesh", "name": "Venkatesh Rudraraju"}}, "url": "https://github.com/apache/hudi/commit/1bdf773ad1851843337674f9cc036926ef2d8ccb", "committedDate": "2020-10-29T18:36:18Z", "message": "Use RateLimiter instead of sleep. Repartition WriteStatus to optimize Hbase index writes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxNzk1NTI3", "url": "https://github.com/apache/hudi/pull/1484#pullrequestreview-521795527", "createdAt": "2020-11-02T16:33:14Z", "commit": {"oid": "1bdf773ad1851843337674f9cc036926ef2d8ccb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3331, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}