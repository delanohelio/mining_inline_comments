{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAxMTM4MzIz", "number": 1500, "title": "[HUDI-772] Make UserDefinedBulkInsertPartitioner configurable for DataSource", "bodyText": "What is the purpose of the pull request\n\n\nThe PR-245 added UserDefinedBulkInsertPartitioner, however API is only available for HoodieWriteClient.\n\n\nThis request allows Spark DataSource uses UserDefinedBulkInsertPartitioner with hoodie.bulkinsert.user_defined.partitioner.class configuration.\n\n\nBrief change log\n\n\"hoodie.bulkinsert.user_defined.partitioner.class\" configuration has been added.\nWhen \"hoodie.bulkinsert.user_defined.partitioner.class\" is configured, data source utils create UserDefinedBulkInsertPartitioner\n\nVerify this pull request\n\nAdded DataSourceUtilsTest to verify the change.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-04-09T00:02:06Z", "url": "https://github.com/apache/hudi/pull/1500", "merged": true, "mergeCommit": {"oid": "ddd105bb3119174b613c6917ee25795f2939f430"}, "closed": true, "closedAt": "2020-04-20T15:38:19Z", "author": {"login": "kwondw"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcW-S1RAFqTM5MTg4MTc1Mg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcZg9SzAFqTM5NjU3MjYwMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxODgxNzUy", "url": "https://github.com/apache/hudi/pull/1500#pullrequestreview-391881752", "createdAt": "2020-04-12T18:02:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQxODowMjo0N1rOGEXgVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQxODowNjo0MFrOGEXiFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzMjU5Ng==", "bodyText": "rename to .user.defined.partitioner.class", "url": "https://github.com/apache/hudi/pull/1500#discussion_r407232596", "createdAt": "2020-04-12T18:02:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -55,6 +55,7 @@\n   private static final String DEFAULT_PARALLELISM = \"1500\";\n   private static final String INSERT_PARALLELISM = \"hoodie.insert.shuffle.parallelism\";\n   private static final String BULKINSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\";\n+  private static final String BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = \"hoodie.bulkinsert.user_defined.partitioner.class\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzMjc1OA==", "bodyText": "theres probably a method in StringUtils that will do this check for you?", "url": "https://github.com/apache/hudi/pull/1500#discussion_r407232758", "createdAt": "2020-04-12T18:03:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -152,6 +154,24 @@ public static KeyGenerator createKeyGenerator(TypedProperties props) throws IOEx\n     }\n   }\n \n+  /**\n+   * Create a UserDefinedBulkInsertPartitioner class via reflection,\n+   * <br>\n+   * if the class name of UserDefinedBulkInsertPartitioner is configured through the HoodieWriteConfig.\n+   * @see HoodieWriteConfig#getUserDefinedBulkInsertPartitionerClass()\n+   */\n+  private static Option<UserDefinedBulkInsertPartitioner> createUserDefinedBulkInsertPartitioner(HoodieWriteConfig config)\n+          throws IOException {\n+    String bulkInsertPartitionerClass = config.getUserDefinedBulkInsertPartitionerClass();\n+    try {\n+      return bulkInsertPartitionerClass == null || bulkInsertPartitionerClass.isEmpty()", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzMjgyNA==", "bodyText": "throw a HoodieException?  this is not an IOException per se?", "url": "https://github.com/apache/hudi/pull/1500#discussion_r407232824", "createdAt": "2020-04-12T18:04:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -152,6 +154,24 @@ public static KeyGenerator createKeyGenerator(TypedProperties props) throws IOEx\n     }\n   }\n \n+  /**\n+   * Create a UserDefinedBulkInsertPartitioner class via reflection,\n+   * <br>\n+   * if the class name of UserDefinedBulkInsertPartitioner is configured through the HoodieWriteConfig.\n+   * @see HoodieWriteConfig#getUserDefinedBulkInsertPartitionerClass()\n+   */\n+  private static Option<UserDefinedBulkInsertPartitioner> createUserDefinedBulkInsertPartitioner(HoodieWriteConfig config)\n+          throws IOException {\n+    String bulkInsertPartitionerClass = config.getUserDefinedBulkInsertPartitionerClass();\n+    try {\n+      return bulkInsertPartitionerClass == null || bulkInsertPartitionerClass.isEmpty()\n+              ? Option.empty() :\n+              Option.of((UserDefinedBulkInsertPartitioner) ReflectionUtils.loadClass(bulkInsertPartitionerClass));\n+    } catch (Throwable e) {\n+      throw new IOException(\"Could not create UserDefinedBulkInsertPartitioner class \" + bulkInsertPartitionerClass, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzMjkxOA==", "bodyText": "I wish our tests were all like this :). .. easy to read.. cc @xushiyan  :)", "url": "https://github.com/apache/hudi/pull/1500#discussion_r407232918", "createdAt": "2020-04-12T18:05:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/test/java/DataSourceUtilsTest.java", "diffHunk": "@@ -59,4 +99,47 @@ public void testAvroRecordsFieldConversion() {\n     assertEquals(\"Hudi Meetup\", DataSourceUtils.getNestedFieldValAsString(record, \"event_name\", true));\n     assertEquals(\"Hudi PMC\", DataSourceUtils.getNestedFieldValAsString(record, \"event_organizer\", true));\n   }\n+\n+  @Test\n+  public void testDoWriteOperationWithoutUserDefinedBulkInsertPartitioner() throws IOException {\n+    DataSourceUtils.doWriteOperation(hoodieWriteClient, hoodieRecords, \"test-time\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzMzA0NA==", "bodyText": "anyway we can use a lambda to implement an anonymuous partitioner instance and avoid this test class", "url": "https://github.com/apache/hudi/pull/1500#discussion_r407233044", "createdAt": "2020-04-12T18:06:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/test/java/org/apache/hudi/table/NoOpBulkInsertPartitioner.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+public class NoOpBulkInsertPartitioner<T extends HoodieRecordPayload>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0MTgxNzcz", "url": "https://github.com/apache/hudi/pull/1500#pullrequestreview-394181773", "createdAt": "2020-04-15T22:35:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMjozNToyMVrOGGN_Fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMjozNToyMVrOGGN_Fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE3Mzc4Mw==", "bodyText": "@kwondw We're moving to Junit 5.. could you please switch to Junit 5 api ?  thank you.", "url": "https://github.com/apache/hudi/pull/1500#discussion_r409173783", "createdAt": "2020-04-15T22:35:21Z", "author": {"login": "xushiyan"}, "path": "hudi-spark/src/test/java/DataSourceUtilsTest.java", "diffHunk": "@@ -17,18 +17,58 @@\n  */\n \n import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.NoOpBulkInsertPartitioner;\n \n import org.apache.avro.Schema;\n import org.apache.avro.generic.GenericData;\n import org.apache.avro.generic.GenericRecord;\n-import org.junit.jupiter.api.Test;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0ODc5MTY1", "url": "https://github.com/apache/hudi/pull/1500#pullrequestreview-394879165", "createdAt": "2020-04-16T18:07:34Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowNzozNFrOGGxQBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowODowNVrOGGxRZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MTU1Nw==", "bodyText": "I guess this wont work with an anonymous class ;/", "url": "https://github.com/apache/hudi/pull/1500#discussion_r409751557", "createdAt": "2020-04-16T18:07:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/test/java/DataSourceUtilsTest.java", "diffHunk": "@@ -59,4 +99,47 @@ public void testAvroRecordsFieldConversion() {\n     assertEquals(\"Hudi Meetup\", DataSourceUtils.getNestedFieldValAsString(record, \"event_name\", true));\n     assertEquals(\"Hudi PMC\", DataSourceUtils.getNestedFieldValAsString(record, \"event_organizer\", true));\n   }\n+\n+  @Test\n+  public void testDoWriteOperationWithoutUserDefinedBulkInsertPartitioner() throws IOException {\n+    DataSourceUtils.doWriteOperation(hoodieWriteClient, hoodieRecords, \"test-time\",\n+            DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL());\n+\n+    verify(hoodieWriteClient, times(1)).bulkInsert(any(hoodieRecords.getClass()), anyString(),\n+            optionCaptor.capture());\n+\n+    assertThat(optionCaptor.getValue(), is(equalTo(Option.empty())));\n+  }\n+\n+  @Test (expected = IOException.class)\n+  public void testDoWriteOperationWithNonExistUserDefinedBulkInsertPartitioner() throws IOException {\n+    verifyAndSetHoodieWriteClient(\"NonExistClassName\");\n+\n+    DataSourceUtils.doWriteOperation(hoodieWriteClient, hoodieRecords, \"test-time\",\n+            DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL());\n+  }\n+\n+  @Test\n+  public void testDoWriteOperationWithUserDefinedBulkInsertPartitioner() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MTkwOQ==", "bodyText": "You are right.. some cases are tricky. I would just suggest to move this as an inner class for the test itself and leave it be", "url": "https://github.com/apache/hudi/pull/1500#discussion_r409751909", "createdAt": "2020-04-16T18:08:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/test/java/org/apache/hudi/table/NoOpBulkInsertPartitioner.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+public class NoOpBulkInsertPartitioner<T extends HoodieRecordPayload>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzMzA0NA=="}, "originalCommit": null, "originalPosition": 25}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "26fb04b94d6acaf34b0cd0ada80ce65d20c1de37", "author": {"user": {"login": "kwondw", "name": "Dongwook"}}, "url": "https://github.com/apache/hudi/commit/26fb04b94d6acaf34b0cd0ada80ce65d20c1de37", "committedDate": "2020-04-17T00:34:19Z", "message": "[HUDI-772] Make UserDefinedBulkInsertPartitioner configurable for DataSource"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2NTcyNjAw", "url": "https://github.com/apache/hudi/pull/1500#pullrequestreview-396572600", "createdAt": "2020-04-20T15:38:06Z", "commit": {"oid": "26fb04b94d6acaf34b0cd0ada80ce65d20c1de37"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3364, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}