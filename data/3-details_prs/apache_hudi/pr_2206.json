{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA5NjI2NDMz", "number": 2206, "title": "[HUDI-1105] Adding dedup support for Bulk Insert w/ Rows", "bodyText": "What is the purpose of the pull request\nAdding dedup support for Bulk Insert w/ Rows\nBrief change log\n\nAdding dedup support for Bulk Insert w/ Rows\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded TestHoodieDatasetBulkInsertHelper to verify the change.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-10-25T17:09:37Z", "url": "https://github.com/apache/hudi/pull/2206", "merged": true, "mergeCommit": {"oid": "16e90d30eaa14e5c1c4632ad0a90497df601c637"}, "closed": true, "closedAt": "2021-07-07T21:38:26Z", "author": {"login": "nsivabalan"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdWDByNAFqTUxNjM3OTU0NA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABeoGuE3gBqjQ5OTAwNTE1OTI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2Mzc5NTQ0", "url": "https://github.com/apache/hudi/pull/2206#pullrequestreview-516379544", "createdAt": "2020-10-25T17:15:13Z", "commit": {"oid": "d46a16f158358ed35528cd916b7e33ce3127904c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzoxNToxM1rOHn7C_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNzoxNToxM1rOHn7C_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMzkzNA==", "bodyText": "@bvaradar : Is there some other option to go about deduping multiple Rows. Bcoz, in Bulksert with Rows, we don't have any HoodiePayload. Hence we have to operate on \"Row\"s only. So, have to group by keys and reduce by a function. In this patch, I have designed the function as this interface.\nSo, two questions.\na. Is there a better option.\nb. Even if we go with this option, I am getting task not serializable when executing this, since avro Schema is also sent along with Row. Also, wondering this might have any performance complications.\nCaused by: java.io.NotSerializableException: org.apache.avro.Schema$RecordSchema\nSerialization stack:\n\t- object not serializable (class: org.apache.avro.Schema$RecordSchema, value: {\"type\":\"record\",\"name\":\"trip\",\"namespace\":\"example.schema\",\"fields\":[{\"name\":\"_row_key\",\"type\":\"string\"},{\"name\":\"partition\",\"type\":\"string\"},{\"name\":\"ts\",\"type\":[\"long\",\"null\"]}]})\n\t- field (class: org.apache.hudi.TestHoodieDatasetBulkInsertHelper, name: schema, type: class org.apache.avro.Schema)\n\t- object (class org.apache.hudi.TestHoodieDatasetBulkInsertHelper, org.apache.hudi.TestHoodieDatasetBulkInsertHelper@8d7718e)\n\t- field (class: org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow, name: this$0, type: class org.apache.hudi.TestHoodieDatasetBulkInsertHelper)\n\t- object (class org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow, org.apache.hudi.TestHoodieDatasetBulkInsertHelper$TestPreCombineRow@3436d3d7)\n\t- element of array (index: 0)\n\t- array (class [Ljava.lang.Object;, size 1)\n\t- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)\n\t- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.apache.hudi.SparkRowWriteHelper, functionalInterfaceMethod=org/apache/spark/api/java/function/ReduceFunction.call:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic org/apache/hudi/SparkRowWriteHelper.lambda$deduplicateRows$14bf715c$1:(Lorg/apache/hudi/PreCombineRow;Lorg/apache/spark/sql/Row;Lorg/apache/spark/sql/Row;)Lorg/apache/spark/sql/Row;, instantiatedMethodType=(Lorg/apache/spark/sql/Row;Lorg/apache/spark/sql/Row;)Lorg/apache/spark/sql/Row;, numCaptured=1])\n\t- writeReplace data (class: java.lang.invoke.SerializedLambda)\n\t- object (class org.apache.hudi.SparkRowWriteHelper$$Lambda$306/2078785618, org.apache.hudi.SparkRowWriteHelper$$Lambda$306/2078785618@19d9ba89)\n\t- field (class: org.apache.spark.sql.KeyValueGroupedDataset$$anonfun$reduceGroups$1, name: f$4, type: interface org.apache.spark.api.java.function.ReduceFunction)\n\t- object (class org.apache.spark.sql.KeyValueGroupedDataset$$anonfun$reduceGroups$1, <function2>)\n\t- field (class: org.apache.spark.sql.expressions.ReduceAggregator, name: func, type: interface scala.Function2)\n\t- object (class org.apache.spark.sql.expressions.ReduceAggregator, org.apache.spark.sql.expressions.ReduceAggregator@14af73e1)", "url": "https://github.com/apache/hudi/pull/2206#discussion_r511623934", "createdAt": "2020-10-25T17:15:13Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/PreCombineRow.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.spark.sql.Row;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Interface used to preCombine two Spark sql Rows.\n+ */\n+public interface PreCombineRow extends Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d46a16f158358ed35528cd916b7e33ce3127904c"}, "originalPosition": 28}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "86ffb2577a0b5d759639ac5bfba6d9f5f00c9a6d", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/86ffb2577a0b5d759639ac5bfba6d9f5f00c9a6d", "committedDate": "2020-12-01T17:04:20Z", "message": "Fixing dedup support"}, "afterCommit": {"oid": "15d0db55fb82ef35633ab1807e0327954ec61233", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/15d0db55fb82ef35633ab1807e0327954ec61233", "committedDate": "2020-12-01T17:06:46Z", "message": "Fixing dedup support"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "15d0db55fb82ef35633ab1807e0327954ec61233", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/15d0db55fb82ef35633ab1807e0327954ec61233", "committedDate": "2020-12-01T17:06:46Z", "message": "Fixing dedup support"}, "afterCommit": {"oid": "01e47cc05db74d78d28a2dee255b8d3cd3f26f22", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/01e47cc05db74d78d28a2dee255b8d3cd3f26f22", "committedDate": "2021-05-23T04:27:32Z", "message": "Fixing dedup support"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c351cff462fd0eb5946f5fc2ad74c04d0078cb54", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/c351cff462fd0eb5946f5fc2ad74c04d0078cb54", "committedDate": "2021-05-23T17:23:49Z", "message": "Fixing build failure"}, "afterCommit": {"oid": "63ca76bbba456b67e5ce764907935731ff35b146", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/63ca76bbba456b67e5ce764907935731ff35b146", "committedDate": "2021-05-24T06:16:17Z", "message": "Fixing build failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "63ca76bbba456b67e5ce764907935731ff35b146", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/63ca76bbba456b67e5ce764907935731ff35b146", "committedDate": "2021-05-24T06:16:17Z", "message": "Fixing build failure"}, "afterCommit": {"oid": "39b3315a7d1ff35da30f91a578c4afe8dd2ad21d", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/39b3315a7d1ff35da30f91a578c4afe8dd2ad21d", "committedDate": "2021-05-24T06:33:08Z", "message": "Fixing build failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "39b3315a7d1ff35da30f91a578c4afe8dd2ad21d", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/39b3315a7d1ff35da30f91a578c4afe8dd2ad21d", "committedDate": "2021-05-24T06:33:08Z", "message": "Fixing build failure"}, "afterCommit": {"oid": "5d68e035eef1ef37b2121bafc67e1f81d85aea74", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/5d68e035eef1ef37b2121bafc67e1f81d85aea74", "committedDate": "2021-06-07T20:00:50Z", "message": "Some refactoring and fixing tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "10043f27bc97e65c476aaf6dd7dbf3fb69d9760c", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/10043f27bc97e65c476aaf6dd7dbf3fb69d9760c", "committedDate": "2021-06-07T20:31:31Z", "message": "Fixing tests"}, "afterCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/031b8fa2a947f69815bce9fa181dc98dd972d07e", "committedDate": "2021-06-08T02:31:16Z", "message": "Fixing tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Njk5NTk1MjIy", "url": "https://github.com/apache/hudi/pull/2206#pullrequestreview-699595222", "createdAt": "2021-07-06T06:56:33Z", "commit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0wNlQwNjo1NjozM1rOJ5gsLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0wNlQwNzowNToyOFrOJ5g_qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NDIwNQ==", "bodyText": "all these configs need to be redone based ConfigProperty/HoodieConfig", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664284205", "createdAt": "2021-07-06T06:56:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -98,6 +98,8 @@\n   public static final String DEFAULT_COMBINE_BEFORE_UPSERT = \"true\";\n   public static final String COMBINE_BEFORE_DELETE_PROP = \"hoodie.combine.before.delete\";\n   public static final String DEFAULT_COMBINE_BEFORE_DELETE = \"true\";\n+  public static final String COMBINE_BEFORE_BULK_INSERT_PROP = \"hoodie.combine.before.bulk.insert\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NTIyOQ==", "bodyText": "So far, we have used one config combine.before.insert to control it for both insert and bulk_insert. Can we keep it the same way? Otherwise, wont it be backwards incompatible, ie a user can be expecting the combine.before.insert continue to take effect for bulk_insert as well and it won't be the case?", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664285229", "createdAt": "2021-07-06T06:58:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -306,6 +308,10 @@ public boolean shouldCombineBeforeInsert() {\n     return Boolean.parseBoolean(props.getProperty(COMBINE_BEFORE_INSERT_PROP));\n   }\n \n+  public boolean shouldCombineBeforeBulkInsert() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NjUzNA==", "bodyText": "I understand that the new config is just used here as of this PR. but from an user standpoint, on the non-row writer path, combine.before.insert was controlling this already. We should just make it consistent.", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664286534", "createdAt": "2021-07-06T07:00:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/HoodieDatasetBulkInsertHelper.java", "diffHunk": "@@ -96,9 +97,15 @@\n                 functions.lit(\"\").cast(DataTypes.StringType))\n             .withColumn(HoodieRecord.FILENAME_METADATA_FIELD,\n                 functions.lit(\"\").cast(DataTypes.StringType));\n+\n+    Dataset<Row> dedupedDf = rowDatasetWithHoodieColumns;\n+    if (config.shouldCombineBeforeBulkInsert()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NjkwOA==", "bodyText": "nit: extra line?", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664286908", "createdAt": "2021-07-06T07:01:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzI0MQ==", "bodyText": "why the singleton etc? Can't we just use a static method?", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664287241", "createdAt": "2021-07-06T07:01:52Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4NzczMA==", "bodyText": "lets use reduceByKey(), which we use for RDD path? groupByKey() can hog memory.", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664287730", "createdAt": "2021-07-06T07:02:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n+  }\n+\n+  public static SparkRowWriteHelper newInstance() {\n+    return SparkRowWriteHelper.WriteHelperHolder.SPARK_WRITE_HELPER;\n+  }\n+\n+  public Dataset<Row> deduplicateRows(Dataset<Row> inputDf, String preCombineField, boolean isGlobalIndex) {\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+\n+    return inputDf.groupByKey(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4ODM1OA==", "bodyText": "have you tested with both Spark 2 and 3?  Some of these classes can be different and actually fail?", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664288358", "createdAt": "2021-07-06T07:03:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark-datasource/hudi-spark/src/main/java/org/apache/hudi/SparkRowWriteHelper.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Helper class to assist in deduplicating Rows for BulkInsert with Rows.\n+ */\n+public class SparkRowWriteHelper {\n+\n+  private SparkRowWriteHelper() {\n+  }\n+\n+  private static class WriteHelperHolder {\n+\n+    private static final SparkRowWriteHelper SPARK_WRITE_HELPER = new SparkRowWriteHelper();\n+  }\n+\n+  public static SparkRowWriteHelper newInstance() {\n+    return SparkRowWriteHelper.WriteHelperHolder.SPARK_WRITE_HELPER;\n+  }\n+\n+  public Dataset<Row> deduplicateRows(Dataset<Row> inputDf, String preCombineField, boolean isGlobalIndex) {\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+\n+    return inputDf.groupByKey(\n+        (MapFunction<Row, String>) value ->\n+            isGlobalIndex ? (value.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD)) :\n+                (value.getAs(HoodieRecord.PARTITION_PATH_METADATA_FIELD) + \"+\" + value.getAs(HoodieRecord.RECORD_KEY_METADATA_FIELD)), Encoders.STRING())\n+        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> {\n+          if (((Comparable) v1.getAs(preCombineField)).compareTo(((Comparable) v2.getAs(preCombineField))) >= 0) {\n+            return v1;\n+          } else {\n+            return v2;\n+          }\n+            }\n+        ).map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n+  }\n+\n+  private ExpressionEncoder getEncoder(StructType schema) {\n+    List<Attribute> attributes = JavaConversions.asJavaCollection(schema.toAttributes()).stream()\n+        .map(Attribute::toAttribute).collect(Collectors.toList());\n+    return RowEncoder.apply(schema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NDI4OTE5NQ==", "bodyText": "do we need to create the index to really check if its Global? Wondering if there are simpler means. (may be not, given we support even user defined indexes)", "url": "https://github.com/apache/hudi/pull/2206#discussion_r664289195", "createdAt": "2021-07-06T07:05:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -333,7 +334,9 @@ private[hudi] object HoodieSparkSqlWriter {\n     log.info(s\"Registered avro schema : ${schema.toString(true)}\")\n     val params = parameters.updated(HoodieWriteConfig.AVRO_SCHEMA, schema.toString)\n     val writeConfig = DataSourceUtils.createHoodieConfig(schema.toString, path.get, tblName, mapAsJavaMap(params))\n-    val hoodieDF = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsert(sqlContext, writeConfig, df, structName, nameSpace)\n+    val isGlobalIndex = SparkHoodieIndex.createIndex(writeConfig).isGlobal", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e"}, "originalPosition": 13}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "031b8fa2a947f69815bce9fa181dc98dd972d07e", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/031b8fa2a947f69815bce9fa181dc98dd972d07e", "committedDate": "2021-06-08T02:31:16Z", "message": "Fixing tests"}, "afterCommit": {"oid": "80ae3446670e4012ef3896477964bb916b71a864", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/80ae3446670e4012ef3896477964bb916b71a864", "committedDate": "2021-07-06T19:42:12Z", "message": "Addressing feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "80ae3446670e4012ef3896477964bb916b71a864", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/80ae3446670e4012ef3896477964bb916b71a864", "committedDate": "2021-07-06T19:42:12Z", "message": "Addressing feedback"}, "afterCommit": {"oid": "0b5cdce9cea59681d7e604c6f6706a4e96a19861", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/0b5cdce9cea59681d7e604c6f6706a4e96a19861", "committedDate": "2021-07-06T21:11:45Z", "message": "Addressing feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0b5cdce9cea59681d7e604c6f6706a4e96a19861", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/0b5cdce9cea59681d7e604c6f6706a4e96a19861", "committedDate": "2021-07-06T21:11:45Z", "message": "Addressing feedback"}, "afterCommit": {"oid": "ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "committedDate": "2021-07-06T21:20:22Z", "message": "Addressing feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/ad1d2d1a2f38cfb396d8f3d30f2f9572a1506e90", "committedDate": "2021-07-06T21:20:22Z", "message": "Addressing feedback"}, "afterCommit": {"oid": "1fa675d9739656c45850d60fa49b66970b18f5ac", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/1fa675d9739656c45850d60fa49b66970b18f5ac", "committedDate": "2021-07-07T04:00:13Z", "message": "Addressing feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf73f313cdfb5e640c16e40eb017151dcd464b41", "author": {"user": {"login": "lamberken", "name": "lamberken"}}, "url": "https://github.com/apache/hudi/commit/bf73f313cdfb5e640c16e40eb017151dcd464b41", "committedDate": "2021-07-07T15:16:04Z", "message": "trigger rebuild"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab8b925db2387548626d762dae3c8458371b43f3", "author": {"user": {"login": "linshan-ma", "name": null}}, "url": "https://github.com/apache/hudi/commit/ab8b925db2387548626d762dae3c8458371b43f3", "committedDate": "2021-07-07T15:16:04Z", "message": "[HUDI-1156] Remove unused dependencies from HoodieDeltaStreamerWrapper Class (#1927)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b436ba3ce44ad7d393e98f11bda8e986888f31fa", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/b436ba3ce44ad7d393e98f11bda8e986888f31fa", "committedDate": "2021-07-07T15:25:35Z", "message": "Adding Dedup support for BulkInsert with Rows"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "280aecde2d0a426616144becf8cfeb75bb86166b", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/280aecde2d0a426616144becf8cfeb75bb86166b", "committedDate": "2021-07-07T15:29:51Z", "message": "Fixing dedup support"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5785cf93a8a4214e8d2bad920edaa00e0d775874", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/5785cf93a8a4214e8d2bad920edaa00e0d775874", "committedDate": "2021-07-07T15:34:05Z", "message": "Fixing build failure"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28d12341dbe50489570dd13e9b2ac0aa9f41fff3", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/28d12341dbe50489570dd13e9b2ac0aa9f41fff3", "committedDate": "2021-07-07T15:35:03Z", "message": "Refactoring and minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8f5b6ef1537c2b3f68d1bef667fbfd56ca748599", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/8f5b6ef1537c2b3f68d1bef667fbfd56ca748599", "committedDate": "2021-07-07T15:36:48Z", "message": "Some refactoring and fixing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4813604f6757ecc09eb117e7e35754d564ef004", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/e4813604f6757ecc09eb117e7e35754d564ef004", "committedDate": "2021-07-07T15:41:31Z", "message": "Removing PreCombine interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c8d6dbb49a3c528190c1464e45fcb9ac137e43ad", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/c8d6dbb49a3c528190c1464e45fcb9ac137e43ad", "committedDate": "2021-07-07T15:41:33Z", "message": "Fixing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51ccc2db570a8cc996ff80725f668d7f6158aa24", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/51ccc2db570a8cc996ff80725f668d7f6158aa24", "committedDate": "2021-07-07T15:54:26Z", "message": "Addressing feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1fa675d9739656c45850d60fa49b66970b18f5ac", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/1fa675d9739656c45850d60fa49b66970b18f5ac", "committedDate": "2021-07-07T04:00:13Z", "message": "Addressing feedback"}, "afterCommit": {"oid": "51ccc2db570a8cc996ff80725f668d7f6158aa24", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/51ccc2db570a8cc996ff80725f668d7f6158aa24", "committedDate": "2021-07-07T15:54:26Z", "message": "Addressing feedback"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4232, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}