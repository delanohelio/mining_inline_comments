{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxMDEyMjQw", "number": 2254, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowOTowMlrOFDuqFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QwNjozMToxNFrOFJUq1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDU0NDg1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowOTowMlrOIDhp0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNjoyNzoxOVrOIHN9qA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2ODAxOQ==", "bodyText": "can you parallelize this? It is very similar to what you did for 'insert_overwrite_table'. Try to reuse the code from there if possible", "url": "https://github.com/apache/hudi/pull/2254#discussion_r540568019", "createdAt": "2020-12-10T23:09:02Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));\n+    result.setWriteStatuses(jsc.emptyRDD());\n+    Map<String, List<String>> partitionToReplaceFileIds = partitions.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQzOTcyMA==", "bodyText": "okay, deletePartition and insertoverwritetable implement insertoverwritecommitexecutor. And reuse the code", "url": "https://github.com/apache/hudi/pull/2254#discussion_r544439720", "createdAt": "2020-12-16T16:27:19Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));\n+    result.setWriteStatuses(jsc.emptyRDD());\n+    Map<String, List<String>> partitionToReplaceFileIds = partitions.stream()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2ODAxOQ=="}, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMDE4NTE3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQyMDo1MTowMlrOIFoC1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNjoyMzozMVrOIHNxMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc2OTg3Ng==", "bodyText": "Do you think using JavaRDD for partitions makes more sense (given all other APIs are using RDD)?", "url": "https://github.com/apache/hudi/pull/2254#discussion_r542769876", "createdAt": "2020-12-14T20:51:02Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -157,6 +157,15 @@ private synchronized FileSystemViewManager getViewManager() {\n    */\n   public abstract HoodieWriteMetadata<O> delete(HoodieEngineContext context, String instantTime, K keys);\n \n+  /**\n+   * Deletes all data of partitions.\n+   * @param context    HoodieEngineContext\n+   * @param instantTime Instant Time for the action\n+   * @param partitions   {@link List} of partition to be deleted\n+   * @return HoodieWriteMetadata\n+   */\n+  public abstract HoodieWriteMetadata deletePartitions(HoodieEngineContext context, String instantTime, List<String> partitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQzNjUyOQ==", "bodyText": "thanks, i thinks \"List\" will be ok. Because flink and spark will implement HoodieTable, use javaRDD not suitable to flink.", "url": "https://github.com/apache/hudi/pull/2254#discussion_r544436529", "createdAt": "2020-12-16T16:23:31Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -157,6 +157,15 @@ private synchronized FileSystemViewManager getViewManager() {\n    */\n   public abstract HoodieWriteMetadata<O> delete(HoodieEngineContext context, String instantTime, K keys);\n \n+  /**\n+   * Deletes all data of partitions.\n+   * @param context    HoodieEngineContext\n+   * @param instantTime Instant Time for the action\n+   * @param partitions   {@link List} of partition to be deleted\n+   * @return HoodieWriteMetadata\n+   */\n+  public abstract HoodieWriteMetadata deletePartitions(HoodieEngineContext context, String instantTime, List<String> partitions);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc2OTg3Ng=="}, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMDIxODI0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQyMDo1NjowOFrOIFoYLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNjozMTowM1rOIHOJGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc3NTM0Mg==", "bodyText": "could you try to simplify this test to avoid code duplication. Also, see if its possible to add similar test for MOR tables.", "url": "https://github.com/apache/hudi/pull/2254#discussion_r542775342", "createdAt": "2020-12-14T20:56:08Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -999,6 +999,110 @@ private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int\n     verifyRecordsWritten(commitTime2, inserts2, statuses);\n   }\n \n+  /**\n+   * Test scenario of writing fewer file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithFewerRecordsFirstPartition() throws Exception {\n+    verifyDeletePartitionsHandling(1000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithSimilarNumberOfRecords() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing more file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingHandlingWithFewerRecordsSecondThirdPartition() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 1000, 1000);\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records for first partition.\n+   *  2) Do write2 (upsert) with 'batch2RecordsCount' number of records for second partition.\n+   *  3) Do write3 (upsert) with 'batch3RecordsCount' number of records for third partition.\n+   *  4) delete first partition and check result.\n+   *  5) delete second and third partition and check result.\n+   *\n+   */\n+  private void verifyDeletePartitionsHandling(int batch1RecordsCount, int batch2RecordsCount, int batch3RecordsCount) throws Exception {\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000, false);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator();\n+\n+    // Do Inserts for DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime1 = \"001\";\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInsertsForPartition(commitTime1, batch1RecordsCount, DEFAULT_FIRST_PARTITION_PATH);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batch1Buckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyRecordsWritten(commitTime1, inserts1, statuses);\n+\n+    // Do Inserts for DEFAULT_SECOND_PARTITION_PATH\n+    String commitTime2 = \"002\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ0MjY0OA==", "bodyText": "thanks , have abstract two method insertPartitionRecordsWithCommit\u3001deletePartitionWithCommit. And reuse it.\nNow deletePartitionAction implement insertOverwriteAction , I think copy on write can cover it.  Add MOR is not very necessary.", "url": "https://github.com/apache/hudi/pull/2254#discussion_r544442648", "createdAt": "2020-12-16T16:31:03Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -999,6 +999,110 @@ private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int\n     verifyRecordsWritten(commitTime2, inserts2, statuses);\n   }\n \n+  /**\n+   * Test scenario of writing fewer file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithFewerRecordsFirstPartition() throws Exception {\n+    verifyDeletePartitionsHandling(1000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithSimilarNumberOfRecords() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing more file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingHandlingWithFewerRecordsSecondThirdPartition() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 1000, 1000);\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records for first partition.\n+   *  2) Do write2 (upsert) with 'batch2RecordsCount' number of records for second partition.\n+   *  3) Do write3 (upsert) with 'batch3RecordsCount' number of records for third partition.\n+   *  4) delete first partition and check result.\n+   *  5) delete second and third partition and check result.\n+   *\n+   */\n+  private void verifyDeletePartitionsHandling(int batch1RecordsCount, int batch2RecordsCount, int batch3RecordsCount) throws Exception {\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000, false);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator();\n+\n+    // Do Inserts for DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime1 = \"001\";\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInsertsForPartition(commitTime1, batch1RecordsCount, DEFAULT_FIRST_PARTITION_PATH);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batch1Buckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyRecordsWritten(commitTime1, inserts1, statuses);\n+\n+    // Do Inserts for DEFAULT_SECOND_PARTITION_PATH\n+    String commitTime2 = \"002\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc3NTM0Mg=="}, "originalCommit": {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNzU5NDI2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxOTo1NzowMVrOIIGbBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxOTo1NzowMVrOIIGbBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM2NDc0MA==", "bodyText": "you may want to assert that other partitions still have base files.", "url": "https://github.com/apache/hudi/pull/2254#discussion_r545364740", "createdAt": "2020-12-17T19:57:01Z", "author": {"login": "satishkotha"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -999,6 +999,103 @@ private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int\n     verifyRecordsWritten(commitTime2, inserts2, statuses);\n   }\n \n+  /**\n+   * Test scenario of writing fewer file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithFewerRecordsFirstPartition() throws Exception {\n+    verifyDeletePartitionsHandling(1000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithSimilarNumberOfRecords() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing more file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingHandlingWithFewerRecordsSecondThirdPartition() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 1000, 1000);\n+  }\n+\n+  private Set<String> insertPartitionRecordsWithCommit(SparkRDDWriteClient client, int recordsCount, String commitTime1, String partitionPath) {\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInsertsForPartition(commitTime1, recordsCount, partitionPath);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batchBuckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyRecordsWritten(commitTime1, inserts1, statuses);\n+    return batchBuckets;\n+  }\n+\n+  private Set<String> deletePartitionWithCommit(SparkRDDWriteClient client, String commitTime, List<String> deletePartitionPath) {\n+    client.startCommitWithTime(commitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n+    HoodieWriteResult writeResult = client.deletePartitions(deletePartitionPath, commitTime);\n+    Set<String> deletePartitionReplaceFileIds =\n+        writeResult.getPartitionToReplaceFileIds().entrySet()\n+            .stream().flatMap(entry -> entry.getValue().stream()).collect(Collectors.toSet());\n+    return deletePartitionReplaceFileIds;\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records for first partition.\n+   *  2) Do write2 (upsert) with 'batch2RecordsCount' number of records for second partition.\n+   *  3) Do write3 (upsert) with 'batch3RecordsCount' number of records for third partition.\n+   *  4) delete first partition and check result.\n+   *  5) delete second and third partition and check result.\n+   *\n+   */\n+  private void verifyDeletePartitionsHandling(int batch1RecordsCount, int batch2RecordsCount, int batch3RecordsCount) throws Exception {\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000, false);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator();\n+\n+    // Do Inserts for DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime1 = \"001\";\n+    Set<String> batch1Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch1RecordsCount, commitTime1, DEFAULT_FIRST_PARTITION_PATH);\n+\n+    // Do Inserts for DEFAULT_SECOND_PARTITION_PATH\n+    String commitTime2 = \"002\";\n+    Set<String> batch2Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch2RecordsCount, commitTime2, DEFAULT_SECOND_PARTITION_PATH);\n+\n+    // Do Inserts for DEFAULT_THIRD_PARTITION_PATH\n+    String commitTime3 = \"003\";\n+    Set<String> batch3Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch3RecordsCount, commitTime3, DEFAULT_THIRD_PARTITION_PATH);\n+\n+    // delete DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime4 = \"004\";\n+    Set<String> deletePartitionReplaceFileIds1 =\n+        deletePartitionWithCommit(client, commitTime4, Arrays.asList(DEFAULT_FIRST_PARTITION_PATH));\n+    assertEquals(batch1Buckets, deletePartitionReplaceFileIds1);\n+    List<HoodieBaseFile> baseFiles = HoodieClientTestUtils.getLatestBaseFiles(basePath, fs,\n+        String.format(\"%s/%s/*\", basePath, DEFAULT_FIRST_PARTITION_PATH));\n+    assertEquals(0, baseFiles.size());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17fbc00fa912a74221fdd2c5b5d2f931dc9cee9d"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1MzIwMDg5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QwNjoyOTo0OFrOILotxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QwNjoyOTo0OFrOILotxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA3MjMyNw==", "bodyText": "can we please use the HoodieTimer class here", "url": "https://github.com/apache/hudi/pull/2254#discussion_r549072327", "createdAt": "2020-12-27T06:29:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime,null, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eecd8a746d51f01eb669be6c4e9076f8b47512e9"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1MzIwMTUwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QwNjozMToxNFrOILouEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yN1QxMTozMTozMlrOILqfCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA3MjQwMQ==", "bodyText": "so we are saying the files have been replaced with an empty file list?", "url": "https://github.com/apache/hudi/pull/2254#discussion_r549072401", "createdAt": "2020-12-27T06:31:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime,null, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));\n+    result.setWriteStatuses(jsc.emptyRDD());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eecd8a746d51f01eb669be6c4e9076f8b47512e9"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTEwMTMyMQ==", "bodyText": "yes, just as insertoverwrite action and  insertoverwritetable action , they have write records, so they set PartitionToReplaceFileIds and also set the sWriteStatuses. But DeletePartitionCommitAction with no records to write , can just setPartitionToReplaceFileIds.  I think it make sense", "url": "https://github.com/apache/hudi/pull/2254#discussion_r549101321", "createdAt": "2020-12-27T11:31:32Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime,null, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));\n+    result.setWriteStatuses(jsc.emptyRDD());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA3MjQwMQ=="}, "originalCommit": {"oid": "eecd8a746d51f01eb669be6c4e9076f8b47512e9"}, "originalPosition": 59}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4174, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}