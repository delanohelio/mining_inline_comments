{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY4MzE4MTM3", "number": 1289, "title": "[HUDI-92] Provide reasonable names for Spark DAG stages in Hudi.", "bodyText": "What is the purpose of the pull request\nHUDI DAG stages do not have any names. The Spark History Server UI shows these stages with the HUDI JAVA filename and the line number.\nThis change provides descriptive names for the stages which makes it easier to visualize the HUDI DAG.\nBrief change log\n\n\nAll code locations where JavaSparkContext.XXX (DAG creation methods like parallelize) are used have been updated with descriptive names for the job with the following API\njsc.setJobGroup(title, description);\n\n\nTitle is the class name so its easy to identify. Description is the activity for that stage.\n\n\nUnit test code has been updates to enable the unit tests to write the Spark event logs so that even the unit tests can be visualized in a locally installed  Spark History Server UI.\n\n\nThe unit tests need to be run as follows:\n\n\nmvn test -DSPARK_EVLOG_DIR=/path/for/spark/event/log\nOnce the unit tests complete, the Spark History Server shows the application logs for all completed unit tests. The unit tests themselves can be identified by the test-class-name as the application name.\nVerify this pull request\n-- This pull request is already covered by existing tests, such as (please describe tests).\n\nAll existing unit tests pass.\nIf the SPARK_EVLOG_DIR property is not set, spark event logging is not enabled (this is defauly behavior)\nManually verified the change by running unit tests locally and observing the application logs on the locally installed Spark History Server.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-01-29T00:37:47Z", "url": "https://github.com/apache/hudi/pull/1289", "merged": true, "mergeCommit": {"oid": "b71f25f210c4004a2dcc97a9967399e74f870fc7"}, "closed": true, "closedAt": "2020-07-19T17:29:26Z", "author": {"login": "prashantwason"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcBMJTrABqjMwMDgzNTQ1NDE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc2gfbcgFqTQ1MTE0OTM2Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU0MjMxNTkw", "url": "https://github.com/apache/hudi/pull/1289#pullrequestreview-354231590", "createdAt": "2020-02-06T06:54:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQwNjo1NDo0OVrOFmQ0pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNlQwNzowNTozMVrOFmQ-mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NTgzMA==", "bodyText": "Change to Execute unschedule operations", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375665830", "createdAt": "2020-02-06T06:54:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java", "diffHunk": "@@ -356,6 +357,7 @@ private ValidationOpResult validateCompactionOperation(HoodieTableMetaClient met\n     } else {\n       LOG.info(\"The following compaction renaming operations needs to be performed to un-schedule\");\n       if (!dryRun) {\n+        jsc.setJobGroup(this.getClass().getSimpleName(), \"Execute renaming operations\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NTk3OA==", "bodyText": "change to Generate compaction unscheduling operations ?", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375665978", "createdAt": "2020-02-06T06:55:35Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java", "diffHunk": "@@ -398,6 +400,7 @@ private ValidationOpResult validateCompactionOperation(HoodieTableMetaClient met\n           \"Number of Compaction Operations :\" + plan.getOperations().size() + \" for instant :\" + compactionInstant);\n       List<CompactionOperation> ops = plan.getOperations().stream()\n           .map(CompactionOperation::convertFromAvroRecordInstance).collect(Collectors.toList());\n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Generate renaming operations\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NzE5MA==", "bodyText": "In general lets provide some context into what higher level context, the action is being performed i.e savepoints, compaction, rollbacks. etc . In that spirit, change to Collecting latest files for savepoint ?\nAlso wonder if we can include the commitTime in the detail i.e Collecting latest files for savepoint 20200205010000. This way, you can just go to past runs on spark history server and relate them to commits on hudi.. Even better, if someone is running deltastreamer in continuous mode, then they can see activity for commits over time", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375667190", "createdAt": "2020-02-06T07:00:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java", "diffHunk": "@@ -586,6 +586,7 @@ public boolean savepoint(String commitTime, String user, String comment) {\n           HoodieTimeline.compareTimestamps(commitTime, lastCommitRetained, HoodieTimeline.GREATER_OR_EQUAL),\n           \"Could not savepoint commit \" + commitTime + \" as this is beyond the lookup window \" + lastCommitRetained);\n \n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Collecting latest files in partition\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NzU2OQ==", "bodyText": "Obtain key ranges for file slices (range pruning=on)", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375667569", "createdAt": "2020-02-06T07:02:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java", "diffHunk": "@@ -254,6 +255,7 @@ private int determineParallelism(int inputParallelism, int totalSubPartitions) {\n \n     if (config.getBloomIndexPruneByRanges()) {\n       // also obtain file ranges, if range pruning is enabled\n+      jsc.setJobDescription(\"Obtain file ranges as range pruning is enabled\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NzY4OA==", "bodyText": "Compacting file slices?", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375667688", "createdAt": "2020-02-06T07:02:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/compact/HoodieMergeOnReadTableCompactor.java", "diffHunk": "@@ -94,6 +94,7 @@\n         .map(CompactionOperation::convertFromAvroRecordInstance).collect(toList());\n     LOG.info(\"Compactor compacting \" + operations + \" files\");\n \n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Compacting files\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2ODAwNg==", "bodyText": "I know the comments say files and you are just using that. but would be nice to stick to our terminologies as much as possible. https://cwiki.apache.org/confluence/display/HUDI/Design+And+Architecture", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375668006", "createdAt": "2020-02-06T07:04:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -298,6 +298,7 @@ public HoodieCleanerPlan scheduleClean(JavaSparkContext jsc) {\n       int cleanerParallelism = Math.min(partitionsToClean.size(), config.getCleanerParallelism());\n       LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n \n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Generates List of files to be cleaned\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2ODM3Nw==", "bodyText": "can we do this in a single line?", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375668377", "createdAt": "2020-02-06T07:05:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/HoodieClientTestHarness.java", "diffHunk": "@@ -107,11 +107,12 @@ protected void initSparkContexts(String appName) {\n   }\n \n   /**\n-   * Initializes the Spark contexts ({@link JavaSparkContext} and {@link SQLContext}) with a default name\n-   * <b>TestHoodieClient</b>.\n+   * Initializes the Spark contexts ({@link JavaSparkContext} and {@link SQLContext}) \n+   * with a default name matching the name of the class.\n    */\n   protected void initSparkContexts() {\n-    initSparkContexts(\"TestHoodieClient\");\n+    String ctxName = this.getClass().getSimpleName() + \"#\" + testName.getMethodName();\n+    initSparkContexts(ctxName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d5265701e812657a229463b1ff7181820f1e9ee", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/3d5265701e812657a229463b1ff7181820f1e9ee", "committedDate": "2020-07-13T17:56:44Z", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "3d5265701e812657a229463b1ff7181820f1e9ee", "author": {"user": {"login": "prashantwason", "name": "Prashant Wason"}}, "url": "https://github.com/apache/hudi/commit/3d5265701e812657a229463b1ff7181820f1e9ee", "committedDate": "2020-07-13T17:56:44Z", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxMTQ5MzYz", "url": "https://github.com/apache/hudi/pull/1289#pullrequestreview-451149363", "createdAt": "2020-07-19T17:29:17Z", "commit": {"oid": "3d5265701e812657a229463b1ff7181820f1e9ee"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4168, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}