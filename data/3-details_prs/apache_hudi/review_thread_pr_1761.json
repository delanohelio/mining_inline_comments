{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM4ODc1NDM2", "number": 1761, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDo1NDo0OVrOEJFyVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMDo1MjowMFrOEKCu1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTY3NDQ3OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDo1NDo0OVrOGpcl0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxODozODozM1rOGqdYuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ==", "bodyText": "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY should be changed to KEYGENERATOR_CLASS_OPT_KEY? and also when specify no partitioning tables, would use NonpartitionedKeyGenerator", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446113235", "createdAt": "2020-06-26T10:54:49Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM4OTcwOQ==", "bodyText": "Yap, seems both are necessary:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            **PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n          \n          \n            \n            **RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n          \n          \n            \n            Default value: `\"uuid\"`<br>\n          \n          \n            \n            \n          \n          \n            \n            **PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using both `KEYGENERATOR_CLASS_OPT_KEY` and `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446389709", "createdAt": "2020-06-26T20:15:23Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ5OTM1OA==", "bodyText": "Yap, seems both are necessary:\n\nThe HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY is used to sync to hive, if you are meaning sync to hive during the write, it makes sense, or it makes no sense.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446499358", "createdAt": "2020-06-27T08:03:54Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjY5NjI1Mg==", "bodyText": "I see under the \"syncing to hive\" section it states \"Both tools above support syncing of the table\u2019s latest schema to Hive metastore\" which implies that syncing table metadata to hive is not mandatory but optional. Is this actually true? It seems that using a metastore is actually mandatory...", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446696252", "createdAt": "2020-06-28T20:45:50Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2MTg1Nw==", "bodyText": "One would only write data into hudi but not syncing to hive, so the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY  is only used when syncing to hive metastore.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446961857", "createdAt": "2020-06-29T13:15:30Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3NDg0MA==", "bodyText": "Will add clarification that HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY is only necessary when using hive.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447174840", "createdAt": "2020-06-29T18:38:33Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, "originalCommit": null, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTY4NTk4OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDo1OToxNlrOGpcs7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxODozNzoyM1rOGqdWGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ==", "bodyText": "hi, why this cannot change across writes? would you please clarify? IIRU, It would be changed across writes.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446115055", "createdAt": "2020-06-26T10:59:16Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM3MjgwMQ==", "bodyText": "Good catch, that was meant to go in the description of the next parameter, TABLE_TYPE_OPT_KEY.\nThe text can be made more clear by using the following language instead\nInstead of \"Note: this cannot change across writes.\"\nUse \"Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark SaveMode.Append mode.\"\nAlso, I believe that the key and partition columns, cannot be changed after the table is first created as well, so this note snippet can be added to those fields as well. Is this correct?", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446372801", "createdAt": "2020-06-26T19:33:40Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM4NDA4NA==", "bodyText": "Btw, there seems to be a mismatch in the data deletion documentation as well. To delete records it seems the only thing necessary is to set OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL and HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY to classOf[GlobalDeleteKeyGenerator].getCanonicalName, however https://hudi.apache.org/docs/writing_data.html#deletes specifies a hard delete procedure that is different from this one.  The procedure there is listed as setting PAYLOAD_CLASS_OPT_KEY to \"org.apache.hudi.EmptyHoodieRecordPayload\".", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446384084", "createdAt": "2020-06-26T20:01:37Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ5ODkzMw==", "bodyText": "in fact, there are some ways to delete records in hudi, 1. we would use \"org.apache.hudi.EmptyHoodieRecordPayload\" to delete a batch of records, 2. we could directly use delete api, 3. we could we _hoodie_is_delete flag to delete record. Also the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY  has nothing to do with the writing path IIRC, it is only related to hive sync.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446498933", "createdAt": "2020-06-27T07:58:37Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ5OTE0Ng==", "bodyText": "Good catch, that was meant to go in the description of the next parameter, TABLE_TYPE_OPT_KEY.\nThe text can be made more clear by using the following language instead\nInstead of \"Note: this cannot change across writes.\"\nUse \"Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark SaveMode.Append mode.\"\nAlso, I believe that the key and partition columns, cannot be changed after the table is first created as well, so this note snippet can be added to those fields as well. Is this correct?\n\nYes, TABLE_TYPE_OPT_KEY is not meant to be changed in next write, and OPERATION_OPT_KEY would be changed during different writes, and maybe I misunderstand what you meant.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446499146", "createdAt": "2020-06-27T08:00:39Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjY5NTk0Mg==", "bodyText": "In regards to deletion: I believe method 2 that you mentioned with delete api, is the same thing as as setting  OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL (DELETE_OPERATION_OPT_VAL has a string value of \"delete\"). In terms of method 3, I believe this flag method is only applicable when using the DeltaStreamer based on this blogpost https://hudi.apache.org/blog/delete-support-in-hudi/ , is this correct? And for method 1 of setting PAYLOAD_CLASS_OPT_KEY to \"org.apache.hudi.EmptyHoodieRecordPayload\", could you please clarify what the difference is between using method 1 and method 2? It seems the steps are the same as first a DataFrame must be prepared including the full records that need to be deleted, and then the write operation must be done using one of the 2 methods..", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446695942", "createdAt": "2020-06-28T20:42:59Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2NTAxNg==", "bodyText": "the flag method is not only applicable when using DeltaStreamer, but also applicable when using HoodieWriteClient/Spark DataSource, in this way, the ingested batch record would consist of both new insert records and delete records, that means you would insert and delete records with one batch, but when setting to EmptyHoodieRecordPayload, the ingested batch record is considered to be all delete records.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446965016", "createdAt": "2020-06-29T13:20:07Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3NDE3MQ==", "bodyText": "Understood. Will clarify the Deletes section with this.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447174171", "createdAt": "2020-06-29T18:37:23Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, "originalCommit": null, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTY5MDA3OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMTowMTowM1rOGpcvnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxOTo1NjowMVrOGps-tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTc0MA==", "bodyText": "TimestampBasedKeyGenerator and GlobalDeleteKeyGenerator are also available.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446115740", "createdAt": "2020-06-26T11:01:03Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM4MTc1MA==", "bodyText": "Good call, also it seems that default is now actually classOf[SlashEncodedDayPartitionValueExtractor].getCanonicalName, which is also available.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            `classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n          \n          \n            \n            `classOf[SlashEncodedDayPartitionValueExtractor].getCanonicalName` (default), `classOf[MultiPartKeysValueExtractor].getCanonicalName`, `classOf[TimestampBasedKeyGenerator].getCanonicalName`, `classOf[NonPartitionedExtractor].getCanonicalName`, `classOf[GlobalDeleteKeyGenerator].getCanonicalName` (to be used when OPERATION_OPT_KEY is set to DELETE_OPERATION_OPT_VAL)", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446381750", "createdAt": "2020-06-26T19:56:01Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTc0MA=="}, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTcxNTUxOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMDo1N1rOGpc_gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxOToxNzoxN1rOGpsAkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTgxMQ==", "bodyText": "would be changed to format(\"hudi\")", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446119811", "createdAt": "2020-06-26T11:10:57Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n+\n+\n+Example:\n+Upsert a DataFrame, specifying the necessary field names for `recordKey => _row_key`, `partitionPath => partition`, and `precombineKey => timestamp`\n \n ```java\n inputDF.write()\n        .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2NTg0Mw==", "bodyText": "This seems to be specified as \"org.apache.hudi\" by all the existing documentation that I can find and when I run the code as well. See example of current usage here: https://hudi.apache.org/docs/writing_data.html#datasource-writer", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446365843", "createdAt": "2020-06-26T19:17:17Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n+\n+\n+Example:\n+Upsert a DataFrame, specifying the necessary field names for `recordKey => _row_key`, `partitionPath => partition`, and `precombineKey => timestamp`\n \n ```java\n inputDF.write()\n        .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTgxMQ=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTcxNTc1OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMTowM1rOGpc_ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxOTozNToyNFrOGpseeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTg0Mw==", "bodyText": "would be changed to format(\"hudi\")", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446119843", "createdAt": "2020-06-26T11:11:03Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n+\n+\n+Example:\n+Upsert a DataFrame, specifying the necessary field names for `recordKey => _row_key`, `partitionPath => partition`, and `precombineKey => timestamp`\n \n ```java\n inputDF.write()\n        .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM3MzQ5Nw==", "bodyText": "marking resolved as duplicate", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446373497", "createdAt": "2020-06-26T19:35:24Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n+\n+\n+Example:\n+Upsert a DataFrame, specifying the necessary field names for `recordKey => _row_key`, `partitionPath => partition`, and `precombineKey => timestamp`\n \n ```java\n inputDF.write()\n        .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTg0Mw=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTcxNzgzOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxMTo0NVrOGpdA2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxOTozNjo0M1rOGpsgiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMDE1NQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446120155", "createdAt": "2020-06-26T11:11:45Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM3NDAyNA==", "bodyText": "Same reply as well,\n\nThis seems to be specified as \"org.apache.hudi\" by all the existing documentation that I can find and when I run the code as well. See example of current usage here: https://hudi.apache.org/docs/writing_data.html#datasource-writer", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446374024", "createdAt": "2020-06-26T19:36:43Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMDE1NQ=="}, "originalCommit": null, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTcyODU1OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMToxNTozMlrOGpdHWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMjoxNDo1OVrOGrQVXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA==", "bodyText": "when partition path is ${tablePath}/a/b/c, the load would be changed to tablePath + \"/*/*/*/*\" accordingly.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446121818", "createdAt": "2020-06-26T11:15:32Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")\n+     .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL())\n+     .load(tablePath + \"/*\") //Include \"/*\" at the end of the path if the table is partitioned", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjY5OTU1Ng==", "bodyText": "You are correct, updating. Checking the actual partitioned directories seems like something that we should add into the hudi framework itself.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446699556", "createdAt": "2020-06-28T21:20:07Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")\n+     .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL())\n+     .load(tablePath + \"/*\") //Include \"/*\" at the end of the path if the table is partitioned", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjcwMzczNg==", "bodyText": "Well, it would be one fewer, no? For each partition level so tablePath + \"/*/*/*\"", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446703736", "createdAt": "2020-06-28T22:03:26Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")\n+     .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL())\n+     .load(tablePath + \"/*\") //Include \"/*\" at the end of the path if the table is partitioned", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2NTkzOQ==", "bodyText": "no, you would refer to http://hudi.apache.org/docs/quick-start-guide.html#query-data", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446965939", "createdAt": "2020-06-29T13:21:27Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")\n+     .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL())\n+     .load(tablePath + \"/*\") //Include \"/*\" at the end of the path if the table is partitioned", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA=="}, "originalCommit": null, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAwOTU2NQ==", "bodyText": "This is a separate point from documentation, but ideally wouldn't it be better for Hudi to figure out which sub-directories need to be read for the partitions, instead of expecting to be passed the tree level of the partitions?", "url": "https://github.com/apache/hudi/pull/1761#discussion_r448009565", "createdAt": "2020-06-30T22:14:59Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")\n+     .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL())\n+     .load(tablePath + \"/*\") //Include \"/*\" at the end of the path if the table is partitioned", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA=="}, "originalCommit": null, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MTM5NDI4OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQyMDoxODo0MFrOGpti0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQyMDoxODo0MFrOGpti0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM5MDk5Mw==", "bodyText": "Add clarity on current limitation\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            `classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n          \n          \n            \n            `classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName` (Non-partitioned tables can currently only have a single key column, [HUDI-1053](https://issues.apache.org/jira/browse/HUDI-1053)), `classOf[ComplexKeyGenerator].getName`", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446390993", "createdAt": "2020-06-26T20:18:40Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4NTYxMDU2OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMzoyMjozNVrOGqQr3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxMjo1OToyMlrOGrk0mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA==", "bodyText": "hi, again the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY is not needed when not syncing to hive.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446966748", "createdAt": "2020-06-29T13:22:35Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3ODA1Ng==", "bodyText": "This notes that HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY only needs to be specified if using hive. Do you have a suggestion on how to phrase it differently?", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447178056", "createdAt": "2020-06-29T18:44:14Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA=="}, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MjQ2Mg==", "bodyText": "how about\nSpecify partitioning/no partitioning using KEYGENERATOR_CLASS_OPT_KEY, and using HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY when syncing to hive ?", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447592462", "createdAt": "2020-06-30T10:50:50Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA=="}, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2ODQ1Nw==", "bodyText": "Building on that, how about:?\nSpecify partitioning/no partitioning using KEYGENERATOR_CLASS_OPT_KEY. If synchronizing to hive, also specify using HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447768457", "createdAt": "2020-06-30T15:20:56Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA=="}, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODM0NTI0MA==", "bodyText": "Building on that, how about:?\nSpecify partitioning/no partitioning using KEYGENERATOR_CLASS_OPT_KEY. If synchronizing to hive, also specify using HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY.\n\nresonable.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r448345240", "createdAt": "2020-07-01T12:59:22Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA=="}, "originalCommit": null, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4NTYxODQxOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMzoyNDoxOVrOGqQwtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxODo0MDoyOFrOGqddMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Nzk5MA==", "bodyText": "same to this section.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446967990", "createdAt": "2020-06-29T13:24:19Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to. Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark `SaveMode.Append` mode.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName` (Non-partitioned tables can currently only have a single key column, [HUDI-1053](https://issues.apache.org/jira/browse/HUDI-1053)), `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3NTk4NQ==", "bodyText": "Also adding clarity that this is only necessary when using hive.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447175985", "createdAt": "2020-06-29T18:40:28Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to. Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark `SaveMode.Append` mode.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName` (Non-partitioned tables can currently only have a single key column, [HUDI-1053](https://issues.apache.org/jira/browse/HUDI-1053)), `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Nzk5MA=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4OTY0ODUyOnYy", "diffSide": "RIGHT", "path": "docs/_docs/1_1_quick_start_guide.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMDo0ODo0N1rOGq2zew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxMzowMDozOFrOGrk3bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTI5MQ==", "bodyText": "hi, would we keep the partitionpath as it is?", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447591291", "createdAt": "2020-06-30T10:48:47Z", "author": {"login": "leesf"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -117,7 +117,7 @@ df.write.format(\"hudi\").\n   options(getQuickstartWriteConfigs).\n   option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n   option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partition_path\").", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0MTU4MA==", "bodyText": "Well in some places it said \"partitionpath\" and in some places it said \"partitionPath\". I do like snake case better for text names, but I'm open to any of the options as long as we're consistent.\nFor instance:\nLine 89: spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\nLine 120: option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447741580", "createdAt": "2020-06-30T14:46:14Z", "author": {"login": "afeldman1"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -117,7 +117,7 @@ df.write.format(\"hudi\").\n   options(getQuickstartWriteConfigs).\n   option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n   option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partition_path\").", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTI5MQ=="}, "originalCommit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODM0NTk2NA==", "bodyText": "Well in some places it said \"partitionpath\" and in some places it said \"partitionPath\". I do like snake case better for text names, but I'm open to any of the options as long as we're consistent.\nFor instance:\nLine 89: spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\nLine 120: option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n\nlet's change to partitionpath.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r448345964", "createdAt": "2020-07-01T13:00:38Z", "author": {"login": "leesf"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -117,7 +117,7 @@ df.write.format(\"hudi\").\n   options(getQuickstartWriteConfigs).\n   option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n   option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partition_path\").", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTI5MQ=="}, "originalCommit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4OTY1OTc0OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMDo1MjowMFrOGq26Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMDo1MjowMFrOGq26Rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzAzMA==", "bodyText": "great.", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447593030", "createdAt": "2020-06-30T10:52:00Z", "author": {"login": "leesf"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -220,9 +253,16 @@ For more info refer to [Delete support in Hudi](https://cwiki.apache.org/conflue\n \n  - **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. \n  This can be simply achieved by ensuring the appropriate fields are nullable in the table schema and simply upserting the table after setting these fields to null.\n- - **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the table. This can be achieved by issuing an upsert with a custom payload implementation\n- via either DataSource or DeltaStreamer which always returns Optional.Empty as the combined value. Hudi ships with a built-in `org.apache.hudi.EmptyHoodieRecordPayload` class that does exactly this.\n  \n+ - **Hard Deletes** : A stronger form of deletion is to physically remove any trace of the record from the table. This can be achieved in 3 different ways.\n+\n+   1) Using DataSource, set `OPERATION_OPT_KEY` to `DELETE_OPERATION_OPT_VAL`. This will remove all records in the DataSet being submitted.\n+   \n+   2) Using DataSource, set `PAYLOAD_CLASS_OPT_KEY` to `\"org.apache.hudi.EmptyHoodieRecordPayload\"`. This will remove all records in the DataSet being submitted. \n+   \n+   3) Using DataSource or DeltaStreamer, add a column named `_hoodie_is_deleted` to DataSet. The value of this column must be set to `true` for all records to be deleted and either `false` or left null for any records to be upserted.\n+    ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd"}, "originalPosition": 78}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4510, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}