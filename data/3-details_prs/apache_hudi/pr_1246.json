{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY0NDEwMDA2", "number": 1246, "title": "[HUDI-552] Fix the schema mismatch in Row-to-Avro conversion", "bodyText": "What is the purpose of the pull request\nThis PR addresses HUDI-552.  When using the FilebasedSchemaProvider to provide the source/target schema in Avro, while ingesting data with the same columns from RowSource, the DeltaStreamer failed.  The root cause is that when writing parquet files in Spark, all fields are automatically converted to be nullable for compatibility reasons.  If the source Avro schema has non-null fields, AvroConversionUtils.createRdd still uses the schema from the Dataframe to convert the Row to Avro record, resulting in a different schema (only nullability difference).\nTo fix this issue, the Avro schema, if exists, is passed to the conversion function to reconstruct the correct StructType for conversion.\nBrief change log\n\nPassed the Avro schema to createRdd to generate the correct StructType for conversion in DeltaSync.readFromSource and AvroConversionUtils.createRdd\nAdded new tests to make sure the logic is correct (before this schema fix some of the new tests failed)\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded tests in TestHoodieDeltaStreamer to test the HoodieDeltaStreamer with ParquetDFSSource under different configurations\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-01-18T09:36:09Z", "url": "https://github.com/apache/hudi/pull/1246", "merged": true, "mergeCommit": {"oid": "d0ee95ed16de6c3568f575169cb993b9c10ced3d"}, "closed": true, "closedAt": "2020-01-19T00:40:57Z", "author": {"login": "yihua"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb7f4HugH2gAyMzY0NDEwMDA2OmIyOGFmMmNlNTJjMDgyMjdiNjE5NDI2ZTIwNWUxYzJjZjE0YmNjMzk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb8WiD5AFqTM0NTU4ODcxNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b28af2ce52c08227b619426e205e1c2cf14bcc39", "author": {"user": {"login": "yihua", "name": "Y Ethan Guo"}}, "url": "https://github.com/apache/hudi/commit/b28af2ce52c08227b619426e205e1c2cf14bcc39", "committedDate": "2020-01-18T09:24:49Z", "message": "[HUDI-552] Fix the schema mismatch in Row-to-Avro conversion"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0OTU2NDQ5", "url": "https://github.com/apache/hudi/pull/1246#pullrequestreview-344956449", "createdAt": "2020-01-18T18:35:29Z", "commit": {"oid": "b28af2ce52c08227b619426e205e1c2cf14bcc39"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ce2019b716fc764373c51d21563c9b2a140638a", "author": {"user": {"login": "yihua", "name": "Y Ethan Guo"}}, "url": "https://github.com/apache/hudi/commit/9ce2019b716fc764373c51d21563c9b2a140638a", "committedDate": "2020-01-19T00:04:39Z", "message": "Fix unit tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ1NTg4NzE3", "url": "https://github.com/apache/hudi/pull/1246#pullrequestreview-345588717", "createdAt": "2020-01-21T01:05:29Z", "commit": {"oid": "9ce2019b716fc764373c51d21563c9b2a140638a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMTowNToyOVrOFfsEcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMTowNToyOVrOFfsEcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc3MjIwOQ==", "bodyText": "is the key to this property right? Isn't \".....target.schema.file\" ?", "url": "https://github.com/apache/hudi/pull/1246#discussion_r368772209", "createdAt": "2020-01-21T01:05:29Z", "author": {"login": "nsivabalan"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -620,6 +636,62 @@ public void testDistributedTestDataSource() {\n     Assert.assertEquals(1000, c);\n   }\n \n+  private static void prepareParquetDFSFiles(int numRecords) throws IOException {\n+    String path = PARQUET_SOURCE_ROOT + \"/1.parquet\";\n+    HoodieTestDataGenerator dataGenerator = new HoodieTestDataGenerator();\n+    Helpers.saveParquetToDFS(Helpers.toGenericRecords(\n+        dataGenerator.generateInserts(\"000\", numRecords), dataGenerator), new Path(path));\n+  }\n+\n+  private void prepareParquetDFSSource(boolean useSchemaProvider, boolean hasTransformer) throws IOException {\n+    // Properties used for testing delta-streamer with Parquet source\n+    TypedProperties parquetProps = new TypedProperties();\n+    parquetProps.setProperty(\"include\", \"base.properties\");\n+    parquetProps.setProperty(\"hoodie.datasource.write.recordkey.field\", \"_row_key\");\n+    parquetProps.setProperty(\"hoodie.datasource.write.partitionpath.field\", \"not_there\");\n+    if (useSchemaProvider) {\n+      parquetProps.setProperty(\"hoodie.deltastreamer.schemaprovider.source.schema.file\", dfsBasePath + \"/source.avsc\");\n+      if (hasTransformer) {\n+        parquetProps.setProperty(\"hoodie.deltastreamer.schemaprovider.source.schema.file\", dfsBasePath + \"/target.avsc\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ce2019b716fc764373c51d21563c9b2a140638a"}, "originalPosition": 86}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4102, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}