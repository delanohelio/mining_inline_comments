{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ0MjgyODQ1", "number": 2366, "title": "[HUDI-1312] [RFC-15] Support for metadata listing for snapshot queries through Hive/SparkSQL", "bodyText": "This provides support for snapshot queries on MOR/COW tables through Hive/SparkSQL. Enabled by setting the following configuration\nset hoodie.metadata.enable=true\n\nTips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\n(For example: This pull request adds quick-start document.)\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-12-22T18:10:57Z", "url": "https://github.com/apache/hudi/pull/2366", "merged": true, "mergeCommit": {"oid": "66ce7f1b14cdb0a073de5edab9dbafb3aa457cb3"}, "closed": true, "closedAt": "2020-12-29T21:09:56Z", "author": {"login": "rmpifer"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdonXhqAH2gAyNTQ0MjgyODQ1OjBkMGFkNzQwNWI3ZDE3MDY4NGRkYjc2NzI1YTlhNzVhYTQwNjE1ODU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdpTA2HgFqTU1ODQ5MTQ1MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/0d0ad7405b7d170684ddb76725a9a75aa4061585", "committedDate": "2020-12-22T09:46:12Z", "message": "[RFC-15] Support for metadata listing for snapshot queries through Hive/SparkSQL"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3Mjk4ODky", "url": "https://github.com/apache/hudi/pull/2366#pullrequestreview-557298892", "createdAt": "2020-12-22T18:59:16Z", "commit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDMwNjkw", "url": "https://github.com/apache/hudi/pull/2366#pullrequestreview-557430690", "createdAt": "2020-12-22T23:50:42Z", "commit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3NDM1OTkw", "url": "https://github.com/apache/hudi/pull/2366#pullrequestreview-557435990", "createdAt": "2020-12-23T00:10:56Z", "commit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxMDo1NlrOIKM08Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwMDoxNDozN1rOIKM4bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NjgzMw==", "bodyText": "hmmm. slightly orthogonal, but the HoodieBaseFile itself should hand us a FileStatus object right? we should probably rethink the need for refreshing file status.", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547566833", "createdAt": "2020-12-23T00:10:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -391,27 +397,48 @@ public static FileStatus getFileStatus(HoodieBaseFile baseFile) throws IOExcepti\n     return grouped;\n   }\n \n+  public static Map<HoodieTableMetaClient, List<Path>> groupSnapshotPathsByMetaClient(\n+          Collection<HoodieTableMetaClient> metaClientList,\n+          List<Path> snapshotPaths\n+  ) {\n+    Map<HoodieTableMetaClient, List<Path>> grouped = new HashMap<>();\n+    metaClientList.forEach(metaClient -> grouped.put(metaClient, new ArrayList<>()));\n+    for (Path path : snapshotPaths) {\n+      // Find meta client associated with the input path\n+      metaClientList.stream().filter(metaClient -> path.toString().contains(metaClient.getBasePath()))\n+              .forEach(metaClient -> grouped.get(metaClient).add(path));\n+    }\n+    return grouped;\n+  }\n+\n   /**\n-   * Filters data files for a snapshot queried table.\n+   * Filters data files under @param paths for a snapshot queried table.\n    * @param job\n-   * @param metadata\n-   * @param fileStatuses\n+   * @param metaClient\n+   * @param paths\n    * @return\n    */\n   public static List<FileStatus> filterFileStatusForSnapshotMode(\n-      JobConf job, HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) throws IOException {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n+          JobConf job, HoodieTableMetaClient metaClient, List<Path> paths) throws IOException {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n+      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metaClient);\n     }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n+\n+    boolean useFileListingFromMetadata = job.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = job.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    HoodieTableFileSystemView fsView = FileSystemViewManager.createInMemoryFileSystemView(metaClient,\n+            useFileListingFromMetadata, verifyFileListing);\n+\n+    List<HoodieBaseFile> filteredBaseFiles = new ArrayList<>();\n+    for (Path p : paths) {\n+      String relativePartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), p);\n+      List<HoodieBaseFile> matched = fsView.getLatestBaseFiles(relativePartitionPath).collect(Collectors.toList());\n+      filteredBaseFiles.addAll(matched);\n+    }\n+\n+    LOG.info(\"Total paths to process after hoodie filter \" + filteredBaseFiles.size());\n     List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n+    for (HoodieBaseFile filteredFile : filteredBaseFiles) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NzA2MQ==", "bodyText": "note to self: doing this by path is ok, since the FileSystemView internally caches per partition.", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547567061", "createdAt": "2020-12-23T00:11:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -391,27 +397,48 @@ public static FileStatus getFileStatus(HoodieBaseFile baseFile) throws IOExcepti\n     return grouped;\n   }\n \n+  public static Map<HoodieTableMetaClient, List<Path>> groupSnapshotPathsByMetaClient(\n+          Collection<HoodieTableMetaClient> metaClientList,\n+          List<Path> snapshotPaths\n+  ) {\n+    Map<HoodieTableMetaClient, List<Path>> grouped = new HashMap<>();\n+    metaClientList.forEach(metaClient -> grouped.put(metaClient, new ArrayList<>()));\n+    for (Path path : snapshotPaths) {\n+      // Find meta client associated with the input path\n+      metaClientList.stream().filter(metaClient -> path.toString().contains(metaClient.getBasePath()))\n+              .forEach(metaClient -> grouped.get(metaClient).add(path));\n+    }\n+    return grouped;\n+  }\n+\n   /**\n-   * Filters data files for a snapshot queried table.\n+   * Filters data files under @param paths for a snapshot queried table.\n    * @param job\n-   * @param metadata\n-   * @param fileStatuses\n+   * @param metaClient\n+   * @param paths\n    * @return\n    */\n   public static List<FileStatus> filterFileStatusForSnapshotMode(\n-      JobConf job, HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) throws IOException {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n+          JobConf job, HoodieTableMetaClient metaClient, List<Path> paths) throws IOException {\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n+      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metaClient);\n     }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n+\n+    boolean useFileListingFromMetadata = job.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = job.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    HoodieTableFileSystemView fsView = FileSystemViewManager.createInMemoryFileSystemView(metaClient,\n+            useFileListingFromMetadata, verifyFileListing);\n+\n+    List<HoodieBaseFile> filteredBaseFiles = new ArrayList<>();\n+    for (Path p : paths) {\n+      String relativePartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), p);\n+      List<HoodieBaseFile> matched = fsView.getLatestBaseFiles(relativePartitionPath).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU2NzcyNA==", "bodyText": "makes sense", "url": "https://github.com/apache/hudi/pull/2366#discussion_r547567724", "createdAt": "2020-12-23T00:14:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "diffHunk": "@@ -63,13 +69,25 @@\n     // TODO(vc): Should we handle also non-hoodie splits here?\n     Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByBasePath(conf, partitionsToParquetSplits.keySet());\n \n+    boolean useFileListingFromMetadata = conf.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = conf.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);\n+    // Create file system cache so metadata table is only instantiated once. Also can benefit normal file listing if\n+    // partition path is listed twice so file groups will already be loaded in file system\n+    Map<HoodieTableMetaClient, HoodieTableFileSystemView> fsCache = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4NDkxNDUw", "url": "https://github.com/apache/hudi/pull/2366#pullrequestreview-558491450", "createdAt": "2020-12-24T12:37:15Z", "commit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQxMjozNzoxNVrOILG6GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNFQxMjozNzoxNVrOILG6GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODUxODQyNA==", "bodyText": "These 2 config options is every where, can we just pass the JobConf to the tool method FileSystemViewManager.createInMemoryFileSystemView and fetch them inside the method ? That would make the invocation more clean, IMO.", "url": "https://github.com/apache/hudi/pull/2366#discussion_r548518424", "createdAt": "2020-12-24T12:37:15Z", "author": {"login": "danny0405"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "diffHunk": "@@ -63,13 +69,25 @@\n     // TODO(vc): Should we handle also non-hoodie splits here?\n     Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getTableMetaClientByBasePath(conf, partitionsToParquetSplits.keySet());\n \n+    boolean useFileListingFromMetadata = conf.getBoolean(METADATA_ENABLE_PROP, DEFAULT_METADATA_ENABLE_FOR_READERS);\n+    boolean verifyFileListing = conf.getBoolean(METADATA_VALIDATE_PROP, DEFAULT_METADATA_VALIDATE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d0ad7405b7d170684ddb76725a9a75aa4061585"}, "originalPosition": 25}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4073, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}