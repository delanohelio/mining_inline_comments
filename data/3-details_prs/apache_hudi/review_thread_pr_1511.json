{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAyNTEwMDc2", "number": 1511, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTo0MDo0MlrODxcn_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMzoxNzowMFrODzhlbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMTc1ODA0OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTo0MDo0MlrOGE7FzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTo0MDo0MlrOGE7FzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTYyOQ==", "bodyText": "use boolean flag ?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r407815629", "createdAt": "2020-04-14T01:40:42Z", "author": {"login": "hmatu"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java", "diffHunk": "@@ -100,6 +100,10 @@ public static void main(String[] args) {\n \n   }\n \n+  private boolean isUpsert() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MTc4MjU4OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwODo1MDozMVrOGGbJcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMjozMDoxMVrOGIAwQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM4OTQyNQ==", "bodyText": "Does this method use to init records for inserting? IMO, we should distinguish it with upsert.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409389425", "createdAt": "2020-04-16T08:50:31Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  private List<GenericRecord> createRecords(Path srcFolder) throws ParseException, IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDAwNTA2OA==", "bodyText": "Does this method use to init records for inserting? IMO, we should distinguish it with upsert.\n\nYes, it is for inserting only, upsert has it's own method.\nhttps://github.com/apache/incubator-hudi/blob/e1a47ff32f900d9c723dc907784210ce756915f9/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java#L284-L286", "url": "https://github.com/apache/hudi/pull/1511#discussion_r410005068", "createdAt": "2020-04-17T05:48:47Z", "author": {"login": "hddong"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  private List<GenericRecord> createRecords(Path srcFolder) throws ParseException, IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM4OTQyNQ=="}, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1NDE0Ng==", "bodyText": "I mean for matching purpose, can we rename it to createInsertRecords?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411054146", "createdAt": "2020-04-20T02:30:11Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  private List<GenericRecord> createRecords(Path srcFolder) throws ParseException, IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM4OTQyNQ=="}, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MTgwNTg3OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwODo1NjoxNVrOGGbYNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwODo1NjoxNVrOGGbYNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5MzIwNg==", "bodyText": "Here exists redundant boxing issues. It would be better to use Double.parseDouble(xxx).", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409393206", "createdAt": "2020-04-16T08:56:15Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MTgxNDgyOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwODo1ODoxOVrOGGbd8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwODo1ODoxOVrOGGbd8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NDY3NA==", "bodyText": "Can we use try-with-resource here?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409394674", "createdAt": "2020-04-16T08:58:19Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MTgyMTEzOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwODo1OTo1MFrOGGbh1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwODo1OTo1MFrOGGbh1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NTY2OA==", "bodyText": "I would suggest one field one line for so many arguments.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409395668", "createdAt": "2020-04-16T08:59:50Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 168}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MTgyMzk5OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwOTowMDozNlrOGGbjvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwOTowMDozNlrOGGbjvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NjE1Ng==", "bodyText": "ditto, try-with-resource?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409396156", "createdAt": "2020-04-16T09:00:36Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MTgyNzg5OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwOTowMTo0MlrOGGbmZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwOTowMTo0MlrOGGbmZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NjgzOQ==", "bodyText": "ParseException would never be thrown.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409396839", "createdAt": "2020-04-16T09:01:42Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0MTg0MTI0OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwOTowNToxMFrOGGbvFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwOTowNToxMFrOGGbvFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5OTA2MA==", "bodyText": "As a good habit, when we override the equals method, it would also be better to override the hashCode even though we did use it here.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409399060", "createdAt": "2020-04-16T09:05:10Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +403,44 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {\n+    double timestamp;\n+    String rowKey;\n+    String rider;\n+    String driver;\n+    double beginLat;\n+    double beginLon;\n+    double endLat;\n+    double endLon;\n+\n+    private HoodieModel(double timestamp, String rowKey, String rider, String driver, double beginLat,\n+        double beginLon, double endLat, double endLon) {\n+      this.timestamp = timestamp;\n+      this.rowKey = rowKey;\n+      this.rider = rider;\n+      this.driver = driver;\n+      this.beginLat = beginLat;\n+      this.beginLon = beginLon;\n+      this.endLat = endLat;\n+      this.endLon = endLon;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 271}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1MzQ0MjU3OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMjoyNDowNVrOGIAqRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMjoyNDowNVrOGIAqRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MjYxNQ==", "bodyText": "It's a bit long. Here, I would suggest using Objects.hash(...).", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411052615", "createdAt": "2020-04-20T02:24:05Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +380,70 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {\n+    double timestamp;\n+    String rowKey;\n+    String rider;\n+    String driver;\n+    double beginLat;\n+    double beginLon;\n+    double endLat;\n+    double endLon;\n+\n+    private HoodieModel(double timestamp, String rowKey, String rider, String driver, double beginLat,\n+        double beginLon, double endLat, double endLon) {\n+      this.timestamp = timestamp;\n+      this.rowKey = rowKey;\n+      this.rider = rider;\n+      this.driver = driver;\n+      this.beginLat = beginLat;\n+      this.beginLon = beginLon;\n+      this.endLat = endLat;\n+      this.endLon = endLon;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      HoodieModel other = (HoodieModel) o;\n+      return timestamp == other.timestamp && rowKey.equals(other.rowKey) && rider.equals(other.rider)\n+          && driver.equals(other.driver) && beginLat == other.beginLat && beginLon == other.beginLon\n+          && endLat == other.endLat && endLon == other.endLon;\n+    }\n+\n+    @Override\n+    public int hashCode() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 332}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1MzQ0OTE1OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMjoyNzo1NVrOGIAtxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMDoxMTozNlrOGINMlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MzUxMQ==", "bodyText": "Can we mark this class to be a static class and rename it to HoodieTripModel?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411053511", "createdAt": "2020-04-20T02:27:55Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +380,70 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI1ODAwNw==", "bodyText": "Can we mark this class to be a static class and rename it to HoodieTripModel?\n\nYes, and change it public, testHDFSParquetImportCommand maybe reuse.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411258007", "createdAt": "2020-04-20T10:11:36Z", "author": {"login": "hddong"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +380,70 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MzUxMQ=="}, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 295}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1MzQ2Mjc2OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMjozNDo1MVrOGIA09A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMjozNDo1MVrOGIA09A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1NTM0OA==", "bodyText": "This method throws IOException, so the .close method should be wrapped into a finally or try-with-resource block. The same issue exists in createRecords method.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411055348", "createdAt": "2020-04-20T02:34:51Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -171,6 +277,30 @@ private void createRecords(Path srcFolder) throws ParseException, IOException {\n       writer.write(record);\n     }\n     writer.close();\n+    return records;\n+  }\n+\n+  private List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n+    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n+    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n+    List<GenericRecord> records = new ArrayList<GenericRecord>();\n+    // 10 for update\n+    for (long recordNum = 0; recordNum < 11; recordNum++) {\n+      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n+          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n+    }\n+    // 4 for insert\n+    for (long recordNum = 96; recordNum < 100; recordNum++) {\n+      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n+          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n+    }\n+    ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n+        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build();\n+    for (GenericRecord record : records) {\n+      writer.write(record);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 223}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1MzU0MjIyOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMzoxNzowMFrOGIBfAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMzoxNzowMFrOGIBfAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA2NjExMg==", "bodyText": "rename to testImportWithInsert?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411066112", "createdAt": "2020-04-20T03:17:00Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -150,14 +166,104 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n       for (Entry<String, Long> e : recordCounts.entrySet()) {\n         assertEquals(\"missing records\", 24, e.getValue().longValue());\n       }\n-    } finally {\n-      if (jsc != null) {\n-        jsc.stop();\n-      }\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 117}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4767, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}