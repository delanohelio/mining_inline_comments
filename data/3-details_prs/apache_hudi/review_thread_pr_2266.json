{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI0NTg1ODM2", "number": 2266, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozNjo0MFrOE-yYeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozODoxN1rOE-ya5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MjcyNjM0OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozNjo0MFrOH8Iacw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo0Nzo1OFrOH8I0Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNDQ1MQ==", "bodyText": "nit: The jsc parameter here seems unnecessary since the constructor already takes it.", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532814451", "createdAt": "2020-11-30T18:36:40Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Interface that supports updating metadata for a given table, as actions complete.\n+ */\n+public interface HoodieTableMetadataWriter extends Serializable {\n+\n+  static HoodieTableMetadataWriter create(Configuration conf, HoodieWriteConfig writeConfig, JavaSparkContext jsc) {\n+    return new FSBackedTableMetadataWriter(conf, writeConfig, jsc);\n+  }\n+\n+  void update(JavaSparkContext jsc, HoodieCommitMetadata commitMetadata, String instantTime);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de733e90f26118700956a08584e1debbc5499d60"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyMTAyMg==", "bodyText": "yes. I also mulled that. I will change", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532821022", "createdAt": "2020-11-30T18:47:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/HoodieTableMetadataWriter.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.metadata;\n+\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieCleanerPlan;\n+import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Interface that supports updating metadata for a given table, as actions complete.\n+ */\n+public interface HoodieTableMetadataWriter extends Serializable {\n+\n+  static HoodieTableMetadataWriter create(Configuration conf, HoodieWriteConfig writeConfig, JavaSparkContext jsc) {\n+    return new FSBackedTableMetadataWriter(conf, writeConfig, jsc);\n+  }\n+\n+  void update(JavaSparkContext jsc, HoodieCommitMetadata commitMetadata, String instantTime);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNDQ1MQ=="}, "originalCommit": {"oid": "de733e90f26118700956a08584e1debbc5499d60"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MjczMjU1OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/metadata/FSBackedTableMetadataWriter.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODozODoxN1rOH8IeLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODo1OToyOFrOH8JOhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNTQwNQ==", "bodyText": "Is FSBased the appropriate name here? FSBased seems to imply that the metadata is being saved directly on filesystem. But technically we are saving the metadata in a hoodie table.", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532815405", "createdAt": "2020-11-30T18:38:17Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/FSBackedTableMetadataWriter.java", "diffHunk": "@@ -74,84 +60,109 @@\n import org.apache.hudi.exception.HoodieMetadataException;\n import org.apache.hudi.metrics.DistributedRegistry;\n import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n \n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n import scala.Tuple2;\n \n+import static org.apache.hudi.metadata.HoodieTableMetadata.METADATA_TABLE_NAME_SUFFIX;\n+import static org.apache.hudi.metadata.HoodieTableMetadata.NON_PARTITIONED_NAME;\n+import static org.apache.hudi.metadata.HoodieTableMetadata.SOLO_COMMIT_TIMESTAMP;\n+\n /**\n  * Writer for Metadata Table.\n  *\n  * Partition and file listing are saved within an internal MOR table called Metadata Table. This table is created\n  * by listing files and partitions (first time) and kept in sync using the instants on the main dataset.\n  */\n-public class HoodieMetadataWriter extends HoodieMetadataReader implements Serializable {\n-  private static final Logger LOG = LogManager.getLogger(HoodieMetadataWriter.class);\n+public class FSBackedTableMetadataWriter implements HoodieTableMetadataWriter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de733e90f26118700956a08584e1debbc5499d60"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyMDYwOQ==", "bodyText": "Good point. Call it - HoodieBackedTableMetadataWriter?  wdyt @prashantwason", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532820609", "createdAt": "2020-11-30T18:47:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/FSBackedTableMetadataWriter.java", "diffHunk": "@@ -74,84 +60,109 @@\n import org.apache.hudi.exception.HoodieMetadataException;\n import org.apache.hudi.metrics.DistributedRegistry;\n import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n \n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n import scala.Tuple2;\n \n+import static org.apache.hudi.metadata.HoodieTableMetadata.METADATA_TABLE_NAME_SUFFIX;\n+import static org.apache.hudi.metadata.HoodieTableMetadata.NON_PARTITIONED_NAME;\n+import static org.apache.hudi.metadata.HoodieTableMetadata.SOLO_COMMIT_TIMESTAMP;\n+\n /**\n  * Writer for Metadata Table.\n  *\n  * Partition and file listing are saved within an internal MOR table called Metadata Table. This table is created\n  * by listing files and partitions (first time) and kept in sync using the instants on the main dataset.\n  */\n-public class HoodieMetadataWriter extends HoodieMetadataReader implements Serializable {\n-  private static final Logger LOG = LogManager.getLogger(HoodieMetadataWriter.class);\n+public class FSBackedTableMetadataWriter implements HoodieTableMetadataWriter {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNTQwNQ=="}, "originalCommit": {"oid": "de733e90f26118700956a08584e1debbc5499d60"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgyNzc4MA==", "bodyText": "Yep, that sounds fine.", "url": "https://github.com/apache/hudi/pull/2266#discussion_r532827780", "createdAt": "2020-11-30T18:59:28Z", "author": {"login": "prashantwason"}, "path": "hudi-client/src/main/java/org/apache/hudi/metadata/FSBackedTableMetadataWriter.java", "diffHunk": "@@ -74,84 +60,109 @@\n import org.apache.hudi.exception.HoodieMetadataException;\n import org.apache.hudi.metrics.DistributedRegistry;\n import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n \n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n import scala.Tuple2;\n \n+import static org.apache.hudi.metadata.HoodieTableMetadata.METADATA_TABLE_NAME_SUFFIX;\n+import static org.apache.hudi.metadata.HoodieTableMetadata.NON_PARTITIONED_NAME;\n+import static org.apache.hudi.metadata.HoodieTableMetadata.SOLO_COMMIT_TIMESTAMP;\n+\n /**\n  * Writer for Metadata Table.\n  *\n  * Partition and file listing are saved within an internal MOR table called Metadata Table. This table is created\n  * by listing files and partitions (first time) and kept in sync using the instants on the main dataset.\n  */\n-public class HoodieMetadataWriter extends HoodieMetadataReader implements Serializable {\n-  private static final Logger LOG = LogManager.getLogger(HoodieMetadataWriter.class);\n+public class FSBackedTableMetadataWriter implements HoodieTableMetadataWriter {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgxNTQwNQ=="}, "originalCommit": {"oid": "de733e90f26118700956a08584e1debbc5499d60"}, "originalPosition": 84}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4191, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}