{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUyOTI0ODYz", "number": 1848, "title": "[HUDI-69] Support Spark Datasource for MOR table - RDD approach", "bodyText": "What is the purpose of the pull request\nThis PR implements Spark Datasource for MOR table in the RDD approach.\nParquetFileFormat approach PR: #1722\nBrief change log\n\nImplemented SnapshotRelation\nImplemented HudiMergeOnReadRDD\nImplemented separate Iterator to handle merge and unmerge record reader.\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded TestMORDataSource to verify this feature.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-07-20T01:06:48Z", "url": "https://github.com/apache/hudi/pull/1848", "merged": true, "mergeCommit": {"oid": "4f74a84607d46249e9bb6e1397246f8dc076b390"}, "closed": true, "closedAt": "2020-08-07T07:28:15Z", "author": {"login": "garyli1019"}, "timelineItems": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc2_L6AgBqjM1NjkxNzEzMTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc8ehXWAH2gAyNDUyOTI0ODYzOjYwY2VjMjBiYTE0MDc2ZTk0ZjM2NTI2OWQyZTY4Y2Y0MWZhYmM1MDg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6617297e0987fe0d49e8b4134268e8975b70ec5f", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/6617297e0987fe0d49e8b4134268e8975b70ec5f", "committedDate": "2020-07-20T00:57:31Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}, "afterCommit": {"oid": "45015db2ab95da640d9ba7ddfdfb5799970cf19d", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/45015db2ab95da640d9ba7ddfdfb5799970cf19d", "committedDate": "2020-07-21T05:14:51Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "45015db2ab95da640d9ba7ddfdfb5799970cf19d", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/45015db2ab95da640d9ba7ddfdfb5799970cf19d", "committedDate": "2020-07-21T05:14:51Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}, "afterCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/bdda1d6650365c616d233503e3ca8994cd0922cc", "committedDate": "2020-07-21T05:22:59Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyMTM1NzM5", "url": "https://github.com/apache/hudi/pull/1848#pullrequestreview-452135739", "createdAt": "2020-07-21T05:28:10Z", "commit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNToyODoxMFrOG0opBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNToyODoxMFrOG0opBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg0NDk5Nw==", "bodyText": "PrunedFilteredScan will change the behavior of ParquetRecordReader inside ParquetFileFormat even we are not using the vectorized reader. Still trying to figure out why... I will follow up with PrunedFilteredScan in a separate PR.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457844997", "createdAt": "2020-07-21T05:28:10Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyMTU4MTI5", "url": "https://github.com/apache/hudi/pull/1848#pullrequestreview-452158129", "createdAt": "2020-07-21T06:24:41Z", "commit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "state": "COMMENTED", "comments": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNjoyNDo0MVrOG0pyig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNzowNjoxNVrOG0q3mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2MzgxOA==", "bodyText": "hows this different from the class we already have in hudi-common : SerializableConfiguration ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457863818", "createdAt": "2020-07-21T06:24:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/config/HadoopSerializableConfiguration.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.config;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.Serializable;\n+\n+public class HadoopSerializableConfiguration implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2Mzk5NA==", "bodyText": "why remove this?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457863994", "createdAt": "2020-07-21T06:25:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java", "diffHunk": "@@ -147,12 +146,4 @@ public Schema getWriterSchema() {\n   public Schema getHiveSchema() {\n     return hiveSchema;\n   }\n-\n-  public long getMaxCompactionMemoryInBytes() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NDQzNQ==", "bodyText": "seems good to avoid the static import here and have the reader realize the class its coming from ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457864435", "createdAt": "2020-07-21T06:26:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java", "diffHunk": "@@ -40,6 +40,8 @@\n import java.io.IOException;\n import java.util.Map;\n \n+import static org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NDc3MA==", "bodyText": "ah ok. its just relocated", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457864770", "createdAt": "2020-07-21T06:26:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java", "diffHunk": "@@ -69,6 +71,17 @@ public static Schema readSchema(Configuration conf, Path filePath) {\n     }\n   }\n \n+  /**\n+   * get the max compaction memory in bytes from JobConf.\n+   */\n+  public static long getMaxCompactionMemoryInBytes(JobConf jobConf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NTI5MQ==", "bodyText": "should we just make a new datasource level config for this. and translate. mixing raw InputFormat level configs here, feels a bit problematic in terms of long term maitenance>", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457865291", "createdAt": "2020-07-21T06:28:21Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -110,6 +112,10 @@ object DataSourceReadOptions {\n    */\n   val INCR_PATH_GLOB_OPT_KEY = \"hoodie.datasource.read.incr.path.glob\"\n   val DEFAULT_INCR_PATH_GLOB_OPT_VAL = \"\"\n+\n+\n+  val REALTIME_SKIP_MERGE_KEY = REALTIME_SKIP_MERGE_PROP", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NTc4Mw==", "bodyText": "rename: getBaseFileOnlyView()", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457865783", "createdAt": "2020-07-21T06:29:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -123,4 +120,25 @@ class DefaultSource extends RelationProvider\n   }\n \n   override def shortName(): String = \"hudi\"\n+\n+  private def getReadOptimizedView(sqlContext: SQLContext,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NjgwNg==", "bodyText": "lets name the classes HoodieMergeOn... not Hudi.. to be consistent with rest of the code.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457866806", "createdAt": "2020-07-21T06:31:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NzM0NQ==", "bodyText": "can we file a JIRA for this. and is it possible to target this for 0.6.0 as well?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457867345", "createdAt": "2020-07-21T06:33:21Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg0NDk5Nw=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NzU3NQ==", "bodyText": "why change this?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457867575", "createdAt": "2020-07-21T06:33:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -67,7 +68,7 @@ class TestDataSource {\n     // Insert Operation\n     val records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100)).toList\n     val inputDF: Dataset[Row] = spark.read.json(spark.sparkContext.parallelize(records, 2))\n-    inputDF.write.format(\"hudi\")\n+    inputDF.write.format(\"org.apache.hudi\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2ODE4Mg==", "bodyText": "nit: space after Logging? Logging { ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457868182", "createdAt": "2020-07-21T06:35:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw==", "bodyText": "would n't this read all the fields out? should we not pass userSchema here?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457871133", "createdAt": "2020-07-21T06:42:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTYxMg==", "bodyText": "if we did PrunedFilteredScan we can also pass in teh filters? in any case, can we pass in the the options?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457871612", "createdAt": "2020-07-21T06:43:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,\n+      filters = Seq.empty,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3Mzk4MQ==", "bodyText": "from the last PR, it seems the biggest change is calling ParquetFileFormat(). and the wrapping as opposed to inheriting the code. Would life be lot simpler if we still defined our own FileFormat and then wrapped ParquetFileFormat and the code you have to read logs as Iterator<InternalRow> ? Just a thought.  https://github.com/apache/spark/blob/8c7d6f9733751503f80d5a1b2463904dfefd6843/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala#L105\nWhat you are doing is probably valid and makes it consistent with how @umehrot2 is also approaching it. if they are equivalent, then I am fine with it.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457873981", "createdAt": "2020-07-21T06:49:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NDI3NQ==", "bodyText": "rename variable  using camel case?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457874275", "createdAt": "2020-07-21T06:50:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,\n+      filters = Seq.empty,\n+      options = Map.empty,\n+      hadoopConf = sqlContext.sparkSession.sessionState.newHadoopConf()\n+    )\n+    val rdd = new HudiMergeOnReadRDD(sqlContext.sparkContext,\n+      sqlContext.sparkSession.sessionState.newHadoopConf(),\n+      parquetReaderFunction, latestSchema, fileIndex)\n+    rdd.asInstanceOf[RDD[Row]]\n+  }\n+\n+  def buildFileIndex(): List[HudiMergeOnReadFileSplit] = {\n+    val inMemoryFileIndex = HudiSparkUtils.createInMemoryFileIndex(sqlContext.sparkSession, globPaths)\n+    val fileStatuses = inMemoryFileIndex.allFiles()\n+    if (fileStatuses.isEmpty) {\n+      throw new HoodieException(\"No files found for reading in user provided path.\")\n+    }\n+\n+    val fsView = new HoodieTableFileSystemView(metaClient,\n+      metaClient.getActiveTimeline.getCommitsTimeline\n+        .filterCompletedInstants, fileStatuses.toArray)\n+    val latestFiles: List[HoodieBaseFile] = fsView.getLatestBaseFiles.iterator().asScala.toList\n+    val latestCommit = fsView.getLastInstant.get().getTimestamp\n+    val fileGroup = HoodieRealtimeInputFormatUtils.groupLogsByBaseFile(conf, latestFiles.asJava).asScala\n+    val FileSplits = fileGroup.map(kv => {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NDY1NQ==", "bodyText": "rename baseFileReadFunction", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457874655", "createdAt": "2020-07-21T06:50:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NDc1Mg==", "bodyText": "hudi vs hoodie", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457874752", "createdAt": "2020-07-21T06:51:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NjIxOQ==", "bodyText": "lets not leak parquet in the naming within this class. we can keep it generic as base vs log files", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457876219", "createdAt": "2020-07-21T06:54:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NjY2Mg==", "bodyText": "print the mergeParquetPartition split variable in the error message?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457876662", "createdAt": "2020-07-21T06:55:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3ODg4NQ==", "bodyText": "this seems to indicate that we will keep scanning the remaining entries in the log and hand them out if dataFileIterator runs out. We need to be careful about how it interplays with split generation. specifically, only works if the each base file has only 1 split..\nHudiMergeOnReadFileSplit(partitionedFile, logPaths, latestCommit,\n         metaClient.getBasePath, maxCompactionMemoryInBytes, skipMerge)\n\nhere partitionedFile has to be a single file and not an input Split. otherwise we will face an issue that the log entry belongs to a different split and the ultimate query will have duplicates.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457878885", "createdAt": "2020-07-21T07:00:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3OTIyNQ==", "bodyText": "why not containsKey. keySet() may create a copy?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457879225", "createdAt": "2020-07-21T07:01:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          val curRow = dataFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (hudiLogRecords.keySet().contains(curKey)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3OTYzMg==", "bodyText": "is the remove needed. this map is often spillable.. we should just make sure the remove does not incur additional I/O or soemethng", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457879632", "createdAt": "2020-07-21T07:02:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          val curRow = dataFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (hudiLogRecords.keySet().contains(curKey)) {\n+            logRecordToRead.remove(curKey)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4MDA1Nw==", "bodyText": "same comment here about double checking if this is actually ok", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457880057", "createdAt": "2020-07-21T07:03:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          val curRow = dataFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (hudiLogRecords.keySet().contains(curKey)) {\n+            logRecordToRead.remove(curKey)\n+            mergeRowWithLog(curRow)\n+          } else {\n+            curRow\n+          }\n+        } else {\n+          val curKey = hudiLogRecordsIterator.next()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4MTQ5OQ==", "bodyText": "this implicitly assumes OvewriteWithLatestPayload ? can we just convert the parquet row as well to Avro and then perform the merge actually calling the right API. HoodieRecordPayload#combineAndGetUpdateValue() ?  This is a correctness issue we need to resolve in the PR..\nideally, adding a test case as well to go along with this would be good", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457881499", "createdAt": "2020-07-21T07:06:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          val curRow = dataFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (hudiLogRecords.keySet().contains(curKey)) {\n+            logRecordToRead.remove(curKey)\n+            mergeRowWithLog(curRow)\n+          } else {\n+            curRow\n+          }\n+        } else {\n+          val curKey = hudiLogRecordsIterator.next()\n+          getAvroRecord(curKey)\n+        }\n+      }\n+\n+      private def getAvroRecord(curKey: String): InternalRow = {\n+        val curAvroRecord = hudiLogRecords.get(curKey).getData.getInsertValue(logSchema).get()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 162}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/bdda1d6650365c616d233503e3ca8994cd0922cc", "committedDate": "2020-07-21T05:22:59Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}, "afterCommit": {"oid": "a4c7069abaafc7faa0476d6a190a6cf55fbd2bbe", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/a4c7069abaafc7faa0476d6a190a6cf55fbd2bbe", "committedDate": "2020-07-22T05:59:19Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a4c7069abaafc7faa0476d6a190a6cf55fbd2bbe", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/a4c7069abaafc7faa0476d6a190a6cf55fbd2bbe", "committedDate": "2020-07-22T05:59:19Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}, "afterCommit": {"oid": "41d1d05d1baa05b89f7bdbdc4458d3e8926f8982", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/41d1d05d1baa05b89f7bdbdc4458d3e8926f8982", "committedDate": "2020-07-22T06:02:19Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzMDA0OTMz", "url": "https://github.com/apache/hudi/pull/1848#pullrequestreview-453004933", "createdAt": "2020-07-22T05:30:28Z", "commit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwNTozMDoyOFrOG1TRaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwNTo1Njo0MFrOG1Tx5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0MzQ2Nw==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458543467", "createdAt": "2020-07-22T05:30:28Z", "author": {"login": "garyli1019"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/config/HadoopSerializableConfiguration.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.config;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.Serializable;\n+\n+public class HadoopSerializableConfiguration implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2MzgxOA=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0NDUzOA==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458544538", "createdAt": "2020-07-22T05:33:55Z", "author": {"login": "garyli1019"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java", "diffHunk": "@@ -40,6 +40,8 @@\n import java.io.IOException;\n import java.util.Map;\n \n+import static org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NDQzNQ=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0NDkxNQ==", "bodyText": "added MERGE_ON_READ_PAYLOAD_KEY and MERGE_ON_READ_ORDERING_KEY. Then we use the payload to do all the merging.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458544915", "createdAt": "2020-07-22T05:35:04Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -110,6 +112,10 @@ object DataSourceReadOptions {\n    */\n   val INCR_PATH_GLOB_OPT_KEY = \"hoodie.datasource.read.incr.path.glob\"\n   val DEFAULT_INCR_PATH_GLOB_OPT_VAL = \"\"\n+\n+\n+  val REALTIME_SKIP_MERGE_KEY = REALTIME_SKIP_MERGE_PROP", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NTI5MQ=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU1MTc4Mw==", "bodyText": "https://issues.apache.org/jira/browse/HUDI-1050", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458551783", "createdAt": "2020-07-22T05:56:40Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg0NDk5Nw=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 54}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "41d1d05d1baa05b89f7bdbdc4458d3e8926f8982", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/41d1d05d1baa05b89f7bdbdc4458d3e8926f8982", "committedDate": "2020-07-22T06:02:19Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}, "afterCommit": {"oid": "f38c76c784b9ba462bfefbf7700a762f70ca2087", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/f38c76c784b9ba462bfefbf7700a762f70ca2087", "committedDate": "2020-07-22T17:35:30Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f38c76c784b9ba462bfefbf7700a762f70ca2087", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/f38c76c784b9ba462bfefbf7700a762f70ca2087", "committedDate": "2020-07-22T17:35:30Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}, "afterCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/5b051e575463e11aa9b30ee62c01ef5546151114", "committedDate": "2020-07-22T17:42:53Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzOTMxNDAy", "url": "https://github.com/apache/hudi/pull/1848#pullrequestreview-453931402", "createdAt": "2020-07-23T08:33:35Z", "commit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwODozMzozNVrOG2BFaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMTowMTo1NVrOG2bczw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI5NDA1Nw==", "bodyText": "Can we try to avoid having to do this part always ? It is not required for the BaseFileOnly case, so it would be good if we can avoid it to decrease the unnecessary overhead for Read Optimized queries.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459294057", "createdAt": "2020-07-23T08:33:35Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -58,26 +61,20 @@ class DefaultSource extends RelationProvider\n       throw new HoodieException(\"'path' must be specified.\")\n     }\n \n+    val fs = FSUtils.getFs(path.get, sqlContext.sparkContext.hadoopConfiguration)\n+    val globPaths = HudiSparkUtils.checkAndGlobPathIfNecessary(Seq(path.get), fs)\n+    val tablePath = DataSourceUtils.getTablePath(fs, globPaths.toArray)\n+    val metaClient = new HoodieTableMetaClient(fs.getConf, tablePath)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI5ODE1MA==", "bodyText": "Shouldn't we call it something like MergeOnReadSnapshotRelation ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459298150", "createdAt": "2020-07-23T08:41:07Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HoodieMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                      logPaths: Option[List[String]],\n+                                      latestCommit: String,\n+                                      tablePath: String,\n+                                      maxCompactionMemoryInBytes: Long,\n+                                      payload: String,\n+                                      orderingVal: String)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMwMDY3Ng==", "bodyText": "Instead of doing this, you can just do:\nval tableSchema = schemaUtil.getTableAvroSchema", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459300676", "createdAt": "2020-07-23T08:45:37Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HoodieMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                      logPaths: Option[List[String]],\n+                                      latestCommit: String,\n+                                      tablePath: String,\n+                                      maxCompactionMemoryInBytes: Long,\n+                                      payload: String,\n+                                      orderingVal: String)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging {\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMxMzUzNA==", "bodyText": "Instead of using prefix Hoodie for all the newly defined classes, shouldn't we be using Hudi. Isn't that where the community is headed towards ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459313534", "createdAt": "2020-07-23T09:08:51Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwNTE2OQ==", "bodyText": "Possibly something like HoodieLogFileFormat might make sense to do in future, as it will clearly extract out the Log files reading logic.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459705169", "createdAt": "2020-07-23T20:20:52Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3Mzk4MQ=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwOTY0OQ==", "bodyText": "We may want to support Column Pruning here. @garyli1019 atleast for the more straight forward parts like reading:\n\nReading base parquet files only without log files to be merged\nUnmerge reading logic\n\nIt should be possible to push the user request columns/schema down without complications. You can possibly introduce another parquetReaderFunction which passes the requested schema down and use it for the above defined cases.\nFor the case where merging is required I agree it may be more complicated. What if the user does not even request reading a column which has been updated in the log file ? Merging in such cases may not even be required.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459709649", "createdAt": "2020-07-23T20:29:42Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxMjk0NA==", "bodyText": "We should try to support filter push-down. Again for the more straight forward scenarios which I mentioned in my other comment, it should be possible to just pass down the user filters in the reader.\nHowever, for Log file merging scenario we may have to take care of the following scenario:\n\nReading base file filtered out say Row X because of filter push-down.\nRow X had been updated in the log file and has an entry.\nWhile merging we need some way to tell that Row X should be filtered out from log file as well, otherwise we may end up still returning it in the result, because based on the merging logic I see that any remaining rows in log file which did not have corresponding key in base file are just appended and returned in the result.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459712944", "createdAt": "2020-07-23T20:36:01Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,\n+      filters = Seq.empty,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTYxMg=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNDc3Mw==", "bodyText": "Just for my understanding, what is this use-case where we want to return un-merged rows ? Do we do something similar for MOR queries through input format where we want to return un-merged rows ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459714773", "createdAt": "2020-07-23T20:39:37Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyMDE1OA==", "bodyText": "Does this need explicit casting ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459720158", "createdAt": "2020-07-23T20:50:26Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNDU3Mg==", "bodyText": "@vinothchandar This seems like something we can consider using at other places in Hudi code like AvroConversionHelper to convert Avro Records to Rows, instead of maintaining the conversion code in-house.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459724572", "createdAt": "2020-07-23T20:59:06Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)\n+      case mergeSplit if !mergeSplit.payload.isEmpty =>\n+        mergeFileIterator(mergeSplit, baseFileIterator)\n+      case _ => throw new HoodieException(s\"Unable to select an Iterator to read the Hoodie MOR File Split for \" +\n+        s\"file path: ${mergeParquetPartition.split.dataFile.filePath}\" +\n+        s\"log paths: ${mergeParquetPartition.split.logPaths.toString}\" +\n+        s\"hoodie table path: ${mergeParquetPartition.split.tablePath}\" +\n+        s\"spark partition Index: ${mergeParquetPartition.index}\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hoodieRealtimeFileSplits.zipWithIndex.map(file => HoodieMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.get()\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                  baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjAzMQ==", "bodyText": "Instead of looping over baseFileIterator and performing the check whether that key exists in logRecords, will it be more efficient to do it the other way round. Loop over logRecords and perform merge. In the end append all the remaining base file rows.\n\n\nAlso is this a good practice to perform the actual fetching in hasNext function, instead of next ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459726031", "createdAt": "2020-07-23T21:01:55Z", "author": {"login": "umehrot2"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)\n+      case mergeSplit if !mergeSplit.payload.isEmpty =>\n+        mergeFileIterator(mergeSplit, baseFileIterator)\n+      case _ => throw new HoodieException(s\"Unable to select an Iterator to read the Hoodie MOR File Split for \" +\n+        s\"file path: ${mergeParquetPartition.split.dataFile.filePath}\" +\n+        s\"log paths: ${mergeParquetPartition.split.logPaths.toString}\" +\n+        s\"hoodie table path: ${mergeParquetPartition.split.tablePath}\" +\n+        s\"spark partition Index: ${mergeParquetPartition.index}\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hoodieRealtimeFileSplits.zipWithIndex.map(file => HoodieMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.get()\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                  baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val logRecords = scanLog(split, logSchema).getRecords\n+      private val logRecordsIterator = logRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        baseFileIterator.hasNext || logRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (baseFileIterator.hasNext) {\n+          baseFileIterator.next()\n+        } else {\n+          val curAvrokey = logRecordsIterator.next()\n+          val curAvroRecord = logRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val avroSchema = getLogAvroSchema(split)\n+      private val sparkSchema = SchemaConverters.toSqlType(avroSchema).dataType.asInstanceOf[StructType]\n+      private val avroToRowConverter = new AvroDeserializer(avroSchema, sparkSchema)\n+      private val rowToAvroConverter = new AvroSerializer(sparkSchema, avroSchema, false)\n+      private val logRecords = scanLog(split, avroSchema).getRecords\n+      private val logRecordToRead = logRecords.keySet()\n+\n+      private var baseFileFinished = false\n+      private var logRecordsIterator: Iterator[String] = _\n+      private var recordToLoad: InternalRow = _\n+\n+      @scala.annotation.tailrec\n+      override def hasNext: Boolean = {\n+        if (baseFileIterator.hasNext) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 144}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU0NTEzMzM1", "url": "https://github.com/apache/hudi/pull/1848#pullrequestreview-454513335", "createdAt": "2020-07-23T21:37:56Z", "commit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMTozNzo1NlrOG2cdaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyMTo1M1rOG2dj7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0MjU2OA==", "bodyText": "I think I can move this into the SNAPSHOT_QUERY session.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459742568", "createdAt": "2020-07-23T21:37:56Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -58,26 +61,20 @@ class DefaultSource extends RelationProvider\n       throw new HoodieException(\"'path' must be specified.\")\n     }\n \n+    val fs = FSUtils.getFs(path.get, sqlContext.sparkContext.hadoopConfiguration)\n+    val globPaths = HudiSparkUtils.checkAndGlobPathIfNecessary(Seq(path.get), fs)\n+    val tablePath = DataSourceUtils.getTablePath(fs, globPaths.toArray)\n+    val metaClient = new HoodieTableMetaClient(fs.getConf, tablePath)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI5NDA1Nw=="}, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0NTU3Ng==", "bodyText": "This problem is tricky. Both sides make sense to me. It seems like impossible to completely switch from hoodie to hudi everywhere. Should we define a standard for the naming convension? Like class name -> hoodie, package name -> hudi @vinothchandar", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459745576", "createdAt": "2020-07-23T21:44:50Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMxMzUzNA=="}, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0NjQyNQ==", "bodyText": "I followed the HadoopRDD implementation here https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L120", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459746425", "createdAt": "2020-07-23T21:46:33Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyMDE1OA=="}, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0NzY3MQ==", "bodyText": "yes we have this option for Hive https://github.com/apache/hudi/blob/master/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java\nThe performance will be better without merging. We can avoid the type conversion at least Row -> Avro -> Merge -> Avro -> Row", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459747671", "createdAt": "2020-07-23T21:49:38Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNDc3Mw=="}, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0ODY3NA==", "bodyText": "agree here. Our AvroConversionHelper is handling Row, which is not an extension of InternalRow. If we don't need Row specifically, I think we can adapt to the Spark Internal serializer.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459748674", "createdAt": "2020-07-23T21:52:05Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)\n+      case mergeSplit if !mergeSplit.payload.isEmpty =>\n+        mergeFileIterator(mergeSplit, baseFileIterator)\n+      case _ => throw new HoodieException(s\"Unable to select an Iterator to read the Hoodie MOR File Split for \" +\n+        s\"file path: ${mergeParquetPartition.split.dataFile.filePath}\" +\n+        s\"log paths: ${mergeParquetPartition.split.logPaths.toString}\" +\n+        s\"hoodie table path: ${mergeParquetPartition.split.tablePath}\" +\n+        s\"spark partition Index: ${mergeParquetPartition.index}\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hoodieRealtimeFileSplits.zipWithIndex.map(file => HoodieMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.get()\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                  baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNDU3Mg=="}, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NDM1Mw==", "bodyText": "The log scanner scans all the log files in one batch and handled the merging internally. The output is a hashmap we can use directly. This logRecordsIterator is just looping through the hashmap and doesn't load the row one by one like the daseFileIterator.\n\n\nThis is a little bit tricky. If the hasNext return true, but next() doesn't return a value, Spark will throw an exception. In our logic, we don't know hasNext will be true of false until we find the qualified record to read. For example, 100 records in base file and 100 delete records in the log file. We will read 0 row and hasNext should return false in the first call, but we have iterated through the whole base file already. There is a test case for this example.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459754353", "createdAt": "2020-07-23T22:05:25Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)\n+      case mergeSplit if !mergeSplit.payload.isEmpty =>\n+        mergeFileIterator(mergeSplit, baseFileIterator)\n+      case _ => throw new HoodieException(s\"Unable to select an Iterator to read the Hoodie MOR File Split for \" +\n+        s\"file path: ${mergeParquetPartition.split.dataFile.filePath}\" +\n+        s\"log paths: ${mergeParquetPartition.split.logPaths.toString}\" +\n+        s\"hoodie table path: ${mergeParquetPartition.split.tablePath}\" +\n+        s\"spark partition Index: ${mergeParquetPartition.index}\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hoodieRealtimeFileSplits.zipWithIndex.map(file => HoodieMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.get()\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                  baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val logRecords = scanLog(split, logSchema).getRecords\n+      private val logRecordsIterator = logRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        baseFileIterator.hasNext || logRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (baseFileIterator.hasNext) {\n+          baseFileIterator.next()\n+        } else {\n+          val curAvrokey = logRecordsIterator.next()\n+          val curAvroRecord = logRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val avroSchema = getLogAvroSchema(split)\n+      private val sparkSchema = SchemaConverters.toSqlType(avroSchema).dataType.asInstanceOf[StructType]\n+      private val avroToRowConverter = new AvroDeserializer(avroSchema, sparkSchema)\n+      private val rowToAvroConverter = new AvroSerializer(sparkSchema, avroSchema, false)\n+      private val logRecords = scanLog(split, avroSchema).getRecords\n+      private val logRecordToRead = logRecords.keySet()\n+\n+      private var baseFileFinished = false\n+      private var logRecordsIterator: Iterator[String] = _\n+      private var recordToLoad: InternalRow = _\n+\n+      @scala.annotation.tailrec\n+      override def hasNext: Boolean = {\n+        if (baseFileIterator.hasNext) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjAzMQ=="}, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NDczNQ==", "bodyText": "good to know. thanks", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459754735", "createdAt": "2020-07-23T22:06:19Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HoodieMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                      logPaths: Option[List[String]],\n+                                      latestCommit: String,\n+                                      tablePath: String,\n+                                      maxCompactionMemoryInBytes: Long,\n+                                      payload: String,\n+                                      orderingVal: String)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging {\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMwMDY3Ng=="}, "originalCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1ODM2MA==", "bodyText": "When I pushdown nothing and pass the full schema as user requested schema, with simply changing from TableScan() to PrunedFilteredScan, the behavior of the parquet reader was changed and not reading the correct schema. I need to dig deeper here.\nLet's focus on making the basic functionality work in this PR. I will figure this out with a follow-up PR.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459758360", "createdAt": "2020-07-23T22:15:43Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MDYyMA==", "bodyText": "Missing the filter for log file probably ok because Spark will apply the filter again after the pushdown filter. Description https://github.com/apache/spark/blob/branch-2.4/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala#L268", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459760620", "createdAt": "2020-07-23T22:21:53Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,\n+      filters = Seq.empty,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTYxMg=="}, "originalCommit": {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc"}, "originalPosition": 81}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/5b051e575463e11aa9b30ee62c01ef5546151114", "committedDate": "2020-07-22T17:42:53Z", "message": "[HUDI-69] Support Spark Datasource for MOR table"}, "afterCommit": {"oid": "eefad4beb4f15864b1ea9925b0d4f54deba776fc", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/eefad4beb4f15864b1ea9925b0d4f54deba776fc", "committedDate": "2020-07-26T23:22:32Z", "message": "[HUDI-69] Support PruneFilteredScan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "eefad4beb4f15864b1ea9925b0d4f54deba776fc", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/eefad4beb4f15864b1ea9925b0d4f54deba776fc", "committedDate": "2020-07-26T23:22:32Z", "message": "[HUDI-69] Support PruneFilteredScan"}, "afterCommit": {"oid": "d8ef9d3dd2eed57983ccb43f235806be848f11e3", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/d8ef9d3dd2eed57983ccb43f235806be848f11e3", "committedDate": "2020-07-27T03:24:55Z", "message": "[HUDI-69] Support PruneFilteredScan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d8ef9d3dd2eed57983ccb43f235806be848f11e3", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/d8ef9d3dd2eed57983ccb43f235806be848f11e3", "committedDate": "2020-07-27T03:24:55Z", "message": "[HUDI-69] Support PruneFilteredScan"}, "afterCommit": {"oid": "4b05f0f7b12e083b66084b11a80a207f33bd7dcc", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/4b05f0f7b12e083b66084b11a80a207f33bd7dcc", "committedDate": "2020-07-27T20:39:19Z", "message": "[HUDI-69] Support PruneFilteredScan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4b05f0f7b12e083b66084b11a80a207f33bd7dcc", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/4b05f0f7b12e083b66084b11a80a207f33bd7dcc", "committedDate": "2020-07-27T20:39:19Z", "message": "[HUDI-69] Support PruneFilteredScan"}, "afterCommit": {"oid": "e6c77567658e73a3db26681ed25e54c8214ca1cd", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/e6c77567658e73a3db26681ed25e54c8214ca1cd", "committedDate": "2020-07-29T04:43:38Z", "message": "[HUDI-69] Support PruneFilteredScan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "995d700073fa5a086c2daeedf30b4fbd0846c5e5", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/995d700073fa5a086c2daeedf30b4fbd0846c5e5", "committedDate": "2020-07-31T23:09:18Z", "message": "[HUDI-69] Remove unnecessary payload config"}, "afterCommit": {"oid": "8f6c63b91d8af0430a35df843c3e6e722d1d6dc5", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/8f6c63b91d8af0430a35df843c3e6e722d1d6dc5", "committedDate": "2020-08-04T05:48:47Z", "message": "[HUDI-69] Remove unnecessary payload config"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "039c951ca23b87d34be5c887aa4fdb8367ec3cd8", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/039c951ca23b87d34be5c887aa4fdb8367ec3cd8", "committedDate": "2020-08-04T05:55:32Z", "message": "placeholder"}, "afterCommit": {"oid": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "committedDate": "2020-08-04T06:01:51Z", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNTI3MDY4", "url": "https://github.com/apache/hudi/pull/1848#pullrequestreview-460527068", "createdAt": "2020-08-04T06:25:17Z", "commit": {"oid": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjoyNToxN1rOG7SvAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo0NTo1MFrOG7TOPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgyNjExMg==", "bodyText": "great!", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464826112", "createdAt": "2020-08-04T06:25:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, GenericRecordBuilder}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.{Partition, SerializableWritable, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{SpecificInternalRow, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(@transient sc: SparkContext,\n+                           @transient config: Configuration,\n+                           fullSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           requiredSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           tableState: HoodieMergeOnReadTableState)\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  private val confBroadcast = sc.broadcast(new SerializableWritable(config))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader)\n+      case skipMergeSplit if skipMergeSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_SKIP_MERGE_OPT_VAL) =>\n+        skipMergeFileIterator(\n+          skipMergeSplit,\n+          read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader),\n+          getConfig\n+        )\n+      case payloadCombineSplit if payloadCombineSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_PAYLOAD_COMBINE_OPT_VAL) =>\n+        payloadCombineFileIterator(\n+          payloadCombineSplit,\n+          read(mergeParquetPartition.split.dataFile, fullSchemaFileReader),\n+          getConfig\n+        )\n+      case _ => throw new HoodieException(s\"Unable to select an Iterator to read the Hoodie MOR File Split for \" +\n+        s\"file path: ${mergeParquetPartition.split.dataFile.filePath}\" +\n+        s\"log paths: ${mergeParquetPartition.split.logPaths.toString}\" +\n+        s\"hoodie table path: ${mergeParquetPartition.split.tablePath}\" +\n+        s\"spark partition Index: ${mergeParquetPartition.index}\" +\n+        s\"merge type: ${mergeParquetPartition.split.mergeType}\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    tableState\n+      .hoodieRealtimeFileSplits\n+      .zipWithIndex\n+      .map(file => HoodieMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig: Configuration = {\n+    val conf = confBroadcast.value.value\n+    HoodieMergeOnReadRDD.CONFIG_INSTANTIATION_LOCK.synchronized {\n+      new Configuration(conf)\n+    }\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def skipMergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                  baseFileIterator: Iterator[InternalRow],\n+                                  config: Configuration): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val tableAvroSchema = new Schema.Parser().parse(tableState.tableAvroSchema)\n+      private val requiredAvroSchema = new Schema.Parser().parse(tableState.requiredAvroSchema)\n+      private val requiredFieldPosition =\n+        tableState.requiredStructSchema\n+          .map(f => tableAvroSchema.getField(f.name).pos()).toList\n+      private val recordBuilder = new GenericRecordBuilder(requiredAvroSchema)\n+      private val deserializer = new AvroDeserializer(requiredAvroSchema, tableState.requiredStructSchema)\n+      private val unsafeProjection = UnsafeProjection.create(tableState.requiredStructSchema)\n+      private val logRecords = HoodieMergeOnReadRDD.scanLog(split, tableAvroSchema, config).getRecords\n+      private val logRecordsKeyIterator = logRecords.keySet().iterator().asScala\n+\n+      private var recordToLoad: InternalRow = _\n+\n+      @scala.annotation.tailrec\n+      override def hasNext: Boolean = {\n+        if (baseFileIterator.hasNext) {\n+          recordToLoad = baseFileIterator.next()\n+          true\n+        } else {\n+          if (logRecordsKeyIterator.hasNext) {\n+            val curAvrokey = logRecordsKeyIterator.next()\n+            val curAvroRecord = logRecords.get(curAvrokey).getData.getInsertValue(tableAvroSchema)\n+            if (!curAvroRecord.isPresent) {\n+              // delete record found, skipping\n+              this.hasNext\n+            } else {\n+              val requiredAvroRecord = AvroConversionUtils\n+                .buildAvroRecordBySchema(curAvroRecord.get(), requiredAvroSchema, requiredFieldPosition, recordBuilder)\n+              recordToLoad = unsafeProjection(deserializer.deserialize(requiredAvroRecord).asInstanceOf[InternalRow])\n+              true\n+            }\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        recordToLoad\n+      }\n+    }\n+\n+  private def payloadCombineFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                baseFileIterator: Iterator[InternalRow],\n+                                config: Configuration): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val tableAvroSchema = new Schema.Parser().parse(tableState.tableAvroSchema)\n+      private val requiredAvroSchema = new Schema.Parser().parse(tableState.requiredAvroSchema)\n+      private val requiredFieldPosition =\n+        tableState.requiredStructSchema\n+          .map(f => tableAvroSchema.getField(f.name).pos()).toList\n+      private val serializer = new AvroSerializer(tableState.tableStructSchema, tableAvroSchema, false)\n+      private val requiredDeserializer = new AvroDeserializer(requiredAvroSchema, tableState.requiredStructSchema)\n+      private val recordBuilder = new GenericRecordBuilder(requiredAvroSchema)\n+      private val unsafeProjection = UnsafeProjection.create(tableState.requiredStructSchema)\n+      private val logRecords = HoodieMergeOnReadRDD.scanLog(split, tableAvroSchema, config).getRecords\n+      private val logRecordsKeyIterator = logRecords.keySet().iterator().asScala\n+      private val keyToSkip = mutable.Set.empty[String]\n+\n+      private var recordToLoad: InternalRow = _\n+\n+      @scala.annotation.tailrec\n+      override def hasNext: Boolean = {\n+        if (baseFileIterator.hasNext) {\n+          val curRow = baseFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (logRecords.containsKey(curKey)) {\n+            // duplicate key found, merging\n+            keyToSkip.add(curKey)\n+            val mergedAvroRecord = mergeRowWithLog(curRow, curKey)\n+            if (!mergedAvroRecord.isPresent) {\n+              // deleted\n+              this.hasNext\n+            } else {\n+              // load merged record as InternalRow with required schema\n+              val requiredAvroRecord = AvroConversionUtils\n+                .buildAvroRecordBySchema(\n+                  mergedAvroRecord.get(),\n+                  requiredAvroSchema,\n+                  requiredFieldPosition,\n+                  recordBuilder\n+                )\n+              recordToLoad = unsafeProjection(requiredDeserializer\n+                .deserialize(requiredAvroRecord).asInstanceOf[InternalRow])\n+              true\n+            }\n+          } else {\n+            // No merge needed, load current row with required schema\n+            recordToLoad = unsafeProjection(createRowWithRequiredSchema(curRow))\n+            true\n+          }\n+        } else {\n+          if (logRecordsKeyIterator.hasNext) {\n+            val curKey = logRecordsKeyIterator.next()\n+            if (keyToSkip.contains(curKey)) {\n+              this.hasNext\n+            } else {\n+              val insertAvroRecord =\n+                logRecords.get(curKey).getData.getInsertValue(tableAvroSchema)\n+              if (!insertAvroRecord.isPresent) {\n+                // stand alone delete record, skipping\n+                this.hasNext\n+              } else {\n+                val requiredAvroRecord = AvroConversionUtils\n+                  .buildAvroRecordBySchema(\n+                    insertAvroRecord.get(),\n+                    requiredAvroSchema,\n+                    requiredFieldPosition,\n+                    recordBuilder\n+                  )\n+                recordToLoad = unsafeProjection(requiredDeserializer\n+                  .deserialize(requiredAvroRecord).asInstanceOf[InternalRow])\n+                true\n+              }\n+            }\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+\n+      override def next(): InternalRow = recordToLoad\n+\n+      private def createRowWithRequiredSchema(row: InternalRow): InternalRow = {\n+        val rowToReturn = new SpecificInternalRow(tableState.requiredStructSchema)\n+        val posIterator = requiredFieldPosition.iterator\n+        var curIndex = 0\n+        tableState.requiredStructSchema.foreach(\n+          f => {\n+            val curPos = posIterator.next()\n+            val curField = row.get(curPos, f.dataType)\n+            rowToReturn.update(curIndex, curField)\n+            curIndex = curIndex + 1\n+          }\n+        )\n+        rowToReturn\n+      }\n+\n+      private def mergeRowWithLog(curRow: InternalRow, curKey: String) = {\n+        val historyAvroRecord = serializer.serialize(curRow).asInstanceOf[GenericRecord]\n+        logRecords.get(curKey).getData.combineAndGetUpdateValue(historyAvroRecord, tableAvroSchema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa"}, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgyNjM5MA==", "bodyText": "remove the >>> ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464826390", "createdAt": "2020-08-04T06:25:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/MergeOnReadSnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, Filter, PrunedFilteredScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HoodieMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                      logPaths: Option[List[String]],\n+                                      latestCommit: String,\n+                                      tablePath: String,\n+                                      maxCompactionMemoryInBytes: Long,\n+                                      mergeType: String)\n+\n+case class HoodieMergeOnReadTableState(tableStructSchema: StructType,\n+                                       requiredStructSchema: StructType,\n+                                       tableAvroSchema: String,\n+                                       requiredAvroSchema: String,\n+                                       hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+\n+class MergeOnReadSnapshotRelation(val sqlContext: SQLContext,\n+                                  val optParams: Map[String, String],\n+                                  val userSchema: StructType,\n+                                  val globPaths: Seq[Path],\n+                                  val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with PrunedFilteredScan with Logging {\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+  private val jobConf = new JobConf(conf)\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val schemaUtil = new TableSchemaResolver(metaClient)\n+  private val tableAvroSchema = schemaUtil.getTableAvroSchema\n+  private val tableStructSchema = AvroConversionUtils.convertAvroSchemaToStructType(tableAvroSchema)\n+  private val mergeType = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_MERGE_OPT_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_MERGE_OPT_VAL)\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(jobConf)\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = tableStructSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = {\n+    log.debug(s\">>> buildScan requiredColumns = ${requiredColumns.mkString(\",\")}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMTAyOQ==", "bodyText": "I think we can still do better here. if the payload is OverwriteWithLatest... , then all we need to do is project the keys alone. right? no need for reading the full schema per se. ?", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464831029", "createdAt": "2020-08-04T06:38:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, GenericRecordBuilder}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.{Partition, SerializableWritable, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{SpecificInternalRow, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(@transient sc: SparkContext,\n+                           @transient config: Configuration,\n+                           fullSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           requiredSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           tableState: HoodieMergeOnReadTableState)\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  private val confBroadcast = sc.broadcast(new SerializableWritable(config))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader)\n+      case skipMergeSplit if skipMergeSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_SKIP_MERGE_OPT_VAL) =>\n+        skipMergeFileIterator(\n+          skipMergeSplit,\n+          read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader),\n+          getConfig\n+        )\n+      case payloadCombineSplit if payloadCombineSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_PAYLOAD_COMBINE_OPT_VAL) =>\n+        payloadCombineFileIterator(\n+          payloadCombineSplit,\n+          read(mergeParquetPartition.split.dataFile, fullSchemaFileReader),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNDEwOQ==", "bodyText": "See HoodieParquetRealtimeInputFormat#addRequiredProjectionFields() for referecne", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464834109", "createdAt": "2020-08-04T06:45:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, GenericRecordBuilder}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.{Partition, SerializableWritable, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{SpecificInternalRow, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(@transient sc: SparkContext,\n+                           @transient config: Configuration,\n+                           fullSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           requiredSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           tableState: HoodieMergeOnReadTableState)\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  private val confBroadcast = sc.broadcast(new SerializableWritable(config))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader)\n+      case skipMergeSplit if skipMergeSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_SKIP_MERGE_OPT_VAL) =>\n+        skipMergeFileIterator(\n+          skipMergeSplit,\n+          read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader),\n+          getConfig\n+        )\n+      case payloadCombineSplit if payloadCombineSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_PAYLOAD_COMBINE_OPT_VAL) =>\n+        payloadCombineFileIterator(\n+          payloadCombineSplit,\n+          read(mergeParquetPartition.split.dataFile, fullSchemaFileReader),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMTAyOQ=="}, "originalCommit": {"oid": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa"}, "originalPosition": 69}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "committedDate": "2020-08-04T06:01:51Z", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table"}, "afterCommit": {"oid": "a2b1467e7961a88ca91a3657807039351b64d834", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/a2b1467e7961a88ca91a3657807039351b64d834", "committedDate": "2020-08-04T06:52:55Z", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table\n\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a2b1467e7961a88ca91a3657807039351b64d834", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/a2b1467e7961a88ca91a3657807039351b64d834", "committedDate": "2020-08-04T06:52:55Z", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table\n\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature."}, "afterCommit": {"oid": "496daba54f981b082b6da8cfd642c5ec748b9b58", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/496daba54f981b082b6da8cfd642c5ec748b9b58", "committedDate": "2020-08-04T17:21:48Z", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table\n\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "496daba54f981b082b6da8cfd642c5ec748b9b58", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/496daba54f981b082b6da8cfd642c5ec748b9b58", "committedDate": "2020-08-04T17:21:48Z", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table\n\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature."}, "afterCommit": {"oid": "52066a3d867682a010a41c1c11b638615edef674", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/52066a3d867682a010a41c1c11b638615edef674", "committedDate": "2020-08-04T20:31:04Z", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table\n\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4694f5f9448578e944866d2f35e15c59b45808ae", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/4694f5f9448578e944866d2f35e15c59b45808ae", "committedDate": "2020-08-04T21:45:59Z", "message": "fix flaky test"}, "afterCommit": {"oid": "9e50d46c03a50e4fbd372f1dffdf6e0e84585c79", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/9e50d46c03a50e4fbd372f1dffdf6e0e84585c79", "committedDate": "2020-08-04T22:44:15Z", "message": "fix flaky test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9e50d46c03a50e4fbd372f1dffdf6e0e84585c79", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/9e50d46c03a50e4fbd372f1dffdf6e0e84585c79", "committedDate": "2020-08-04T22:44:15Z", "message": "fix flaky test"}, "afterCommit": {"oid": "1bbb23d04a483802407f85d98c89668f19be881b", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/1bbb23d04a483802407f85d98c89668f19be881b", "committedDate": "2020-08-05T13:50:43Z", "message": "Adding debug statements for test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1bbb23d04a483802407f85d98c89668f19be881b", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/1bbb23d04a483802407f85d98c89668f19be881b", "committedDate": "2020-08-05T13:50:43Z", "message": "Adding debug statements for test failure"}, "afterCommit": {"oid": "a053967b65240ee394e2a33c228fe617c203d8f6", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/a053967b65240ee394e2a33c228fe617c203d8f6", "committedDate": "2020-08-05T16:20:46Z", "message": "Adding debug statements for test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a053967b65240ee394e2a33c228fe617c203d8f6", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/a053967b65240ee394e2a33c228fe617c203d8f6", "committedDate": "2020-08-05T16:20:46Z", "message": "Adding debug statements for test failure"}, "afterCommit": {"oid": "fc5425ddeb2ac075f230793c5ff7906786fc66f3", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/fc5425ddeb2ac075f230793c5ff7906786fc66f3", "committedDate": "2020-08-06T05:35:01Z", "message": "Adding debug statements for test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fc5425ddeb2ac075f230793c5ff7906786fc66f3", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/fc5425ddeb2ac075f230793c5ff7906786fc66f3", "committedDate": "2020-08-06T05:35:01Z", "message": "Adding debug statements for test failure"}, "afterCommit": {"oid": "dcd26632c4c17f72fb6d96b1c65e8fed5a83e437", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/dcd26632c4c17f72fb6d96b1c65e8fed5a83e437", "committedDate": "2020-08-06T06:02:04Z", "message": "Adding debug statements for test failure"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMjEwOTE4", "url": "https://github.com/apache/hudi/pull/1848#pullrequestreview-462210918", "createdAt": "2020-08-06T06:06:52Z", "commit": {"oid": "dcd26632c4c17f72fb6d96b1c65e8fed5a83e437"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNjowNjo1MlrOG8kh0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNjowNjo1MlrOG8kh0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE2NjIyNA==", "bodyText": "@vinothchandar Looks like some unrelated change was added during the rebase. Maybe this is related to the issue.", "url": "https://github.com/apache/hudi/pull/1848#discussion_r466166224", "createdAt": "2020-08-06T06:06:52Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkUtils.scala", "diffHunk": "@@ -27,9 +27,9 @@ import org.apache.spark.sql.types.{StringType, StructField, StructType}\n import scala.collection.JavaConverters._\n \n \n-object HudiSparkUtils {\n+object HoodieSparkUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcd26632c4c17f72fb6d96b1c65e8fed5a83e437"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dcd26632c4c17f72fb6d96b1c65e8fed5a83e437", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/dcd26632c4c17f72fb6d96b1c65e8fed5a83e437", "committedDate": "2020-08-06T06:02:04Z", "message": "Adding debug statements for test failure"}, "afterCommit": {"oid": "15f95a2c560fa5bc03b97db0591d518635257e3a", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/15f95a2c560fa5bc03b97db0591d518635257e3a", "committedDate": "2020-08-06T06:36:05Z", "message": "Adding debug statements for test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "15f95a2c560fa5bc03b97db0591d518635257e3a", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/15f95a2c560fa5bc03b97db0591d518635257e3a", "committedDate": "2020-08-06T06:36:05Z", "message": "Adding debug statements for test failure"}, "afterCommit": {"oid": "a9533a099cacd86e79b65c483d41397ad62ff76f", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/a9533a099cacd86e79b65c483d41397ad62ff76f", "committedDate": "2020-08-06T07:32:54Z", "message": "Adding debug statements for test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a9533a099cacd86e79b65c483d41397ad62ff76f", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/a9533a099cacd86e79b65c483d41397ad62ff76f", "committedDate": "2020-08-06T07:32:54Z", "message": "Adding debug statements for test failure"}, "afterCommit": {"oid": "f70258dacf59e80888f5c95c93dfbc7e5ab90164", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/f70258dacf59e80888f5c95c93dfbc7e5ab90164", "committedDate": "2020-08-06T07:48:32Z", "message": "Adding debug statements for test failure"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f70258dacf59e80888f5c95c93dfbc7e5ab90164", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/f70258dacf59e80888f5c95c93dfbc7e5ab90164", "committedDate": "2020-08-06T07:48:32Z", "message": "Adding debug statements for test failure"}, "afterCommit": {"oid": "52a4532e75ae50cffbc4c3daaddbb6827c251975", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/52a4532e75ae50cffbc4c3daaddbb6827c251975", "committedDate": "2020-08-06T08:30:50Z", "message": "Adding debug statements for test failure"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4731d25e1223ca025df6a146ae8760f2f81d99d8", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/4731d25e1223ca025df6a146ae8760f2f81d99d8", "committedDate": "2020-08-06T21:08:50Z", "message": "[HUDI-69] Support Spark Datasource for MOR table\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d8beca97ef56b1fd1aac2ea9241d82d41103776f", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/d8beca97ef56b1fd1aac2ea9241d82d41103776f", "committedDate": "2020-08-06T21:12:09Z", "message": "[MINOR] fix some tests to pass CI"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "52a4532e75ae50cffbc4c3daaddbb6827c251975", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/52a4532e75ae50cffbc4c3daaddbb6827c251975", "committedDate": "2020-08-06T08:30:50Z", "message": "Adding debug statements for test failure"}, "afterCommit": {"oid": "d8beca97ef56b1fd1aac2ea9241d82d41103776f", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/d8beca97ef56b1fd1aac2ea9241d82d41103776f", "committedDate": "2020-08-06T21:12:09Z", "message": "[MINOR] fix some tests to pass CI"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzMDQyMDU4", "url": "https://github.com/apache/hudi/pull/1848#pullrequestreview-463042058", "createdAt": "2020-08-07T05:26:13Z", "commit": {"oid": "d8beca97ef56b1fd1aac2ea9241d82d41103776f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNToyNjoxM1rOG9M7KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNToyNjoxM1rOG9M7KA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyODA3Mg==", "bodyText": "@bvaradar keeping this code actually. given now, we are adding support for MOR snapshot query, if we don't unset this, then if you do a RO query and then a snapshot query, then this will filter out all except latest base files. which will be problematic.\ncc @garyli1019 does this make sense?  Let me see if I can add a test case for this", "url": "https://github.com/apache/hudi/pull/1848#discussion_r466828072", "createdAt": "2020-08-07T05:26:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -132,11 +132,15 @@ class DefaultSource extends RelationProvider\n \n     log.info(\"Constructing hoodie (as parquet) data source with options :\" + optParams)\n     // simply return as a regular parquet relation\n-    DataSource.apply(\n+    val relation =  DataSource.apply(\n       sparkSession = sqlContext.sparkSession,\n       userSpecifiedSchema = Option(schema),\n       className = \"parquet\",\n       options = optParams)\n       .resolveRelation()\n+\n+    sqlContext.sparkContext.hadoopConfiguration.unset(\"mapreduce.input.pathFilter.class\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d8beca97ef56b1fd1aac2ea9241d82d41103776f"}, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "60cec20ba14076e94f365269d2e68cf41fabc508", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/60cec20ba14076e94f365269d2e68cf41fabc508", "committedDate": "2020-08-07T06:35:08Z", "message": "Clean up test file name, add tests for mixed query type tests\n\n - We can now revert the change made in DefaultSource"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2994, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}