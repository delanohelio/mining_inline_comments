{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAzNDUwNzY3", "number": 2176, "reviewThreads": {"totalCount": 45, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNzo0NzozOFrOE1-lbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDozNzoyMlrOE52Lpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDM1MzczOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNzo0NzozOFrOHujQrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNjo1OTowOFrOHvg96Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3NDI1NQ==", "bodyText": "Since this is the entry point, it would be better to add some description.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518574255", "createdAt": "2020-11-06T07:47:38Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU4NTI1Nw==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519585257", "createdAt": "2020-11-09T06:59:08Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3NDI1NQ=="}, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDM1ODY3OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNzo0OTozM1rOHujTrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNjo1OToxNlrOHvg-FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3NTAyMw==", "bodyText": "Please add a description here, why we must hard-code this config option.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518575023", "createdAt": "2020-11-06T07:49:33Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU4NTMwMA==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519585300", "createdAt": "2020-11-09T06:59:16Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU3NTAyMw=="}, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDU1ODg3OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwODo1MzozNFrOHulNPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwODo1MzozNFrOHulNPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYwNjE0Mg==", "bodyText": "The indent is bad.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518606142", "createdAt": "2020-11-06T08:53:34Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new KafkaJson2HoodieRecord(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .setParallelism(1)\n+        .keyBy(HoodieRecord::getPartitionPath)\n+        .transform(WriteProcessOperator.NAME, TypeInformation.of(new TypeHint<Tuple3<String, List<WriteStatus>, Integer>>() {\n+        }), new WriteProcessOperator(new KeyedWriteProcessFunction())).name(\"write_process\").uid(\"write_process_uid\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDU2MTU0OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwODo1NDoyMFrOHulO3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNjo1OToyNFrOHvg-PQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYwNjU1Ng==", "bodyText": "Add a description about why hard code this parallelism.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518606556", "createdAt": "2020-11-06T08:54:20Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new KafkaJson2HoodieRecord(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .setParallelism(1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU4NTM0MQ==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519585341", "createdAt": "2020-11-09T06:59:24Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new KafkaJson2HoodieRecord(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .setParallelism(1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYwNjU1Ng=="}, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDY1MzI1OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwOToyMDoxMlrOHumGjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwOToyMDoxMlrOHumGjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYyMDgxNA==", "bodyText": "Hudi write job via Flink? And can we add some variables to distinguish different hudi job? e.g. table name?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518620814", "createdAt": "2020-11-06T09:20:12Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.WriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.KafkaJson2HoodieRecord;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new KafkaJson2HoodieRecord(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .setParallelism(1)\n+        .keyBy(HoodieRecord::getPartitionPath)\n+        .transform(WriteProcessOperator.NAME, TypeInformation.of(new TypeHint<Tuple3<String, List<WriteStatus>, Integer>>() {\n+        }), new WriteProcessOperator(new KeyedWriteProcessFunction())).name(\"write_process\").uid(\"write_process_uid\")\n+        .setParallelism(env.getParallelism())\n+        .addSink(new CommitSink()).name(\"commit_sink\").uid(\"commit_sink_uid\")\n+        .setParallelism(1);\n+\n+    env.execute(\"Hudi write via Flink\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDcyMTQxOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwOTozODowNFrOHumvsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwOTozODowNFrOHumvsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYzMTM0Nw==", "bodyText": "keep the same style as others?  add the access modifier", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518631347", "createdAt": "2020-11-06T09:38:04Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieWriteConfig writeConfig;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  List<String> latestInstantList = new ArrayList<>(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDcyMTYwOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwOTozODowN1rOHumvzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNjo1OTo0MlrOHvg-ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYzMTM3NA==", "bodyText": "split the static fields and non-static fields", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518631374", "createdAt": "2020-11-06T09:38:07Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU4NTQ0Mw==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519585443", "createdAt": "2020-11-09T06:59:42Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODYzMTM3NA=="}, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDc5MjU4OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwOTo1NzoyNlrOHunbsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwOTo1NzoyNlrOHunbsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY0MjYxMA==", "bodyText": "It seems bufferedRecords sounds better?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518642610", "createdAt": "2020-11-06T09:57:26Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieWriteConfig writeConfig;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> records = new LinkedList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDgxNDQ1OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMDowMzoyOVrOHunpfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxMDowMzoyOVrOHunpfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODY0NjE0MQ==", "bodyText": "Why do two times call this method?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r518646141", "createdAt": "2020-11-06T10:03:29Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieWriteConfig writeConfig;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> records = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      records.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), writeConfig);\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!records.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+    super.prepareSnapshotPreBarrier(checkpointId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0982523325a76046430bf3ed8c0e7360fca0a115"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1Njc5NTUyOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/WriteProcessOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwMzoyNDo0MFrOHvd01g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTo1NDo0N1rOHvm1MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUzMzc4Mg==", "bodyText": "WDYT about renaming to KeyedWriteProcessOperator? Since you use KeyedWriteProcessFunction.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519533782", "createdAt": "2020-11-09T03:24:40Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/WriteProcessOperator.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class WriteProcessOperator extends KeyedProcessOperator<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY4MTMyOQ==", "bodyText": "WDYT about renaming to KeyedWriteProcessOperator? Since you use KeyedWriteProcessFunction.\n\nyes, it's better", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519681329", "createdAt": "2020-11-09T09:54:47Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/WriteProcessOperator.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class WriteProcessOperator extends KeyedProcessOperator<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTUzMzc4Mg=="}, "originalCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzA2NTExOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/source/KafkaJson2HoodieRecord.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNjoxNzowMVrOHvgNqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTo1NToxOFrOHvm2lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU3MjkwNw==", "bodyText": "It would be better to add xxxFunction as the suffix. wdyt?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519572907", "createdAt": "2020-11-09T06:17:01Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/source/KafkaJson2HoodieRecord.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.source;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.keygen.KeyGenerator;\n+import org.apache.hudi.schema.FilebasedSchemaProvider;\n+import org.apache.hudi.util.AvroConvertor;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+public class KafkaJson2HoodieRecord implements MapFunction<String, HoodieRecord> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY4MTY4Nw==", "bodyText": "It would be better to add xxxFunction as the suffix. wdyt?\nyes , it is better", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519681687", "createdAt": "2020-11-09T09:55:18Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/source/KafkaJson2HoodieRecord.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.source;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.keygen.KeyGenerator;\n+import org.apache.hudi.schema.FilebasedSchemaProvider;\n+import org.apache.hudi.util.AvroConvertor;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+public class KafkaJson2HoodieRecord implements MapFunction<String, HoodieRecord> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU3MjkwNw=="}, "originalCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzA2NTYyOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/source/KafkaJson2HoodieRecord.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNjoxNzoyMlrOHvgN-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNjoxNzoyMlrOHvgN-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTU3Mjk4NQ==", "bodyText": "Add a new empty line.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519572985", "createdAt": "2020-11-09T06:17:22Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/source/KafkaJson2HoodieRecord.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.source;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.flink.api.common.functions.MapFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.keygen.KeyGenerator;\n+import org.apache.hudi.schema.FilebasedSchemaProvider;\n+import org.apache.hudi.util.AvroConvertor;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+public class KafkaJson2HoodieRecord implements MapFunction<String, HoodieRecord> {\n+\n+  private static Logger LOG = LoggerFactory.getLogger(KafkaJson2HoodieRecord.class);\n+\n+  private final HudiFlinkStreamer.Config cfg;\n+  private TypedProperties props;\n+  private KeyGenerator keyGenerator;\n+  private AvroConvertor convertor;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f5b5a94c56f47282659dbc2535785fa1d38736b5"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzI0OTkzOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNzozMDo0NFrOHvh7lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwNzozMDo0NFrOHvh7lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTYwMTA0NA==", "bodyText": "Can be a local variable.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519601044", "createdAt": "2020-11-09T07:30:44Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9bbaef1c6844bca962598ce37c42c8deb7fc9fc9"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzU3NTcyOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTowNTo1OFrOHvk7ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTowNTo1OFrOHvk7ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY1MDE0OQ==", "bodyText": "Never thrown exception.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519650149", "createdAt": "2020-11-09T09:05:58Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+\n+    indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(new org.apache.hadoop.conf.Configuration());\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(serializableHadoopConf, new FlinkTaskContextSupplier(getRuntimeContext()));\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(context, writeConfig);\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzU5NjAzOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOToxMToxNFrOHvlHiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOToxMToxNFrOHvlHiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY1MzI1Nw==", "bodyText": "I am thinking one thing: if the interval of the checkpoint is too long. If this buffer would cause OOM?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519653257", "createdAt": "2020-11-09T09:11:14Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+\n+    indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(new org.apache.hadoop.conf.Configuration());\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(serializableHadoopConf, new FlinkTaskContextSupplier(getRuntimeContext()));\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(context, writeConfig);\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    // get latest requested instant\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    List<String> latestInstants = writeClient.getInflightsAndRequestedInstants(commitType);\n+    latestInstant = latestInstants.isEmpty() ? null : latestInstants.get(0);\n+\n+    if (output != null && latestInstant != null && records.size() > 0) {\n+      hasRecordsIn = true;\n+      String instantTimestamp = latestInstant;\n+      LOG.info(\"Upsert records, subtask id = [{}]  checkpoint_id = [{}}] instant = [{}], record size = [{}]\", indexOfThisSubtask, context.getCheckpointId(), instantTimestamp, records.size());\n+\n+      List<WriteStatus> writeStatus;\n+      switch (cfg.operation) {\n+        case INSERT:\n+          writeStatus = writeClient.insert(records, instantTimestamp);\n+          break;\n+        case UPSERT:\n+          writeStatus = writeClient.upsert(records, instantTimestamp);\n+          break;\n+        default:\n+          throw new HoodieFlinkStreamerException(\"Unknown operation : \" + cfg.operation);\n+      }\n+      output.collect(new Tuple3<>(instantTimestamp, writeStatus, indexOfThisSubtask));\n+      records.clear();\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext functionInitializationContext) throws Exception {\n+    // no operation\n+  }\n+\n+  @Override\n+  public void processElement(HoodieRecord hoodieRecord, Context context, Collector<Tuple3<String, List<WriteStatus>, Integer>> collector) throws Exception {\n+    records.add(hoodieRecord);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzU5ODIzOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOToxMTo0N1rOHvlI2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOToxMTo0N1rOHvlI2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY1MzU5Mw==", "bodyText": "never used Exception.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519653593", "createdAt": "2020-11-09T09:11:47Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+\n+    indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(new org.apache.hadoop.conf.Configuration());\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(serializableHadoopConf, new FlinkTaskContextSupplier(getRuntimeContext()));\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(context, writeConfig);\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    // get latest requested instant\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    List<String> latestInstants = writeClient.getInflightsAndRequestedInstants(commitType);\n+    latestInstant = latestInstants.isEmpty() ? null : latestInstants.get(0);\n+\n+    if (output != null && latestInstant != null && records.size() > 0) {\n+      hasRecordsIn = true;\n+      String instantTimestamp = latestInstant;\n+      LOG.info(\"Upsert records, subtask id = [{}]  checkpoint_id = [{}}] instant = [{}], record size = [{}]\", indexOfThisSubtask, context.getCheckpointId(), instantTimestamp, records.size());\n+\n+      List<WriteStatus> writeStatus;\n+      switch (cfg.operation) {\n+        case INSERT:\n+          writeStatus = writeClient.insert(records, instantTimestamp);\n+          break;\n+        case UPSERT:\n+          writeStatus = writeClient.upsert(records, instantTimestamp);\n+          break;\n+        default:\n+          throw new HoodieFlinkStreamerException(\"Unknown operation : \" + cfg.operation);\n+      }\n+      output.collect(new Tuple3<>(instantTimestamp, writeStatus, indexOfThisSubtask));\n+      records.clear();\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext functionInitializationContext) throws Exception {\n+    // no operation\n+  }\n+\n+  @Override\n+  public void processElement(HoodieRecord hoodieRecord, Context context, Collector<Tuple3<String, List<WriteStatus>, Integer>> collector) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzU5OTQzOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOToxMjowNFrOHvlJjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOToxMjowNFrOHvlJjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY1Mzc3NQ==", "bodyText": "never used exception", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519653775", "createdAt": "2020-11-09T09:12:04Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/KeyedWriteProcessFunction.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieFlinkStreamerException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.util.Collector;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+public class KeyedWriteProcessFunction extends KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(KeyedWriteProcessFunction.class);\n+  private List<HoodieRecord> records = new LinkedList<>();\n+  private Collector<Tuple3<String, List<WriteStatus>, Integer>> output;\n+  private int indexOfThisSubtask;\n+  private String latestInstant;\n+  private boolean hasRecordsIn;\n+\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+  /**\n+   * Serializable hadoop conf.\n+   */\n+  private SerializableConfiguration serializableHadoopConf;\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+\n+    indexOfThisSubtask = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(new org.apache.hadoop.conf.Configuration());\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    HoodieFlinkEngineContext context = new HoodieFlinkEngineContext(serializableHadoopConf, new FlinkTaskContextSupplier(getRuntimeContext()));\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(context, writeConfig);\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    // get latest requested instant\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    List<String> latestInstants = writeClient.getInflightsAndRequestedInstants(commitType);\n+    latestInstant = latestInstants.isEmpty() ? null : latestInstants.get(0);\n+\n+    if (output != null && latestInstant != null && records.size() > 0) {\n+      hasRecordsIn = true;\n+      String instantTimestamp = latestInstant;\n+      LOG.info(\"Upsert records, subtask id = [{}]  checkpoint_id = [{}}] instant = [{}], record size = [{}]\", indexOfThisSubtask, context.getCheckpointId(), instantTimestamp, records.size());\n+\n+      List<WriteStatus> writeStatus;\n+      switch (cfg.operation) {\n+        case INSERT:\n+          writeStatus = writeClient.insert(records, instantTimestamp);\n+          break;\n+        case UPSERT:\n+          writeStatus = writeClient.upsert(records, instantTimestamp);\n+          break;\n+        default:\n+          throw new HoodieFlinkStreamerException(\"Unknown operation : \" + cfg.operation);\n+      }\n+      output.collect(new Tuple3<>(instantTimestamp, writeStatus, indexOfThisSubtask));\n+      records.clear();\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext functionInitializationContext) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzY5ODY4OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/WriteProcessOperator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTozNjowOVrOHvmFbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTozNjowOVrOHvmFbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY2OTEwMg==", "bodyText": "It would be better to explain why do we need to implement the snapshotState method both in UDF and operator?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519669102", "createdAt": "2020-11-09T09:36:09Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/WriteProcessOperator.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.KeyedProcessFunction;\n+import org.apache.flink.streaming.api.operators.KeyedProcessOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class WriteProcessOperator extends KeyedProcessOperator<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> {\n+  public static final String NAME = \"WriteProcessOperator\";\n+  private static final Logger LOG = LoggerFactory.getLogger(WriteProcessOperator.class);\n+  private KeyedWriteProcessFunction function;\n+\n+  public WriteProcessOperator(KeyedProcessFunction<String, HoodieRecord, Tuple3<String, List<WriteStatus>, Integer>> function) {\n+    super(function);\n+    this.function = (KeyedWriteProcessFunction) function;\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzcwODg4OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTozODozM1rOHvmLng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTozODozM1rOHvmLng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY3MDY4Ng==", "bodyText": "It would be better to make the value variable more reasonable.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519670686", "createdAt": "2020-11-09T09:38:33Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sink;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class CommitSink extends RichSinkFunction<Tuple3<String, List<WriteStatus>, Integer>> {\n+  private static final Logger LOG = LoggerFactory.getLogger(CommitSink.class);\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  private Map<String, List<List<WriteStatus>>> bufferedWriteStatus = new HashMap<>();\n+\n+  private Integer upsertParallelSize = 0;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+    upsertParallelSize = getRuntimeContext().getExecutionConfig().getParallelism();\n+\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(new HoodieFlinkEngineContext(new FlinkTaskContextSupplier(null)), writeConfig);\n+  }\n+\n+  @Override\n+  public void invoke(Tuple3<String, List<WriteStatus>, Integer> value, Context context) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzcxNTYzOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTo0MDowOVrOHvmPsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTo0MDowOVrOHvmPsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY3MTcyOA==", "bodyText": "You do not use the returned value, right?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519671728", "createdAt": "2020-11-09T09:40:09Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sink;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class CommitSink extends RichSinkFunction<Tuple3<String, List<WriteStatus>, Integer>> {\n+  private static final Logger LOG = LoggerFactory.getLogger(CommitSink.class);\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  private Map<String, List<List<WriteStatus>>> bufferedWriteStatus = new HashMap<>();\n+\n+  private Integer upsertParallelSize = 0;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+    upsertParallelSize = getRuntimeContext().getExecutionConfig().getParallelism();\n+\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(new HoodieFlinkEngineContext(new FlinkTaskContextSupplier(null)), writeConfig);\n+  }\n+\n+  @Override\n+  public void invoke(Tuple3<String, List<WriteStatus>, Integer> value, Context context) throws Exception {\n+    LOG.info(\"Receive records, instantTime = [{}], subtaskId = [{}], records size = [{}]\", value.f0, value.f2, value.f1.size());\n+    try {\n+      if (bufferedWriteStatus.containsKey(value.f0)) {\n+        bufferedWriteStatus.get(value.f0).add(value.f1);\n+      } else {\n+        List<List<WriteStatus>> oneBatchData = new ArrayList<>(upsertParallelSize);\n+        oneBatchData.add(value.f1);\n+        bufferedWriteStatus.put(value.f0, oneBatchData);\n+      }\n+      // check and commit\n+      checkAndCommit(value.f0);\n+    } catch (Exception e) {\n+      LOG.error(\"Invoke sink error: \" + Thread.currentThread().getId() + \";\" + this);\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Check and commit if all subtask completed.\n+   *\n+   * @throws Exception\n+   */\n+  private boolean checkAndCommit(String instantTime) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1NzcxOTMwOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTo0MTowMFrOHvmR8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQwOTo0MTowMFrOHvmR8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTY3MjMwNw==", "bodyText": "public -> private", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519672307", "createdAt": "2020-11-09T09:41:00Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/sink/CommitSink.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sink;\n+\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class CommitSink extends RichSinkFunction<Tuple3<String, List<WriteStatus>, Integer>> {\n+  private static final Logger LOG = LoggerFactory.getLogger(CommitSink.class);\n+  /**\n+   * Job conf.\n+   */\n+  private HudiFlinkStreamer.Config cfg;\n+\n+  /**\n+   * HoodieWriteConfig.\n+   */\n+  private HoodieWriteConfig writeConfig;\n+\n+  /**\n+   * Write Client.\n+   */\n+  private transient HoodieFlinkWriteClient writeClient;\n+\n+  private Map<String, List<List<WriteStatus>>> bufferedWriteStatus = new HashMap<>();\n+\n+  private Integer upsertParallelSize = 0;\n+\n+  @Override\n+  public void open(Configuration parameters) throws Exception {\n+    super.open(parameters);\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+    upsertParallelSize = getRuntimeContext().getExecutionConfig().getParallelism();\n+\n+    // HoodieWriteConfig\n+    writeConfig = StreamerUtil.getHoodieClientConfig(cfg);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient<>(new HoodieFlinkEngineContext(new FlinkTaskContextSupplier(null)), writeConfig);\n+  }\n+\n+  @Override\n+  public void invoke(Tuple3<String, List<WriteStatus>, Integer> value, Context context) throws Exception {\n+    LOG.info(\"Receive records, instantTime = [{}], subtaskId = [{}], records size = [{}]\", value.f0, value.f2, value.f1.size());\n+    try {\n+      if (bufferedWriteStatus.containsKey(value.f0)) {\n+        bufferedWriteStatus.get(value.f0).add(value.f1);\n+      } else {\n+        List<List<WriteStatus>> oneBatchData = new ArrayList<>(upsertParallelSize);\n+        oneBatchData.add(value.f1);\n+        bufferedWriteStatus.put(value.f0, oneBatchData);\n+      }\n+      // check and commit\n+      checkAndCommit(value.f0);\n+    } catch (Exception e) {\n+      LOG.error(\"Invoke sink error: \" + Thread.currentThread().getId() + \";\" + this);\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * Check and commit if all subtask completed.\n+   *\n+   * @throws Exception\n+   */\n+  private boolean checkAndCommit(String instantTime) throws Exception {\n+    if (bufferedWriteStatus.get(instantTime).size() == upsertParallelSize) {\n+      LOG.info(\"Instant [{}] process complete, start commit\uff01\", instantTime);\n+      doCommit(instantTime);\n+      bufferedWriteStatus.clear();\n+      LOG.info(\"Instant [{}] commit completed!\", instantTime);\n+      return true;\n+    } else {\n+      LOG.info(\"Instant [{}], can not commit yet, subtask completed : [{}/{}]\", instantTime, bufferedWriteStatus.get(instantTime).size(), upsertParallelSize);\n+      return false;\n+    }\n+  }\n+\n+  public void doCommit(String instantTime) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07097e7fec159248ae93f154b35e873b96cd2515"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1ODIwNTIwOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMTo0NToyMVrOHvq4pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMjozMDoyNVrOHvsWGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc0Nzc0OQ==", "bodyText": "It seems that this variable: commitTimeout  means check retry number? If you think it a timeout solution, then it is not exact counting.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519747749", "createdAt": "2020-11-09T11:45:21Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc1MDQ4MA==", "bodyText": "IMO, we can introduce both retry number and timeout conditions. WDYT?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519750480", "createdAt": "2020-11-09T11:50:26Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc0Nzc0OQ=="}, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc3MTY3Mg==", "bodyText": "IMO, we can introduce both retry number and timeout conditions. WDYT?\n\ngood idea", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519771672", "createdAt": "2020-11-09T12:30:25Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc0Nzc0OQ=="}, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1ODIxMjY2OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMTo0NzoyOVrOHvq9IA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMzoxOTo1OVrOHvunmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc0ODg5Ng==", "bodyText": "IMO, checker is a noun. Can we name it to be doCheck or checkXXX ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519748896", "createdAt": "2020-11-09T11:47:29Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTgwODkyMQ==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519808921", "createdAt": "2020-11-09T13:19:59Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc0ODg5Ng=="}, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1ODI0MDIxOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMTo1NToxMlrOHvrOCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMzowMjo0NVrOHvtfHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc1MzIyNg==", "bodyText": "Chinese? Why this is a clear action to string buffer?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519753226", "createdAt": "2020-11-09T11:55:12Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {\n+      tryTimes++;\n+      StringBuffer sb = new StringBuffer();\n+      if (rollbackPendingCommits.contains(latestInstant)) {\n+        //\u6e05\u7a7a sb", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc5MDM2NQ==", "bodyText": "Chinese? Why this is a clear action to string buffer?\n\nmy bad", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519790365", "createdAt": "2020-11-09T13:02:45Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {\n+      tryTimes++;\n+      StringBuffer sb = new StringBuffer();\n+      if (rollbackPendingCommits.contains(latestInstant)) {\n+        //\u6e05\u7a7a sb", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc1MzIyNg=="}, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1ODI1MDU1OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMTo1Nzo1NFrOHvrUEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMzowMDo1MlrOHvtYPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc1NDc2OQ==", "bodyText": "What's the result if the path exists? Adding a log warning message?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519754769", "createdAt": "2020-11-09T11:57:54Z", "author": {"login": "yanghua"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {\n+      tryTimes++;\n+      StringBuffer sb = new StringBuffer();\n+      if (rollbackPendingCommits.contains(latestInstant)) {\n+        //\u6e05\u7a7a sb\n+        rollbackPendingCommits.forEach(x -> sb.append(x).append(\",\"));\n+\n+        LOG.warn(\"Latest transaction [{}] is not completed! unCompleted transaction:[{}],try times [{}]\", latestInstant, sb.toString(), tryTimes);\n+\n+        Thread.sleep(1000);\n+        rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+      } else {\n+        LOG.warn(\"Latest transaction [{}] is completed! Completed transaction, try times [{}]\", latestInstant, tryTimes);\n+        return;\n+      }\n+    }\n+    throw new InterruptedException(\"Last instant costs more than ten second, stop task now\");\n+  }\n+\n+\n+  /**\n+   * Create table if not exists.\n+   */\n+  private void initTable() throws IOException {\n+    if (!fs.exists(new Path(cfg.targetBasePath))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc4ODYwNA==", "bodyText": "What's the result if the path exists? Adding a log warning message?\nok", "url": "https://github.com/apache/hudi/pull/2176#discussion_r519788604", "createdAt": "2020-11-09T13:00:52Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/operator/InstantGenerateOperator.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.operator;\n+\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.HudiFlinkStreamer;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.util.StreamerUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+/**\n+ * Operator helps to generate globally unique instant, it must be executed in one parallelism. Before generate a new\n+ * instant , {@link InstantGenerateOperator} will always check whether the last instant has completed. if it is\n+ * completed, a new instant will be generated immediately, otherwise, wait and check the state of last instant until\n+ * time out and throw an exception.\n+ */\n+public class InstantGenerateOperator extends AbstractStreamOperator<HoodieRecord> implements OneInputStreamOperator<HoodieRecord, HoodieRecord> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(InstantGenerateOperator.class);\n+  public static final String NAME = \"InstantGenerateOperator\";\n+\n+  private HudiFlinkStreamer.Config cfg;\n+  private HoodieFlinkWriteClient writeClient;\n+  private SerializableConfiguration serializableHadoopConf;\n+  private transient FileSystem fs;\n+  private String latestInstant = \"\";\n+  private List<String> latestInstantList = new ArrayList<>(1);\n+  private transient ListState<String> latestInstantState;\n+  private List<StreamRecord> bufferedRecords = new LinkedList();\n+  private transient ListState<StreamRecord> recordsState;\n+  private Integer commitTimeout;\n+\n+  @Override\n+  public void processElement(StreamRecord<HoodieRecord> streamRecord) throws Exception {\n+    if (streamRecord.getValue() != null) {\n+      bufferedRecords.add(streamRecord);\n+      output.collect(streamRecord);\n+    }\n+  }\n+\n+  @Override\n+  public void open() throws Exception {\n+    super.open();\n+    // get configs from runtimeContext\n+    cfg = (HudiFlinkStreamer.Config) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();\n+\n+    // timeout\n+    commitTimeout = Integer.valueOf(cfg.flinkCommitTimeout);\n+\n+    // hadoopConf\n+    serializableHadoopConf = new SerializableConfiguration(StreamerUtil.getHadoopConf());\n+\n+    // Hadoop FileSystem\n+    fs = FSUtils.getFs(cfg.targetBasePath, serializableHadoopConf.get());\n+\n+    TaskContextSupplier taskContextSupplier = new FlinkTaskContextSupplier(null);\n+\n+    // writeClient\n+    writeClient = new HoodieFlinkWriteClient(new HoodieFlinkEngineContext(taskContextSupplier), StreamerUtil.getHoodieClientConfig(cfg));\n+\n+    // init table, create it if not exists.\n+    initTable();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    super.prepareSnapshotPreBarrier(checkpointId);\n+    // check whether the last instant is completed, if not, wait 10s and then throws an exception\n+    if (!StringUtils.isNullOrEmpty(latestInstant)) {\n+      doChecker();\n+      // last instant completed, set it empty\n+      latestInstant = \"\";\n+    }\n+\n+    // no data no new instant\n+    if (!bufferedRecords.isEmpty()) {\n+      latestInstant = startNewInstant(checkpointId);\n+    }\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    // instantState\n+    ListStateDescriptor<String> latestInstantStateDescriptor = new ListStateDescriptor<String>(\"latestInstant\", String.class);\n+    latestInstantState = context.getOperatorStateStore().getListState(latestInstantStateDescriptor);\n+\n+    // recordState\n+    ListStateDescriptor<StreamRecord> recordsStateDescriptor = new ListStateDescriptor<StreamRecord>(\"recordsState\", StreamRecord.class);\n+    recordsState = context.getOperatorStateStore().getListState(recordsStateDescriptor);\n+\n+    if (context.isRestored()) {\n+      Iterator<String> latestInstantIterator = latestInstantState.get().iterator();\n+      latestInstantIterator.forEachRemaining(x -> latestInstant = x);\n+      LOG.info(\"InstantGenerateOperator initializeState get latestInstant [{}]\", latestInstant);\n+\n+      Iterator<StreamRecord> recordIterator = recordsState.get().iterator();\n+      bufferedRecords.clear();\n+      recordIterator.forEachRemaining(x -> bufferedRecords.add(x));\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext functionSnapshotContext) throws Exception {\n+    if (latestInstantList.isEmpty()) {\n+      latestInstantList.add(latestInstant);\n+    } else {\n+      latestInstantList.set(0, latestInstant);\n+    }\n+    latestInstantState.update(latestInstantList);\n+    LOG.info(\"Update latest instant [{}]\", latestInstant);\n+\n+    recordsState.update(bufferedRecords);\n+    LOG.info(\"Update records state size = [{}]\", bufferedRecords.size());\n+    bufferedRecords.clear();\n+  }\n+\n+  /**\n+   * Create a new instant.\n+   *\n+   * @param checkpointId\n+   */\n+  private String startNewInstant(long checkpointId) {\n+    String newTime = writeClient.startCommit();\n+    LOG.info(\"create instant [{}], at checkpoint [{}]\", newTime, checkpointId);\n+    return newTime;\n+  }\n+\n+  /**\n+   * Check the status of last instant.\n+   */\n+  private void doChecker() throws InterruptedException {\n+    // query the requested and inflight commit/deltacommit instants\n+    String commitType = cfg.tableType.equals(HoodieTableType.COPY_ON_WRITE.name()) ? HoodieTimeline.COMMIT_ACTION : HoodieTimeline.DELTA_COMMIT_ACTION;\n+    LOG.info(\"Query latest instant [{}]\", latestInstant);\n+    List<String> rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+    int tryTimes = 0;\n+    while (tryTimes < commitTimeout) {\n+      tryTimes++;\n+      StringBuffer sb = new StringBuffer();\n+      if (rollbackPendingCommits.contains(latestInstant)) {\n+        //\u6e05\u7a7a sb\n+        rollbackPendingCommits.forEach(x -> sb.append(x).append(\",\"));\n+\n+        LOG.warn(\"Latest transaction [{}] is not completed! unCompleted transaction:[{}],try times [{}]\", latestInstant, sb.toString(), tryTimes);\n+\n+        Thread.sleep(1000);\n+        rollbackPendingCommits = writeClient.getInflightsAndRequestedInstants(commitType);\n+      } else {\n+        LOG.warn(\"Latest transaction [{}] is completed! Completed transaction, try times [{}]\", latestInstant, tryTimes);\n+        return;\n+      }\n+    }\n+    throw new InterruptedException(\"Last instant costs more than ten second, stop task now\");\n+  }\n+\n+\n+  /**\n+   * Create table if not exists.\n+   */\n+  private void initTable() throws IOException {\n+    if (!fs.exists(new Path(cfg.targetBasePath))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTc1NDc2OQ=="}, "originalCommit": {"oid": "0cedf8c1aa4e9a0cb80acd8b0547c180a7667e7e"}, "originalPosition": 203}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NzYyODExOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/schema/FilebasedSchemaProvider.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTozMDoyNFrOHxFOfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwMToyNjoyNlrOHxltdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIyNzkwMA==", "bodyText": "we have TableSchemaResolver to handle the schema", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521227900", "createdAt": "2020-11-11T09:30:24Z", "author": {"login": "garyli1019"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/schema/FilebasedSchemaProvider.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.schema;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+\n+/**\n+ * A simple schema provider, that reads off files on DFS.\n+ */\n+public class FilebasedSchemaProvider extends SchemaProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTI4MDYwNA==", "bodyText": "we have TableSchemaResolver to handle the schema\n\nTableSchemaResolver  helps to read schema from data files and log files, here we need a Util to read schema from job config", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521280604", "createdAt": "2020-11-11T11:03:45Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/schema/FilebasedSchemaProvider.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.schema;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+\n+/**\n+ * A simple schema provider, that reads off files on DFS.\n+ */\n+public class FilebasedSchemaProvider extends SchemaProvider {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIyNzkwMA=="}, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTM0MTk3Ng==", "bodyText": "ah ok, I got confused by the class name. Should we move this to TableSchemaResolver then, so we have schema related stuff in one place. WDYT?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521341976", "createdAt": "2020-11-11T13:01:14Z", "author": {"login": "garyli1019"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/schema/FilebasedSchemaProvider.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.schema;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+\n+/**\n+ * A simple schema provider, that reads off files on DFS.\n+ */\n+public class FilebasedSchemaProvider extends SchemaProvider {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIyNzkwMA=="}, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTc2MDExNg==", "bodyText": "ah ok, I got confused by the class name. Should we move this to TableSchemaResolver then, so we have schema related stuff in one place. WDYT?\n\nYes, keep schema related stuff together makes sense.\nMaybe we can leave TableSchemaResolver where it is and move SchemaProvider stuff to hudi-client-common , because TableSchemaResolver  basically will not be modified in the future, while SchemaProvider  is expansion-oriented, more likely to add new child implementation, and more like client stuff.\nbesides, this SchemaProvider comes from hudi-utilities module, and there are still lots of utils can be reused there, we can file a new ticket to do things like this together.\nWDYT?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521760116", "createdAt": "2020-11-12T01:26:26Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/schema/FilebasedSchemaProvider.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.schema;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+\n+/**\n+ * A simple schema provider, that reads off files on DFS.\n+ */\n+public class FilebasedSchemaProvider extends SchemaProvider {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIyNzkwMA=="}, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NzY0ODkyOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTozNjoxNVrOHxFbbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTozNjoxNVrOHxFbbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIzMTIxNQ==", "bodyText": "would you add a bit more comments here? not quite familiar with Flink but a bit worried about if this will impact the performance", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521231215", "createdAt": "2020-11-11T09:36:15Z", "author": {"login": "garyli1019"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.KeyedWriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.JsonStringToHoodieRecordMapFunction;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+\n+/**\n+ * An Utility which can incrementally consume data from Kafka and apply it to the target table.\n+ * currently, it only support MOR table and insert, upsert operation.\n+ */\n+public class HudiFlinkStreamer {\n+  public static void main(String[] args) throws Exception {\n+    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    env.enableCheckpointing(cfg.checkpointInterval);\n+    env.getConfig().setGlobalJobParameters(cfg);\n+    // We use checkpoint to trigger write operation, including instant generating and committing,\n+    // There can only be one checkpoint at one time.\n+    env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n+    env.disableOperatorChaining();\n+\n+    if (cfg.flinkCheckPointPath != null) {\n+      env.setStateBackend(new FsStateBackend(cfg.flinkCheckPointPath));\n+    }\n+\n+    Properties kafkaProps = StreamerUtil.getKafkaProps(cfg);\n+\n+    // Read from kafka source\n+    DataStream<HoodieRecord> inputRecords =\n+        env.addSource(new FlinkKafkaConsumer<>(cfg.kafkaTopic, new SimpleStringSchema(), kafkaProps))\n+            .map(new JsonStringToHoodieRecordMapFunction(cfg))\n+            .name(\"kafka_to_hudi_record\")\n+            .uid(\"kafka_to_hudi_record_uid\");\n+\n+    // InstantGenerateOperator helps to emit globally unique instantTime, it must be executed in one parallelism\n+    inputRecords.transform(InstantGenerateOperator.NAME, TypeInformation.of(HoodieRecord.class), new InstantGenerateOperator())\n+        .name(\"instant_generator\")\n+        .uid(\"instant_generator_id\")\n+        .setParallelism(1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NzY1Njc0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/upgrade/OneToZeroDowngradeHandler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwOTozODoyM1rOHxFgQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxMTowNzowOVrOHxIjmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIzMjQ1MQ==", "bodyText": "what is the diff between version 1 and 0?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521232451", "createdAt": "2020-11-11T09:38:23Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/upgrade/OneToZeroDowngradeHandler.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.MarkerFiles;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Downgrade handle to assist in downgrading hoodie table from version 1 to 0.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTI4MjQ1Ng==", "bodyText": "what is the diff between version 1 and 0?\n\n0  - hudi version < 0.6.0\n1 - hudi version >= 0.6.0", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521282456", "createdAt": "2020-11-11T11:07:09Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/upgrade/OneToZeroDowngradeHandler.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.MarkerFiles;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Downgrade handle to assist in downgrading hoodie table from version 1 to 0.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTIzMjQ1MQ=="}, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2Nzc0Nzg4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxMDowMjo1OFrOHxGYog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxMDowMjo1OFrOHxGYog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTI0Njg4Mg==", "bodyText": "Is this only dedup data in one batch but not upserting into the historical hudi table?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r521246882", "createdAt": "2020-11-11T10:02:58Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50a26bb33185788a1f4dc49b08a0a93189ead07d"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3ODYwNjEyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo0OTo0OFrOHyvAmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNFQxNDozMTo1M1rOHzLXHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MTA0OQ==", "bodyText": "would be changed to RuntimeContext?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522961049", "createdAt": "2020-11-13T13:49:48Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.EngineProperty;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+\n+import java.util.function.Supplier;\n+\n+/**\n+ * Flink task context supplier.\n+ */\n+public class FlinkTaskContextSupplier extends TaskContextSupplier {\n+  private org.apache.flink.api.common.functions.RuntimeContext flinkRuntimeContext;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzQyNTU2Ng==", "bodyText": "would be changed to RuntimeContext?\n\ndone", "url": "https://github.com/apache/hudi/pull/2176#discussion_r523425566", "createdAt": "2020-11-14T14:31:53Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.EngineProperty;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+\n+import java.util.function.Supplier;\n+\n+/**\n+ * Flink task context supplier.\n+ */\n+public class FlinkTaskContextSupplier extends TaskContextSupplier {\n+  private org.apache.flink.api.common.functions.RuntimeContext flinkRuntimeContext;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MTA0OQ=="}, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3ODYwODM5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MDoyMFrOHyvB4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNFQxNDozMTozOFrOHzLXFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MTM3Ng==", "bodyText": "// TODO need to check again?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522961376", "createdAt": "2020-11-13T13:50:20Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.EngineProperty;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+\n+import java.util.function.Supplier;\n+\n+/**\n+ * Flink task context supplier.\n+ */\n+public class FlinkTaskContextSupplier extends TaskContextSupplier {\n+  private org.apache.flink.api.common.functions.RuntimeContext flinkRuntimeContext;\n+\n+  public FlinkTaskContextSupplier(RuntimeContext flinkRuntimeContext) {\n+    this.flinkRuntimeContext = flinkRuntimeContext;\n+  }\n+\n+  public RuntimeContext getFlinkRuntimeContext() {\n+    return flinkRuntimeContext;\n+  }\n+\n+  @Override\n+  public Supplier<Integer> getPartitionIdSupplier() {\n+    return () -> this.flinkRuntimeContext.getIndexOfThisSubtask();\n+  }\n+\n+  @Override\n+  public Supplier<Integer> getStageIdSupplier() {\n+    // need to check again", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzQyNTU1Ng==", "bodyText": "// TODO need to check again?\n\nhere flink has no concept of stage\uff0c so I used NumberOfParallelSubtasks not sure if it is suitable\nany idea ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r523425556", "createdAt": "2020-11-14T14:31:38Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/FlinkTaskContextSupplier.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.EngineProperty;\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.flink.api.common.functions.RuntimeContext;\n+\n+import java.util.function.Supplier;\n+\n+/**\n+ * Flink task context supplier.\n+ */\n+public class FlinkTaskContextSupplier extends TaskContextSupplier {\n+  private org.apache.flink.api.common.functions.RuntimeContext flinkRuntimeContext;\n+\n+  public FlinkTaskContextSupplier(RuntimeContext flinkRuntimeContext) {\n+    this.flinkRuntimeContext = flinkRuntimeContext;\n+  }\n+\n+  public RuntimeContext getFlinkRuntimeContext() {\n+    return flinkRuntimeContext;\n+  }\n+\n+  @Override\n+  public Supplier<Integer> getPartitionIdSupplier() {\n+    return () -> this.flinkRuntimeContext.getIndexOfThisSubtask();\n+  }\n+\n+  @Override\n+  public Supplier<Integer> getStageIdSupplier() {\n+    // need to check again", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MTM3Ng=="}, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3ODYxNDYzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MjowMlrOHyvFhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNFQxNDozNjoyMlrOHzLYsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MjMwOA==", "bodyText": "would be moved to super class ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522962308", "createdAt": "2020-11-13T13:52:02Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                Option<EmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  @Override\n+  protected HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createIndex(HoodieWriteConfig writeConfig) {\n+    return FlinkHoodieIndex.createIndex((HoodieFlinkEngineContext) context, config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieFlinkTable<T> table = HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS).bootstrap(context, extraMetadata);\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzQyNTk3MQ==", "bodyText": "would be moved to super class ?\n\nmy bad, this override is unnecessary, we can remove it", "url": "https://github.com/apache/hudi/pull/2176#discussion_r523425971", "createdAt": "2020-11-14T14:36:22Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                Option<EmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  @Override\n+  protected HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createIndex(HoodieWriteConfig writeConfig) {\n+    return FlinkHoodieIndex.createIndex((HoodieFlinkEngineContext) context, config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieFlinkTable<T> table = HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS).bootstrap(context, extraMetadata);\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MjMwOA=="}, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3ODYxOTQ1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MzoyNFrOHyvIcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1MzoyNFrOHyvIcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2MzA1OA==", "bodyText": "implement delete here?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522963058", "createdAt": "2020-11-13T13:53:24Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                Option<EmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  @Override\n+  protected HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createIndex(HoodieWriteConfig writeConfig) {\n+    return FlinkHoodieIndex.createIndex((HoodieFlinkEngineContext) context, config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieFlinkTable<T> table = HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS).bootstrap(context, extraMetadata);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.upsert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.INSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.INSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.insert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> bulkInsert(List<HoodieRecord<T>> records, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> bulkInsert(List<HoodieRecord<T>> records, String instantTime, Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> userDefinedBulkInsertPartitioner) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> bulkInsertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime, Option<BulkInsertPartitioner<List<HoodieRecord<T>>>> bulkInsertPartitioner) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> delete(List<HoodieKey> keys, String instantTime) {\n+    // TODO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3ODYyNjEzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1NToxN1rOHyvMmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNFQxNDo0MjoyMVrOHzLasg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2NDEyMg==", "bodyText": "should we implement some index here ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522964122", "createdAt": "2020-11-13T13:55:17Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.state.FlinkInMemoryStateIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.List;\n+\n+/**\n+ * Base flink implementation of {@link HoodieIndex}.\n+ * @param <T> payload type\n+ */\n+public abstract class FlinkHoodieIndex<T extends HoodieRecordPayload> extends HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+  protected FlinkHoodieIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  public static FlinkHoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    // first use index class config to create index.\n+    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n+      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n+      if (!(instance instanceof HoodieIndex)) {\n+        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n+      }\n+      return (FlinkHoodieIndex) instance;\n+    }\n+    switch (config.getIndexType()) {\n+      case HBASE:\n+        // TODO\n+        return null;\n+      case INMEMORY:\n+        return new FlinkInMemoryStateIndex<>(context, config);\n+      case BLOOM:\n+        // TODO\n+        return null;\n+      case GLOBAL_BLOOM:\n+        // TODO\n+        return null;\n+      case SIMPLE:\n+        // TODO\n+        return null;\n+      case GLOBAL_SIMPLE:\n+        // TODO\n+        return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzQyNjQ4Mg==", "bodyText": "should we implement some index here ?\n\nhow about another pr to add other indexes\uff1f This pr we can focus on basic functions", "url": "https://github.com/apache/hudi/pull/2176#discussion_r523426482", "createdAt": "2020-11-14T14:42:21Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.state.FlinkInMemoryStateIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.List;\n+\n+/**\n+ * Base flink implementation of {@link HoodieIndex}.\n+ * @param <T> payload type\n+ */\n+public abstract class FlinkHoodieIndex<T extends HoodieRecordPayload> extends HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+  protected FlinkHoodieIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  public static FlinkHoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    // first use index class config to create index.\n+    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n+      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n+      if (!(instance instanceof HoodieIndex)) {\n+        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n+      }\n+      return (FlinkHoodieIndex) instance;\n+    }\n+    switch (config.getIndexType()) {\n+      case HBASE:\n+        // TODO\n+        return null;\n+      case INMEMORY:\n+        return new FlinkInMemoryStateIndex<>(context, config);\n+      case BLOOM:\n+        // TODO\n+        return null;\n+      case GLOBAL_BLOOM:\n+        // TODO\n+        return null;\n+      case SIMPLE:\n+        // TODO\n+        return null;\n+      case GLOBAL_SIMPLE:\n+        // TODO\n+        return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2NDEyMg=="}, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3ODYyOTYyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1NjoxNlrOHyvO1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1NjoxNlrOHyvO1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2NDY5Mg==", "bodyText": "throw exception here ?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522964692", "createdAt": "2020-11-13T13:56:16Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(FlinkInMemoryStateIndex.class);\n+  private MapState<HoodieKey, HoodieRecordLocation> mapState;\n+\n+  public FlinkInMemoryStateIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    super(config);\n+    if (context.getRuntimeContext() != null) {\n+      MapStateDescriptor<HoodieKey, HoodieRecordLocation> indexStateDesc =\n+          new MapStateDescriptor<>(\"indexState\", TypeInformation.of(HoodieKey.class), TypeInformation.of(HoodieRecordLocation.class));\n+      if (context.getRuntimeContext() != null) {\n+        mapState = context.getRuntimeContext().getMapState(indexStateDesc);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records,\n+                                           HoodieEngineContext context,\n+                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) throws HoodieIndexException {\n+    return context.map(records, record -> {\n+      try {\n+        if (mapState.contains(record.getKey())) {\n+          record.unseal();\n+          record.setCurrentLocation(mapState.get(record.getKey()));\n+          record.seal();\n+        }\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"Tag record location failed, key = %s, %s\", record.getRecordKey(), e.getMessage()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3ODYzMTI4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1Njo0MlrOHyvP-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxMzo1Njo0MlrOHyvP-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjk2NDk4NQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2176#discussion_r522964985", "createdAt": "2020-11-13T13:56:42Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(FlinkInMemoryStateIndex.class);\n+  private MapState<HoodieKey, HoodieRecordLocation> mapState;\n+\n+  public FlinkInMemoryStateIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    super(config);\n+    if (context.getRuntimeContext() != null) {\n+      MapStateDescriptor<HoodieKey, HoodieRecordLocation> indexStateDesc =\n+          new MapStateDescriptor<>(\"indexState\", TypeInformation.of(HoodieKey.class), TypeInformation.of(HoodieRecordLocation.class));\n+      if (context.getRuntimeContext() != null) {\n+        mapState = context.getRuntimeContext().getMapState(indexStateDesc);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records,\n+                                           HoodieEngineContext context,\n+                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) throws HoodieIndexException {\n+    return context.map(records, record -> {\n+      try {\n+        if (mapState.contains(record.getKey())) {\n+          record.unseal();\n+          record.setCurrentLocation(mapState.get(record.getKey()));\n+          record.seal();\n+        }\n+      } catch (Exception e) {\n+        LOG.error(String.format(\"Tag record location failed, key = %s, %s\", record.getRecordKey(), e.getMessage()));\n+      }\n+      return record;\n+    }, 0);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> updateLocation(List<WriteStatus> writeStatuses,\n+                                          HoodieEngineContext context,\n+                                          HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) throws HoodieIndexException {\n+    return context.map(writeStatuses, writeStatus -> {\n+      for (HoodieRecord record : writeStatus.getWrittenRecords()) {\n+        if (!writeStatus.isErrored(record.getKey())) {\n+          HoodieKey key = record.getKey();\n+          Option<HoodieRecordLocation> newLocation = record.getNewLocation();\n+          if (newLocation.isPresent()) {\n+            try {\n+              mapState.put(key, newLocation.get());\n+            } catch (Exception e) {\n+              LOG.error(String.format(\"Update record location failed, key = %s, %s\", record.getRecordKey(), e.getMessage()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3OTg4MDA5OnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/constant/Operation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODo1OToyNVrOHy7VLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNTo0MTo1MlrOH0lJPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MjkyNA==", "bodyText": "can we reuse WriteOperationType, that already exists in hudi-common?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r523162924", "createdAt": "2020-11-13T18:59:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/constant/Operation.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.constant;\n+\n+public enum Operation {\n+  UPSERT, INSERT", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg5NjU3NQ==", "bodyText": "can we reuse WriteOperationType, that already exists in hudi-common?\n\nsure, will do tonight", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524896575", "createdAt": "2020-11-17T05:41:52Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/constant/Operation.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.constant;\n+\n+public enum Operation {\n+  UPSERT, INSERT", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MjkyNA=="}, "originalCommit": {"oid": "96e8b469c8601bc3a43c67957b628f6e5b5aa390"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDg5MzYwOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/constant/Operation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDoyMzowNVrOH0j3XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxMDo1MjoyOVrOH0vN0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NTYxMg==", "bodyText": "can we reuse the WriteOperationType enum?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524875612", "createdAt": "2020-11-17T04:23:05Z", "author": {"login": "vinothchandar"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/constant/Operation.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.constant;\n+\n+public enum Operation {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA2MTU4Nw==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2176#discussion_r525061587", "createdAt": "2020-11-17T10:52:29Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/constant/Operation.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.constant;\n+\n+public enum Operation {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NTYxMg=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDkwMTMzOnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDoyNzowMVrOH0j7mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNTo0MjozNVrOH0lKGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NjY5Ng==", "bodyText": "could we just call this hudi-flink instead of hudi-flink-writer. It gives us the change to also build the reading support into the same module/bundle?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524876696", "createdAt": "2020-11-17T04:27:01Z", "author": {"login": "vinothchandar"}, "path": "pom.xml", "diffHunk": "@@ -53,6 +53,8 @@\n     <module>hudi-integ-test</module>\n     <module>packaging/hudi-integ-test-bundle</module>\n     <module>hudi-examples</module>\n+    <module>hudi-flink-writer</module>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg5Njc5Mw==", "bodyText": "could we just call this hudi-flink instead of hudi-flink-writer. It gives us the change to also build the reading support into the same module/bundle?\n\nyes, good idea", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524896793", "createdAt": "2020-11-17T05:42:35Z", "author": {"login": "wangxianghu"}, "path": "pom.xml", "diffHunk": "@@ -53,6 +53,8 @@\n     <module>hudi-integ-test</module>\n     <module>packaging/hudi-integ-test-bundle</module>\n     <module>hudi-examples</module>\n+    <module>hudi-flink-writer</module>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NjY5Ng=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDkwMzQxOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDoyODoxNlrOH0j8xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDoyODoxNlrOH0j8xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3Njk5Ng==", "bodyText": "In the future, I guess we can do more refactoring and make stuff work end-end with deltastreamer as well . We can provide a migration path for users then. For now, this is a solid approach to get stuff working end to end.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524876996", "createdAt": "2020-11-17T04:28:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.KeyedWriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.JsonStringToHoodieRecordMapFunction;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Properties;\n+\n+/**\n+ * An Utility which can incrementally consume data from Kafka and apply it to the target table.\n+ * currently, it only support MOR table and insert, upsert operation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDkwNDEzOnYy", "diffSide": "RIGHT", "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDoyODo0MlrOH0j9PQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNTo0NToxNFrOH0lNCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NzExNw==", "bodyText": "can we make sure the program throws errors if using COW etc.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524877117", "createdAt": "2020-11-17T04:28:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.KeyedWriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.JsonStringToHoodieRecordMapFunction;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Properties;\n+\n+/**\n+ * An Utility which can incrementally consume data from Kafka and apply it to the target table.\n+ * currently, it only support MOR table and insert, upsert operation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg5NzU0NA==", "bodyText": "can we make sure the program throws errors if using COW etc.\n\nMy bad. this pr support COW only. will support MOR in another pr", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524897544", "createdAt": "2020-11-17T05:45:14Z", "author": {"login": "wangxianghu"}, "path": "hudi-flink-writer/src/main/java/org/apache/hudi/HudiFlinkStreamer.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import com.beust.jcommander.IStringConverter;\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.ParameterException;\n+import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.typeinfo.TypeHint;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.tuple.Tuple3;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;\n+import org.apache.hudi.constant.Operation;\n+import org.apache.hudi.operator.InstantGenerateOperator;\n+import org.apache.hudi.operator.KeyedWriteProcessFunction;\n+import org.apache.hudi.operator.KeyedWriteProcessOperator;\n+import org.apache.hudi.sink.CommitSink;\n+import org.apache.hudi.source.JsonStringToHoodieRecordMapFunction;\n+import org.apache.hudi.util.StreamerUtil;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.Properties;\n+\n+/**\n+ * An Utility which can incrementally consume data from Kafka and apply it to the target table.\n+ * currently, it only support MOR table and insert, upsert operation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3NzExNw=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDkxMDgxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDozMjowM1rOH0kA1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwOToyOToxNlrOH0r2jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3ODAzOA==", "bodyText": "throw errors here, instead of returning null?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524878038", "createdAt": "2020-11-17T04:32:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.state.FlinkInMemoryStateIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.List;\n+\n+/**\n+ * Base flink implementation of {@link HoodieIndex}.\n+ * @param <T> payload type\n+ */\n+public abstract class FlinkHoodieIndex<T extends HoodieRecordPayload> extends HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+  protected FlinkHoodieIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  public static FlinkHoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    // first use index class config to create index.\n+    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n+      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n+      if (!(instance instanceof HoodieIndex)) {\n+        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n+      }\n+      return (FlinkHoodieIndex) instance;\n+    }\n+    switch (config.getIndexType()) {\n+      case HBASE:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTAwNjQ3Ng==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/2176#discussion_r525006476", "createdAt": "2020-11-17T09:29:16Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/FlinkHoodieIndex.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.ApiMaturityLevel;\n+import org.apache.hudi.PublicAPIMethod;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.state.FlinkInMemoryStateIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.List;\n+\n+/**\n+ * Base flink implementation of {@link HoodieIndex}.\n+ * @param <T> payload type\n+ */\n+public abstract class FlinkHoodieIndex<T extends HoodieRecordPayload> extends HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+  protected FlinkHoodieIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  public static FlinkHoodieIndex createIndex(HoodieFlinkEngineContext context, HoodieWriteConfig config) {\n+    // first use index class config to create index.\n+    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n+      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n+      if (!(instance instanceof HoodieIndex)) {\n+        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n+      }\n+      return (FlinkHoodieIndex) instance;\n+    }\n+    switch (config.getIndexType()) {\n+      case HBASE:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3ODAzOA=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDkxNDMyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDozNDoyMVrOH0kC9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMToxNDo0MVrOH1R9lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3ODU4Mg==", "bodyText": "is this always in memory? or can it be rocksDB backed as well?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524878582", "createdAt": "2020-11-17T04:34:21Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg5OTY1OQ==", "bodyText": "is this always in memory? or can it be rocksDB backed as well?\n\nFor now, if we configured  --flink-checkpoint-path,  the index data will be periodically persisted to HDFS.\nRockDB backend has some issues now, I'll fix it.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524899659", "createdAt": "2020-11-17T05:52:18Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3ODU4Mg=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5ODUwOA==", "bodyText": "@wangxianghu sounds good. please file a JIRA if there is not one already.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r525598508", "createdAt": "2020-11-17T23:41:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3ODU4Mg=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMDg2OA==", "bodyText": "@wangxianghu sounds good. please file a JIRA if there is not one already.\n\nfiled here : https://issues.apache.org/jira/browse/HUDI-981", "url": "https://github.com/apache/hudi/pull/2176#discussion_r525630868", "createdAt": "2020-11-18T01:14:41Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/state/FlinkInMemoryStateIndex.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.state;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.flink.api.common.state.MapState;\n+import org.apache.flink.api.common.state.MapStateDescriptor;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+\n+/**\n+ * Hoodie index implementation backed by flink state.\n+ *\n+ * @param <T> type of payload\n+ */\n+public class FlinkInMemoryStateIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3ODU4Mg=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDkxODc3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDozNjo0MFrOH0kFWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMToxMjoxOVrOH1R6ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3OTE5Mg==", "bodyText": "unused?", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524879192", "createdAt": "2020-11-17T04:36:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYzMDA3NQ==", "bodyText": "used now :)", "url": "https://github.com/apache/hudi/pull/2176#discussion_r525630075", "createdAt": "2020-11-18T01:12:19Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3OTE5Mg=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDkyMDA2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNDozNzoyMlrOH0kGDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNTo0ODo1OVrOH0lRHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3OTM3NQ==", "bodyText": "same comment. can we throw an Unsupported exception here instead of returning null.", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524879375", "createdAt": "2020-11-17T04:37:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                Option<EmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  @Override\n+  protected HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createIndex(HoodieWriteConfig writeConfig) {\n+    return FlinkHoodieIndex.createIndex((HoodieFlinkEngineContext) context, config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieFlinkTable<T> table = HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.upsert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.INSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.INSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.insert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg5ODU5MQ==", "bodyText": "same comment. can we throw an Unsupported exception here instead of returning null.\n\nsure, I'll fix them  so as the index", "url": "https://github.com/apache/hudi/pull/2176#discussion_r524898591", "createdAt": "2020-11-17T05:48:59Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/client/HoodieFlinkWriteClient.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieFlinkEngineContext;\n+import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.upgrade.FlinkUpgradeDowngrade;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class HoodieFlinkWriteClient<T extends HoodieRecordPayload> extends\n+    AbstractHoodieWriteClient<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> {\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieFlinkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                Option<EmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Complete changes performed at the given instantTime marker with specified action.\n+   */\n+  @Override\n+  protected HoodieIndex<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createIndex(HoodieWriteConfig writeConfig) {\n+    return FlinkHoodieIndex.createIndex((HoodieFlinkEngineContext) context, config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, List<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata, String commitActionType, Map<String, List<String>> partitionToReplacedFileIds) {\n+    List<HoodieWriteStat> writeStats = writeStatuses.parallelStream().map(WriteStatus::getStat).collect(Collectors.toList());\n+    return commitStats(instantTime, writeStats, extraMetadata, commitActionType, partitionToReplacedFileIds);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> filterExists(List<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieFlinkTable<T> table = HoodieFlinkTable.create(config, (HoodieFlinkEngineContext) context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    List<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.stream().filter(v1 -> !v1.isCurrentLocationKnown()).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.upsert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> upsertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insert(List<HoodieRecord<T>> records, String instantTime) {\n+    HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> table =\n+        getTableAndInitCtx(WriteOperationType.INSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.INSERT);\n+    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<List<WriteStatus>> result = table.insert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public List<WriteStatus> insertPreppedRecords(List<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    // TODO\n+    return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg3OTM3NQ=="}, "originalCommit": {"oid": "b63e126c245ed063359c005dd7c1d2f79f9881f2"}, "originalPosition": 135}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4093, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}