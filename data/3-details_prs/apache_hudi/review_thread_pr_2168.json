{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxMDc1MTA3", "number": 2168, "reviewThreads": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMVQwNDoxNDoxNlrOEsV6EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTo1NToxN1rOFCnWAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0OTMxNzI4OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/DelayNode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMVQwNDoxNDoxNlrOHfkURw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMVQwNDoxNDoxNlrOHfkURw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg2MjkxOQ==", "bodyText": "as of now, I have kept it simple with thread.sleep. but open to ideas on how to add delays. Initially I thought we could schedule testsuitejob every time, but haven't thought through completely on whether we need a single test suite job to run and manage delays on its own or can we initiate a test suite job everytime from cron job kind of thing.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r502862919", "createdAt": "2020-10-11T04:14:16Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/DelayNode.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Delay Node to add delays between each group of test runs.\n+ */\n+public class DelayNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+  private int delayMins;\n+\n+  public DelayNode(int delayMins) {\n+    this.delayMins = delayMins;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+    log.warn(\"Waiting for \"+ delayMins+\" mins before going for next test run\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7ba49cc8652da0c77da6af506af715740c127d5"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDYxOTE0OnYy", "diffSide": "RIGHT", "path": "docker/demo/config/test-suite/cow-validate-cumulative-multiple-rounds.yaml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToxODozNVrOHwB_MA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToxODozNVrOHwB_MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEyNjI1Ng==", "bodyText": "@nsivabalan why do we need 2 same yaml dags one for once and one for multiple ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520126256", "createdAt": "2020-11-09T21:18:35Z", "author": {"login": "n3nash"}, "path": "docker/demo/config/test-suite/cow-validate-cumulative-multiple-rounds.yaml", "diffHunk": "@@ -0,0 +1,59 @@\n+# Licensed to the Apache Software Foundation (ASF) under one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDYyNTc2OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToyMDoyNFrOHwCDBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToyMDoyNFrOHwCDBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEyNzIzNg==", "bodyText": "spark sql engine works with hive 1.x ? Have we tested it out ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520127236", "createdAt": "2020-11-09T21:20:24Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/README.md", "diffHunk": "@@ -267,3 +269,170 @@ spark-submit \\\n --table-type MERGE_ON_READ \\\n --compact-scheduling-minshare 1\n ``` \n+\n+For long running test suite, validation has to be done differently. Idea is to run same dag in a repeated manner. \n+Hence \"ValidateDatasetNode\" is introduced which will read entire input data and compare it with hudi contents both via \n+spark datasource and hive table via spark sql engine.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDYyODkwOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToyMToyN1rOHwCE2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToyMToyN1rOHwCE2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEyNzcwNQ==", "bodyText": "What happens if the execution of one round takes more than 10 mins ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520127705", "createdAt": "2020-11-09T21:21:27Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/README.md", "diffHunk": "@@ -267,3 +269,170 @@ spark-submit \\\n --table-type MERGE_ON_READ \\\n --compact-scheduling-minshare 1\n ``` \n+\n+For long running test suite, validation has to be done differently. Idea is to run same dag in a repeated manner. \n+Hence \"ValidateDatasetNode\" is introduced which will read entire input data and compare it with hudi contents both via \n+spark datasource and hive table via spark sql engine.\n+\n+If you have \"ValidateDatasetNode\" in your dag, do not replace hive jars as instructed above. Spark sql engine does not \n+go well w/ hive2* jars. So, after running docker setup, just copy test.properties and your dag of interest and you are \n+good to go ahead. \n+\n+For repeated runs, two additional configs need to be set. \"--num-rounds N\" and \"--delay-between-rounds-mins Y\". \n+This means that your dag will be repeated for N times w/ a delay of Y mins between each round.\n+\n+Also, ValidateDatasetNode can be configured in two ways. Either with \"delete_input_data: true\" set or not set. \n+When \"delete_input_data\" is set for ValidateDatasetNode, once validation is complete, entire input data will be deleted. \n+So, suggestion is to use this ValidateDatasetNode as the last node in the dag with \"delete_input_data\". \n+Example dag: \n+```\n+     Insert\n+     Upsert\n+     ValidateDatasetNode with delete_input_data = true\n+```\n+\n+If above dag is run with \"--num-rounds 10 --delay-between-rounds-mins 10\", then this dag will run for 10 times with 10 ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDY2NjA3OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMTozMjowNFrOHwCbPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMTozMjowNFrOHwCbPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEzMzQzNg==", "bodyText": "@nsivabalan\nCan we avoid introducing these high level arguments numRound and delayMins to the dagScheduler ? This messes with a simpler design and adds unnecessary overloading to the constructor.\nInstead, we could do something like this\nIntroduce a top level yaml change around this\ndag_name:\ndag_rounds:\ndag_intermittent_delay:\ndag_props:\ndag_content:\ninsert_node:\n...\nupsert_node:\n...\nYou can then introduce a concept called RuntimeComposableDagNode that basically wraps this concept.\nLet me know.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520133436", "createdAt": "2020-11-09T21:32:04Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/HoodieTestSuiteJob.java", "diffHunk": "@@ -147,7 +147,7 @@ public void runTestSuite() {\n       long startTime = System.currentTimeMillis();\n       WriterContext writerContext = new WriterContext(jsc, props, cfg, keyGenerator, sparkSession);\n       writerContext.initContext(jsc);\n-      DagScheduler dagScheduler = new DagScheduler(workflowDag, writerContext);\n+      DagScheduler dagScheduler = new DagScheduler(workflowDag, writerContext, jsc, cfg.numRounds, cfg.delayMins);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDY2OTM3OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/converter/UpdateConverter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMTozMzowN1rOHwCdWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMTozMzowN1rOHwCdWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEzMzk3Nw==", "bodyText": "Can you explain why we need to introduce these 2 fields ? Also, please comment why the preCombineFieldValue is \"int\" ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520133977", "createdAt": "2020-11-09T21:33:07Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/converter/UpdateConverter.java", "diffHunk": "@@ -35,19 +37,23 @@\n   private final List<String> partitionPathFields;\n   private final List<String> recordKeyFields;\n   private final int minPayloadSize;\n+  private final String preCombineField;\n+  private final int preCombineFieldValue;\n \n-  public UpdateConverter(String schemaStr, int minPayloadSize, List<String> partitionPathFields,\n-      List<String> recordKeyFields) {\n+  public  UpdateConverter(String schemaStr, int minPayloadSize, List<String> partitionPathFields,\n+      List<String> recordKeyFields, String preCombineField, int preCombineFieldValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDg0MzAyOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNzowMlrOHwEGiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNzowMlrOHwEGiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MDkwNA==", "bodyText": "This is problematic since we cannot test other types of partitioning. It's OK for now, can you add a TODO here ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520160904", "createdAt": "2020-11-09T22:27:02Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDg0NDI0OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNzoyM1rOHwEHRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNzoyM1rOHwEHRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MTA5Mw==", "bodyText": "There should be a config for inputBasePath ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520161093", "createdAt": "2020-11-09T22:27:23Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/../input/\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDg0NjI2OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyODowNVrOHwEIlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyODowNVrOHwEIlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MTQyOQ==", "bodyText": "Can you make this \"TODO\" ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520161429", "createdAt": "2020-11-09T22:28:05Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/../input/\";\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    // fix hard coded fields from configs.\n+    // read input and resolve insert, updates, etc.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDg1MDA4OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyOTozMFrOHwELCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyOTozMFrOHwELCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MjA1Nw==", "bodyText": "Can we take the database names and table names from config ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520162057", "createdAt": "2020-11-09T22:29:30Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/../input/\";\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    // fix hard coded fields from configs.\n+    // read input and resolve insert, updates, etc.\n+    Dataset<Row> inputDf = session.read().format(\"avro\").load(inputPath);\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+    Dataset<Row> inputSnapshotDf = inputDf.groupByKey(\n+        (MapFunction<Row, String>) value -> value.getAs(\"timestamp\") + \"+\" + value.getAs(\"_row_key\"), Encoders.STRING())\n+        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> {\n+          long ts1 = v1.getAs(\"ts\");\n+          long ts2 = v2.getAs(\"ts\");\n+          if (ts1 > ts2) {\n+            return v1;\n+          } else {\n+            return v2;\n+          }\n+        })\n+        .map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n+\n+    // read from hudi and remove meta columns.\n+    Dataset<Row> hudiDf = session.read().format(\"hudi\").load(hudiPath);\n+    Dataset<Row> trimmedDf = hudiDf.drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n+        .drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.FILENAME_METADATA_FIELD);\n+\n+    Dataset<Row> intersectionDf = inputSnapshotDf.intersect(trimmedDf);\n+    // the intersected df should be same as inputDf. if not, there is some mismatch.\n+    if (inputSnapshotDf.except(intersectionDf).count() != 0) {\n+      log.error(\"Data set validation failed. Total count in hudi \" + trimmedDf.count() + \", input df count \" + inputSnapshotDf.count());\n+      throw new AssertionError(\"Hudi contents does not match contents input data. \");\n+    } \n+\n+    log.warn(\"Validating hive table \");\n+    Dataset<Row> cowDf = session.sql(\"SELECT * FROM testdb.table1\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDg1MDU3OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyOTo0MVrOHwELUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyOTo0MVrOHwELUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MjEzMQ==", "bodyText": "Same comment here", "url": "https://github.com/apache/hudi/pull/2168#discussion_r520162131", "createdAt": "2020-11-09T22:29:41Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/../input/\";\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    // fix hard coded fields from configs.\n+    // read input and resolve insert, updates, etc.\n+    Dataset<Row> inputDf = session.read().format(\"avro\").load(inputPath);\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+    Dataset<Row> inputSnapshotDf = inputDf.groupByKey(\n+        (MapFunction<Row, String>) value -> value.getAs(\"timestamp\") + \"+\" + value.getAs(\"_row_key\"), Encoders.STRING())\n+        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> {\n+          long ts1 = v1.getAs(\"ts\");\n+          long ts2 = v2.getAs(\"ts\");\n+          if (ts1 > ts2) {\n+            return v1;\n+          } else {\n+            return v2;\n+          }\n+        })\n+        .map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n+\n+    // read from hudi and remove meta columns.\n+    Dataset<Row> hudiDf = session.read().format(\"hudi\").load(hudiPath);\n+    Dataset<Row> trimmedDf = hudiDf.drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n+        .drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.FILENAME_METADATA_FIELD);\n+\n+    Dataset<Row> intersectionDf = inputSnapshotDf.intersect(trimmedDf);\n+    // the intersected df should be same as inputDf. if not, there is some mismatch.\n+    if (inputSnapshotDf.except(intersectionDf).count() != 0) {\n+      log.error(\"Data set validation failed. Total count in hudi \" + trimmedDf.count() + \", input df count \" + inputSnapshotDf.count());\n+      throw new AssertionError(\"Hudi contents does not match contents input data. \");\n+    } \n+\n+    log.warn(\"Validating hive table \");\n+    Dataset<Row> cowDf = session.sql(\"SELECT * FROM testdb.table1\");\n+    Dataset<Row> trimmedCowDf = cowDf.drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n+        .drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.FILENAME_METADATA_FIELD);\n+    intersectionDf = inputSnapshotDf.intersect(trimmedDf);\n+    // the intersected df should be same as inputDf. if not, there is some mismatch.\n+    if (inputSnapshotDf.except(intersectionDf).count() != 0) {\n+      log.error(\"Data set validation failed for COW hive table. Total count in hudi \" + trimmedCowDf.count() + \", input df count \" + inputSnapshotDf.count());\n+      throw new AssertionError(\"Hudi hive table contents does not match contents input data. \");\n+    }\n+\n+    // if delete input data is enabled, erase input data.\n+    if (config.isDeleteInputData()) {\n+      // clean up input data for current group of writes.\n+      inputPathStr = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/../input/\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8feb5a0995ba602ca5042a0fb7ed0f68dc251f4b"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMxNzE1OnYy", "diffSide": "RIGHT", "path": "docker/demo/config/test-suite/cow-per-round-mixed-validate.yaml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTozNjoxMFrOH3w26Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTozNjoxMFrOH3w26Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNDIxNw==", "bodyText": "I haven't fixed these dags yet for long running test-suite. Once we have consensus that things are looking good, I will update all dags. Do check unit test dags under resources dir in hudi-integ-test module for updated version.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528234217", "createdAt": "2020-11-21T19:36:10Z", "author": {"login": "nsivabalan"}, "path": "docker/demo/config/test-suite/cow-per-round-mixed-validate.yaml", "diffHunk": "@@ -0,0 +1,65 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+first_insert:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMxODMwOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/README.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTozNzozMVrOH3w3eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTozNzozMVrOH3w3eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNDM2MA==", "bodyText": "as per our discussion, I have added this new field from within the code. So essentially this field will be appended to both source schema and target schema and will be used for source ordering. batch_id is used as values.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528234360", "createdAt": "2020-11-21T19:37:31Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/README.md", "diffHunk": "@@ -214,7 +216,7 @@ spark-submit \\\n --conf spark.sql.catalogImplementation=hive \\\n --class org.apache.hudi.integ.testsuite.HoodieTestSuiteJob \\\n /opt/hudi-integ-test-bundle-0.6.1-SNAPSHOT.jar \\\n---source-ordering-field timestamp \\\n+--source-ordering-field test_suite_source_ordering_field \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMxODgwOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/DagUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTozODoxOVrOH3w3sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTozODoxOVrOH3w3sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNDQxOA==", "bodyText": "high level config param parsing.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528234418", "createdAt": "2020-11-21T19:38:19Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/DagUtils.java", "diffHunk": "@@ -62,15 +71,38 @@ public static WorkflowDag convertYamlPathToDag(FileSystem fs, String path) throw\n    * Converts a YAML representation to {@link WorkflowDag}.\n    */\n   public static WorkflowDag convertYamlToDag(String yaml) throws IOException {\n+    int dagRounds = DEFAULT_DAG_ROUNDS;\n+    int intermittentDelayMins = DEFAULT_INTERMITTENT_DELAY_MINS;\n+    String dagName = DEFAULT_DAG_NAME;\n     Map<String, DagNode> allNodes = new HashMap<>();\n     final ObjectMapper yamlReader = new ObjectMapper(new YAMLFactory());\n     final JsonNode jsonNode = yamlReader.readTree(yaml);\n     Iterator<Entry<String, JsonNode>> itr = jsonNode.fields();\n     while (itr.hasNext()) {\n       Entry<String, JsonNode> dagNode = itr.next();\n-      allNodes.put(dagNode.getKey(), convertJsonToDagNode(allNodes, dagNode.getKey(), dagNode.getValue()));\n+      String key = dagNode.getKey();\n+      switch (key) {\n+        case DAG_NAME:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMxOTEwOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/DagUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTozODo0NVrOH3w32A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTozODo0NVrOH3w32A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNDQ1Ng==", "bodyText": "as agreed, all node contents go into \"dag_content\"", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528234456", "createdAt": "2020-11-21T19:38:45Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/DagUtils.java", "diffHunk": "@@ -62,15 +71,38 @@ public static WorkflowDag convertYamlPathToDag(FileSystem fs, String path) throw\n    * Converts a YAML representation to {@link WorkflowDag}.\n    */\n   public static WorkflowDag convertYamlToDag(String yaml) throws IOException {\n+    int dagRounds = DEFAULT_DAG_ROUNDS;\n+    int intermittentDelayMins = DEFAULT_INTERMITTENT_DELAY_MINS;\n+    String dagName = DEFAULT_DAG_NAME;\n     Map<String, DagNode> allNodes = new HashMap<>();\n     final ObjectMapper yamlReader = new ObjectMapper(new YAMLFactory());\n     final JsonNode jsonNode = yamlReader.readTree(yaml);\n     Iterator<Entry<String, JsonNode>> itr = jsonNode.fields();\n     while (itr.hasNext()) {\n       Entry<String, JsonNode> dagNode = itr.next();\n-      allNodes.put(dagNode.getKey(), convertJsonToDagNode(allNodes, dagNode.getKey(), dagNode.getValue()));\n+      String key = dagNode.getKey();\n+      switch (key) {\n+        case DAG_NAME:\n+          dagName = dagNode.getValue().asText();\n+          break;\n+        case DAG_ROUNDS:\n+          dagRounds = dagNode.getValue().asInt();\n+          break;\n+        case DAG_INTERMITTENT_DELAY_MINS:\n+          intermittentDelayMins = dagNode.getValue().asInt();\n+          break;\n+        case DAG_CONTENT:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMyMDY1OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0MDoxNFrOH3w4hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0MDoxNFrOH3w4hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNDYzMA==", "bodyText": "may I know where or how to fetch the record key and partition path field. I don't see any thing exposed in deltastreamer config. as of now, I am hardcoding it here.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528234630", "createdAt": "2020-11-21T19:40:14Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+import org.apache.hudi.integ.testsuite.schema.SchemaUtils;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    // todo: Fix partitioning schemes. For now, assumes data based partitioning.\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().inputBasePath;\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    // todo: fix hard coded fields from configs.\n+    // read input and resolve insert, updates, etc.\n+    Dataset<Row> inputDf = session.read().format(\"avro\").load(inputPath);\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+    Dataset<Row> inputSnapshotDf = inputDf.groupByKey(\n+        (MapFunction<Row, String>) value -> value.getAs(\"timestamp\") + \"+\" + value.getAs(\"_row_key\"), Encoders.STRING())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMyMTg0OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/scheduler/DagScheduler.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0MTozOFrOH3w5CQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0MTozOFrOH3w5CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNDc2MQ==", "bodyText": "I didn't make a whole lot of change from last time. I exposed the global params in workflowDag and using it wherever required. like rounds, delay etc. Felt this is simple and achieves our goal.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528234761", "createdAt": "2020-11-21T19:41:38Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/scheduler/DagScheduler.java", "diffHunk": "@@ -77,33 +79,47 @@ public void schedule() throws Exception {\n    * Method to start executing the nodes in workflow DAGs.\n    *\n    * @param service ExecutorService\n-   * @param nodes Nodes to be executed\n+   * @param workflowDag instance of workflow dag that needs to be executed\n    * @throws Exception will be thrown if ant error occurred\n    */\n-  private void execute(ExecutorService service, List<DagNode> nodes) throws Exception {\n+  private void execute(ExecutorService service, WorkflowDag workflowDag) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMyMzU4OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/schema/TestSuiteFileBasedSchemaProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0NDoyMFrOH3w50A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0NDoyMFrOH3w50A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNDk2MA==", "bodyText": "Added this schemaProvider to append the source ordering field to source and target schema.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528234960", "createdAt": "2020-11-21T19:44:20Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/schema/TestSuiteFileBasedSchemaProvider.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.schema;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.integ.testsuite.dag.WriterContext;\n+import org.apache.hudi.utilities.schema.FilebasedSchemaProvider;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class TestSuiteFileBasedSchemaProvider extends FilebasedSchemaProvider {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMyNDE0OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/writer/DFSDeltaWriterAdapter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0NDo0NFrOH3w6DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0NDo0NFrOH3w6DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNTAyMA==", "bodyText": "this is where the source ordering field is set.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528235020", "createdAt": "2020-11-21T19:44:44Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/writer/DFSDeltaWriterAdapter.java", "diffHunk": "@@ -30,22 +32,29 @@\n  */\n public class DFSDeltaWriterAdapter implements DeltaWriterAdapter<GenericRecord> {\n \n-  private DeltaInputWriter deltaInputGenerator;\n+  private DeltaInputWriter deltaInputWriter;\n   private List<DeltaWriteStats> metrics = new ArrayList<>();\n+  private int preCombineFieldVal = 0;\n+\n+  public DFSDeltaWriterAdapter(DeltaInputWriter<GenericRecord> deltaInputWriter, int preCombineFieldVal) {\n+    this.deltaInputWriter = deltaInputWriter;\n+    this.preCombineFieldVal = preCombineFieldVal;\n+  }\n \n-  public DFSDeltaWriterAdapter(DeltaInputWriter<GenericRecord> deltaInputGenerator) {\n-    this.deltaInputGenerator = deltaInputGenerator;\n+  public DFSDeltaWriterAdapter(DeltaInputWriter<GenericRecord> deltaInputWriter) {\n+    this.deltaInputWriter = deltaInputWriter;\n   }\n \n   @Override\n   public List<DeltaWriteStats> write(Iterator<GenericRecord> input) throws IOException {\n     while (input.hasNext()) {\n       GenericRecord next = input.next();\n-      if (this.deltaInputGenerator.canWrite()) {\n-        this.deltaInputGenerator.writeData(next);\n-      } else if (input.hasNext()) {\n+      next.put(SchemaUtils.SOURCE_ORDERING_FIELD, preCombineFieldVal);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMyNTA3OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/resources/unit-test-cow-dag.yaml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0NTozOFrOH3w6eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0NTozOFrOH3w6eA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNTEyOA==", "bodyText": "Here is the example of how the dag looks like w/ global params. I have tested this dag for few rounds of validation.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528235128", "createdAt": "2020-11-21T19:45:38Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/test/resources/unit-test-cow-dag.yaml", "diffHunk": "@@ -13,58 +13,62 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-first_insert:\n-  config:\n-    record_size: 70000\n-    num_partitions_insert: 1\n-    repeat_count: 2\n-    num_records_insert: 100\n-  type: InsertNode\n-  deps: none\n-second_insert:\n-  config:\n-    record_size: 70000\n-    num_partitions_insert: 1\n-    repeat_count: 1\n-    num_records_insert: 100\n-  type: InsertNode\n-  deps: first_insert\n-first_rollback:\n-  config:\n-  deps: second_insert\n-  type: RollbackNode\n-third_insert:\n-  config:\n-    record_size: 70000\n-    num_partitions_insert: 1\n-    repeat_count: 1\n-    num_records_insert: 100\n-  type: InsertNode\n-  deps: first_rollback\n-first_upsert:\n-  config:\n-    record_size: 70000\n-    num_partitions_upsert: 1\n-    repeat_count: 1\n-    num_records_upsert: 100\n-  type: UpsertNode\n-  deps: third_insert\n-first_hive_sync:\n-  config:\n-    queue_name: \"adhoc\"\n-    engine: \"mr\"\n-  type: HiveSyncNode\n-  deps: first_upsert\n-first_hive_query:\n-  config:\n-    hive_props:\n-      prop2: \"set spark.yarn.queue=\"\n-      prop3: \"set hive.strict.checks.large.query=false\"\n-      prop4: \"set hive.stats.autogather=false\"\n-    hive_queries:\n-      query1: \"select count(*) from testdb1.table1\"\n-      result1: 300\n-      query2: \"select count(*) from testdb1.table1 group   by `_row_key` having count(*) > 1\"\n-      result2: 0\n-  type: HiveQueryNode\n-  deps: first_hive_sync\n\\ No newline at end of file\n+dag_name: unit-test-cow-dag", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMyNTY0OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/FilebasedSchemaProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0NjoyNVrOH3w6wA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxOTo0NjoyNVrOH3w6wA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNTIwMA==", "bodyText": "since I am introducing a test schema provider to append source ordering field, I had to make this protected since the test schema provider extends from this.", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528235200", "createdAt": "2020-11-21T19:46:25Z", "author": {"login": "nsivabalan"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/schema/FilebasedSchemaProvider.java", "diffHunk": "@@ -46,9 +46,9 @@\n \n   private final FileSystem fs;\n \n-  private final Schema sourceSchema;\n+  protected Schema sourceSchema;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f91cb6a43c1d6c98d50715fdfef5e9696614033d"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMzNzg3OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQyMDowMDozMFrOH3xAPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDo0Mzo0NVrOIDcfnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNjYwNQ==", "bodyText": "is this the right way to fetch record key and partition path field ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528236605", "createdAt": "2020-11-21T20:00:30Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+import org.apache.hudi.integ.testsuite.schema.SchemaUtils;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    // todo: Fix partitioning schemes. For now, assumes data based partitioning.\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().inputBasePath;\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    String recordKeyField = context.getWriterContext().getProps().getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb66e344ded7f737da619f08f23efce9e3409fa0"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgzMjcyOA==", "bodyText": "@nsivabalan This will just fetch the KEY names, I'm guessing you need the values for these keys ?", "url": "https://github.com/apache/hudi/pull/2168#discussion_r538832728", "createdAt": "2020-12-08T21:49:13Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+import org.apache.hudi.integ.testsuite.schema.SchemaUtils;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    // todo: Fix partitioning schemes. For now, assumes data based partitioning.\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().inputBasePath;\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    String recordKeyField = context.getWriterContext().getProps().getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNjYwNQ=="}, "originalCommit": {"oid": "bb66e344ded7f737da619f08f23efce9e3409fa0"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ4MzQ4Nw==", "bodyText": "nope. just the record key and partition path field names. we need to group by Hoodiekey in df\ninputDf.groupByKey(\n        (MapFunction<Row, String>) value -> partitionPathField + \"+\" + recordKeyField, Encoders.STRING())", "url": "https://github.com/apache/hudi/pull/2168#discussion_r540483487", "createdAt": "2020-12-10T20:43:45Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+import org.apache.hudi.integ.testsuite.schema.SchemaUtils;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    // todo: Fix partitioning schemes. For now, assumes data based partitioning.\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().inputBasePath;\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    String recordKeyField = context.getWriterContext().getProps().getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNjYwNQ=="}, "originalCommit": {"oid": "bb66e344ded7f737da619f08f23efce9e3409fa0"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMjMzODIxOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQyMDowMTowMVrOH3xAZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTo1MToxNFrOIB30Kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNjY0Ng==", "bodyText": "again, couldn't find a better way to fetch db and table name. LMK if there are other better ways", "url": "https://github.com/apache/hudi/pull/2168#discussion_r528236646", "createdAt": "2020-11-21T20:01:01Z", "author": {"login": "nsivabalan"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+import org.apache.hudi.integ.testsuite.schema.SchemaUtils;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    // todo: Fix partitioning schemes. For now, assumes data based partitioning.\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().inputBasePath;\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    String recordKeyField = context.getWriterContext().getProps().getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY());\n+    String partitionPathField = context.getWriterContext().getProps().getString(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY());\n+    // todo: fix hard coded fields from configs.\n+    // read input and resolve insert, updates, etc.\n+    Dataset<Row> inputDf = session.read().format(\"avro\").load(inputPath);\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+    Dataset<Row> inputSnapshotDf = inputDf.groupByKey(\n+        (MapFunction<Row, String>) value -> partitionPathField + \"+\" + recordKeyField, Encoders.STRING())\n+        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> {\n+          int ts1 = v1.getAs(SchemaUtils.SOURCE_ORDERING_FIELD);\n+          int ts2 = v2.getAs(SchemaUtils.SOURCE_ORDERING_FIELD);\n+          if (ts1 > ts2) {\n+            return v1;\n+          } else {\n+            return v2;\n+          }\n+        })\n+        .map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n+\n+    // read from hudi and remove meta columns.\n+    Dataset<Row> hudiDf = session.read().format(\"hudi\").load(hudiPath);\n+    Dataset<Row> trimmedDf = hudiDf.drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n+        .drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.FILENAME_METADATA_FIELD);\n+\n+    Dataset<Row> intersectionDf = inputSnapshotDf.intersect(trimmedDf);\n+    // the intersected df should be same as inputDf. if not, there is some mismatch.\n+    if (inputSnapshotDf.except(intersectionDf).count() != 0) {\n+      log.error(\"Data set validation failed. Total count in hudi \" + trimmedDf.count() + \", input df count \" + inputSnapshotDf.count());\n+      throw new AssertionError(\"Hudi contents does not match contents input data. \");\n+    }\n+\n+    String database = context.getWriterContext().getProps().getString(\"hoodie.datasource.hive_sync.database\");\n+    String tableName = context.getWriterContext().getProps().getString(\"hoodie.datasource.hive_sync.table\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb66e344ded7f737da619f08f23efce9e3409fa0"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgzMzk2Mg==", "bodyText": "Can you use this config instead of the hard coding -> https://github.com/apache/hudi/blob/master/hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala#L298 ? Same for the second one as well", "url": "https://github.com/apache/hudi/pull/2168#discussion_r538833962", "createdAt": "2020-12-08T21:51:14Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/nodes/ValidateDatasetNode.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.testsuite.dag.nodes;\n+\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.integ.testsuite.configuration.DeltaConfig.Config;\n+import org.apache.hudi.integ.testsuite.dag.ExecutionContext;\n+import org.apache.hudi.integ.testsuite.schema.SchemaUtils;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.ReduceFunction;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.types.StructType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This nodes validates contents from input path are in tact with Hudi. This nodes uses spark datasource for comparison purposes. By default no configs are required for this node. But there is an\n+ * optional config \"delete_input_data\" that you can set for this node. If set, once validation completes, contents from inputPath are deleted. This will come in handy for long running test suites.\n+ * README has more details under docker set up for usages of this node.\n+ */\n+public class ValidateDatasetNode extends DagNode<Boolean> {\n+\n+  private static Logger log = LoggerFactory.getLogger(ValidateDatasetNode.class);\n+\n+  public ValidateDatasetNode(Config config) {\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public void execute(ExecutionContext context) throws Exception {\n+\n+    SparkSession session = SparkSession.builder().sparkContext(context.getJsc().sc()).getOrCreate();\n+\n+    // todo: Fix partitioning schemes. For now, assumes data based partitioning.\n+    String inputPath = context.getHoodieTestSuiteWriter().getCfg().inputBasePath + \"/*/*\";\n+    String hudiPath = context.getHoodieTestSuiteWriter().getCfg().targetBasePath + \"/*/*/*\";\n+    log.warn(\"ValidateDataset Node: Input path \" + inputPath + \", hudi path \" + hudiPath);\n+    // listing batches to be validated\n+    String inputPathStr = context.getHoodieTestSuiteWriter().getCfg().inputBasePath;\n+    FileSystem fs = new Path(inputPathStr)\n+        .getFileSystem(context.getHoodieTestSuiteWriter().getConfiguration());\n+    FileStatus[] fileStatuses = fs.listStatus(new Path(inputPathStr));\n+    for (FileStatus fileStatus : fileStatuses) {\n+      log.debug(\"Listing all Micro batches to be validated :: \" + fileStatus.getPath().toString());\n+    }\n+\n+    String recordKeyField = context.getWriterContext().getProps().getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY());\n+    String partitionPathField = context.getWriterContext().getProps().getString(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY());\n+    // todo: fix hard coded fields from configs.\n+    // read input and resolve insert, updates, etc.\n+    Dataset<Row> inputDf = session.read().format(\"avro\").load(inputPath);\n+    ExpressionEncoder encoder = getEncoder(inputDf.schema());\n+    Dataset<Row> inputSnapshotDf = inputDf.groupByKey(\n+        (MapFunction<Row, String>) value -> partitionPathField + \"+\" + recordKeyField, Encoders.STRING())\n+        .reduceGroups((ReduceFunction<Row>) (v1, v2) -> {\n+          int ts1 = v1.getAs(SchemaUtils.SOURCE_ORDERING_FIELD);\n+          int ts2 = v2.getAs(SchemaUtils.SOURCE_ORDERING_FIELD);\n+          if (ts1 > ts2) {\n+            return v1;\n+          } else {\n+            return v2;\n+          }\n+        })\n+        .map((MapFunction<Tuple2<String, Row>, Row>) value -> value._2, encoder);\n+\n+    // read from hudi and remove meta columns.\n+    Dataset<Row> hudiDf = session.read().format(\"hudi\").load(hudiPath);\n+    Dataset<Row> trimmedDf = hudiDf.drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n+        .drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.FILENAME_METADATA_FIELD);\n+\n+    Dataset<Row> intersectionDf = inputSnapshotDf.intersect(trimmedDf);\n+    // the intersected df should be same as inputDf. if not, there is some mismatch.\n+    if (inputSnapshotDf.except(intersectionDf).count() != 0) {\n+      log.error(\"Data set validation failed. Total count in hudi \" + trimmedDf.count() + \", input df count \" + inputSnapshotDf.count());\n+      throw new AssertionError(\"Hudi contents does not match contents input data. \");\n+    }\n+\n+    String database = context.getWriterContext().getProps().getString(\"hoodie.datasource.hive_sync.database\");\n+    String tableName = context.getWriterContext().getProps().getString(\"hoodie.datasource.hive_sync.table\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIzNjY0Ng=="}, "originalCommit": {"oid": "bb66e344ded7f737da619f08f23efce9e3409fa0"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4Mjg2MDgxOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/DagUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTo1NToxN1rOIB39jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTo1NToxN1rOIB39jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgzNjM2Nw==", "bodyText": "The name can be in caps (TestDagName), no need for camecasing..", "url": "https://github.com/apache/hudi/pull/2168#discussion_r538836367", "createdAt": "2020-12-08T21:55:17Z", "author": {"login": "n3nash"}, "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/dag/DagUtils.java", "diffHunk": "@@ -48,6 +48,15 @@\n  */\n public class DagUtils {\n \n+  public static final String DAG_NAME = \"dag_name\";\n+  public static final String DAG_ROUNDS = \"dag_rounds\";\n+  public static final String DAG_INTERMITTENT_DELAY_MINS = \"dag_intermittent_delay_mins\";\n+  public static final String DAG_CONTENT = \"dag_content\";\n+\n+  public static int DEFAULT_DAG_ROUNDS = 1;\n+  public static int DEFAULT_INTERMITTENT_DELAY_MINS = 10;\n+  public static String DEFAULT_DAG_NAME = \"testDagName\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb66e344ded7f737da619f08f23efce9e3409fa0"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4085, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}