{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU0NzI2ODM5", "number": 1858, "title": "[HUDI-1014] Adding Upgrade and downgrade infra for smooth transitioning from list based rollback to marker based rollback", "bodyText": "What is the purpose of the pull request\n\n\n\nThis pull request adds upgrade/downgrade infra for smooth transition from list based rollback to marker based rollback*\n\n\n\n\nA new property called hoodie.table.version is added to hoodie.properties file as part of this. Whenever hoodie is launched with newer table version i.e 1(or moving from pre 0.6.0 to 0.6.0), an upgrade step will be executed automatically to adhere to marker based rollback.*\n\n\n\n\nThis automatic upgrade step will happen just once per dataset as the hoodie.table.version will be updated in property file after upgrade is completed once*\n\n\n\n\nSimilarly, a command line tool for Downgrading is added if incase some user wants to downgrade hoodie from table version 1 to 0 or move from hoodie 0.6.0 to pre 0.6.0*\n\n\n\nBrief change log\n\nAdded UpgradeDowngradeUtil to assist in upgrading or downgrading hoodie table\nAdded Interfaces for upgrade and downgrade and concrete implementations for upgrading from 0 to 1 and downgrading from 1 to 0.\nMade some changes to ListingBasedRollbackHelper to expose just rollback stats w/o performing actual rollback, which will be consumed by Upgrade infra\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded TestUpgradeDowngradeUtil to verify the change.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-07-21T20:23:36Z", "url": "https://github.com/apache/hudi/pull/1858", "merged": true, "mergeCommit": {"oid": "ff53e8f0b62448815a487285ce0cc337398272f2"}, "closed": true, "closedAt": "2020-08-09T22:32:44Z", "author": {"login": "nsivabalan"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc3Na2ygFqTQ1MjgwMTUxNg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc9T_e-ABqjM2MzY3NzAxMDE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyODAxNTE2", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-452801516", "createdAt": "2020-07-21T20:24:24Z", "commit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMDoyNDoyNVrOG1Ib2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMDoyNjoyOFrOG1IgTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NTkxNQ==", "bodyText": "@vinothchandar : is this the right place to call upgrade/downgrade. If not, please advise.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458365915", "createdAt": "2020-07-21T20:24:25Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -190,6 +192,7 @@ public HoodieMetrics getMetrics() {\n    */\n   protected HoodieTable getTableAndInitCtx(WriteOperationType operationType) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n+    mayBeUpradeOrDowngrade(metaClient);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2NzA1NA==", "bodyText": "@vinothchandar : after updating the hoodie.properties file, I haven't reloaded the meta client as of now. It is just the table version in memory that has changed and no other code blocks should access table.version. Do you think is reload of metaclient mandatory if we update hoodie.properties w/ new table version?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458367054", "createdAt": "2020-07-21T20:26:28Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.exception.HoodieException;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeHelper {\n+\n+  public static final String HOODIE_ORIG_PROPERTY_FILE = \"hoodie.properties.orig\";\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie verion was upgraded to 0.6.0, there are some upgrade steps need\n+   * to be executed before doing any writes.\n+   * Similarly, if a dataset was created using 0.6.0 and then hoodie was downgraded, some downgrade steps need to be executed before proceeding w/ any writes.\n+   * On a high level, these are the steps performed\n+   * Step1 : Understand current hoodie version and table version from hoodie.properties file\n+   * Step2 : Fix any residues from previous upgrade/downgrade\n+   * Step3 : Check for version upgrade/downgrade.\n+   * Step4 : If upgrade/downgrade is required, perform the steps required for the same.\n+   * Step5 : Copy hoodie.properties -> hoodie.properties.orig\n+   * Step6 : Update hoodie.properties file with current table version\n+   * Step7 : Delete hoodie.properties.orig\n+   * </p>\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @throws IOException\n+   */\n+  public static void doUpgradeOrDowngrade(HoodieTableMetaClient metaClient) throws IOException {\n+    // Fetch version from property file and current version\n+    HoodieTableVersion versionFromPropertyFile = metaClient.getTableConfig().getHoodieTableVersionFromPropertyFile();\n+    HoodieTableVersion currentVersion = metaClient.getTableConfig().getCurrentHoodieTableVersion();\n+\n+    Path metaPath = new Path(metaClient.getMetaPath());\n+    Path originalHoodiePropertyPath = getOrigHoodiePropertyFilePath(metaPath.toString());\n+\n+    boolean updateTableVersionInPropertyFile = false;\n+\n+    if (metaClient.getFs().exists(originalHoodiePropertyPath)) {\n+      // if hoodie.properties.orig exists, rename to hoodie.properties and skip upgrade/downgrade step\n+      metaClient.getFs().rename(originalHoodiePropertyPath, getHoodiePropertyFilePath(metaPath.toString()));\n+      updateTableVersionInPropertyFile = true;\n+    } else {\n+      // upgrade or downgrade if there is a version mismatch\n+      if (versionFromPropertyFile != currentVersion) {\n+        updateTableVersionInPropertyFile = true;\n+        if (versionFromPropertyFile == HoodieTableVersion.PRE_ZERO_SIZE_ZERO && currentVersion == HoodieTableVersion.ZERO_SIX_ZERO) {\n+          upgradeFromOlderToZeroSixZero();\n+        } else if (versionFromPropertyFile == HoodieTableVersion.ZERO_SIX_ZERO && currentVersion == HoodieTableVersion.PRE_ZERO_SIZE_ZERO) {\n+          downgradeFromZeroSixZeroToPreZeroSixZero();\n+        } else {\n+          throw new HoodieException(\"Illegal state wrt table versions. Version from proerpty file \" + versionFromPropertyFile + \" and current version \" + currentVersion);\n+        }\n+      }\n+    }\n+\n+    /**\n+     * If table version needs to be updated in hoodie.properties file.\n+     * Step1: Copy hoodie.properties to hoodie.properties.orig\n+     * Step2: add table.version to hoodie.properties\n+     * Step3: delete hoodie.properties.orig\n+     */\n+    if (updateTableVersionInPropertyFile) {\n+      updateTableVersionInMetaPath(metaClient);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 95}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyOTY3MDg1", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-452967085", "createdAt": "2020-07-22T03:19:22Z", "commit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMzoxOToyMlrOG1ROJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMzoxOToyMlrOG1ROJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwOTg2Mg==", "bodyText": "@vinothchandar : sorry forgot to ask this question earlier. May I know how to fetch current hoodie version in use?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r458509862", "createdAt": "2020-07-22T03:19:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -151,6 +154,27 @@ public HoodieTableType getTableType() {\n         : Option.empty();\n   }\n \n+  /**\n+   * @return the table version from .hoodie properties file.\n+   */\n+  public HoodieTableVersion getHoodieTableVersionFromPropertyFile() {\n+    if (props.contains(HOODIE_TABLE_VERSION_PROP_NAME)) {\n+      String propValue = props.getProperty(HOODIE_TABLE_VERSION_PROP_NAME);\n+      if (propValue.equals(HoodieTableVersion.ZERO_SIX_ZERO.version)) {\n+        return HoodieTableVersion.ZERO_SIX_ZERO;\n+      }\n+    }\n+    return DEFAULT_TABLE_VERSION;\n+  }\n+\n+  /**\n+   * @return the current hoodie table version.\n+   */\n+  public HoodieTableVersion getCurrentHoodieTableVersion() {\n+    // TODO: fetch current version dynamically", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "296f39101e95442a59dcdd11a41c88f76c90286d"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU2MTg2NTgz", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-456186583", "createdAt": "2020-07-27T22:46:48Z", "commit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo0Njo0OFrOG32Opg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMjo1NToxOFrOG32acQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxMzM1MA==", "bodyText": "@vinothchandar : I have added a flag here to say where delete has to be done or just stats need to be collected. Since I don't want to duplicate code, tried my best to re-use. If you can think of any other ways, lmk.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461213350", "createdAt": "2020-07-27T22:46:48Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "diffHunk": "@@ -159,24 +161,32 @@ private void rollBackIndex() {\n     LOG.info(\"Index rolled back for commits \" + instantToRollback);\n   }\n \n-  public List<HoodieRollbackStat> doRollbackAndGetStats() {\n-    final String instantTimeToRollback = instantToRollback.getTimestamp();\n-    final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())\n-        && !instantToRollback.isCompleted();\n-    validateSavepointRollbacks();\n-    if (!isPendingCompaction) {\n-      validateRollbackCommitSequence();\n-    }\n-\n-    try {\n-      List<HoodieRollbackStat> stats = executeRollback();\n-      LOG.info(\"Rolled back inflight instant \" + instantTimeToRollback);\n+  public List<HoodieRollbackStat> mayBeRollbackAndGetStats(boolean doDelete) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxMzU1Mg==", "bodyText": "this is the else part where we just collect stats.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461213552", "createdAt": "2020-07-27T22:47:26Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "diffHunk": "@@ -159,24 +161,32 @@ private void rollBackIndex() {\n     LOG.info(\"Index rolled back for commits \" + instantToRollback);\n   }\n \n-  public List<HoodieRollbackStat> doRollbackAndGetStats() {\n-    final String instantTimeToRollback = instantToRollback.getTimestamp();\n-    final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())\n-        && !instantToRollback.isCompleted();\n-    validateSavepointRollbacks();\n-    if (!isPendingCompaction) {\n-      validateRollbackCommitSequence();\n-    }\n-\n-    try {\n-      List<HoodieRollbackStat> stats = executeRollback();\n-      LOG.info(\"Rolled back inflight instant \" + instantTimeToRollback);\n+  public List<HoodieRollbackStat> mayBeRollbackAndGetStats(boolean doDelete) {\n+    if(doDelete) {\n+      final String instantTimeToRollback = instantToRollback.getTimestamp();\n+      final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())\n+          && !instantToRollback.isCompleted();\n+      validateSavepointRollbacks();\n       if (!isPendingCompaction) {\n-        rollBackIndex();\n+        validateRollbackCommitSequence();\n+      }\n+\n+      try {\n+        List<HoodieRollbackStat> stats = executeRollback(doDelete);\n+        LOG.info(\"Rolled back inflight instant \" + instantTimeToRollback);\n+        if (!isPendingCompaction) {\n+          rollBackIndex();\n+        }\n+        return stats;\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Unable to execute rollback \", e);\n+      }\n+    } else{\n+      try {\n+        return executeRollback(doDelete);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxMzc5MQ==", "bodyText": "and if doDelete is false, we call into executeRollbackUsingFileListing directly.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461213791", "createdAt": "2020-07-27T22:48:10Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -80,10 +82,16 @@ public CopyOnWriteRollbackActionExecutor(JavaSparkContext jsc,\n     if (!resolvedInstant.isRequested()) {\n       // delete all the data files for this commit\n       LOG.info(\"Clean out all base files generated for commit: \" + resolvedInstant);\n-      stats = getRollbackStrategy().execute(resolvedInstant);\n+      if(doDelete) {\n+        stats = getRollbackStrategy().execute(resolvedInstant);\n+      } else{\n+        stats = executeRollbackUsingFileListing(resolvedInstant, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNDM3Mw==", "bodyText": "disintegrated ListingBasedRollbackHelper into two apis, performRollback and collectRollbackStats where first calls into 2nd. Incase of actual rollback, we call performRollback and incase of collecting stats for upgrade, we call into collectRollbackStats. I am repurposing HoodieRollbackStat to hold the info on file path to be rolledback.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461214373", "createdAt": "2020-07-27T22:49:44Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/CopyOnWriteRollbackActionExecutor.java", "diffHunk": "@@ -100,8 +108,13 @@ public CopyOnWriteRollbackActionExecutor(JavaSparkContext jsc,\n   }\n \n   @Override\n-  protected List<HoodieRollbackStat> executeRollbackUsingFileListing(HoodieInstant instantToRollback) {\n+  protected List<HoodieRollbackStat> executeRollbackUsingFileListing(HoodieInstant instantToRollback, boolean doDelete) {\n     List<ListingBasedRollbackRequest> rollbackRequests = generateRollbackRequestsByListing();\n-    return new ListingBasedRollbackHelper(table.getMetaClient(), config).performRollback(jsc, instantToRollback, rollbackRequests);\n+    ListingBasedRollbackHelper listingBasedRollbackHelper = new ListingBasedRollbackHelper(table.getMetaClient(), config);\n+    if(doDelete) {\n+      return listingBasedRollbackHelper.performRollback(jsc, instantToRollback, rollbackRequests);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNDYyMA==", "bodyText": "this filter was used only within one method and hence moved it within the resp method.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461214620", "createdAt": "2020-07-27T22:50:18Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -68,34 +69,38 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n    * Performs all rollback actions that we have collected in parallel.\n    */\n   public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<ListingBasedRollbackRequest> rollbackRequests) {\n-    SerializablePathFilter filter = (path) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNTE3Ng==", "bodyText": "can you help me understand how to differentiate between CREATE and MERGE in these code blocks.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461215176", "createdAt": "2020-07-27T22:51:57Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -130,39 +137,55 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n               1L\n           );\n           return new Tuple2<>(rollbackRequest.getPartitionPath(),\n-                  HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n-                          .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n         }\n         default:\n           throw new IllegalStateException(\"Unknown Rollback action \" + rollbackRequest);\n       }\n-    }).reduceByKey(RollbackUtils::mergeRollbackStat).map(Tuple2::_2).collect();\n+    });\n   }\n \n \n-\n   /**\n    * Common method used for cleaning out base files under a partition path during rollback of a set of commits.\n    */\n-  private Map<FileStatus, Boolean> deleteCleanedFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n-                                                      String partitionPath, PathFilter filter) throws IOException {\n+  private Map<FileStatus, Boolean> deleteBaseAndLogFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNTUwOA==", "bodyText": "incase of just collecting stats, all files are added to success list.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461215508", "createdAt": "2020-07-27T22:52:53Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/ListingBasedRollbackHelper.java", "diffHunk": "@@ -130,39 +137,55 @@ public ListingBasedRollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteC\n               1L\n           );\n           return new Tuple2<>(rollbackRequest.getPartitionPath(),\n-                  HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n-                          .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n+              HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())\n+                  .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());\n         }\n         default:\n           throw new IllegalStateException(\"Unknown Rollback action \" + rollbackRequest);\n       }\n-    }).reduceByKey(RollbackUtils::mergeRollbackStat).map(Tuple2::_2).collect();\n+    });\n   }\n \n \n-\n   /**\n    * Common method used for cleaning out base files under a partition path during rollback of a set of commits.\n    */\n-  private Map<FileStatus, Boolean> deleteCleanedFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n-                                                      String partitionPath, PathFilter filter) throws IOException {\n+  private Map<FileStatus, Boolean> deleteBaseAndLogFiles(HoodieTableMetaClient metaClient, HoodieWriteConfig config,\n+      String commit, String partitionPath, boolean doDelete) throws IOException {\n     LOG.info(\"Cleaning path \" + partitionPath);\n+    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n+    SerializablePathFilter filter = (path) -> {\n+      if (path.toString().endsWith(basefileExtension)) {\n+        String fileCommitTime = FSUtils.getCommitTime(path.getName());\n+        return commit.equals(fileCommitTime);\n+      } else if (FSUtils.isLogFile(path)) {\n+        // Since the baseCommitTime is the only commit for new log files, it's okay here\n+        String fileCommitTime = FSUtils.getBaseCommitTimeFromLogPath(path);\n+        return commit.equals(fileCommitTime);\n+      }\n+      return false;\n+    };\n+\n     final Map<FileStatus, Boolean> results = new HashMap<>();\n     FileSystem fs = metaClient.getFs();\n     FileStatus[] toBeDeleted = fs.listStatus(FSUtils.getPartitionPath(config.getBasePath(), partitionPath), filter);\n     for (FileStatus file : toBeDeleted) {\n-      boolean success = fs.delete(file.getPath(), false);\n-      results.put(file, success);\n-      LOG.info(\"Delete file \" + file.getPath() + \"\\t\" + success);\n+      if(doDelete) {\n+        boolean success = fs.delete(file.getPath(), false);\n+        results.put(file, success);\n+        LOG.info(\"Delete file \" + file.getPath() + \"\\t\" + success);\n+      } else{\n+        results.put(file, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNTgzMw==", "bodyText": "have added this to hold file status fully to be used for upgrade", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461215833", "createdAt": "2020-07-27T22:53:52Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/HoodieRollbackStat.java", "diffHunk": "@@ -39,12 +40,15 @@\n   // Count of HoodieLogFile to commandBlocks written for a particular rollback\n   private final Map<FileStatus, Long> commandBlocksCount;\n \n+  private final List<FileStatus> filesToRollback;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNjE3MA==", "bodyText": "yet to figure out how to differentiate CREATE and MERGE from fileStatus", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461216170", "createdAt": "2020-07-27T22:54:49Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeHelper {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngradeHelper.class);\n+  public static final String HOODIE_ORIG_PROPERTY_FILE = \"hoodie.properties.orig\";\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths. Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3),\n+   * and Hoodie version was upgraded to 0.6.0, Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded to pre 0.6.0 or to Hoodie table version 0,\n+   * then some downgrade steps need to be executed before proceeding w/ any writes.\n+   * On a high level, these are the steps performed\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Fix any residues from previous upgrade/downgrade\n+   * Step3 : If there are no residues, Check for version upgrade/downgrade. If version mismatch, perform upgrade/downgrade.\n+   * Step4 : If there are residues, clean them up and skip upgrade/downgrade since those steps would have been completed last time.\n+   * Step5 : Copy hoodie.properties -> hoodie.properties.orig\n+   * Step6 : Update hoodie.properties file with current table version\n+   * Step7 : Delete hoodie.properties.orig\n+   * </p>\n+   *\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @param toVersion version to which upgrade or downgrade has to be done.\n+   */\n+  public static void doUpgradeOrDowngrade(HoodieTableMetaClient metaClient, HoodieTableVersion toVersion, HoodieWriteConfig config, JavaSparkContext jsc) throws IOException {\n+    // Fetch version from property file and current version\n+    HoodieTableVersion versionFromPropertyFile = metaClient.getTableConfig().getHoodieTableVersionFromPropertyFile();\n+\n+    Path metaPath = new Path(metaClient.getMetaPath());\n+    Path originalHoodiePropertyFile = getOrigHoodiePropertyFilePath(metaPath.toString());\n+\n+    boolean updateTableVersionInPropertyFile = false;\n+\n+    if (metaClient.getFs().exists(originalHoodiePropertyFile)) {\n+      // if hoodie.properties.orig exists, rename to hoodie.properties and skip upgrade/downgrade step\n+      metaClient.getFs().rename(originalHoodiePropertyFile, getHoodiePropertyFilePath(metaPath.toString()));\n+      updateTableVersionInPropertyFile = true;\n+    } else {\n+      // upgrade or downgrade if there is a version mismatch\n+      if (versionFromPropertyFile != toVersion) {\n+        updateTableVersionInPropertyFile = true;\n+        if (versionFromPropertyFile == HoodieTableVersion.ONE && toVersion == HoodieTableVersion.ZERO) {\n+          upgradeFromZeroToOne(config, jsc.hadoopConfiguration(), jsc);\n+        } else if (versionFromPropertyFile == HoodieTableVersion.ZERO && toVersion == HoodieTableVersion.ONE) {\n+          downgradeFromOneToZero();\n+        } else {\n+          throw new HoodieException(\"Illegal state wrt table versions. Version from proerpty file \" + versionFromPropertyFile + \" and current version \" + toVersion);\n+        }\n+      }\n+    }\n+\n+    /**\n+     * If table version needs to be updated in hoodie.properties file.\n+     * Step1: Copy hoodie.properties to hoodie.properties.orig\n+     * Step2: add table.version to hoodie.properties\n+     * Step3: delete hoodie.properties.orig\n+     */\n+    if (updateTableVersionInPropertyFile) {\n+      updateTableVersionInHoodiePropertyFile(metaClient, toVersion);\n+    }\n+  }\n+\n+  /**\n+   * Upgrade steps to be done to upgrade from hoodie table version 0 to 1.\n+   */\n+  private static void upgradeFromZeroToOne(HoodieWriteConfig config, Configuration hadoopConf, JavaSparkContext jsc) {\n+    // fetch pending commit info\n+    HoodieTable table = HoodieTable.create(config, hadoopConf);\n+    HoodieTimeline inflightTimeline = table.getMetaClient().getCommitsTimeline().filterPendingExcludingCompaction();\n+    List<String> commits = inflightTimeline.getReverseOrderedInstants().map(HoodieInstant::getTimestamp)\n+        .collect(Collectors.toList());\n+    for (String commit : commits) {\n+      // for every pending commit, delete old marker files and re-SparkMaincreate marker files in new format\n+      recreateMarkerFiles(commit, table, jsc);\n+    }\n+  }\n+\n+  public static void recreateMarkerFiles(final String commitInstantTime, HoodieTable table, JavaSparkContext jsc) throws HoodieRollbackException {\n+    try {\n+      Option<HoodieInstant> commitInstantOpt = Option.fromJavaOptional(table.getActiveTimeline().getCommitsTimeline().getInstants()\n+          .filter(instant -> HoodieActiveTimeline.EQUALS.test(instant.getTimestamp(), commitInstantTime))\n+          .findFirst());\n+      if (commitInstantOpt.isPresent()) {\n+        MarkerFiles markerFiles = new MarkerFiles(table, commitInstantTime);\n+        markerFiles.quietDeleteMarkerDir();\n+\n+        List<HoodieRollbackStat> rollbackStats = new CopyOnWriteRollbackActionExecutor(jsc, table.getConfig(), table, \"\", commitInstantOpt.get(), false).mayBeRollbackAndGetStats(false);\n+\n+        for (HoodieRollbackStat rollbackStat : rollbackStats) {\n+          for (FileStatus fileStatus : rollbackStat.getFilesToRollback()) {\n+            String path = fileStatus.getPath().toString();\n+            String dataFileName = path.substring(path.lastIndexOf(\"/\") + 1);\n+            markerFiles.create(rollbackStat.getPartitionPath(), dataFileName, path.endsWith(table.getBaseFileExtension()) ? IOType.CREATE : IOType.MERGE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTIxNjM2OQ==", "bodyText": "I assume every entry in commandBlocks will be an APPEND. Correct me if I am wrong.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461216369", "createdAt": "2020-07-27T22:55:18Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/UpgradeDowngradeHelper.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieRollbackException;\n+import org.apache.hudi.io.IOType;\n+import org.apache.hudi.table.action.rollback.CopyOnWriteRollbackActionExecutor;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeHelper {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngradeHelper.class);\n+  public static final String HOODIE_ORIG_PROPERTY_FILE = \"hoodie.properties.orig\";\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths. Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3),\n+   * and Hoodie version was upgraded to 0.6.0, Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded to pre 0.6.0 or to Hoodie table version 0,\n+   * then some downgrade steps need to be executed before proceeding w/ any writes.\n+   * On a high level, these are the steps performed\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Fix any residues from previous upgrade/downgrade\n+   * Step3 : If there are no residues, Check for version upgrade/downgrade. If version mismatch, perform upgrade/downgrade.\n+   * Step4 : If there are residues, clean them up and skip upgrade/downgrade since those steps would have been completed last time.\n+   * Step5 : Copy hoodie.properties -> hoodie.properties.orig\n+   * Step6 : Update hoodie.properties file with current table version\n+   * Step7 : Delete hoodie.properties.orig\n+   * </p>\n+   *\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @param toVersion version to which upgrade or downgrade has to be done.\n+   */\n+  public static void doUpgradeOrDowngrade(HoodieTableMetaClient metaClient, HoodieTableVersion toVersion, HoodieWriteConfig config, JavaSparkContext jsc) throws IOException {\n+    // Fetch version from property file and current version\n+    HoodieTableVersion versionFromPropertyFile = metaClient.getTableConfig().getHoodieTableVersionFromPropertyFile();\n+\n+    Path metaPath = new Path(metaClient.getMetaPath());\n+    Path originalHoodiePropertyFile = getOrigHoodiePropertyFilePath(metaPath.toString());\n+\n+    boolean updateTableVersionInPropertyFile = false;\n+\n+    if (metaClient.getFs().exists(originalHoodiePropertyFile)) {\n+      // if hoodie.properties.orig exists, rename to hoodie.properties and skip upgrade/downgrade step\n+      metaClient.getFs().rename(originalHoodiePropertyFile, getHoodiePropertyFilePath(metaPath.toString()));\n+      updateTableVersionInPropertyFile = true;\n+    } else {\n+      // upgrade or downgrade if there is a version mismatch\n+      if (versionFromPropertyFile != toVersion) {\n+        updateTableVersionInPropertyFile = true;\n+        if (versionFromPropertyFile == HoodieTableVersion.ONE && toVersion == HoodieTableVersion.ZERO) {\n+          upgradeFromZeroToOne(config, jsc.hadoopConfiguration(), jsc);\n+        } else if (versionFromPropertyFile == HoodieTableVersion.ZERO && toVersion == HoodieTableVersion.ONE) {\n+          downgradeFromOneToZero();\n+        } else {\n+          throw new HoodieException(\"Illegal state wrt table versions. Version from proerpty file \" + versionFromPropertyFile + \" and current version \" + toVersion);\n+        }\n+      }\n+    }\n+\n+    /**\n+     * If table version needs to be updated in hoodie.properties file.\n+     * Step1: Copy hoodie.properties to hoodie.properties.orig\n+     * Step2: add table.version to hoodie.properties\n+     * Step3: delete hoodie.properties.orig\n+     */\n+    if (updateTableVersionInPropertyFile) {\n+      updateTableVersionInHoodiePropertyFile(metaClient, toVersion);\n+    }\n+  }\n+\n+  /**\n+   * Upgrade steps to be done to upgrade from hoodie table version 0 to 1.\n+   */\n+  private static void upgradeFromZeroToOne(HoodieWriteConfig config, Configuration hadoopConf, JavaSparkContext jsc) {\n+    // fetch pending commit info\n+    HoodieTable table = HoodieTable.create(config, hadoopConf);\n+    HoodieTimeline inflightTimeline = table.getMetaClient().getCommitsTimeline().filterPendingExcludingCompaction();\n+    List<String> commits = inflightTimeline.getReverseOrderedInstants().map(HoodieInstant::getTimestamp)\n+        .collect(Collectors.toList());\n+    for (String commit : commits) {\n+      // for every pending commit, delete old marker files and re-SparkMaincreate marker files in new format\n+      recreateMarkerFiles(commit, table, jsc);\n+    }\n+  }\n+\n+  public static void recreateMarkerFiles(final String commitInstantTime, HoodieTable table, JavaSparkContext jsc) throws HoodieRollbackException {\n+    try {\n+      Option<HoodieInstant> commitInstantOpt = Option.fromJavaOptional(table.getActiveTimeline().getCommitsTimeline().getInstants()\n+          .filter(instant -> HoodieActiveTimeline.EQUALS.test(instant.getTimestamp(), commitInstantTime))\n+          .findFirst());\n+      if (commitInstantOpt.isPresent()) {\n+        MarkerFiles markerFiles = new MarkerFiles(table, commitInstantTime);\n+        markerFiles.quietDeleteMarkerDir();\n+\n+        List<HoodieRollbackStat> rollbackStats = new CopyOnWriteRollbackActionExecutor(jsc, table.getConfig(), table, \"\", commitInstantOpt.get(), false).mayBeRollbackAndGetStats(false);\n+\n+        for (HoodieRollbackStat rollbackStat : rollbackStats) {\n+          for (FileStatus fileStatus : rollbackStat.getFilesToRollback()) {\n+            String path = fileStatus.getPath().toString();\n+            String dataFileName = path.substring(path.lastIndexOf(\"/\") + 1);\n+            markerFiles.create(rollbackStat.getPartitionPath(), dataFileName, path.endsWith(table.getBaseFileExtension()) ? IOType.CREATE : IOType.MERGE);\n+          }\n+          for (FileStatus fileStatus : rollbackStat.getCommandBlocksCount().keySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 145}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU2NTEyMjQz", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-456512243", "createdAt": "2020-07-28T10:42:20Z", "commit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMDo0MjoyMFrOG4G7DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxMDo0MjoyMFrOG4G7DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQ4Njg2MQ==", "bodyText": "@vinothchandar : forgot to remind you yesterday when we discussed to move the collectStats method to a separate class and call directly for upgrade. These validations steps (validateSavepointRollbacks, validateRollbackCommitSequence) might be required as well right ? So, few bits and pieces in this class is required in upgrade step as well.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r461486861", "createdAt": "2020-07-28T10:42:20Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "diffHunk": "@@ -159,24 +161,32 @@ private void rollBackIndex() {\n     LOG.info(\"Index rolled back for commits \" + instantToRollback);\n   }\n \n-  public List<HoodieRollbackStat> doRollbackAndGetStats() {\n-    final String instantTimeToRollback = instantToRollback.getTimestamp();\n-    final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())\n-        && !instantToRollback.isCompleted();\n-    validateSavepointRollbacks();\n-    if (!isPendingCompaction) {\n-      validateRollbackCommitSequence();\n-    }\n-\n-    try {\n-      List<HoodieRollbackStat> stats = executeRollback();\n-      LOG.info(\"Rolled back inflight instant \" + instantTimeToRollback);\n+  public List<HoodieRollbackStat> mayBeRollbackAndGetStats(boolean doDelete) {\n+    if(doDelete) {\n+      final String instantTimeToRollback = instantToRollback.getTimestamp();\n+      final boolean isPendingCompaction = Objects.equals(HoodieTimeline.COMPACTION_ACTION, instantToRollback.getAction())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216"}, "originalPosition": 51}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/a04f2fa1d198b5c5a7d3e26c96c3c7b2364c7216", "committedDate": "2020-07-27T21:33:48Z", "message": "Adding actual upgrade steps from HoodieTableVersion 0 to 1"}, "afterCommit": {"oid": "0ea132a70e9bfe914b3f99e8823111aec8b4fe68", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/0ea132a70e9bfe914b3f99e8823111aec8b4fe68", "committedDate": "2020-07-28T12:49:49Z", "message": "Adding actual upgrade steps from HoodieTableVersion 0 to 1"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "04accb1729fc6e4f10e0785f89b550c4e76f75b0", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/04accb1729fc6e4f10e0785f89b550c4e76f75b0", "committedDate": "2020-07-29T17:11:59Z", "message": "Adding tests for upgrading from 0 to 1"}, "afterCommit": {"oid": "42159f615061f66effb40b0aa94c4bc8ba20485b", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/42159f615061f66effb40b0aa94c4bc8ba20485b", "committedDate": "2020-07-29T17:24:19Z", "message": "Adding tests for upgrading from 0 to 1"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3NzMwMzA3", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-457730307", "createdAt": "2020-07-29T17:18:50Z", "commit": {"oid": "04accb1729fc6e4f10e0785f89b550c4e76f75b0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxODo1MFrOG5CWQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNzoxODo1MFrOG5CWQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ2MDQ4MA==", "bodyText": "no changes in this file. just formatting changes.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r462460480", "createdAt": "2020-07-29T17:18:50Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/BaseRollbackActionExecutor.java", "diffHunk": "@@ -59,31 +61,31 @@\n   protected final boolean useMarkerBasedStrategy;\n \n   public BaseRollbackActionExecutor(JavaSparkContext jsc,\n-                                    HoodieWriteConfig config,\n-                                    HoodieTable<?> table,\n-                                    String instantTime,\n-                                    HoodieInstant instantToRollback,\n-                                    boolean deleteInstants) {\n+      HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "04accb1729fc6e4f10e0785f89b550c4e76f75b0"}, "originalPosition": 25}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "42159f615061f66effb40b0aa94c4bc8ba20485b", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/42159f615061f66effb40b0aa94c4bc8ba20485b", "committedDate": "2020-07-29T17:24:19Z", "message": "Adding tests for upgrading from 0 to 1"}, "afterCommit": {"oid": "baf702280d215704d0f16d5d54c6c07c90cbd033", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/baf702280d215704d0f16d5d54c6c07c90cbd033", "committedDate": "2020-07-29T18:02:04Z", "message": "Adding tests for upgrading from 0 to 1"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/2183d2f50d04452447ec06448a583d46f955070e", "committedDate": "2020-08-01T23:13:24Z", "message": "Fixing downgrade from 1 to 0"}, "afterCommit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/e218461f40d8322b7ecd28b7878423dc7a766197", "committedDate": "2020-08-04T06:13:42Z", "message": "[HUDI-1014] Adding Upgrade and downgrade infra for smooth transitioning from list based rollback to marker based rollback\n\n- This pull request adds upgrade/downgrade infra for smooth transition from list based rollback to marker based rollback*\n - A new property called hoodie.table.version is added to hoodie.properties file as part of this. Whenever hoodie is launched with newer table version i.e 1(or moving from pre 0.6.0 to 0.6.0), an upgrade step will be executed automatically to adhere to marker based rollback.*\n - This automatic upgrade step will happen just once per dataset as the hoodie.table.version will be updated in property file after upgrade is completed once*\n - Similarly, a command line tool for Downgrading is added if incase some user wants to downgrade hoodie from table version 1 to 0 or move from hoodie 0.6.0 to pre 0.6.0*\n - *Added UpgradeDowngradeUtil to assist in upgrading or downgrading hoodie table*\n - *Added Interfaces for upgrade and downgrade and concrete implementations for upgrading from 0 to 1 and downgrading from 1 to 0.*\n - *Made some changes to ListingBasedRollbackHelper to expose just rollback stats w/o performing actual rollback, which will be consumed by Upgrade infra*"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjY1MDQ2", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-461265046", "createdAt": "2020-08-05T00:38:28Z", "commit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDozODoyOFrOG72G2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDozODoyOFrOG72G2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQwNTY1OA==", "bodyText": "@nsivabalan Can a user control the hoodie layout version manually from the HoodieWriteConfig. Say, choose the older timeline layout for 0.6.0 in which case there is no need to upgrade ?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r465405658", "createdAt": "2020-08-05T00:38:28Z", "author": {"login": "n3nash"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngradeUtil.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngradeUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjY1MTYx", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-461265161", "createdAt": "2020-08-05T00:38:56Z", "commit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e218461f40d8322b7ecd28b7878423dc7a766197", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/e218461f40d8322b7ecd28b7878423dc7a766197", "committedDate": "2020-08-04T06:13:42Z", "message": "[HUDI-1014] Adding Upgrade and downgrade infra for smooth transitioning from list based rollback to marker based rollback\n\n- This pull request adds upgrade/downgrade infra for smooth transition from list based rollback to marker based rollback*\n - A new property called hoodie.table.version is added to hoodie.properties file as part of this. Whenever hoodie is launched with newer table version i.e 1(or moving from pre 0.6.0 to 0.6.0), an upgrade step will be executed automatically to adhere to marker based rollback.*\n - This automatic upgrade step will happen just once per dataset as the hoodie.table.version will be updated in property file after upgrade is completed once*\n - Similarly, a command line tool for Downgrading is added if incase some user wants to downgrade hoodie from table version 1 to 0 or move from hoodie 0.6.0 to pre 0.6.0*\n - *Added UpgradeDowngradeUtil to assist in upgrading or downgrading hoodie table*\n - *Added Interfaces for upgrade and downgrade and concrete implementations for upgrading from 0 to 1 and downgrading from 1 to 0.*\n - *Made some changes to ListingBasedRollbackHelper to expose just rollback stats w/o performing actual rollback, which will be consumed by Upgrade infra*"}, "afterCommit": {"oid": "ea18853c5d5c52145eabf56b99e7e639beb5e5db", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/ea18853c5d5c52145eabf56b99e7e639beb5e5db", "committedDate": "2020-08-06T07:39:48Z", "message": "[HUDI-1014] Adding Upgrade and downgrade infra for smooth transitioning from list based rollback to marker based rollback\n\n- This pull request adds upgrade/downgrade infra for smooth transition from list based rollback to marker based rollback*\n - A new property called hoodie.table.version is added to hoodie.properties file as part of this. Whenever hoodie is launched with newer table version i.e 1(or moving from pre 0.6.0 to 0.6.0), an upgrade step will be executed automatically to adhere to marker based rollback.*\n - This automatic upgrade step will happen just once per dataset as the hoodie.table.version will be updated in property file after upgrade is completed once*\n - Similarly, a command line tool for Downgrading is added if incase some user wants to downgrade hoodie from table version 1 to 0 or move from hoodie 0.6.0 to pre 0.6.0*\n - *Added UpgradeDowngradeUtil to assist in upgrading or downgrading hoodie table*\n - *Added Interfaces for upgrade and downgrade and concrete implementations for upgrading from 0 to 1 and downgrading from 1 to 0.*\n - *Made some changes to ListingBasedRollbackHelper to expose just rollback stats w/o performing actual rollback, which will be consumed by Upgrade infra*"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNDAxMDEy", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-460401012", "createdAt": "2020-08-03T23:27:07Z", "commit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMzoyNzowOFrOG7L3ZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwODo1OToyM1rOG8pyBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMzU3Mg==", "bodyText": "rename: UpgradeDowngradeUtil.migrate(..)", "url": "https://github.com/apache/hudi/pull/1858#discussion_r464713572", "createdAt": "2020-08-03T23:27:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -329,9 +341,34 @@ private static int deleteSavepoint(JavaSparkContext jsc, String savepointTime, S\n     }\n   }\n \n+  /**\n+   * Upgrade or downgrade hoodie table.\n+   * @param jsc instance of {@link JavaSparkContext} to use.\n+   * @param basePath base path of the dataset.\n+   * @param toVersion version to which upgrade/downgrade to be done.\n+   * @return 0 if success, else -1.\n+   * @throws Exception\n+   */\n+  protected static int upgradeOrDowngradeHoodieDataset(JavaSparkContext jsc, String basePath, String toVersion) throws Exception {\n+    HoodieWriteConfig config = getWriteConfig(basePath);\n+    HoodieTableMetaClient metaClient = ClientUtils.createMetaClient(jsc.hadoopConfiguration(), config, false);\n+    try {\n+      UpgradeDowngradeUtil.doUpgradeOrDowngrade(metaClient, HoodieTableVersion.valueOf(toVersion), config, jsc, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNDY4Mg==", "bodyText": "we should do this no matter, whether rollback using markers is on /off", "url": "https://github.com/apache/hudi/pull/1858#discussion_r464714682", "createdAt": "2020-08-03T23:30:55Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -186,10 +188,14 @@ public HoodieMetrics getMetrics() {\n    * Get HoodieTable and init {@link Timer.Context}.\n    *\n    * @param operationType write operation type\n+   * @param instantTime current inflight instant time\n    * @return HoodieTable\n    */\n-  protected HoodieTable getTableAndInitCtx(WriteOperationType operationType) {\n+  protected HoodieTable getTableAndInitCtx(WriteOperationType operationType, String instantTime) {\n     HoodieTableMetaClient metaClient = createMetaClient(true);\n+    if (config.shouldRollbackUsingMarkers()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNTExNw==", "bodyText": "is this just moving code in bulk?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r464715117", "createdAt": "2020-08-03T23:32:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java", "diffHunk": "@@ -63,4 +84,156 @@ static HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRoll\n     return new HoodieRollbackStat(stat1.getPartitionPath(), successDeleteFiles, failedDeleteFiles, commandBlocksCount);\n   }\n \n+  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2183d2f50d04452447ec06448a583d46f955070e"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI1MjI5NA==", "bodyText": "we should keep these to just HoodieClientTestUtils. since markers are just an artifact of the client", "url": "https://github.com/apache/hudi/pull/1858#discussion_r466252294", "createdAt": "2020-08-06T08:59:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/testutils/HoodieTestUtils.java", "diffHunk": "@@ -279,6 +287,23 @@ public static String createDataFile(String basePath, String partitionPath, Strin\n     return fileID;\n   }\n \n+  public static void createMarkerFile(String basePath, String partitionPath, String instantTime, String dataFileName) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea18853c5d5c52145eabf56b99e7e639beb5e5db"}, "originalPosition": 19}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a6d380e369c4f5043258f6c9d4259c415942bf8", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/7a6d380e369c4f5043258f6c9d4259c415942bf8", "committedDate": "2020-08-09T08:13:57Z", "message": "[HUDI-1014] Adding Upgrade and downgrade infra for smooth transitioning from list based rollback to marker based rollback\n\n- This pull request adds upgrade/downgrade infra for smooth transition from list based rollback to marker based rollback*\n - A new property called hoodie.table.version is added to hoodie.properties file as part of this. Whenever hoodie is launched with newer table version i.e 1(or moving from pre 0.6.0 to 0.6.0), an upgrade step will be executed automatically to adhere to marker based rollback.*\n - This automatic upgrade step will happen just once per dataset as the hoodie.table.version will be updated in property file after upgrade is completed once*\n - Similarly, a command line tool for Downgrading is added if incase some user wants to downgrade hoodie from table version 1 to 0 or move from hoodie 0.6.0 to pre 0.6.0*\n - *Added UpgradeDowngradeUtil to assist in upgrading or downgrading hoodie table*\n - *Added Interfaces for upgrade and downgrade and concrete implementations for upgrading from 0 to 1 and downgrading from 1 to 0.*\n - *Made some changes to ListingBasedRollbackHelper to expose just rollback stats w/o performing actual rollback, which will be consumed by Upgrade infra*"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ea18853c5d5c52145eabf56b99e7e639beb5e5db", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/ea18853c5d5c52145eabf56b99e7e639beb5e5db", "committedDate": "2020-08-06T07:39:48Z", "message": "[HUDI-1014] Adding Upgrade and downgrade infra for smooth transitioning from list based rollback to marker based rollback\n\n- This pull request adds upgrade/downgrade infra for smooth transition from list based rollback to marker based rollback*\n - A new property called hoodie.table.version is added to hoodie.properties file as part of this. Whenever hoodie is launched with newer table version i.e 1(or moving from pre 0.6.0 to 0.6.0), an upgrade step will be executed automatically to adhere to marker based rollback.*\n - This automatic upgrade step will happen just once per dataset as the hoodie.table.version will be updated in property file after upgrade is completed once*\n - Similarly, a command line tool for Downgrading is added if incase some user wants to downgrade hoodie from table version 1 to 0 or move from hoodie 0.6.0 to pre 0.6.0*\n - *Added UpgradeDowngradeUtil to assist in upgrading or downgrading hoodie table*\n - *Added Interfaces for upgrade and downgrade and concrete implementations for upgrading from 0 to 1 and downgrading from 1 to 0.*\n - *Made some changes to ListingBasedRollbackHelper to expose just rollback stats w/o performing actual rollback, which will be consumed by Upgrade infra*"}, "afterCommit": {"oid": "27e1c4170c6abae8cc4a51b4189675368f0e237d", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/27e1c4170c6abae8cc4a51b4189675368f0e237d", "committedDate": "2020-08-09T08:16:23Z", "message": "Reworking failure handling for upgrade/downgrade\n\n - Changed tests accordingly, added one test around left over cleanup\n - New tables now write table version into hoodie.properties\n - Clean up code naming, abstractions."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "27e1c4170c6abae8cc4a51b4189675368f0e237d", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/27e1c4170c6abae8cc4a51b4189675368f0e237d", "committedDate": "2020-08-09T08:16:23Z", "message": "Reworking failure handling for upgrade/downgrade\n\n - Changed tests accordingly, added one test around left over cleanup\n - New tables now write table version into hoodie.properties\n - Clean up code naming, abstractions."}, "afterCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/9cc565e5af28e454dc70680beb96ac54c14e8b21", "committedDate": "2020-08-09T09:01:40Z", "message": "Reworking failure handling for upgrade/downgrade\n\n - Changed tests accordingly, added one test around left over cleanup\n - New tables now write table version into hoodie.properties\n - Clean up code naming, abstractions."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzODc3NDUx", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-463877451", "createdAt": "2020-08-09T14:12:46Z", "commit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNDoxMjo0N1rOG97WHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNDoxMjo0N1rOG97WHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU4ODYzNw==", "bodyText": "this needs fixing. This will issue commit via client and since you have removed the guard to execute upgrade (even if marker based is disabled), upgrade would have already been executed. if I am not wrong, below command of\nUpgradeDowngrade.run(metaClient, HoodieTableVersion.ONE, cfg, jsc, null); \n\nis a no op since already the table version is upgraded.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467588637", "createdAt": "2020-08-09T14:12:47Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,405 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";\n+\n+  public static Stream<Arguments> configParams() {\n+    Object[][] data = new Object[][] {\n+            {true, HoodieTableType.COPY_ON_WRITE}, {false, HoodieTableType.COPY_ON_WRITE},\n+            {true, HoodieTableType.MERGE_ON_READ}, {false, HoodieTableType.MERGE_ON_READ}\n+    };\n+    return Stream.of(data).map(Arguments::of);\n+  }\n+\n+  @Test\n+  public void testLeftOverUpdatedPropFileCleanup() throws IOException {\n+    testUpgradeInternal(true, true, HoodieTableType.MERGE_ON_READ);\n+  }\n+\n+  @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @MethodSource(\"configParams\")\n+  public void testUpgrade(boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    testUpgradeInternal(false, deletePartialMarkerFiles, tableType);\n+  }\n+\n+  public void testUpgradeInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    // init config, table and client.\n+    Map<String, String> params = new HashMap<>();\n+    if (tableType == HoodieTableType.MERGE_ON_READ) {\n+      params.put(HOODIE_TABLE_TYPE_PROP_NAME, HoodieTableType.MERGE_ON_READ.name());\n+      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n+    HoodieWriteClient client = getHoodieWriteClient(cfg);\n+\n+    // prepare data. Make 2 commits, in which 2nd is not committed.\n+    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n+    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n+    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzODc3NTM3", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-463877537", "createdAt": "2020-08-09T14:13:48Z", "commit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzODg1OTMw", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-463885930", "createdAt": "2020-08-09T16:14:04Z", "commit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNjoxNDowNVrOG98IBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxNjo0NzoyNFrOG98Ulw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMTQxNA==", "bodyText": "probably you might have to follow something like this in TestUpgradeDowngrade.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467601414", "createdAt": "2020-08-09T16:14:05Z", "author": {"login": "nsivabalan"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestUpgradeDowngradeCommand.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.testutils.AbstractShellIntegrationTest;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Properties;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+/**\n+ * Tests {@link UpgradeOrDowngradeCommand}.\n+ */\n+public class TestUpgradeDowngradeCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @BeforeEach\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    //Create some commits files and parquet files\n+    String commitTime1 = \"100\";\n+    String commitTime2 = \"101\";\n+    HoodieTestDataGenerator.writePartitionMetadata(fs, HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS, tablePath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzIyMA==", "bodyText": "updated/upgraded(file is named as HOODIE_UPDATED_PROPERTY_FILE). Lets use same terminology everywhere. Ignore addressing renaming/java docs/refactoring comments for now. Let's get the patch in for now. But leaving comments so that I can take it up after 0.6.0 release.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467603220", "createdAt": "2020-08-09T16:32:44Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Date;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngrade {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngrade.class);\n+  public static final String HOODIE_UPDATED_PROPERTY_FILE = \"hoodie.properties.updated\";\n+\n+  private HoodieTableMetaClient metaClient;\n+  private HoodieWriteConfig config;\n+  private JavaSparkContext jsc;\n+  private transient FileSystem fs;\n+  private Path updatedPropsFilePath;\n+  private Path propsFilePath;\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   *\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie version was upgraded to 0.6.0,\n+   * Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded\n+   * to pre 0.6.0 or to Hoodie table version 0, then some downgrade steps need to be executed before proceeding w/ any writes.\n+   *\n+   * On a high level, these are the steps performed\n+   *\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Delete any left over .upgraded from previous upgrade/downgrade", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzM2OQ==", "bodyText": "why use same name for this method and for the other method too?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467603369", "createdAt": "2020-08-09T16:34:25Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/upgrade/UpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Date;\n+import java.util.Properties;\n+\n+/**\n+ * Helper class to assist in upgrading/downgrading Hoodie when there is a version change.\n+ */\n+public class UpgradeDowngrade {\n+\n+  private static final Logger LOG = LogManager.getLogger(UpgradeDowngrade.class);\n+  public static final String HOODIE_UPDATED_PROPERTY_FILE = \"hoodie.properties.updated\";\n+\n+  private HoodieTableMetaClient metaClient;\n+  private HoodieWriteConfig config;\n+  private JavaSparkContext jsc;\n+  private transient FileSystem fs;\n+  private Path updatedPropsFilePath;\n+  private Path propsFilePath;\n+\n+  /**\n+   * Perform Upgrade or Downgrade steps if required and updated table version if need be.\n+   * <p>\n+   * Starting from version 0.6.0, this upgrade/downgrade step will be added in all write paths.\n+   *\n+   * Essentially, if a dataset was created using any pre 0.6.0(for eg 0.5.3), and Hoodie version was upgraded to 0.6.0,\n+   * Hoodie table version gets bumped to 1 and there are some upgrade steps need to be executed before doing any writes.\n+   * Similarly, if a dataset was created using Hoodie version 0.6.0 or Hoodie table version 1 and then hoodie was downgraded\n+   * to pre 0.6.0 or to Hoodie table version 0, then some downgrade steps need to be executed before proceeding w/ any writes.\n+   *\n+   * On a high level, these are the steps performed\n+   *\n+   * Step1 : Understand current hoodie table version and table version from hoodie.properties file\n+   * Step2 : Delete any left over .upgraded from previous upgrade/downgrade\n+   * Step3 : If version are different, perform upgrade/downgrade.\n+   * Step4 : Copy hoodie.properties -> hoodie.properties.upgraded with the version updated\n+   * Step6 : Rename hoodie.properties.updated to hoodie.properties\n+   * </p>\n+   *\n+   * @param metaClient instance of {@link HoodieTableMetaClient} to use\n+   * @param toVersion version to which upgrade or downgrade has to be done.\n+   * @param config instance of {@link HoodieWriteConfig} to use.\n+   * @param jsc instance of {@link JavaSparkContext} to use.\n+   * @param instantTime current instant time that should not be touched.\n+   */\n+  public static void run(HoodieTableMetaClient metaClient, HoodieTableVersion toVersion, HoodieWriteConfig config,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwMzk2Nw==", "bodyText": "comments need fixing.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467603967", "createdAt": "2020-08-09T16:40:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,405 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";\n+\n+  public static Stream<Arguments> configParams() {\n+    Object[][] data = new Object[][] {\n+            {true, HoodieTableType.COPY_ON_WRITE}, {false, HoodieTableType.COPY_ON_WRITE},\n+            {true, HoodieTableType.MERGE_ON_READ}, {false, HoodieTableType.MERGE_ON_READ}\n+    };\n+    return Stream.of(data).map(Arguments::of);\n+  }\n+\n+  @Test\n+  public void testLeftOverUpdatedPropFileCleanup() throws IOException {\n+    testUpgradeInternal(true, true, HoodieTableType.MERGE_ON_READ);\n+  }\n+\n+  @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @MethodSource(\"configParams\")\n+  public void testUpgrade(boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    testUpgradeInternal(false, deletePartialMarkerFiles, tableType);\n+  }\n+\n+  public void testUpgradeInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    // init config, table and client.\n+    Map<String, String> params = new HashMap<>();\n+    if (tableType == HoodieTableType.MERGE_ON_READ) {\n+      params.put(HOODIE_TABLE_TYPE_PROP_NAME, HoodieTableType.MERGE_ON_READ.name());\n+      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n+    HoodieWriteClient client = getHoodieWriteClient(cfg);\n+\n+    // prepare data. Make 2 commits, in which 2nd is not committed.\n+    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n+    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n+    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n+\n+    HoodieTable<?> table = this.getHoodieTable(metaClient, cfg);\n+    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n+\n+    // delete one of the marker files in 2nd commit if need be.\n+    MarkerFiles markerFiles = new MarkerFiles(table, commitInstant.getTimestamp());\n+    List<String> markerPaths = markerFiles.allMarkerFilePaths();\n+    if (deletePartialMarkerFiles) {\n+      String toDeleteMarkerFile = markerPaths.get(0);\n+      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n+      markerPaths.remove(toDeleteMarkerFile);\n+    }\n+\n+    // set hoodie.table.version to 0 in hoodie.properties file\n+    metaClient.getTableConfig().setTableVersion(HoodieTableVersion.ZERO);\n+\n+    // if induce residues are set, copy property file to orig file.\n+    if (induceResiduesFromPrevUpgrade) {\n+      createResidualFile();\n+    }\n+\n+    // should re-create marker files for 2nd commit since its pending. If there was any residues, no upgrade steps should happen except for updating the hoodie.table.version", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYwNDYzMQ==", "bodyText": "sorry, I don't get why we need this here. If properties contain table type and table name, why bail out?", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467604631", "createdAt": "2020-08-09T16:47:24Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableConfig.java", "diffHunk": "@@ -96,6 +97,8 @@ public HoodieTableConfig(FileSystem fs, String metaPath, String payloadClassName\n       throw new HoodieIOException(\"Could not load Hoodie properties from \" + propertyPath, e);\n     }\n     this.props = props;\n+    ValidationUtils.checkArgument(props.containsKey(HOODIE_TABLE_TYPE_PROP_NAME) && props.containsKey(HOODIE_TABLE_NAME_PROP_NAME),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 48}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/9cc565e5af28e454dc70680beb96ac54c14e8b21", "committedDate": "2020-08-09T09:01:40Z", "message": "Reworking failure handling for upgrade/downgrade\n\n - Changed tests accordingly, added one test around left over cleanup\n - New tables now write table version into hoodie.properties\n - Clean up code naming, abstractions."}, "afterCommit": {"oid": "3ca5adc0039c68235ee08623336b4d90d6f1f338", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/3ca5adc0039c68235ee08623336b4d90d6f1f338", "committedDate": "2020-08-09T18:12:20Z", "message": "Reworking failure handling for upgrade/downgrade\n\n - Changed tests accordingly, added one test around left over cleanup\n - New tables now write table version into hoodie.properties\n - Clean up code naming, abstractions."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzODkzOTQ0", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-463893944", "createdAt": "2020-08-09T18:26:56Z", "commit": {"oid": "3ca5adc0039c68235ee08623336b4d90d6f1f338"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxODoyNjo1NlrOG984xQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxODoyNjo1NlrOG984xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxMzg5Mw==", "bodyText": "nvm. I overlooked this line\nmetaClient.getTableConfig().setTableVersion(HoodieTableVersion.ZERO);\n\nTests are fine.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467613893", "createdAt": "2020-08-09T18:26:56Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,405 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";\n+\n+  public static Stream<Arguments> configParams() {\n+    Object[][] data = new Object[][] {\n+            {true, HoodieTableType.COPY_ON_WRITE}, {false, HoodieTableType.COPY_ON_WRITE},\n+            {true, HoodieTableType.MERGE_ON_READ}, {false, HoodieTableType.MERGE_ON_READ}\n+    };\n+    return Stream.of(data).map(Arguments::of);\n+  }\n+\n+  @Test\n+  public void testLeftOverUpdatedPropFileCleanup() throws IOException {\n+    testUpgradeInternal(true, true, HoodieTableType.MERGE_ON_READ);\n+  }\n+\n+  @ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @MethodSource(\"configParams\")\n+  public void testUpgrade(boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    testUpgradeInternal(false, deletePartialMarkerFiles, tableType);\n+  }\n+\n+  public void testUpgradeInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n+    // init config, table and client.\n+    Map<String, String> params = new HashMap<>();\n+    if (tableType == HoodieTableType.MERGE_ON_READ) {\n+      params.put(HOODIE_TABLE_TYPE_PROP_NAME, HoodieTableType.MERGE_ON_READ.name());\n+      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n+    }\n+    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n+    HoodieWriteClient client = getHoodieWriteClient(cfg);\n+\n+    // prepare data. Make 2 commits, in which 2nd is not committed.\n+    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n+    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n+    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzU4ODYzNw=="}, "originalCommit": {"oid": "9cc565e5af28e454dc70680beb96ac54c14e8b21"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzODk2MjA5", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-463896209", "createdAt": "2020-08-09T19:05:21Z", "commit": {"oid": "3ca5adc0039c68235ee08623336b4d90d6f1f338"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxOTowNToyMVrOG99G0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxOTowNToyMVrOG99G0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxNzQ5MA==", "bodyText": "minor: there are only 2 args.", "url": "https://github.com/apache/hudi/pull/1858#discussion_r467617490", "createdAt": "2020-08-09T19:05:21Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.upgrade;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroup;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.common.testutils.HoodieTestUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+import org.apache.hudi.testutils.Assertions;\n+import org.apache.hudi.testutils.HoodieClientTestBase;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.common.table.HoodieTableConfig.HOODIE_TABLE_TYPE_PROP_NAME;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+import static org.apache.hudi.common.testutils.HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit tests {@link UpgradeDowngrade}.\n+ */\n+public class TestUpgradeDowngrade extends HoodieClientTestBase {\n+\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with induceResiduesFromPrevUpgrade={0}, deletePartialMarkerFiles={1} and TableType = {2}\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ca5adc0039c68235ee08623336b4d90d6f1f338"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzODk2Njk5", "url": "https://github.com/apache/hudi/pull/1858#pullrequestreview-463896699", "createdAt": "2020-08-09T19:13:17Z", "commit": {"oid": "3ca5adc0039c68235ee08623336b4d90d6f1f338"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "94b5d9e096eb739fbe1cd75367c1d64350bb8570", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/94b5d9e096eb739fbe1cd75367c1d64350bb8570", "committedDate": "2020-08-09T20:52:47Z", "message": "Reworking failure handling for upgrade/downgrade\n\n - Changed tests accordingly, added one test around left over cleanup\n - New tables now write table version into hoodie.properties\n - Clean up code naming, abstractions."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3ca5adc0039c68235ee08623336b4d90d6f1f338", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/3ca5adc0039c68235ee08623336b4d90d6f1f338", "committedDate": "2020-08-09T18:12:20Z", "message": "Reworking failure handling for upgrade/downgrade\n\n - Changed tests accordingly, added one test around left over cleanup\n - New tables now write table version into hoodie.properties\n - Clean up code naming, abstractions."}, "afterCommit": {"oid": "94b5d9e096eb739fbe1cd75367c1d64350bb8570", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/94b5d9e096eb739fbe1cd75367c1d64350bb8570", "committedDate": "2020-08-09T20:52:47Z", "message": "Reworking failure handling for upgrade/downgrade\n\n - Changed tests accordingly, added one test around left over cleanup\n - New tables now write table version into hoodie.properties\n - Clean up code naming, abstractions."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4675, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}