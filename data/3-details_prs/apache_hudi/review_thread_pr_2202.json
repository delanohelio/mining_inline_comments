{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA5MjQ0Mzg4", "number": 2202, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNDo1NDo0MVrOEy2jZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNjo0NDozNVrOE1vpvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzU4MDUyOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNDo1NDo0MVrOHpuOhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNDowOToxNFrOHqdltw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxMTA0NA==", "bodyText": "@satishkotha AVRO has a way to add description with a type called \"doc\", we can either use that or add these comments at the beginning of the avro file.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513511044", "createdAt": "2020-10-28T14:54:41Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieClusteringGroup\",\n+   \"type\":\"record\",\n+   \"fields\":[\n+      {\n+         /* Group of files that needs to merged. All the slices in a group will belong to same partition initially.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxMzgyOQ==", "bodyText": "@satishkotha Also, can we not create multiple avsc files for each \"Pojo\" that you need, you can just define them all under the HoodieClusteringPlan,  take a look at this for example -> https://github.com/apache/hudi/blob/master/hudi-common/src/main/avro/HoodieCleanerPlan.avsc#L78. Once the code-gen creates the POJO from the AVSC, you will have access to them.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513513829", "createdAt": "2020-10-28T14:57:56Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieClusteringGroup\",\n+   \"type\":\"record\",\n+   \"fields\":[\n+      {\n+         /* Group of files that needs to merged. All the slices in a group will belong to same partition initially.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxMTA0NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3NTkxMQ==", "bodyText": "From earlier reviews, creating multiple avsc was preferred because we can reuse pojos for other operations. Please see discussion in #2048 . Let me know if there are any advantages of keeping them together in one file", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513675911", "createdAt": "2020-10-28T18:35:37Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieClusteringGroup\",\n+   \"type\":\"record\",\n+   \"fields\":[\n+      {\n+         /* Group of files that needs to merged. All the slices in a group will belong to same partition initially.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxMTA0NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDI4NzAzMQ==", "bodyText": "Hmm, I see. If there are use-cases where we plan to reuse them to create different POJO's, I'm fine with this.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r514287031", "createdAt": "2020-10-29T14:09:14Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieClusteringGroup\",\n+   \"type\":\"record\",\n+   \"fields\":[\n+      {\n+         /* Group of files that needs to merged. All the slices in a group will belong to same partition initially.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxMTA0NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzYwMTQ1OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieClusteringStrategy.avsc", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNDo1ODo0M1rOHpub9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQyMzozNDozMlrOHrq3UA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNDQ4NA==", "bodyText": "At a high level, don't think we need a clustering strategy avsc for this, we can just drive this from Reflection and classnames, just like CompactionPlans ?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513514484", "createdAt": "2020-10-28T14:58:43Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieClusteringStrategy.avsc", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3Njk4MA==", "bodyText": "There needs to be coordination between scheduling and executing clustering. For example, when scheduling, we specify parameters such as 'sortColumns', 'targetFileSize' etc. These needs to be passed to async job that clusters data. These parameters are serialized and stored in the strategy/plan.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513676980", "createdAt": "2020-10-28T18:37:30Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieClusteringStrategy.avsc", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNDQ4NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDI5MDIzMg==", "bodyText": "The parameters such as 'sortColumns', 'targetFileSize' etc serialized to the clustering job makes sense to me, my question is around the strategy class name itself being written to the file. Is it just for tracking the strategy or will the async clustering job actually make use of this strategy to read our the strategyParams ?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r514290232", "createdAt": "2020-10-29T14:13:19Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieClusteringStrategy.avsc", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNDQ4NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU1MzEwNA==", "bodyText": "Yes, async strategy job would read the strategy to instantiate the class. That strategy object will be used to write clustered records (Example: strategy may use custom partitioner to respect sortColumns)", "url": "https://github.com/apache/hudi/pull/2202#discussion_r515553104", "createdAt": "2020-10-31T23:34:32Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieClusteringStrategy.avsc", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNDQ4NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzYwNjQ1OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieRequestedReplaceMetadata.avsc", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNDo1OTo0MVrOHpufOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQyMzozNjo0NVrOHrq4Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNTMyMQ==", "bodyText": "Same comment, just roll up all the datastructures that you need into the HoodieClusteringPlan, is it possible to merge the clustering plan with the insert-overwrite plan ?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513515321", "createdAt": "2020-10-28T14:59:41Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieRequestedReplaceMetadata.avsc", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3OTI5MA==", "bodyText": "I think this model is flexible for future operations. Replace could be used for other operations such as column pruning and many other use cases. Trying to fit metadata for all these special operations into one format may cause problems (InsertOverwrite doesnt really have a plan today.  so we have to generate plan just for sake of creating it). We also dont want to introduce other top level commits because its lot of work, so i want to keep this as generic as possible.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513679290", "createdAt": "2020-10-28T18:41:25Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieRequestedReplaceMetadata.avsc", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNTMyMQ=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDI5MjE1OQ==", "bodyText": "Keeping the model flexible sounds fair to me, I'm not following how that is connected to introducing other top level commits, could you please expand ?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r514292159", "createdAt": "2020-10-29T14:15:46Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieRequestedReplaceMetadata.avsc", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNTMyMQ=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU1MzMzMA==", "bodyText": "initially we were considering replacecommit, clusteringcommit as separate things. Only reason not to introduce new commit is to keep it simple as post commit metadata looks very similar. I was trying to say that if these were separate top level commits, this file is not needed.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r515553330", "createdAt": "2020-10-31T23:36:45Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieRequestedReplaceMetadata.avsc", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNTMyMQ=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzYwODI2OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNTowMDowMFrOHpugSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQyMzozNzowNFrOHrq4Vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNTU5NA==", "bodyText": "Same comment, please roll this up into HoodieClusteringPlan.avsc", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513515594", "createdAt": "2020-10-28T15:00:00Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MDA1OA==", "bodyText": "I see value in keeping these in separate files. It was suggested to me in earlier reviews as well. Please tell me if there is any value in combining these. We can perhaps add this to coding guidelines.Without that, different reviewers are giving conflicting suggestions.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513680058", "createdAt": "2020-10-28T18:42:51Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNTU5NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDI5NDg3Mw==", "bodyText": "@satishkotha For code reviews, there may be different opinions from different reviewers. We should just treat that as opportunities to weed out finer details and  the idea should be drive consensus on a single solution. We need to take most of these case by case, I think this particular case you want to reuse the building blocks of the different avsc in the future for other plans etc, that makes sense to me.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r514294873", "createdAt": "2020-10-29T14:19:31Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNTU5NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU1MzM2Ng==", "bodyText": "sounds good. thanks", "url": "https://github.com/apache/hudi/pull/2202#discussion_r515553366", "createdAt": "2020-10-31T23:37:04Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUxNTU5NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzYzNjE3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/ClusteringOpDTO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNTowNToyNVrOHpuxsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxODo0MzoxM1rOHp4jlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMDA0OA==", "bodyText": "Alter comments accordingly", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513520048", "createdAt": "2020-10-28T15:05:25Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/ClusteringOpDTO.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.timeline.dto;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+/**\n+ * The data transfer object of compaction.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MDI3OQ==", "bodyText": "Will fix it. thanks", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513680279", "createdAt": "2020-10-28T18:43:13Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/ClusteringOpDTO.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.timeline.dto;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+/**\n+ * The data transfer object of compaction.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMDA0OA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzY0MTczOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/ClusteringOpDTO.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNTowNjoyOVrOHpu1Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNDoyMDoyOFrOHqeHDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMDkyMw==", "bodyText": "What does a fileID mean here ? Is this the lowest representation of the clustering plan, will the clustering plan contain a bunch of ClusteringOpDTO's ?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513520923", "createdAt": "2020-10-28T15:06:29Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/ClusteringOpDTO.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.timeline.dto;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+/**\n+ * The data transfer object of compaction.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class ClusteringOpDTO {\n+\n+  @JsonProperty(\"id\")\n+  private String fileId;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MTgwMg==", "bodyText": "This is only used for RemoteFileSystemView implementation. RemoteFSV uses DTOs to serialize and deserialize the data over wire. One of the APIs filesystem view provides is to list all fileIds that have pending clustering scheduled. So this file contains all the information we need to send over network. It has nothing to do with clustering plan stored in avro format. Its just there to support getFilesInPendingClusteringOperation API", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513681802", "createdAt": "2020-10-28T18:45:44Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/ClusteringOpDTO.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.timeline.dto;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+/**\n+ * The data transfer object of compaction.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class ClusteringOpDTO {\n+\n+  @JsonProperty(\"id\")\n+  private String fileId;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMDkyMw=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDI5NTU2NA==", "bodyText": "Thanks for the explanation.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r514295564", "createdAt": "2020-10-29T14:20:28Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/dto/ClusteringOpDTO.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.timeline.dto;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+/**\n+ * The data transfer object of compaction.\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class ClusteringOpDTO {\n+\n+  @JsonProperty(\"id\")\n+  private String fileId;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMDkyMw=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzY1MTg0OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNTowODoyNVrOHpu7Tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxODo0NjoxNFrOHp4q5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMjUxMA==", "bodyText": "Is this comment the right description for this API ?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513522510", "createdAt": "2020-10-28T15:08:25Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -710,6 +721,40 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract void removePendingCompactionOperations(Stream<Pair<String, CompactionOperation>> operations);\n \n+  /**\n+   * Check if there is an outstanding clustering operation (requested/inflight) scheduled for this file.\n+   *\n+   * @param fgId File-Group Id\n+   * @return true if there is a pending clustering, false otherwise\n+   */\n+  protected abstract boolean isPendingClusteringScheduledForFileId(HoodieFileGroupId fgId);\n+\n+  /**\n+   *  Get pending clustering instant time for specified file group. Return None if file group is not in pending\n+   *  clustering operation.\n+   */\n+  protected abstract Option<HoodieInstant> getPendingClusteringInstant(final HoodieFileGroupId fileGroupId);\n+\n+  /**\n+   * Fetch all file groups in pending clustering.\n+   */\n+  protected abstract Stream<Pair<HoodieFileGroupId, HoodieInstant>> fetchFileGroupsInPendingClustering();\n+\n+  /**\n+   * resets the pending clustering operation and overwrite with the new list.\n+   */\n+  abstract void resetFileGroupsInPendingClustering(Map<HoodieFileGroupId, HoodieInstant> fgIdToInstantMap);\n+\n+  /**\n+   * Add metadata for pending clustering operation to store for given instant.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MjE0OQ==", "bodyText": "I will expand it to explain better", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513682149", "createdAt": "2020-10-28T18:46:14Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -710,6 +721,40 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract void removePendingCompactionOperations(Stream<Pair<String, CompactionOperation>> operations);\n \n+  /**\n+   * Check if there is an outstanding clustering operation (requested/inflight) scheduled for this file.\n+   *\n+   * @param fgId File-Group Id\n+   * @return true if there is a pending clustering, false otherwise\n+   */\n+  protected abstract boolean isPendingClusteringScheduledForFileId(HoodieFileGroupId fgId);\n+\n+  /**\n+   *  Get pending clustering instant time for specified file group. Return None if file group is not in pending\n+   *  clustering operation.\n+   */\n+  protected abstract Option<HoodieInstant> getPendingClusteringInstant(final HoodieFileGroupId fileGroupId);\n+\n+  /**\n+   * Fetch all file groups in pending clustering.\n+   */\n+  protected abstract Stream<Pair<HoodieFileGroupId, HoodieInstant>> fetchFileGroupsInPendingClustering();\n+\n+  /**\n+   * resets the pending clustering operation and overwrite with the new list.\n+   */\n+  abstract void resetFileGroupsInPendingClustering(Map<HoodieFileGroupId, HoodieInstant> fgIdToInstantMap);\n+\n+  /**\n+   * Add metadata for pending clustering operation to store for given instant.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMjUxMA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzY1NDIwOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNTowODo1NFrOHpu8yQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxODo0ODoxOVrOHp4vmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMjg4OQ==", "bodyText": "How will this API be used ?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513522889", "createdAt": "2020-10-28T15:08:54Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -710,6 +721,40 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract void removePendingCompactionOperations(Stream<Pair<String, CompactionOperation>> operations);\n \n+  /**\n+   * Check if there is an outstanding clustering operation (requested/inflight) scheduled for this file.\n+   *\n+   * @param fgId File-Group Id\n+   * @return true if there is a pending clustering, false otherwise\n+   */\n+  protected abstract boolean isPendingClusteringScheduledForFileId(HoodieFileGroupId fgId);\n+\n+  /**\n+   *  Get pending clustering instant time for specified file group. Return None if file group is not in pending\n+   *  clustering operation.\n+   */\n+  protected abstract Option<HoodieInstant> getPendingClusteringInstant(final HoodieFileGroupId fileGroupId);\n+\n+  /**\n+   * Fetch all file groups in pending clustering.\n+   */\n+  protected abstract Stream<Pair<HoodieFileGroupId, HoodieInstant>> fetchFileGroupsInPendingClustering();\n+\n+  /**\n+   * resets the pending clustering operation and overwrite with the new list.\n+   */\n+  abstract void resetFileGroupsInPendingClustering(Map<HoodieFileGroupId, HoodieInstant> fgIdToInstantMap);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MzM1NA==", "bodyText": "This is called from FileSystemView#init method to refresh list of files in pending clustering. The list is used to block updates  HUDI-1354 and scheduling further clustering operations.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513683354", "createdAt": "2020-10-28T18:48:19Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java", "diffHunk": "@@ -710,6 +721,40 @@ private String formatPartitionKey(String partitionStr) {\n    */\n   abstract void removePendingCompactionOperations(Stream<Pair<String, CompactionOperation>> operations);\n \n+  /**\n+   * Check if there is an outstanding clustering operation (requested/inflight) scheduled for this file.\n+   *\n+   * @param fgId File-Group Id\n+   * @return true if there is a pending clustering, false otherwise\n+   */\n+  protected abstract boolean isPendingClusteringScheduledForFileId(HoodieFileGroupId fgId);\n+\n+  /**\n+   *  Get pending clustering instant time for specified file group. Return None if file group is not in pending\n+   *  clustering operation.\n+   */\n+  protected abstract Option<HoodieInstant> getPendingClusteringInstant(final HoodieFileGroupId fileGroupId);\n+\n+  /**\n+   * Fetch all file groups in pending clustering.\n+   */\n+  protected abstract Stream<Pair<HoodieFileGroupId, HoodieInstant>> fetchFileGroupsInPendingClustering();\n+\n+  /**\n+   * resets the pending clustering operation and overwrite with the new list.\n+   */\n+  abstract void resetFileGroupsInPendingClustering(Map<HoodieFileGroupId, HoodieInstant> fgIdToInstantMap);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyMjg4OQ=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzY4Njg4OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNToxNToyMVrOHpvRAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxODo0ODozMVrOHp4wDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyODA2Ng==", "bodyText": "Can we change this loop to java 8 stream style, fileSliceGroups.stream().foreach()..", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513528066", "createdAt": "2020-10-28T15:15:21Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Helper class to generate clustering plan from metadata.\n+ */\n+public class ClusteringUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(ClusteringUtils.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  /**\n+   * Get all pending clustering plans along with their instants.\n+   */\n+  public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    List<HoodieInstant> pendingReplaceInstants =\n+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());\n+    return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))\n+        .filter(Option::isPresent).map(Option::get);\n+  }\n+\n+  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant requestedReplaceInstant) {\n+    try {\n+      Option<byte[]> content = metaClient.getActiveTimeline().getInstantDetails(requestedReplaceInstant);\n+      if (!content.isPresent() || content.get().length == 0) {\n+        // few operations create requested file without any content. Assume these are not clustering\n+        LOG.warn(\"No content found in requested file for instant \" + requestedReplaceInstant);\n+        return Option.empty();\n+      }\n+      HoodieRequestedReplaceMetadata requestedReplaceMetadata = TimelineMetadataUtils.deserializeRequestedReplaceMetadta(content.get());\n+      if (WriteOperationType.CLUSTER.name().equals(requestedReplaceMetadata.getOperationType())) {\n+        return Option.of(Pair.of(requestedReplaceInstant, requestedReplaceMetadata.getClusteringPlan()));\n+      }\n+      return Option.empty();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Error reading clustering plan \" + requestedReplaceInstant.getTimestamp(), e);\n+    }\n+  }\n+\n+  /**\n+   * Get filegroups to pending clustering instant mapping for all pending clustering plans.\n+   * This includes all clustering operattions in 'requested' and 'inflight' states.\n+   */\n+  public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    Stream<Pair<HoodieInstant, HoodieClusteringPlan>> pendingClusteringPlans = getAllPendingClusteringPlans(metaClient);\n+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = pendingClusteringPlans.flatMap(clusteringPlan ->\n+        // get all filegroups in the plan\n+        getFileGroupEntriesInClusteringPlan(clusteringPlan.getLeft(), clusteringPlan.getRight()));\n+\n+    Map<HoodieFileGroupId, HoodieInstant> resultMap = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    LOG.info(\"Found \" + resultMap.size() + \" files in pending clustering operations\");\n+    return resultMap;\n+  }\n+\n+  public static Stream<Pair<HoodieFileGroupId, HoodieInstant>> getFileGroupsInPendingClusteringInstant(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    Stream<HoodieFileGroupId> partitionToFileIdLists = clusteringPlan.getInputGroups().stream().flatMap(ClusteringUtils::getFileGroupsFromClusteringGroup);\n+    return partitionToFileIdLists.map(e -> Pair.of(e, instant));\n+  }\n+\n+  private static Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> getFileGroupEntriesInClusteringPlan(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    return getFileGroupsInPendingClusteringInstant(instant, clusteringPlan).map(entry ->\n+        new AbstractMap.SimpleEntry<>(entry.getLeft(), entry.getRight()));\n+  }\n+\n+  private static Stream<HoodieFileGroupId> getFileGroupsFromClusteringGroup(HoodieClusteringGroup group) {\n+    return group.getSlices().stream().map(slice -> new HoodieFileGroupId(slice.getPartitionPath(), slice.getFileId()));\n+  }\n+\n+  /**\n+   * Create clustering plan from input fileSliceGroups.\n+   */\n+  public static HoodieClusteringPlan createClusteringPlan(String strategyClassName,\n+                                                          Map<String, String> strategyParams,\n+                                                          List<FileSlice>[] fileSliceGroups) {\n+    List<HoodieClusteringGroup> clusteringGroups = new ArrayList<>();\n+    for (int i = 0; i < fileSliceGroups.length; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MzQ3MA==", "bodyText": "Sure. will fix it.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513683470", "createdAt": "2020-10-28T18:48:31Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Helper class to generate clustering plan from metadata.\n+ */\n+public class ClusteringUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(ClusteringUtils.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  /**\n+   * Get all pending clustering plans along with their instants.\n+   */\n+  public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    List<HoodieInstant> pendingReplaceInstants =\n+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());\n+    return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))\n+        .filter(Option::isPresent).map(Option::get);\n+  }\n+\n+  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant requestedReplaceInstant) {\n+    try {\n+      Option<byte[]> content = metaClient.getActiveTimeline().getInstantDetails(requestedReplaceInstant);\n+      if (!content.isPresent() || content.get().length == 0) {\n+        // few operations create requested file without any content. Assume these are not clustering\n+        LOG.warn(\"No content found in requested file for instant \" + requestedReplaceInstant);\n+        return Option.empty();\n+      }\n+      HoodieRequestedReplaceMetadata requestedReplaceMetadata = TimelineMetadataUtils.deserializeRequestedReplaceMetadta(content.get());\n+      if (WriteOperationType.CLUSTER.name().equals(requestedReplaceMetadata.getOperationType())) {\n+        return Option.of(Pair.of(requestedReplaceInstant, requestedReplaceMetadata.getClusteringPlan()));\n+      }\n+      return Option.empty();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Error reading clustering plan \" + requestedReplaceInstant.getTimestamp(), e);\n+    }\n+  }\n+\n+  /**\n+   * Get filegroups to pending clustering instant mapping for all pending clustering plans.\n+   * This includes all clustering operattions in 'requested' and 'inflight' states.\n+   */\n+  public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    Stream<Pair<HoodieInstant, HoodieClusteringPlan>> pendingClusteringPlans = getAllPendingClusteringPlans(metaClient);\n+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = pendingClusteringPlans.flatMap(clusteringPlan ->\n+        // get all filegroups in the plan\n+        getFileGroupEntriesInClusteringPlan(clusteringPlan.getLeft(), clusteringPlan.getRight()));\n+\n+    Map<HoodieFileGroupId, HoodieInstant> resultMap = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    LOG.info(\"Found \" + resultMap.size() + \" files in pending clustering operations\");\n+    return resultMap;\n+  }\n+\n+  public static Stream<Pair<HoodieFileGroupId, HoodieInstant>> getFileGroupsInPendingClusteringInstant(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    Stream<HoodieFileGroupId> partitionToFileIdLists = clusteringPlan.getInputGroups().stream().flatMap(ClusteringUtils::getFileGroupsFromClusteringGroup);\n+    return partitionToFileIdLists.map(e -> Pair.of(e, instant));\n+  }\n+\n+  private static Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> getFileGroupEntriesInClusteringPlan(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    return getFileGroupsInPendingClusteringInstant(instant, clusteringPlan).map(entry ->\n+        new AbstractMap.SimpleEntry<>(entry.getLeft(), entry.getRight()));\n+  }\n+\n+  private static Stream<HoodieFileGroupId> getFileGroupsFromClusteringGroup(HoodieClusteringGroup group) {\n+    return group.getSlices().stream().map(slice -> new HoodieFileGroupId(slice.getPartitionPath(), slice.getFileId()));\n+  }\n+\n+  /**\n+   * Create clustering plan from input fileSliceGroups.\n+   */\n+  public static HoodieClusteringPlan createClusteringPlan(String strategyClassName,\n+                                                          Map<String, String> strategyParams,\n+                                                          List<FileSlice>[] fileSliceGroups) {\n+    List<HoodieClusteringGroup> clusteringGroups = new ArrayList<>();\n+    for (int i = 0; i < fileSliceGroups.length; i++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyODA2Ng=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzY5Mzk3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNToxNjo0MlrOHpvVVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQyMDowODozMlrOHp7gHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyOTE3NA==", "bodyText": "What happens if we don't setExtraMetadata to new Hashmap ? In general, we should only set the params which we need to and rest of the uninitialized params should be handled internally in the HoodieClusteringPlan.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513529174", "createdAt": "2020-10-28T15:16:42Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Helper class to generate clustering plan from metadata.\n+ */\n+public class ClusteringUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(ClusteringUtils.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  /**\n+   * Get all pending clustering plans along with their instants.\n+   */\n+  public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    List<HoodieInstant> pendingReplaceInstants =\n+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());\n+    return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))\n+        .filter(Option::isPresent).map(Option::get);\n+  }\n+\n+  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant requestedReplaceInstant) {\n+    try {\n+      Option<byte[]> content = metaClient.getActiveTimeline().getInstantDetails(requestedReplaceInstant);\n+      if (!content.isPresent() || content.get().length == 0) {\n+        // few operations create requested file without any content. Assume these are not clustering\n+        LOG.warn(\"No content found in requested file for instant \" + requestedReplaceInstant);\n+        return Option.empty();\n+      }\n+      HoodieRequestedReplaceMetadata requestedReplaceMetadata = TimelineMetadataUtils.deserializeRequestedReplaceMetadta(content.get());\n+      if (WriteOperationType.CLUSTER.name().equals(requestedReplaceMetadata.getOperationType())) {\n+        return Option.of(Pair.of(requestedReplaceInstant, requestedReplaceMetadata.getClusteringPlan()));\n+      }\n+      return Option.empty();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Error reading clustering plan \" + requestedReplaceInstant.getTimestamp(), e);\n+    }\n+  }\n+\n+  /**\n+   * Get filegroups to pending clustering instant mapping for all pending clustering plans.\n+   * This includes all clustering operattions in 'requested' and 'inflight' states.\n+   */\n+  public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    Stream<Pair<HoodieInstant, HoodieClusteringPlan>> pendingClusteringPlans = getAllPendingClusteringPlans(metaClient);\n+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = pendingClusteringPlans.flatMap(clusteringPlan ->\n+        // get all filegroups in the plan\n+        getFileGroupEntriesInClusteringPlan(clusteringPlan.getLeft(), clusteringPlan.getRight()));\n+\n+    Map<HoodieFileGroupId, HoodieInstant> resultMap = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    LOG.info(\"Found \" + resultMap.size() + \" files in pending clustering operations\");\n+    return resultMap;\n+  }\n+\n+  public static Stream<Pair<HoodieFileGroupId, HoodieInstant>> getFileGroupsInPendingClusteringInstant(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    Stream<HoodieFileGroupId> partitionToFileIdLists = clusteringPlan.getInputGroups().stream().flatMap(ClusteringUtils::getFileGroupsFromClusteringGroup);\n+    return partitionToFileIdLists.map(e -> Pair.of(e, instant));\n+  }\n+\n+  private static Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> getFileGroupEntriesInClusteringPlan(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    return getFileGroupsInPendingClusteringInstant(instant, clusteringPlan).map(entry ->\n+        new AbstractMap.SimpleEntry<>(entry.getLeft(), entry.getRight()));\n+  }\n+\n+  private static Stream<HoodieFileGroupId> getFileGroupsFromClusteringGroup(HoodieClusteringGroup group) {\n+    return group.getSlices().stream().map(slice -> new HoodieFileGroupId(slice.getPartitionPath(), slice.getFileId()));\n+  }\n+\n+  /**\n+   * Create clustering plan from input fileSliceGroups.\n+   */\n+  public static HoodieClusteringPlan createClusteringPlan(String strategyClassName,\n+                                                          Map<String, String> strategyParams,\n+                                                          List<FileSlice>[] fileSliceGroups) {\n+    List<HoodieClusteringGroup> clusteringGroups = new ArrayList<>();\n+    for (int i = 0; i < fileSliceGroups.length; i++) {\n+      List<FileSlice> fileSliceGroup = fileSliceGroups[i];\n+      Map<String, Double> groupMetrics = buildMetrics(fileSliceGroup);\n+      List<HoodieSliceInfo> sliceInfos = getFileSliceInfo(fileSliceGroup);\n+      clusteringGroups.add(HoodieClusteringGroup.newBuilder().setSlices(sliceInfos).setMetrics(groupMetrics).build());\n+    }\n+\n+    HoodieClusteringStrategy strategy = HoodieClusteringStrategy.newBuilder()\n+        .setStrategyClassName(strategyClassName).setStrategyParams(strategyParams)\n+        .build();\n+\n+    HoodieClusteringPlan plan = HoodieClusteringPlan.newBuilder()\n+        .setInputGroups(clusteringGroups)\n+        .setExtraMetadata(new HashMap<>())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzcyODU0Mg==", "bodyText": "fixed", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513728542", "createdAt": "2020-10-28T20:08:32Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Helper class to generate clustering plan from metadata.\n+ */\n+public class ClusteringUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(ClusteringUtils.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  /**\n+   * Get all pending clustering plans along with their instants.\n+   */\n+  public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    List<HoodieInstant> pendingReplaceInstants =\n+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());\n+    return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))\n+        .filter(Option::isPresent).map(Option::get);\n+  }\n+\n+  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant requestedReplaceInstant) {\n+    try {\n+      Option<byte[]> content = metaClient.getActiveTimeline().getInstantDetails(requestedReplaceInstant);\n+      if (!content.isPresent() || content.get().length == 0) {\n+        // few operations create requested file without any content. Assume these are not clustering\n+        LOG.warn(\"No content found in requested file for instant \" + requestedReplaceInstant);\n+        return Option.empty();\n+      }\n+      HoodieRequestedReplaceMetadata requestedReplaceMetadata = TimelineMetadataUtils.deserializeRequestedReplaceMetadta(content.get());\n+      if (WriteOperationType.CLUSTER.name().equals(requestedReplaceMetadata.getOperationType())) {\n+        return Option.of(Pair.of(requestedReplaceInstant, requestedReplaceMetadata.getClusteringPlan()));\n+      }\n+      return Option.empty();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Error reading clustering plan \" + requestedReplaceInstant.getTimestamp(), e);\n+    }\n+  }\n+\n+  /**\n+   * Get filegroups to pending clustering instant mapping for all pending clustering plans.\n+   * This includes all clustering operattions in 'requested' and 'inflight' states.\n+   */\n+  public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    Stream<Pair<HoodieInstant, HoodieClusteringPlan>> pendingClusteringPlans = getAllPendingClusteringPlans(metaClient);\n+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = pendingClusteringPlans.flatMap(clusteringPlan ->\n+        // get all filegroups in the plan\n+        getFileGroupEntriesInClusteringPlan(clusteringPlan.getLeft(), clusteringPlan.getRight()));\n+\n+    Map<HoodieFileGroupId, HoodieInstant> resultMap = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    LOG.info(\"Found \" + resultMap.size() + \" files in pending clustering operations\");\n+    return resultMap;\n+  }\n+\n+  public static Stream<Pair<HoodieFileGroupId, HoodieInstant>> getFileGroupsInPendingClusteringInstant(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    Stream<HoodieFileGroupId> partitionToFileIdLists = clusteringPlan.getInputGroups().stream().flatMap(ClusteringUtils::getFileGroupsFromClusteringGroup);\n+    return partitionToFileIdLists.map(e -> Pair.of(e, instant));\n+  }\n+\n+  private static Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> getFileGroupEntriesInClusteringPlan(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    return getFileGroupsInPendingClusteringInstant(instant, clusteringPlan).map(entry ->\n+        new AbstractMap.SimpleEntry<>(entry.getLeft(), entry.getRight()));\n+  }\n+\n+  private static Stream<HoodieFileGroupId> getFileGroupsFromClusteringGroup(HoodieClusteringGroup group) {\n+    return group.getSlices().stream().map(slice -> new HoodieFileGroupId(slice.getPartitionPath(), slice.getFileId()));\n+  }\n+\n+  /**\n+   * Create clustering plan from input fileSliceGroups.\n+   */\n+  public static HoodieClusteringPlan createClusteringPlan(String strategyClassName,\n+                                                          Map<String, String> strategyParams,\n+                                                          List<FileSlice>[] fileSliceGroups) {\n+    List<HoodieClusteringGroup> clusteringGroups = new ArrayList<>();\n+    for (int i = 0; i < fileSliceGroups.length; i++) {\n+      List<FileSlice> fileSliceGroup = fileSliceGroups[i];\n+      Map<String, Double> groupMetrics = buildMetrics(fileSliceGroup);\n+      List<HoodieSliceInfo> sliceInfos = getFileSliceInfo(fileSliceGroup);\n+      clusteringGroups.add(HoodieClusteringGroup.newBuilder().setSlices(sliceInfos).setMetrics(groupMetrics).build());\n+    }\n+\n+    HoodieClusteringStrategy strategy = HoodieClusteringStrategy.newBuilder()\n+        .setStrategyClassName(strategyClassName).setStrategyParams(strategyParams)\n+        .build();\n+\n+    HoodieClusteringPlan plan = HoodieClusteringPlan.newBuilder()\n+        .setInputGroups(clusteringGroups)\n+        .setExtraMetadata(new HashMap<>())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyOTE3NA=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzY5NjY3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNToxNzoxM1rOHpvW-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQyMDowOToxOVrOHp7hyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyOTU5NQ==", "bodyText": "Should the dataFilePath be empty string or empty Option ?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513529595", "createdAt": "2020-10-28T15:17:13Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Helper class to generate clustering plan from metadata.\n+ */\n+public class ClusteringUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(ClusteringUtils.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  /**\n+   * Get all pending clustering plans along with their instants.\n+   */\n+  public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    List<HoodieInstant> pendingReplaceInstants =\n+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());\n+    return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))\n+        .filter(Option::isPresent).map(Option::get);\n+  }\n+\n+  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant requestedReplaceInstant) {\n+    try {\n+      Option<byte[]> content = metaClient.getActiveTimeline().getInstantDetails(requestedReplaceInstant);\n+      if (!content.isPresent() || content.get().length == 0) {\n+        // few operations create requested file without any content. Assume these are not clustering\n+        LOG.warn(\"No content found in requested file for instant \" + requestedReplaceInstant);\n+        return Option.empty();\n+      }\n+      HoodieRequestedReplaceMetadata requestedReplaceMetadata = TimelineMetadataUtils.deserializeRequestedReplaceMetadta(content.get());\n+      if (WriteOperationType.CLUSTER.name().equals(requestedReplaceMetadata.getOperationType())) {\n+        return Option.of(Pair.of(requestedReplaceInstant, requestedReplaceMetadata.getClusteringPlan()));\n+      }\n+      return Option.empty();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Error reading clustering plan \" + requestedReplaceInstant.getTimestamp(), e);\n+    }\n+  }\n+\n+  /**\n+   * Get filegroups to pending clustering instant mapping for all pending clustering plans.\n+   * This includes all clustering operattions in 'requested' and 'inflight' states.\n+   */\n+  public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    Stream<Pair<HoodieInstant, HoodieClusteringPlan>> pendingClusteringPlans = getAllPendingClusteringPlans(metaClient);\n+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = pendingClusteringPlans.flatMap(clusteringPlan ->\n+        // get all filegroups in the plan\n+        getFileGroupEntriesInClusteringPlan(clusteringPlan.getLeft(), clusteringPlan.getRight()));\n+\n+    Map<HoodieFileGroupId, HoodieInstant> resultMap = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    LOG.info(\"Found \" + resultMap.size() + \" files in pending clustering operations\");\n+    return resultMap;\n+  }\n+\n+  public static Stream<Pair<HoodieFileGroupId, HoodieInstant>> getFileGroupsInPendingClusteringInstant(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    Stream<HoodieFileGroupId> partitionToFileIdLists = clusteringPlan.getInputGroups().stream().flatMap(ClusteringUtils::getFileGroupsFromClusteringGroup);\n+    return partitionToFileIdLists.map(e -> Pair.of(e, instant));\n+  }\n+\n+  private static Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> getFileGroupEntriesInClusteringPlan(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    return getFileGroupsInPendingClusteringInstant(instant, clusteringPlan).map(entry ->\n+        new AbstractMap.SimpleEntry<>(entry.getLeft(), entry.getRight()));\n+  }\n+\n+  private static Stream<HoodieFileGroupId> getFileGroupsFromClusteringGroup(HoodieClusteringGroup group) {\n+    return group.getSlices().stream().map(slice -> new HoodieFileGroupId(slice.getPartitionPath(), slice.getFileId()));\n+  }\n+\n+  /**\n+   * Create clustering plan from input fileSliceGroups.\n+   */\n+  public static HoodieClusteringPlan createClusteringPlan(String strategyClassName,\n+                                                          Map<String, String> strategyParams,\n+                                                          List<FileSlice>[] fileSliceGroups) {\n+    List<HoodieClusteringGroup> clusteringGroups = new ArrayList<>();\n+    for (int i = 0; i < fileSliceGroups.length; i++) {\n+      List<FileSlice> fileSliceGroup = fileSliceGroups[i];\n+      Map<String, Double> groupMetrics = buildMetrics(fileSliceGroup);\n+      List<HoodieSliceInfo> sliceInfos = getFileSliceInfo(fileSliceGroup);\n+      clusteringGroups.add(HoodieClusteringGroup.newBuilder().setSlices(sliceInfos).setMetrics(groupMetrics).build());\n+    }\n+\n+    HoodieClusteringStrategy strategy = HoodieClusteringStrategy.newBuilder()\n+        .setStrategyClassName(strategyClassName).setStrategyParams(strategyParams)\n+        .build();\n+\n+    HoodieClusteringPlan plan = HoodieClusteringPlan.newBuilder()\n+        .setInputGroups(clusteringGroups)\n+        .setExtraMetadata(new HashMap<>())\n+        .setStrategy(strategy)\n+        .build();\n+\n+    return plan;\n+  }\n+\n+  private static List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    FileSlice slice1 = slices.get(0);\n+    System.out.println(slice1.getLogFiles().count());\n+    System.out.println(slice1.getBaseFile().get());\n+    System.out.println(slice1.getPartitionPath());\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzcyODk3MA==", "bodyText": "Because this is avro, I'm not sure if Option would work. Changed it to null to match with compaction.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513728970", "createdAt": "2020-10-28T20:09:19Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Helper class to generate clustering plan from metadata.\n+ */\n+public class ClusteringUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(ClusteringUtils.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  /**\n+   * Get all pending clustering plans along with their instants.\n+   */\n+  public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    List<HoodieInstant> pendingReplaceInstants =\n+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());\n+    return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))\n+        .filter(Option::isPresent).map(Option::get);\n+  }\n+\n+  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant requestedReplaceInstant) {\n+    try {\n+      Option<byte[]> content = metaClient.getActiveTimeline().getInstantDetails(requestedReplaceInstant);\n+      if (!content.isPresent() || content.get().length == 0) {\n+        // few operations create requested file without any content. Assume these are not clustering\n+        LOG.warn(\"No content found in requested file for instant \" + requestedReplaceInstant);\n+        return Option.empty();\n+      }\n+      HoodieRequestedReplaceMetadata requestedReplaceMetadata = TimelineMetadataUtils.deserializeRequestedReplaceMetadta(content.get());\n+      if (WriteOperationType.CLUSTER.name().equals(requestedReplaceMetadata.getOperationType())) {\n+        return Option.of(Pair.of(requestedReplaceInstant, requestedReplaceMetadata.getClusteringPlan()));\n+      }\n+      return Option.empty();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Error reading clustering plan \" + requestedReplaceInstant.getTimestamp(), e);\n+    }\n+  }\n+\n+  /**\n+   * Get filegroups to pending clustering instant mapping for all pending clustering plans.\n+   * This includes all clustering operattions in 'requested' and 'inflight' states.\n+   */\n+  public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    Stream<Pair<HoodieInstant, HoodieClusteringPlan>> pendingClusteringPlans = getAllPendingClusteringPlans(metaClient);\n+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = pendingClusteringPlans.flatMap(clusteringPlan ->\n+        // get all filegroups in the plan\n+        getFileGroupEntriesInClusteringPlan(clusteringPlan.getLeft(), clusteringPlan.getRight()));\n+\n+    Map<HoodieFileGroupId, HoodieInstant> resultMap = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    LOG.info(\"Found \" + resultMap.size() + \" files in pending clustering operations\");\n+    return resultMap;\n+  }\n+\n+  public static Stream<Pair<HoodieFileGroupId, HoodieInstant>> getFileGroupsInPendingClusteringInstant(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    Stream<HoodieFileGroupId> partitionToFileIdLists = clusteringPlan.getInputGroups().stream().flatMap(ClusteringUtils::getFileGroupsFromClusteringGroup);\n+    return partitionToFileIdLists.map(e -> Pair.of(e, instant));\n+  }\n+\n+  private static Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> getFileGroupEntriesInClusteringPlan(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    return getFileGroupsInPendingClusteringInstant(instant, clusteringPlan).map(entry ->\n+        new AbstractMap.SimpleEntry<>(entry.getLeft(), entry.getRight()));\n+  }\n+\n+  private static Stream<HoodieFileGroupId> getFileGroupsFromClusteringGroup(HoodieClusteringGroup group) {\n+    return group.getSlices().stream().map(slice -> new HoodieFileGroupId(slice.getPartitionPath(), slice.getFileId()));\n+  }\n+\n+  /**\n+   * Create clustering plan from input fileSliceGroups.\n+   */\n+  public static HoodieClusteringPlan createClusteringPlan(String strategyClassName,\n+                                                          Map<String, String> strategyParams,\n+                                                          List<FileSlice>[] fileSliceGroups) {\n+    List<HoodieClusteringGroup> clusteringGroups = new ArrayList<>();\n+    for (int i = 0; i < fileSliceGroups.length; i++) {\n+      List<FileSlice> fileSliceGroup = fileSliceGroups[i];\n+      Map<String, Double> groupMetrics = buildMetrics(fileSliceGroup);\n+      List<HoodieSliceInfo> sliceInfos = getFileSliceInfo(fileSliceGroup);\n+      clusteringGroups.add(HoodieClusteringGroup.newBuilder().setSlices(sliceInfos).setMetrics(groupMetrics).build());\n+    }\n+\n+    HoodieClusteringStrategy strategy = HoodieClusteringStrategy.newBuilder()\n+        .setStrategyClassName(strategyClassName).setStrategyParams(strategyParams)\n+        .build();\n+\n+    HoodieClusteringPlan plan = HoodieClusteringPlan.newBuilder()\n+        .setInputGroups(clusteringGroups)\n+        .setExtraMetadata(new HashMap<>())\n+        .setStrategy(strategy)\n+        .build();\n+\n+    return plan;\n+  }\n+\n+  private static List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    FileSlice slice1 = slices.get(0);\n+    System.out.println(slice1.getLogFiles().count());\n+    System.out.println(slice1.getBaseFile().get());\n+    System.out.println(slice1.getPartitionPath());\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUyOTU5NQ=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 156}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNzY5OTMwOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNToxNzo1MFrOHpvYqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQyMDowOTozMFrOHp7iMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUzMDAyNQ==", "bodyText": "Same question here for data file path empty", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513530025", "createdAt": "2020-10-28T15:17:50Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Helper class to generate clustering plan from metadata.\n+ */\n+public class ClusteringUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(ClusteringUtils.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  /**\n+   * Get all pending clustering plans along with their instants.\n+   */\n+  public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    List<HoodieInstant> pendingReplaceInstants =\n+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());\n+    return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))\n+        .filter(Option::isPresent).map(Option::get);\n+  }\n+\n+  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant requestedReplaceInstant) {\n+    try {\n+      Option<byte[]> content = metaClient.getActiveTimeline().getInstantDetails(requestedReplaceInstant);\n+      if (!content.isPresent() || content.get().length == 0) {\n+        // few operations create requested file without any content. Assume these are not clustering\n+        LOG.warn(\"No content found in requested file for instant \" + requestedReplaceInstant);\n+        return Option.empty();\n+      }\n+      HoodieRequestedReplaceMetadata requestedReplaceMetadata = TimelineMetadataUtils.deserializeRequestedReplaceMetadta(content.get());\n+      if (WriteOperationType.CLUSTER.name().equals(requestedReplaceMetadata.getOperationType())) {\n+        return Option.of(Pair.of(requestedReplaceInstant, requestedReplaceMetadata.getClusteringPlan()));\n+      }\n+      return Option.empty();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Error reading clustering plan \" + requestedReplaceInstant.getTimestamp(), e);\n+    }\n+  }\n+\n+  /**\n+   * Get filegroups to pending clustering instant mapping for all pending clustering plans.\n+   * This includes all clustering operattions in 'requested' and 'inflight' states.\n+   */\n+  public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    Stream<Pair<HoodieInstant, HoodieClusteringPlan>> pendingClusteringPlans = getAllPendingClusteringPlans(metaClient);\n+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = pendingClusteringPlans.flatMap(clusteringPlan ->\n+        // get all filegroups in the plan\n+        getFileGroupEntriesInClusteringPlan(clusteringPlan.getLeft(), clusteringPlan.getRight()));\n+\n+    Map<HoodieFileGroupId, HoodieInstant> resultMap = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    LOG.info(\"Found \" + resultMap.size() + \" files in pending clustering operations\");\n+    return resultMap;\n+  }\n+\n+  public static Stream<Pair<HoodieFileGroupId, HoodieInstant>> getFileGroupsInPendingClusteringInstant(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    Stream<HoodieFileGroupId> partitionToFileIdLists = clusteringPlan.getInputGroups().stream().flatMap(ClusteringUtils::getFileGroupsFromClusteringGroup);\n+    return partitionToFileIdLists.map(e -> Pair.of(e, instant));\n+  }\n+\n+  private static Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> getFileGroupEntriesInClusteringPlan(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    return getFileGroupsInPendingClusteringInstant(instant, clusteringPlan).map(entry ->\n+        new AbstractMap.SimpleEntry<>(entry.getLeft(), entry.getRight()));\n+  }\n+\n+  private static Stream<HoodieFileGroupId> getFileGroupsFromClusteringGroup(HoodieClusteringGroup group) {\n+    return group.getSlices().stream().map(slice -> new HoodieFileGroupId(slice.getPartitionPath(), slice.getFileId()));\n+  }\n+\n+  /**\n+   * Create clustering plan from input fileSliceGroups.\n+   */\n+  public static HoodieClusteringPlan createClusteringPlan(String strategyClassName,\n+                                                          Map<String, String> strategyParams,\n+                                                          List<FileSlice>[] fileSliceGroups) {\n+    List<HoodieClusteringGroup> clusteringGroups = new ArrayList<>();\n+    for (int i = 0; i < fileSliceGroups.length; i++) {\n+      List<FileSlice> fileSliceGroup = fileSliceGroups[i];\n+      Map<String, Double> groupMetrics = buildMetrics(fileSliceGroup);\n+      List<HoodieSliceInfo> sliceInfos = getFileSliceInfo(fileSliceGroup);\n+      clusteringGroups.add(HoodieClusteringGroup.newBuilder().setSlices(sliceInfos).setMetrics(groupMetrics).build());\n+    }\n+\n+    HoodieClusteringStrategy strategy = HoodieClusteringStrategy.newBuilder()\n+        .setStrategyClassName(strategyClassName).setStrategyParams(strategyParams)\n+        .build();\n+\n+    HoodieClusteringPlan plan = HoodieClusteringPlan.newBuilder()\n+        .setInputGroups(clusteringGroups)\n+        .setExtraMetadata(new HashMap<>())\n+        .setStrategy(strategy)\n+        .build();\n+\n+    return plan;\n+  }\n+\n+  private static List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    FileSlice slice1 = slices.get(0);\n+    System.out.println(slice1.getLogFiles().count());\n+    System.out.println(slice1.getBaseFile().get());\n+    System.out.println(slice1.getPartitionPath());\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))\n+        .setDeltaFilePaths(slice.getLogFiles().map(f -> f.getPath().getName()).collect(Collectors.toList()))\n+        .setBootstrapFilePath(slice.getBaseFile().map(bf -> bf.getBootstrapBaseFile().map(bbf -> bbf.getPath()).orElse(\"\")).orElse(\"\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzcyOTA3Mg==", "bodyText": "Changed it to null to match with compaction.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r513729072", "createdAt": "2020-10-28T20:09:30Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ClusteringUtils.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Helper class to generate clustering plan from metadata.\n+ */\n+public class ClusteringUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(ClusteringUtils.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  /**\n+   * Get all pending clustering plans along with their instants.\n+   */\n+  public static Stream<Pair<HoodieInstant, HoodieClusteringPlan>> getAllPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    List<HoodieInstant> pendingReplaceInstants =\n+        metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().collect(Collectors.toList());\n+    return pendingReplaceInstants.stream().map(instant -> getClusteringPlan(metaClient, instant))\n+        .filter(Option::isPresent).map(Option::get);\n+  }\n+\n+  public static Option<Pair<HoodieInstant, HoodieClusteringPlan>> getClusteringPlan(HoodieTableMetaClient metaClient, HoodieInstant requestedReplaceInstant) {\n+    try {\n+      Option<byte[]> content = metaClient.getActiveTimeline().getInstantDetails(requestedReplaceInstant);\n+      if (!content.isPresent() || content.get().length == 0) {\n+        // few operations create requested file without any content. Assume these are not clustering\n+        LOG.warn(\"No content found in requested file for instant \" + requestedReplaceInstant);\n+        return Option.empty();\n+      }\n+      HoodieRequestedReplaceMetadata requestedReplaceMetadata = TimelineMetadataUtils.deserializeRequestedReplaceMetadta(content.get());\n+      if (WriteOperationType.CLUSTER.name().equals(requestedReplaceMetadata.getOperationType())) {\n+        return Option.of(Pair.of(requestedReplaceInstant, requestedReplaceMetadata.getClusteringPlan()));\n+      }\n+      return Option.empty();\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Error reading clustering plan \" + requestedReplaceInstant.getTimestamp(), e);\n+    }\n+  }\n+\n+  /**\n+   * Get filegroups to pending clustering instant mapping for all pending clustering plans.\n+   * This includes all clustering operattions in 'requested' and 'inflight' states.\n+   */\n+  public static Map<HoodieFileGroupId, HoodieInstant> getAllFileGroupsInPendingClusteringPlans(\n+      HoodieTableMetaClient metaClient) {\n+    Stream<Pair<HoodieInstant, HoodieClusteringPlan>> pendingClusteringPlans = getAllPendingClusteringPlans(metaClient);\n+    Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> resultStream = pendingClusteringPlans.flatMap(clusteringPlan ->\n+        // get all filegroups in the plan\n+        getFileGroupEntriesInClusteringPlan(clusteringPlan.getLeft(), clusteringPlan.getRight()));\n+\n+    Map<HoodieFileGroupId, HoodieInstant> resultMap = resultStream.collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    LOG.info(\"Found \" + resultMap.size() + \" files in pending clustering operations\");\n+    return resultMap;\n+  }\n+\n+  public static Stream<Pair<HoodieFileGroupId, HoodieInstant>> getFileGroupsInPendingClusteringInstant(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    Stream<HoodieFileGroupId> partitionToFileIdLists = clusteringPlan.getInputGroups().stream().flatMap(ClusteringUtils::getFileGroupsFromClusteringGroup);\n+    return partitionToFileIdLists.map(e -> Pair.of(e, instant));\n+  }\n+\n+  private static Stream<Map.Entry<HoodieFileGroupId, HoodieInstant>> getFileGroupEntriesInClusteringPlan(\n+      HoodieInstant instant, HoodieClusteringPlan clusteringPlan) {\n+    return getFileGroupsInPendingClusteringInstant(instant, clusteringPlan).map(entry ->\n+        new AbstractMap.SimpleEntry<>(entry.getLeft(), entry.getRight()));\n+  }\n+\n+  private static Stream<HoodieFileGroupId> getFileGroupsFromClusteringGroup(HoodieClusteringGroup group) {\n+    return group.getSlices().stream().map(slice -> new HoodieFileGroupId(slice.getPartitionPath(), slice.getFileId()));\n+  }\n+\n+  /**\n+   * Create clustering plan from input fileSliceGroups.\n+   */\n+  public static HoodieClusteringPlan createClusteringPlan(String strategyClassName,\n+                                                          Map<String, String> strategyParams,\n+                                                          List<FileSlice>[] fileSliceGroups) {\n+    List<HoodieClusteringGroup> clusteringGroups = new ArrayList<>();\n+    for (int i = 0; i < fileSliceGroups.length; i++) {\n+      List<FileSlice> fileSliceGroup = fileSliceGroups[i];\n+      Map<String, Double> groupMetrics = buildMetrics(fileSliceGroup);\n+      List<HoodieSliceInfo> sliceInfos = getFileSliceInfo(fileSliceGroup);\n+      clusteringGroups.add(HoodieClusteringGroup.newBuilder().setSlices(sliceInfos).setMetrics(groupMetrics).build());\n+    }\n+\n+    HoodieClusteringStrategy strategy = HoodieClusteringStrategy.newBuilder()\n+        .setStrategyClassName(strategyClassName).setStrategyParams(strategyParams)\n+        .build();\n+\n+    HoodieClusteringPlan plan = HoodieClusteringPlan.newBuilder()\n+        .setInputGroups(clusteringGroups)\n+        .setExtraMetadata(new HashMap<>())\n+        .setStrategy(strategy)\n+        .build();\n+\n+    return plan;\n+  }\n+\n+  private static List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    FileSlice slice1 = slices.get(0);\n+    System.out.println(slice1.getLogFiles().count());\n+    System.out.println(slice1.getBaseFile().get());\n+    System.out.println(slice1.getPartitionPath());\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))\n+        .setDeltaFilePaths(slice.getLogFiles().map(f -> f.getPath().getName()).collect(Collectors.toList()))\n+        .setBootstrapFilePath(slice.getBaseFile().map(bf -> bf.getBootstrapBaseFile().map(bbf -> bbf.getPath()).orElse(\"\")).orElse(\"\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUzMDAyNQ=="}, "originalCommit": {"oid": "5071687bfd54c1645eb6770398290e9712eb3e0d"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTA3NDIzOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoxMDozOFrOHs4k4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNjo0MzoyOVrOHuMNgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyNjMzNg==", "bodyText": "I wonder if we should start out with the assumption of having all files in a given partition. Does this simplify anything in the design now? I would love to implement this generically right away, otherwise we will spend cycles thinking about migrations, eetc etc down the line.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r516826336", "createdAt": "2020-11-03T17:10:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieClusteringGroup\",\n+   \"type\":\"record\",\n+   \"fields\":[\n+      {\n+         /* Group of files that needs to merged. All the slices in a group will belong to same partition initially.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyNzY5OA==", "bodyText": "or you are just suggesting that the clustreing strategy currently only does it within partitions? (totally reasonable approach!)", "url": "https://github.com/apache/hudi/pull/2202#discussion_r516827698", "createdAt": "2020-11-03T17:12:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieClusteringGroup\",\n+   \"type\":\"record\",\n+   \"fields\":[\n+      {\n+         /* Group of files that needs to merged. All the slices in a group will belong to same partition initially.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyNjMzNg=="}, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc3OTgyMw==", "bodyText": "Yes, strategy only groups within a partition. But the metadata is flexible in case we need to implement more complex disk layouts in future.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r517779823", "createdAt": "2020-11-05T04:02:14Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieClusteringGroup\",\n+   \"type\":\"record\",\n+   \"fields\":[\n+      {\n+         /* Group of files that needs to merged. All the slices in a group will belong to same partition initially.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyNjMzNg=="}, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE5NjYxMA==", "bodyText": "My concern was mostly about metadata being able to model that. So we good here", "url": "https://github.com/apache/hudi/pull/2202#discussion_r518196610", "createdAt": "2020-11-05T16:43:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieClusteringGroup\",\n+   \"type\":\"record\",\n+   \"fields\":[\n+      {\n+         /* Group of files that needs to merged. All the slices in a group will belong to same partition initially.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyNjMzNg=="}, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTA3OTQ3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoxMjowNFrOHs4oOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNjo0OTozOVrOHuMfNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyNzE5NA==", "bodyText": "this can be fetched from the bootstrap index as well right? its immutable anyway?", "url": "https://github.com/apache/hudi/pull/2202#discussion_r516827194", "createdAt": "2020-11-03T17:12:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieSliceInfo\",\n+   \"fields\":[\n+      {\n+        \"name\":\"dataFilePath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"deltaFilePaths\",\n+        \"type\":[\"null\", {\n+           \"type\":\"array\",\n+           \"items\":\"string\"\n+        }],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"fileId\",\n+        \"type\":[\"null\",\"string\"]\n+      },\n+      {\n+        \"name\":\"partitionPath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"bootstrapFilePath\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc4MDI5NA==", "bodyText": "I tried to keep  sliceInfo similar to compaction. But I can remove it, if you think we can change the approach to lookup from index.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r517780294", "createdAt": "2020-11-05T04:04:31Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieSliceInfo\",\n+   \"fields\":[\n+      {\n+        \"name\":\"dataFilePath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"deltaFilePaths\",\n+        \"type\":[\"null\", {\n+           \"type\":\"array\",\n+           \"items\":\"string\"\n+        }],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"fileId\",\n+        \"type\":[\"null\",\"string\"]\n+      },\n+      {\n+        \"name\":\"partitionPath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"bootstrapFilePath\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyNzE5NA=="}, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODIwMTE0MA==", "bodyText": "its okay. I recall some discussion with @bvaradar on why we did that. but blanking out now. let's just go with this as-is", "url": "https://github.com/apache/hudi/pull/2202#discussion_r518201140", "createdAt": "2020-11-05T16:49:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieSliceInfo\",\n+   \"fields\":[\n+      {\n+        \"name\":\"dataFilePath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"deltaFilePaths\",\n+        \"type\":[\"null\", {\n+           \"type\":\"array\",\n+           \"items\":\"string\"\n+        }],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"fileId\",\n+        \"type\":[\"null\",\"string\"]\n+      },\n+      {\n+        \"name\":\"partitionPath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"bootstrapFilePath\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyNzE5NA=="}, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTA4Njc2OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoxNDowMlrOHs4s4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwNDowNjoyN1rOHty08A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyODM4Ng==", "bodyText": "lets ensure this also contains a version field? in fact, any of these new avsc should have a version at the start preferably, so we can evolve.", "url": "https://github.com/apache/hudi/pull/2202#discussion_r516828386", "createdAt": "2020-11-03T17:14:02Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieSliceInfo\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzc4MDcyMA==", "bodyText": "Fixed", "url": "https://github.com/apache/hudi/pull/2202#discussion_r517780720", "createdAt": "2020-11-05T04:06:27Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieSliceInfo\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgyODM4Ng=="}, "originalCommit": {"oid": "d20e11d3e47be7af8a62d5c3f9cd0efa500adce3"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NzkwNzE3OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNjo0NDozNVrOHuMQ5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNjo0ODozMFrOHuMcCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE5NzQ3Ng==", "bodyText": "Can we add this as the first field (so no issues with reordering etc) we should always be able to read this", "url": "https://github.com/apache/hudi/pull/2202#discussion_r518197476", "createdAt": "2020-11-05T16:44:35Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieSliceInfo\",\n+   \"fields\":[\n+      {\n+        \"name\":\"dataFilePath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"deltaFilePaths\",\n+        \"type\":[\"null\", {\n+           \"type\":\"array\",\n+           \"items\":\"string\"\n+        }],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"fileId\",\n+        \"type\":[\"null\",\"string\"]\n+      },\n+      {\n+        \"name\":\"partitionPath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"bootstrapFilePath\",\n+        \"type\":[\"null\", \"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"version\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5973890d4772603e729b3c1de3c3d87e4c5094f3"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODIwMDMyOQ==", "bodyText": "Realize we can achieve this by passing writer/reader schema and also there are places where this is not being done uniformly. So treat this more as a nit", "url": "https://github.com/apache/hudi/pull/2202#discussion_r518200329", "createdAt": "2020-11-05T16:48:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieSliceInfo.avsc", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+{\n+   \"namespace\":\"org.apache.hudi.avro.model\",\n+   \"type\":\"record\",\n+   \"name\":\"HoodieSliceInfo\",\n+   \"fields\":[\n+      {\n+        \"name\":\"dataFilePath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"deltaFilePaths\",\n+        \"type\":[\"null\", {\n+           \"type\":\"array\",\n+           \"items\":\"string\"\n+        }],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"fileId\",\n+        \"type\":[\"null\",\"string\"]\n+      },\n+      {\n+        \"name\":\"partitionPath\",\n+        \"type\":[\"null\",\"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"bootstrapFilePath\",\n+        \"type\":[\"null\", \"string\"],\n+        \"default\": null\n+      },\n+      {\n+        \"name\":\"version\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE5NzQ3Ng=="}, "originalCommit": {"oid": "5973890d4772603e729b3c1de3c3d87e4c5094f3"}, "originalPosition": 51}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4128, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}