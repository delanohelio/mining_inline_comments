{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkwMDM5MDAx", "number": 1416, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNjoyNTowOFrODplQdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQwOTozMDo0NlrODqtW7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0OTI4NjI4OnYy", "diffSide": "RIGHT", "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNjoyNTowOFrOF42WVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMTo0NDozMFrOF5Bdew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE1NTAzMQ==", "bodyText": "How does this work for non hdfs FS like cloud stores ?", "url": "https://github.com/apache/hudi/pull/1416#discussion_r395155031", "createdAt": "2020-03-19T16:25:08Z", "author": {"login": "bvaradar"}, "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -198,7 +198,8 @@ private String getPartitionClause(String partition) {\n     for (String partition : partitions) {\n       String partitionClause = getPartitionClause(partition);\n       Path partitionPath = FSUtils.getPartitionPath(syncConfig.basePath, partition);\n-      String fullPartitionPath = partitionPath.toUri().getScheme().equals(StorageSchemes.HDFS.getScheme())\n+      String partitionScheme = partitionPath.toUri().getScheme();\n+      String fullPartitionPath = StorageSchemes.HDFS.getScheme().equals(partitionScheme)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMwODkwMg==", "bodyText": "The original code itself does not seem to support non HDFS sources. Are non HDFS cloud stores even supported by Hive?\nThis fix has only changed the order of comparsion as getScheme() may return NULL when there is no scheme prefix like in the unit-tests. The boolean result of the comparison remains the same.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r395308902", "createdAt": "2020-03-19T20:44:24Z", "author": {"login": "prashantwason"}, "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -198,7 +198,8 @@ private String getPartitionClause(String partition) {\n     for (String partition : partitions) {\n       String partitionClause = getPartitionClause(partition);\n       Path partitionPath = FSUtils.getPartitionPath(syncConfig.basePath, partition);\n-      String fullPartitionPath = partitionPath.toUri().getScheme().equals(StorageSchemes.HDFS.getScheme())\n+      String partitionScheme = partitionPath.toUri().getScheme();\n+      String fullPartitionPath = StorageSchemes.HDFS.getScheme().equals(partitionScheme)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE1NTAzMQ=="}, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMzNzA4Mw==", "bodyText": "No, I think it does support cloud stores. This special handling was part of https://jira.apache.org/jira/browse/HUDI-325  to get correct behavior for HDFS  for alter partitions.  This should be fine.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r395337083", "createdAt": "2020-03-19T21:44:30Z", "author": {"login": "bvaradar"}, "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -198,7 +198,8 @@ private String getPartitionClause(String partition) {\n     for (String partition : partitions) {\n       String partitionClause = getPartitionClause(partition);\n       Path partitionPath = FSUtils.getPartitionPath(syncConfig.basePath, partition);\n-      String fullPartitionPath = partitionPath.toUri().getScheme().equals(StorageSchemes.HDFS.getScheme())\n+      String partitionScheme = partitionPath.toUri().getScheme();\n+      String fullPartitionPath = StorageSchemes.HDFS.getScheme().equals(partitionScheme)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE1NTAzMQ=="}, "originalCommit": null, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MDQyMjkyOnYy", "diffSide": "RIGHT", "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMTo1NTo1OVrOF5BwCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxODozOTowOFrOF93IbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM0MTgzMg==", "bodyText": "What is the reason for adding 1 to schema fields count ? Is it to account for partition field ? Can you add a comment here.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r395341832", "createdAt": "2020-03-19T21:55:59Z", "author": {"login": "bvaradar"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQxMDczMg==", "bodyText": "Yes, the partition field is extra. The message within the assertEquals has the information on +1.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r400410732", "createdAt": "2020-03-30T18:39:08Z", "author": {"login": "prashantwason"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM0MTgzMg=="}, "originalCommit": null, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2MTA4MTc3OnYy", "diffSide": "RIGHT", "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQwOToyNjoyNVrOF6nhnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxOTowNjozMVrOF94HVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzAwOTMwOA==", "bodyText": "Is this message correct ? We only added 1 partition right and thats why count went from 5 => 6 ?", "url": "https://github.com/apache/hudi/pull/1416#discussion_r397009308", "createdAt": "2020-03-24T09:26:25Z", "author": {"login": "umehrot2"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);\n+    assertEquals(\"Table partitions should match the number of partitions we wrote\", 5,\n+        hiveClientRT.scanTablePartitions(snapshotTableName).size());\n+\n+    // Now lets create more partitions and these are the only ones which needs to be synced\n+    DateTime dateTime = DateTime.now().plusDays(6);\n+    String commitTime2 = \"102\";\n+    String deltaCommitTime2 = \"103\";\n+\n+    TestUtil.addCOWPartitions(1, true, dateTime, commitTime2);\n+    TestUtil.addMORPartitions(1, true, false, dateTime, commitTime2, deltaCommitTime2);\n+    // Lets do the sync\n+    tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+    hiveClientRT = new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    // Schema being read from the log files\n+    assertEquals(\"Hive Schema should match the evolved table schema + partition field\",\n+        hiveClientRT.getTableSchema(snapshotTableName).size(), SchemaTestUtil.getEvolvedSchema().getFields().size() + 1);\n+    // Sync should add the one partition\n+    assertEquals(\"The 2 partitions we wrote should be added to hive\", 6, hiveClientRT.scanTablePartitions(snapshotTableName).size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNjgzNg==", "bodyText": "Updated the message.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r400426836", "createdAt": "2020-03-30T19:06:31Z", "author": {"login": "prashantwason"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);\n+    assertEquals(\"Table partitions should match the number of partitions we wrote\", 5,\n+        hiveClientRT.scanTablePartitions(snapshotTableName).size());\n+\n+    // Now lets create more partitions and these are the only ones which needs to be synced\n+    DateTime dateTime = DateTime.now().plusDays(6);\n+    String commitTime2 = \"102\";\n+    String deltaCommitTime2 = \"103\";\n+\n+    TestUtil.addCOWPartitions(1, true, dateTime, commitTime2);\n+    TestUtil.addMORPartitions(1, true, false, dateTime, commitTime2, deltaCommitTime2);\n+    // Lets do the sync\n+    tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+    hiveClientRT = new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    // Schema being read from the log files\n+    assertEquals(\"Hive Schema should match the evolved table schema + partition field\",\n+        hiveClientRT.getTableSchema(snapshotTableName).size(), SchemaTestUtil.getEvolvedSchema().getFields().size() + 1);\n+    // Sync should add the one partition\n+    assertEquals(\"The 2 partitions we wrote should be added to hive\", 6, hiveClientRT.scanTablePartitions(snapshotTableName).size());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzAwOTMwOA=="}, "originalCommit": null, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2MTA5OTMyOnYy", "diffSide": "RIGHT", "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQwOTozMDo0NlrOF6ns0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxOToxMDoyOVrOF94QSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzAxMjE3OQ==", "bodyText": "Is this the only reason to add this test, so that schema is read from base file ? Otherwise this seems exactly like testSyncMergeOnRead ? I guess I am also trying to understand how this test relates to the change you are making in this review.\nOn that note name of the test testSchemeFromMOR can probably be changed to something more meaningful to reflect what this test is trying to achieve differently.", "url": "https://github.com/apache/hudi/pull/1416#discussion_r397012179", "createdAt": "2020-03-24T09:30:46Z", "author": {"login": "umehrot2"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyOTEzMQ==", "bodyText": "Yes, wanted to add tests for schema reading from both base and and log files. I guess the first (reading schema from base files) is already covered by other unit tests.\n\n\nI guess I am also trying to understand how this test relates to the change you are making in this review.\nThe changes  made are specifically tested in testBasicSync. I added more tests to this unit test.\n\n\nI have renamed the test to testReadSchemaForMOR", "url": "https://github.com/apache/hudi/pull/1416#discussion_r400429131", "createdAt": "2020-03-30T19:10:29Z", "author": {"login": "prashantwason"}, "path": "hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -363,4 +404,51 @@ public void testMultiPartitionKeySync() throws Exception {\n     assertEquals(\"The last commit that was sycned should be updated in the TBLPROPERTIES\", commitTime,\n         hiveClient.getLastCommitTimeSynced(hiveSyncConfig.tableName).get());\n   }\n+\n+  @Test\n+  public void testSchemeFromMOR() throws Exception {\n+    TestUtil.hiveSyncConfig.useJdbc = this.useJdbc;\n+    String commitTime = \"100\";\n+    String snapshotTableName = TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE;\n+    TestUtil.createMORTable(commitTime, \"\", 5, false);\n+    HoodieHiveClient hiveClientRT =\n+        new HoodieHiveClient(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+\n+    assertFalse(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should not exist initially\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Lets do the sync\n+    HiveSyncTool tool = new HiveSyncTool(TestUtil.hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);\n+    tool.syncHoodieTable();\n+\n+    assertTrue(\"Table \" + TestUtil.hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE\n+        + \" should exist after sync completes\", hiveClientRT.doesTableExist(snapshotTableName));\n+\n+    // Schema being read from compacted base files\n+    assertEquals(\"Hive Schema should match the table schema + partition field\", hiveClientRT.getTableSchema(snapshotTableName).size(),\n+        SchemaTestUtil.getSimpleSchema().getFields().size() + 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzAxMjE3OQ=="}, "originalCommit": null, "originalPosition": 92}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4934, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}