{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ5NDgzNjU5", "number": 1834, "title": "[HUDI-1013] Adding Bulk Insert V2 implementation", "bodyText": "What is the purpose of the pull request\n\nAdding support for \"bulk_insert_dataset\" which has better performance compared to existing \"bulk_insert\".\nThis implementation uses Datasource for writing to storage.\nSupport for key generators to operate on Row(rather than HoodieRecords as per existing \"bulk_insert\") is added.\n\nBrief change log\n\nAdded support for \"bulk_insert_dataset\" which uses Datasource for writing.\nThis path introduces a new datasource called \"org.apache.hudi.internal\" and all supporting cast like DefaultSource, DataSourceWriter(HoodieDataSourceInternalWriter), DataWriterFactory(HoodieBulkInsertDataInternalWriterFactory), DataWriter(HoodieBulkInsertDataInternalWriter), etc for the same.\nThis patch also introduces HoodieRowCreateHandle, HoodieInternalRowFileWriter, HoodieInternalRowParquetWriter, etc to assist in writing InternalRows to parquet(and respective factory classes).\nHave added HoodieInternalRow which wraps InternalRow and exposes meta columns.\nHave added HoodieInternalWriteStatus to hold write status (instead of WriteStatus used in HoodieRecord write paths), since this hold Rows instead of HoodieRecords.\nThis patch adds changes to KeyGenerator to ensure getRecordKey and getPartitionPath is supported with Row for \"bulk_insert_dataset\". New apis are added to KeyGenerator, but default implementation is added so as to not have any breaking change. All keygenerator implementations have been fixed on this regards.\nAdded HoodieDatasetBulkInsertHelper to assist in prepping the dataset before calling into datasource write. For commit, HoodieWriteClient is leveraged.\nSome additional functionalities on top of bulk inserts are not covered in this patch. Have create HUDI-1014, HUDI-1105 and HUDI-1106. Namely, UserDefinedCustomPartitioner, Dedup and drop duplicates.\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded tests in HoodieSparkSqlWriterSuite to test end to end bulk_insert_dataset\nAdded tests to test HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieDatasetBulkInsertHelper, HoodieBulkInsertDataInternalWriter\nAdded tests to test HoodieDataSourceInternalWriter for commit, abort, large writes and multiple writes\nAdded tests to test all key generators for new apis with Row\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-07-15T13:32:12Z", "url": "https://github.com/apache/hudi/pull/1834", "merged": true, "mergeCommit": {"oid": "379cf0786fe9fea94ec8c0da7d467ae2fb30dd0b"}, "closed": true, "closedAt": "2020-08-13T07:33:40Z", "author": {"login": "nsivabalan"}, "timelineItems": {"totalCount": 37, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc2cmQLgBqjM1NjI1NDUzMjY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-fhRRgFqTQ2NjczMDY2NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "a5f608ca3be3922e0e8ca58b556aa406638526ec", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/a5f608ca3be3922e0e8ca58b556aa406638526ec", "committedDate": "2020-07-19T23:52:54Z", "message": "Fixing fileId generation within HoodieBulkInsertDataInternalWriter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a5f608ca3be3922e0e8ca58b556aa406638526ec", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/a5f608ca3be3922e0e8ca58b556aa406638526ec", "committedDate": "2020-07-19T23:52:54Z", "message": "Fixing fileId generation within HoodieBulkInsertDataInternalWriter"}, "afterCommit": {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/e5d4939fc7136e846c3123d2084939f75c6fed40", "committedDate": "2020-07-22T11:19:34Z", "message": "Bulk Insert Dataset Base Implementation using Datasource to improve performance"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzMjQzODY2", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-453243866", "createdAt": "2020-07-22T11:53:27Z", "commit": {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMTo1MzoyN1rOG1fBMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMTo1NToyOFrOG1fFUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNTkyMg==", "bodyText": "can we keep this in hudi-spark?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r458735922", "createdAt": "2020-07-22T11:53:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieInternalWriteStatus.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+/**\n+ * Hoodie's internal write status used in datasource implementation of bulk insert.\n+ */\n+public class HoodieInternalWriteStatus implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNjk3OQ==", "bodyText": "we should probably assert that this is not null?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r458736979", "createdAt": "2020-07-22T11:55:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -670,7 +670,9 @@ public Builder withPath(String basePath) {\n     }\n \n     public Builder withSchema(String schemaStr) {\n-      props.setProperty(AVRO_SCHEMA, schemaStr);\n+      if (null != schemaStr) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3e779529be0bdd7b0df28284429694ed838e78aa", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/3e779529be0bdd7b0df28284429694ed838e78aa", "committedDate": "2020-07-23T23:42:22Z", "message": "Adding tests for HoodieInternalRow and HoodieInternalWriteStatus"}, "afterCommit": {"oid": "15382beed32f8db95a72d3d5e8fe6bf0d09fcc3a", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/15382beed32f8db95a72d3d5e8fe6bf0d09fcc3a", "committedDate": "2020-07-24T02:02:50Z", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow and HoodieRowCreateHandle"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "15382beed32f8db95a72d3d5e8fe6bf0d09fcc3a", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/15382beed32f8db95a72d3d5e8fe6bf0d09fcc3a", "committedDate": "2020-07-24T02:02:50Z", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow and HoodieRowCreateHandle"}, "afterCommit": {"oid": "3de1e77efae8e645ffe48176349f86e417d64692", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/3de1e77efae8e645ffe48176349f86e417d64692", "committedDate": "2020-07-24T05:01:33Z", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieBulkInsertDataInternalWriter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3de1e77efae8e645ffe48176349f86e417d64692", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/3de1e77efae8e645ffe48176349f86e417d64692", "committedDate": "2020-07-24T05:01:33Z", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieBulkInsertDataInternalWriter"}, "afterCommit": {"oid": "af27762c4ec3291107e14d3ad38d7e07b66f67c3", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/af27762c4ec3291107e14d3ad38d7e07b66f67c3", "committedDate": "2020-07-24T06:16:07Z", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieBulkInsertDataInternalWriter, HoodieDataSourceInternalWriter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "af27762c4ec3291107e14d3ad38d7e07b66f67c3", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/af27762c4ec3291107e14d3ad38d7e07b66f67c3", "committedDate": "2020-07-24T06:16:07Z", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieBulkInsertDataInternalWriter, HoodieDataSourceInternalWriter"}, "afterCommit": {"oid": "1a11d1c84346383a9b8ad6bfed989cdd1c8b44d8", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/1a11d1c84346383a9b8ad6bfed989cdd1c8b44d8", "committedDate": "2020-07-24T06:19:05Z", "message": "Adding tests for HoodieInternalRow, HoodieInternalWriteStatus, HoodieInternalRow, HoodieRowCreateHandle, HoodieInternalRowParquetWriter, HoodieBulkInsertDataInternalWriter, HoodieDataSourceInternalWriter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3debefccc630604a1ed16974fd2ee331c5234501", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/3debefccc630604a1ed16974fd2ee331c5234501", "committedDate": "2020-07-24T21:32:07Z", "message": "Adding more tests"}, "afterCommit": {"oid": "332f78af7c43f8efe78925a9b428cacef4232e9a", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/332f78af7c43f8efe78925a9b428cacef4232e9a", "committedDate": "2020-08-06T11:47:05Z", "message": "Adding more java docs and minor fixes in tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "332f78af7c43f8efe78925a9b428cacef4232e9a", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/332f78af7c43f8efe78925a9b428cacef4232e9a", "committedDate": "2020-08-06T11:47:05Z", "message": "Adding more java docs and minor fixes in tests"}, "afterCommit": {"oid": "c9570086ecb6866268b200ae6f41b4ca67065af3", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/c9570086ecb6866268b200ae6f41b4ca67065af3", "committedDate": "2020-08-06T11:48:23Z", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c9570086ecb6866268b200ae6f41b4ca67065af3", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/c9570086ecb6866268b200ae6f41b4ca67065af3", "committedDate": "2020-08-06T11:48:23Z", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance"}, "afterCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/dacd635367b59e7d6b8de91f6b785b337dd851eb", "committedDate": "2020-08-06T11:59:01Z", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyNDgwNDEx", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-462480411", "createdAt": "2020-08-06T12:42:58Z", "commit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "state": "COMMENTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxMjo0Mjo1OFrOG8xzfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQxNDoyMDoyN1rOG81tvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4Mzc0MA==", "bodyText": "will address all feedback together.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466383740", "createdAt": "2020-08-06T12:42:58Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieInternalWriteStatus.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+/**\n+ * Hoodie's internal write status used in datasource implementation of bulk insert.\n+ */\n+public class HoodieInternalWriteStatus implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNTkyMg=="}, "originalCommit": {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4NTEyMA==", "bodyText": "Note to reviewer: if this statement fails, we had to consider it as global error and not as per record error since we don't have record key yet. This is different from how HoodieRecord write happens. So, in these cases, rowCreateHandle will throw exception and caller is expected to close the rowCreateHandle.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466385120", "createdAt": "2020-08-06T12:45:21Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieRowCreateHandle.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.model.HoodieInternalRow;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.io.storage.HoodieInternalRowFileWriter;\n+import org.apache.hudi.io.storage.HoodieInternalRowFileWriterFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * Create handle with InternalRow for datasource implemention of bulk insert.\n+ */\n+public class HoodieRowCreateHandle implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(HoodieRowCreateHandle.class);\n+  private static final AtomicLong SEQGEN = new AtomicLong(1);\n+  private final String instantTime;\n+  private final int taskPartitionId;\n+  private final long taskId;\n+  private final long taskEpochId;\n+  private final HoodieTable table;\n+  private final HoodieWriteConfig writeConfig;\n+  private final HoodieInternalRowFileWriter fileWriter;\n+  private final String partitionPath;\n+  private final Path path;\n+  private final String fileId;\n+  private final FileSystem fs;\n+  private final HoodieInternalWriteStatus writeStatus;\n+  private final HoodieTimer currTimer;\n+\n+  public HoodieRowCreateHandle(HoodieTable table, HoodieWriteConfig writeConfig, String partitionPath, String fileId,\n+      String instantTime, int taskPartitionId, long taskId, long taskEpochId,\n+      StructType structType) {\n+    this.partitionPath = partitionPath;\n+    this.table = table;\n+    this.writeConfig = writeConfig;\n+    this.instantTime = instantTime;\n+    this.taskPartitionId = taskPartitionId;\n+    this.taskId = taskId;\n+    this.taskEpochId = taskEpochId;\n+    this.fileId = fileId;\n+    this.currTimer = new HoodieTimer();\n+    this.currTimer.startTimer();\n+    this.fs = table.getMetaClient().getFs();\n+    this.path = makeNewPath(partitionPath);\n+    this.writeStatus = new HoodieInternalWriteStatus(!table.getIndex().isImplicitWithStorage(),\n+        writeConfig.getWriteStatusFailureFraction());\n+    writeStatus.setPartitionPath(partitionPath);\n+    writeStatus.setFileId(fileId);\n+    try {\n+      HoodiePartitionMetadata partitionMetadata =\n+          new HoodiePartitionMetadata(\n+              fs,\n+              instantTime,\n+              new Path(writeConfig.getBasePath()),\n+              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath));\n+      partitionMetadata.trySave(taskPartitionId);\n+      createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));\n+      this.fileWriter = createNewFileWriter(path, table, writeConfig, structType);\n+    } catch (IOException e) {\n+      throw new HoodieInsertException(\"Failed to initialize file writer for path \" + path, e);\n+    }\n+    LOG.info(\"New handle created for partition :\" + partitionPath + \" with fileId \" + fileId);\n+  }\n+\n+  /**\n+   * Writes an {@link InternalRow} to the underlying HoodieInternalRowFileWriter. Before writing, value for meta columns are computed as required\n+   * and wrapped in {@link HoodieInternalRow}. {@link HoodieInternalRow} is what gets written to HoodieInternalRowFileWriter.\n+   * @param record instance of {@link InternalRow} that needs to be written to the fileWriter.\n+   * @throws IOException\n+   */\n+  public void write(InternalRow record) throws IOException {\n+    try {\n+      String partitionPath = record.getUTF8String(HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.get(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4NTQ1NQ==", "bodyText": "Note to reviewer: these methods are copied from HoodieWriteHandle for now.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466385455", "createdAt": "2020-08-06T12:46:00Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieRowCreateHandle.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.model.HoodieInternalRow;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.table.HoodieTableConfig;\n+import org.apache.hudi.common.util.HoodieTimer;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.io.storage.HoodieInternalRowFileWriter;\n+import org.apache.hudi.io.storage.HoodieInternalRowFileWriterFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.MarkerFiles;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * Create handle with InternalRow for datasource implemention of bulk insert.\n+ */\n+public class HoodieRowCreateHandle implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(HoodieRowCreateHandle.class);\n+  private static final AtomicLong SEQGEN = new AtomicLong(1);\n+  private final String instantTime;\n+  private final int taskPartitionId;\n+  private final long taskId;\n+  private final long taskEpochId;\n+  private final HoodieTable table;\n+  private final HoodieWriteConfig writeConfig;\n+  private final HoodieInternalRowFileWriter fileWriter;\n+  private final String partitionPath;\n+  private final Path path;\n+  private final String fileId;\n+  private final FileSystem fs;\n+  private final HoodieInternalWriteStatus writeStatus;\n+  private final HoodieTimer currTimer;\n+\n+  public HoodieRowCreateHandle(HoodieTable table, HoodieWriteConfig writeConfig, String partitionPath, String fileId,\n+      String instantTime, int taskPartitionId, long taskId, long taskEpochId,\n+      StructType structType) {\n+    this.partitionPath = partitionPath;\n+    this.table = table;\n+    this.writeConfig = writeConfig;\n+    this.instantTime = instantTime;\n+    this.taskPartitionId = taskPartitionId;\n+    this.taskId = taskId;\n+    this.taskEpochId = taskEpochId;\n+    this.fileId = fileId;\n+    this.currTimer = new HoodieTimer();\n+    this.currTimer.startTimer();\n+    this.fs = table.getMetaClient().getFs();\n+    this.path = makeNewPath(partitionPath);\n+    this.writeStatus = new HoodieInternalWriteStatus(!table.getIndex().isImplicitWithStorage(),\n+        writeConfig.getWriteStatusFailureFraction());\n+    writeStatus.setPartitionPath(partitionPath);\n+    writeStatus.setFileId(fileId);\n+    try {\n+      HoodiePartitionMetadata partitionMetadata =\n+          new HoodiePartitionMetadata(\n+              fs,\n+              instantTime,\n+              new Path(writeConfig.getBasePath()),\n+              FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath));\n+      partitionMetadata.trySave(taskPartitionId);\n+      createMarkerFile(partitionPath, FSUtils.makeDataFileName(this.instantTime, getWriteToken(), this.fileId, table.getBaseFileExtension()));\n+      this.fileWriter = createNewFileWriter(path, table, writeConfig, structType);\n+    } catch (IOException e) {\n+      throw new HoodieInsertException(\"Failed to initialize file writer for path \" + path, e);\n+    }\n+    LOG.info(\"New handle created for partition :\" + partitionPath + \" with fileId \" + fileId);\n+  }\n+\n+  /**\n+   * Writes an {@link InternalRow} to the underlying HoodieInternalRowFileWriter. Before writing, value for meta columns are computed as required\n+   * and wrapped in {@link HoodieInternalRow}. {@link HoodieInternalRow} is what gets written to HoodieInternalRowFileWriter.\n+   * @param record instance of {@link InternalRow} that needs to be written to the fileWriter.\n+   * @throws IOException\n+   */\n+  public void write(InternalRow record) throws IOException {\n+    try {\n+      String partitionPath = record.getUTF8String(HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.get(\n+          HoodieRecord.PARTITION_PATH_METADATA_FIELD)).toString();\n+      String seqId = HoodieRecord.generateSequenceId(instantTime, taskPartitionId, SEQGEN.getAndIncrement());\n+      String recordKey = record.getUTF8String(HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.get(\n+          HoodieRecord.RECORD_KEY_METADATA_FIELD)).toString();\n+      HoodieInternalRow internalRow = new HoodieInternalRow(instantTime, seqId, recordKey, partitionPath, path.getName(),\n+          record);\n+      try {\n+        fileWriter.writeRow(recordKey, internalRow);\n+        writeStatus.markSuccess(recordKey);\n+      } catch (Throwable t) {\n+        writeStatus.markFailure(recordKey, t);\n+      }\n+    } catch (Throwable ge) {\n+      writeStatus.setGlobalError(ge);\n+      throw ge;\n+    }\n+  }\n+\n+  /**\n+   * @returns {@code true} if this handle can take in more writes. else {@code false}.\n+   */\n+  public boolean canWrite() {\n+    return fileWriter.canWrite();\n+  }\n+\n+  /**\n+   * Closes the {@link HoodieRowCreateHandle} and returns an instance of {@link HoodieInternalWriteStatus} containing the stats and\n+   * status of the writes to this handle.\n+   * @return the {@link HoodieInternalWriteStatus} containing the stats and status of the writes to this handle.\n+   * @throws IOException\n+   */\n+  public HoodieInternalWriteStatus close() throws IOException {\n+    fileWriter.close();\n+    HoodieWriteStat stat = new HoodieWriteStat();\n+    stat.setPartitionPath(partitionPath);\n+    stat.setNumWrites(writeStatus.getTotalRecords());\n+    stat.setNumDeletes(0);\n+    stat.setNumInserts(writeStatus.getTotalRecords());\n+    stat.setPrevCommit(HoodieWriteStat.NULL_COMMIT);\n+    stat.setFileId(fileId);\n+    stat.setPath(new Path(writeConfig.getBasePath()), path);\n+    long fileSizeInBytes = FSUtils.getFileSize(table.getMetaClient().getFs(), path);\n+    stat.setTotalWriteBytes(fileSizeInBytes);\n+    stat.setFileSizeInBytes(fileSizeInBytes);\n+    stat.setTotalWriteErrors(writeStatus.getFailedRowsSize());\n+    HoodieWriteStat.RuntimeStats runtimeStats = new HoodieWriteStat.RuntimeStats();\n+    runtimeStats.setTotalCreateTime(currTimer.endTimer());\n+    stat.setRuntimeStats(runtimeStats);\n+    writeStatus.setStat(stat);\n+    return writeStatus;\n+  }\n+\n+  public String getFileName() {\n+    return path.getName();\n+  }\n+\n+  private Path makeNewPath(String partitionPath) {\n+    Path path = FSUtils.getPartitionPath(writeConfig.getBasePath(), partitionPath);\n+    try {\n+      fs.mkdirs(path); // create a new partition as needed.\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to make dir \" + path, e);\n+    }\n+    HoodieTableConfig tableConfig = table.getMetaClient().getTableConfig();\n+    return new Path(path.toString(), FSUtils.makeDataFileName(instantTime, getWriteToken(), fileId,\n+        tableConfig.getBaseFileFormat().getFileExtension()));\n+  }\n+\n+  /**\n+   * Creates an empty marker file corresponding to storage writer path.\n+   *\n+   * @param partitionPath Partition path\n+   */\n+  private void createMarkerFile(String partitionPath, String dataFileName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4NTgwMQ==", "bodyText": "Note to reviewer: moved this file to hudi-spark, as this needs access to AvroConversionUtils", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466385801", "createdAt": "2020-08-06T12:46:37Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -1,88 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.keygen;\n-\n-import org.apache.hudi.common.config.TypedProperties;\n-import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.exception.HoodieKeyException;\n-\n-import org.apache.avro.generic.GenericRecord;\n-\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Base class for all the built-in key generators. Contains methods structured for\n- * code reuse amongst them.\n- */\n-public abstract class BuiltinKeyGenerator extends KeyGenerator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4Njg5Mw==", "bodyText": "Note to reviewer: introduced these new apis for Row based KeyGen. All Built in generators have implemented these. If any user has custom key generator, they don't need to implement these apis if not for \"bulk_insert_dataset\". But if they wish to use \"bulk_insert_dataset\", they might have to give implementations to these methods.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466386893", "createdAt": "2020-08-06T12:48:36Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/keygen/KeyGenerator.java", "diffHunk": "@@ -51,4 +53,32 @@ protected KeyGenerator(TypedProperties config) {\n     throw new UnsupportedOperationException(\"Bootstrap not supported for key generator. \"\n         + \"Please override this method in your custom key generator.\");\n   }\n+\n+  /**\n+   * Initializes {@link KeyGenerator} for {@link Row} based operations.\n+   * @param structType structype of the dataset.\n+   * @param structName struct name of the dataset.\n+   * @param recordNamespace record namespace of the dataset.\n+   */\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4ODMyMw==", "bodyText": "Note to reviewer: as mentioned above, if there is some error parsing partition path or record key, it will result in global error for the handle and not per record/row error.\nI couldn't repro/test per record error. I tried writing a different datatype to one of the data column expecting the write to fail, but it didn't fail. So, as of now, there are no tests for per record failures. Same applies to RowFileWriter, InternalWriter etc.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466388323", "createdAt": "2020-08-06T12:51:03Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/io/TestHoodieRowCreateHandle.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestHarness;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.UUID;\n+\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getInternalRowWithError;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.toInternalRows;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+/**\n+ * Unit tests {@link HoodieRowCreateHandle}.\n+ */\n+public class TestHoodieRowCreateHandle extends HoodieClientTestHarness {\n+\n+  private static final Random RANDOM = new Random();\n+\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initSparkContexts(\"TestHoodieRowCreateHandle\");\n+    initPath();\n+    initFileSystem();\n+    initTestDataGenerator();\n+    initMetaClient();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  @Test\n+  public void testRowCreateHandle() throws IOException {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    HoodieTable table = HoodieTable.create(metaClient, cfg, hadoopConf);\n+    List<String> fileNames = new ArrayList<>();\n+    List<String> fileAbsPaths = new ArrayList<>();\n+\n+    Dataset<Row> totalInputRows = null;\n+    // one round per partition\n+    for (int i = 0; i < 5; i++) {\n+      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[i % 3];\n+\n+      // init some args\n+      String fileId = UUID.randomUUID().toString();\n+      String instantTime = \"000\";\n+\n+      HoodieRowCreateHandle handle = new HoodieRowCreateHandle(table, cfg, partitionPath, fileId, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE);\n+      int size = 10 + RANDOM.nextInt(1000);\n+      // Generate inputs\n+      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n+      if (totalInputRows == null) {\n+        totalInputRows = inputRows;\n+      } else {\n+        totalInputRows = totalInputRows.union(inputRows);\n+      }\n+\n+      // issue writes\n+      HoodieInternalWriteStatus writeStatus = writeAndGetWriteStatus(inputRows, handle);\n+\n+      fileAbsPaths.add(basePath + \"/\" + writeStatus.getStat().getPath());\n+      fileNames.add(handle.getFileName());\n+      // verify output\n+      assertOutput(writeStatus, size, fileId, partitionPath, instantTime, totalInputRows, fileNames, fileAbsPaths);\n+    }\n+  }\n+\n+  /**\n+   * Issue some corrupted or wrong schematized InternalRow after few valid InternalRows so that global error is thrown. write batch 1 of valid records write batch 2 of invalid records Global Error\n+   * should be thrown.\n+   */\n+  @Test\n+  public void testGlobalFailure() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4OTU1NA==", "bodyText": "Note to reviewer: Can't leverage HoodieTestDataGenerator since each record is expected to be in certain format (meta columns followed by data columns). Hence introduced a new schema for testing \"bulk insert dataset\"", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466389554", "createdAt": "2020-08-06T12:53:20Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieDatasetTestUtils.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+import static org.apache.hudi.common.testutils.FileSystemTestUtils.RANDOM;\n+\n+/**\n+ * Dataset test utils.\n+ */\n+public class HoodieDatasetTestUtils {\n+\n+  public static final StructType STRUCT_TYPE = new StructType(new StructField[] {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM5MDQ4Mw==", "bodyText": "Note to reviewer: Have unified code across Simple and Complex key gens.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466390483", "createdAt": "2020-08-06T12:54:58Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  private List<String> recordKeyFields;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM5MTQzMA==", "bodyText": "Note to reviewer: had to move this to hudi-spark as we need to access AvroConversionUtils for Row to GenericRecord converter function.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466391430", "createdAt": "2020-08-06T12:56:31Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0MTMzMw==", "bodyText": "Note to reviewer: returning -1 only in case  of partition path. So, that  getNestedFieldVal(Row row, List positions) will return DEFAULT_PARTITION_PATH if partition path field is not found.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466441333", "createdAt": "2020-08-06T14:10:56Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/RowKeyGeneratorHelper.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import scala.Option;\n+\n+import static org.apache.hudi.keygen.KeyGenUtils.DEFAULT_PARTITION_PATH;\n+import static org.apache.hudi.keygen.KeyGenUtils.DEFAULT_PARTITION_PATH_SEPARATOR;\n+import static org.apache.hudi.keygen.KeyGenUtils.EMPTY_RECORDKEY_PLACEHOLDER;\n+import static org.apache.hudi.keygen.KeyGenUtils.NULL_RECORDKEY_PLACEHOLDER;\n+\n+/**\n+ * Helper class to fetch fields from Row.\n+ */\n+public class RowKeyGeneratorHelper {\n+\n+  /**\n+   * Generates record key for the corresponding {@link Row}.\n+   * @param row instance of {@link Row} of interest\n+   * @param recordKeyFields record key fields as a list\n+   * @param recordKeyPositions record key positions for the corresponding record keys in {@code recordKeyFields}\n+   * @param prefixFieldName {@code true} if field name need to be prefixed in the returned result. {@code false} otherwise.\n+   * @return the record key thus generated\n+   */\n+  public static String getRecordKeyFromRow(Row row, List<String> recordKeyFields, Map<String, List<Integer>> recordKeyPositions, boolean prefixFieldName) {\n+    AtomicBoolean keyIsNullOrEmpty = new AtomicBoolean(true);\n+    String toReturn = IntStream.range(0, recordKeyFields.size()).mapToObj(idx -> {\n+      String field = recordKeyFields.get(idx);\n+      String val = null;\n+      List<Integer> fieldPositions = recordKeyPositions.get(field);\n+      if (fieldPositions.size() == 1) { // simple field\n+        Integer fieldPos = fieldPositions.get(0);\n+        if (row.isNullAt(fieldPos)) {\n+          val = NULL_RECORDKEY_PLACEHOLDER;\n+        } else {\n+          val = row.getAs(field).toString();\n+          if (val.isEmpty()) {\n+            val = EMPTY_RECORDKEY_PLACEHOLDER;\n+          } else {\n+            keyIsNullOrEmpty.set(false);\n+          }\n+        }\n+      } else { // nested fields\n+        val = getNestedFieldVal(row, recordKeyPositions.get(field)).toString();\n+        if (!val.contains(NULL_RECORDKEY_PLACEHOLDER) && !val.contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+          keyIsNullOrEmpty.set(false);\n+        }\n+      }\n+      return prefixFieldName ? (field + \":\" + val) : val;\n+    }).collect(Collectors.joining(\",\"));\n+    if (keyIsNullOrEmpty.get()) {\n+      throw new HoodieKeyException(\"recordKey value: \\\"\" + toReturn + \"\\\" for fields: \\\"\" + Arrays.toString(recordKeyFields.toArray()) + \"\\\" cannot be null or empty.\");\n+    }\n+    return toReturn;\n+  }\n+\n+  /**\n+   * Generates partition path for the corresponding {@link Row}.\n+   * @param row instance of {@link Row} of interest\n+   * @param partitionPathFields partition path fields as a list\n+   * @param hiveStylePartitioning {@code true} if hive style partitioning is set. {@code false} otherwise\n+   * @param partitionPathPositions partition path positions for the corresponding fields in {@code partitionPathFields}\n+   * @return the generated partition path for the row\n+   */\n+  public static String getPartitionPathFromRow(Row row, List<String> partitionPathFields, boolean hiveStylePartitioning, Map<String, List<Integer>> partitionPathPositions) {\n+    return IntStream.range(0, partitionPathFields.size()).mapToObj(idx -> {\n+      String field = partitionPathFields.get(idx);\n+      String val = null;\n+      List<Integer> fieldPositions = partitionPathPositions.get(field);\n+      if (fieldPositions.size() == 1) { // simple\n+        Integer fieldPos = fieldPositions.get(0);\n+        // for partition path, if field is not found, index will be set to -1\n+        if (fieldPos == -1 || row.isNullAt(fieldPos)) {\n+          val = DEFAULT_PARTITION_PATH;\n+        } else {\n+          val = row.getAs(field).toString();\n+          if (val.isEmpty()) {\n+            val = DEFAULT_PARTITION_PATH;\n+          }\n+        }\n+        if (hiveStylePartitioning) {\n+          val = field + \"=\" + val;\n+        }\n+      } else { // nested\n+        Object nestedVal = getNestedFieldVal(row, partitionPathPositions.get(field));\n+        if (nestedVal.toString().contains(NULL_RECORDKEY_PLACEHOLDER) || nestedVal.toString().contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+          val = hiveStylePartitioning ? field + \"=\" + DEFAULT_PARTITION_PATH : DEFAULT_PARTITION_PATH;\n+        } else {\n+          val = hiveStylePartitioning ? field + \"=\" + nestedVal.toString() : nestedVal.toString();\n+        }\n+      }\n+      return val;\n+    }).collect(Collectors.joining(DEFAULT_PARTITION_PATH_SEPARATOR));\n+  }\n+\n+  /**\n+   * Fetch the field value located at the positions requested for.\n+   * @param row instance of {@link Row} of interest\n+   * @param positions tree style positions where the leaf node need to be fetched and returned\n+   * @return the field value as per the positions requested for.\n+   */\n+  public static Object getNestedFieldVal(Row row, List<Integer> positions) {\n+    if (positions.size() == 1 && positions.get(0) == -1) {\n+      return DEFAULT_PARTITION_PATH;\n+    }\n+    int index = 0;\n+    int totalCount = positions.size();\n+    Row valueToProcess = row;\n+    Object toReturn = null;\n+\n+    while (index < totalCount) {\n+      if (index < totalCount - 1) {\n+        if (valueToProcess.isNullAt(positions.get(index))) {\n+          toReturn = NULL_RECORDKEY_PLACEHOLDER;\n+          break;\n+        }\n+        valueToProcess = (Row) valueToProcess.get(positions.get(index));\n+      } else { // last index\n+        if (valueToProcess.getAs(positions.get(index)).toString().isEmpty()) {\n+          toReturn = EMPTY_RECORDKEY_PLACEHOLDER;\n+          break;\n+        }\n+        toReturn = valueToProcess.getAs(positions.get(index));\n+      }\n+      index++;\n+    }\n+    return toReturn;\n+  }\n+\n+  /**\n+   * Generate the tree style positions for the field requested for as per the defined struct type.\n+   * @param structType schema of interest\n+   * @param field field of interest for which the positions are requested for\n+   * @param isRecordKey {@code true} if the field requested for is a record key. {@code false} incase of a partition path.\n+   * @return the positions of the field as per the struct type.\n+   */\n+  public static List<Integer> getNestedFieldIndices(StructType structType, String field, boolean isRecordKey) {\n+    String[] slices = field.split(\"\\\\.\");\n+    List<Integer> positions = new ArrayList<>();\n+    int index = 0;\n+    int totalCount = slices.length;\n+    while (index < totalCount) {\n+      String slice = slices[index];\n+      Option<Object> curIndexOpt = structType.getFieldIndex(slice);\n+      if (curIndexOpt.isDefined()) {\n+        int curIndex = (int) curIndexOpt.get();\n+        positions.add(curIndex);\n+        final StructField nestedField = structType.fields()[curIndex];\n+        if (index < totalCount - 1) {\n+          if (!(nestedField.dataType() instanceof StructType)) {\n+            if (isRecordKey) {\n+              throw new HoodieKeyException(\"Nested field should be of type StructType \" + nestedField);\n+            } else {\n+              positions = Collections.singletonList(-1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0MTk3MQ==", "bodyText": "Note to reviewer: getNestedFieldIndices(StructType structType, String field, boolean isRecordKey) will return -1 for partitionPathIndices if partition path field is not found.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466441971", "createdAt": "2020-08-06T14:11:51Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/RowKeyGeneratorHelper.java", "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import scala.Option;\n+\n+import static org.apache.hudi.keygen.KeyGenUtils.DEFAULT_PARTITION_PATH;\n+import static org.apache.hudi.keygen.KeyGenUtils.DEFAULT_PARTITION_PATH_SEPARATOR;\n+import static org.apache.hudi.keygen.KeyGenUtils.EMPTY_RECORDKEY_PLACEHOLDER;\n+import static org.apache.hudi.keygen.KeyGenUtils.NULL_RECORDKEY_PLACEHOLDER;\n+\n+/**\n+ * Helper class to fetch fields from Row.\n+ */\n+public class RowKeyGeneratorHelper {\n+\n+  /**\n+   * Generates record key for the corresponding {@link Row}.\n+   * @param row instance of {@link Row} of interest\n+   * @param recordKeyFields record key fields as a list\n+   * @param recordKeyPositions record key positions for the corresponding record keys in {@code recordKeyFields}\n+   * @param prefixFieldName {@code true} if field name need to be prefixed in the returned result. {@code false} otherwise.\n+   * @return the record key thus generated\n+   */\n+  public static String getRecordKeyFromRow(Row row, List<String> recordKeyFields, Map<String, List<Integer>> recordKeyPositions, boolean prefixFieldName) {\n+    AtomicBoolean keyIsNullOrEmpty = new AtomicBoolean(true);\n+    String toReturn = IntStream.range(0, recordKeyFields.size()).mapToObj(idx -> {\n+      String field = recordKeyFields.get(idx);\n+      String val = null;\n+      List<Integer> fieldPositions = recordKeyPositions.get(field);\n+      if (fieldPositions.size() == 1) { // simple field\n+        Integer fieldPos = fieldPositions.get(0);\n+        if (row.isNullAt(fieldPos)) {\n+          val = NULL_RECORDKEY_PLACEHOLDER;\n+        } else {\n+          val = row.getAs(field).toString();\n+          if (val.isEmpty()) {\n+            val = EMPTY_RECORDKEY_PLACEHOLDER;\n+          } else {\n+            keyIsNullOrEmpty.set(false);\n+          }\n+        }\n+      } else { // nested fields\n+        val = getNestedFieldVal(row, recordKeyPositions.get(field)).toString();\n+        if (!val.contains(NULL_RECORDKEY_PLACEHOLDER) && !val.contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+          keyIsNullOrEmpty.set(false);\n+        }\n+      }\n+      return prefixFieldName ? (field + \":\" + val) : val;\n+    }).collect(Collectors.joining(\",\"));\n+    if (keyIsNullOrEmpty.get()) {\n+      throw new HoodieKeyException(\"recordKey value: \\\"\" + toReturn + \"\\\" for fields: \\\"\" + Arrays.toString(recordKeyFields.toArray()) + \"\\\" cannot be null or empty.\");\n+    }\n+    return toReturn;\n+  }\n+\n+  /**\n+   * Generates partition path for the corresponding {@link Row}.\n+   * @param row instance of {@link Row} of interest\n+   * @param partitionPathFields partition path fields as a list\n+   * @param hiveStylePartitioning {@code true} if hive style partitioning is set. {@code false} otherwise\n+   * @param partitionPathPositions partition path positions for the corresponding fields in {@code partitionPathFields}\n+   * @return the generated partition path for the row\n+   */\n+  public static String getPartitionPathFromRow(Row row, List<String> partitionPathFields, boolean hiveStylePartitioning, Map<String, List<Integer>> partitionPathPositions) {\n+    return IntStream.range(0, partitionPathFields.size()).mapToObj(idx -> {\n+      String field = partitionPathFields.get(idx);\n+      String val = null;\n+      List<Integer> fieldPositions = partitionPathPositions.get(field);\n+      if (fieldPositions.size() == 1) { // simple\n+        Integer fieldPos = fieldPositions.get(0);\n+        // for partition path, if field is not found, index will be set to -1\n+        if (fieldPos == -1 || row.isNullAt(fieldPos)) {\n+          val = DEFAULT_PARTITION_PATH;\n+        } else {\n+          val = row.getAs(field).toString();\n+          if (val.isEmpty()) {\n+            val = DEFAULT_PARTITION_PATH;\n+          }\n+        }\n+        if (hiveStylePartitioning) {\n+          val = field + \"=\" + val;\n+        }\n+      } else { // nested\n+        Object nestedVal = getNestedFieldVal(row, partitionPathPositions.get(field));\n+        if (nestedVal.toString().contains(NULL_RECORDKEY_PLACEHOLDER) || nestedVal.toString().contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+          val = hiveStylePartitioning ? field + \"=\" + DEFAULT_PARTITION_PATH : DEFAULT_PARTITION_PATH;\n+        } else {\n+          val = hiveStylePartitioning ? field + \"=\" + nestedVal.toString() : nestedVal.toString();\n+        }\n+      }\n+      return val;\n+    }).collect(Collectors.joining(DEFAULT_PARTITION_PATH_SEPARATOR));\n+  }\n+\n+  /**\n+   * Fetch the field value located at the positions requested for.\n+   * @param row instance of {@link Row} of interest\n+   * @param positions tree style positions where the leaf node need to be fetched and returned\n+   * @return the field value as per the positions requested for.\n+   */\n+  public static Object getNestedFieldVal(Row row, List<Integer> positions) {\n+    if (positions.size() == 1 && positions.get(0) == -1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0MjcxNA==", "bodyText": "Note to reviewer: no changes here. just moved code to a private method for re-use", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466442714", "createdAt": "2020-08-06T14:12:51Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -129,45 +134,54 @@ public String getPartitionPath(GenericRecord record) {\n     if (partitionVal == null) {\n       partitionVal = 1L;\n     }\n+    try {\n+      return getPartitionPath(partitionVal);\n+    } catch (Exception e) {\n+      throw new HoodieDeltaStreamerException(\"Unable to parse input partition field :\" + partitionVal, e);\n+    }\n+  }\n \n+  /**\n+   * Parse and fetch partition path based on data type.\n+   *\n+   * @param partitionVal partition path object value fetched from record/row\n+   * @return the parsed partition path based on data type\n+   * @throws ParseException on any parse exception\n+   */\n+  private String getPartitionPath(Object partitionVal) throws ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0NDMwOA==", "bodyText": "Note to reviewer: no changes in the else section which is same as before for all write operations. Github does not show the difference well. Anyways, if possible just ensure there are no change as I had to manually resolve lot of conflicts during rebase.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466444308", "createdAt": "2020-08-06T14:15:15Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -108,262 +106,280 @@ private[hudi] object HoodieSparkSqlWriter {\n           throw new HoodieException(s\"hoodie table with name $existingTableName already exist at $basePath\")\n         }\n       }\n-      val (writeStatuses, writeClient: HoodieWriteClient[HoodieRecordPayload[Nothing]]) =\n-        if (!operation.equalsIgnoreCase(DELETE_OPERATION_OPT_VAL)) {\n-          // register classes & schemas\n-          val structName = s\"${tblName}_record\"\n-          val nameSpace = s\"hoodie.${tblName}\"\n-          sparkContext.getConf.registerKryoClasses(\n-            Array(classOf[org.apache.avro.generic.GenericData],\n-              classOf[org.apache.avro.Schema]))\n-          val schema = AvroConversionUtils.convertStructTypeToAvroSchema(df.schema, structName, nameSpace)\n-          sparkContext.getConf.registerAvroSchemas(schema)\n-          log.info(s\"Registered avro schema : ${schema.toString(true)}\")\n-\n-          // Convert to RDD[HoodieRecord]\n-          val keyGenerator = DataSourceUtils.createKeyGenerator(toProperties(parameters))\n-          val genericRecords: RDD[GenericRecord] = AvroConversionUtils.createRdd(df, structName, nameSpace)\n-          val hoodieAllIncomingRecords = genericRecords.map(gr => {\n-            val orderingVal = HoodieAvroUtils.getNestedFieldVal(gr, parameters(PRECOMBINE_FIELD_OPT_KEY), false)\n-              .asInstanceOf[Comparable[_]]\n-            DataSourceUtils.createHoodieRecord(gr,\n-              orderingVal, keyGenerator.getKey(gr),\n-              parameters(PAYLOAD_CLASS_OPT_KEY))\n-          }).toJavaRDD()\n-\n-          // Handle various save modes\n-          if (mode == SaveMode.ErrorIfExists && exists) {\n-            throw new HoodieException(s\"hoodie table at $basePath already exists.\")\n-          }\n \n-          if (mode == SaveMode.Overwrite && exists) {\n-            log.warn(s\"hoodie table at $basePath already exists. Deleting existing data & overwriting with new data.\")\n-            fs.delete(basePath, true)\n-            exists = false\n-          }\n+      val (writeSuccessfulRetVal: Boolean, commitTimeRetVal: common.util.Option[String], compactionInstantRetVal: common.util.Option[String],\n+      writeClientRetVal: HoodieWriteClient[HoodieRecordPayload[Nothing]], tableConfigRetVal: HoodieTableConfig) =\n+         if (operation.equalsIgnoreCase(BULK_INSERT_DATASET_OPERATION_OPT_VAL)) {\n+        // register classes & schemas\n+        val structName = s\"${tblName}_record\"\n+        val nameSpace = s\"hoodie.${tblName}\"\n \n-          // Create the table if not present\n-          if (!exists) {\n-            //FIXME(bootstrap): bootstrapIndexClass needs to be set when bootstrap index class is integrated.\n-            val tableMetaClient = HoodieTableMetaClient.initTableTypeWithBootstrap(sparkContext.hadoopConfiguration,\n-              path.get, HoodieTableType.valueOf(tableType),\n-              tblName, \"archived\", parameters(PAYLOAD_CLASS_OPT_KEY), null, null, null)\n-            tableConfig = tableMetaClient.getTableConfig\n-          }\n+        // Handle various save modes\n+        if (mode == SaveMode.ErrorIfExists && exists) {\n+          throw new HoodieException(s\"hoodie table at $basePath already exists.\")\n+        }\n \n-          // Create a HoodieWriteClient & issue the write.\n-          val client = hoodieWriteClient.getOrElse(DataSourceUtils.createHoodieClient(jsc, schema.toString, path.get,\n-            tblName, mapAsJavaMap(parameters)\n-          )).asInstanceOf[HoodieWriteClient[HoodieRecordPayload[Nothing]]]\n+        val (success, commitTime: common.util.Option[String]) =\n+          if (mode == SaveMode.Ignore && exists) {\n+            log.warn(s\"hoodie table at $basePath already exists. Ignoring & not performing actual writes.\")\n+            (false, common.util.Option.ofNullable(instantTime))\n+          } else {\n+            if (mode == SaveMode.Overwrite && exists) {\n+              log.warn(s\"hoodie table at $basePath already exists. Deleting existing data & overwriting with new data.\")\n+              fs.delete(basePath, true)\n+              exists = false\n+            }\n \n-          if (asyncCompactionTriggerFn.isDefined &&\n-            isAsyncCompactionEnabled(client, tableConfig, parameters, jsc.hadoopConfiguration())) {\n-            asyncCompactionTriggerFn.get.apply(client)\n-          }\n+            // Create the table if not present\n+            if (!exists) {\n+              //FIXME(bootstrap): bootstrapIndexClass needs to be set when bootstrap index class is integrated.\n+              val tableMetaClient = HoodieTableMetaClient.initTableTypeWithBootstrap(sparkContext.hadoopConfiguration,\n+                path.get, HoodieTableType.valueOf(tableType),\n+                tblName, \"archived\", parameters(PAYLOAD_CLASS_OPT_KEY), null, null, null)\n+              tableConfig = tableMetaClient.getTableConfig\n+            }\n \n-          val hoodieRecords =\n-            if (parameters(INSERT_DROP_DUPS_OPT_KEY).toBoolean) {\n-              DataSourceUtils.dropDuplicates(jsc, hoodieAllIncomingRecords, mapAsJavaMap(parameters))\n+            val writeConfig = DataSourceUtils.createHoodieConfig(null, path.get, tblName,\n+              mapAsJavaMap(parameters))\n+\n+            val hoodieDF = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsert(sqlContext, writeConfig, df, structName, nameSpace)\n+            hoodieDF.write.format(\"org.apache.hudi.internal\").option(INSTANT_TIME, instantTime)\n+              .options(parameters).save()\n+            val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+            val syncHiveSucess = if (hiveSyncEnabled) {\n+              log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")\n+              val fs = FSUtils.getFs(basePath.toString, jsc.hadoopConfiguration)\n+              syncHive(basePath, fs, parameters)\n             } else {\n-              hoodieAllIncomingRecords\n+              true\n             }\n-\n-          if (hoodieRecords.isEmpty()) {\n-            log.info(\"new batch has no new records, skipping...\")\n-            (true, common.util.Option.empty())\n+            (syncHiveSucess, common.util.Option.ofNullable(instantTime))\n           }\n-          client.startCommitWithTime(instantTime)\n-          val writeStatuses = DataSourceUtils.doWriteOperation(client, hoodieRecords, instantTime, operation)\n-          (writeStatuses, client)\n-        } else {\n+        (success, commitTime, common.util.Option.of(\"\"), hoodieWriteClient.orNull, tableConfig)\n+       } else {\n+        val (writeStatuses, writeClient: HoodieWriteClient[HoodieRecordPayload[Nothing]]) =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0NTk4Mg==", "bodyText": "Note to reviewer: here is the only place where we test abort for Datasource path. We couldn't test it elsewhere (TestHoodieRowCreateHandle, TestHoodieInternalRowParquetWriter, TestHoodieBulkInsertDataInternalWriter)", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466445982", "createdAt": "2020-08-06T14:17:46Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.testutils.HoodieClientTestHarness;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.HoodieDatasetTestUtils.toInternalRows;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+\n+/**\n+ * Unit tests {@link HoodieDataSourceInternalWriter}.\n+ */\n+public class TestHoodieDataSourceInternalWriter extends HoodieClientTestHarness {\n+\n+  private static final Random RANDOM = new Random();\n+\n+  @BeforeEach\n+  public void setUp() throws Exception {\n+    initSparkContexts(\"TestHoodieDataSourceInternalWriter\");\n+    initPath();\n+    initFileSystem();\n+    initTestDataGenerator();\n+    initMetaClient();\n+  }\n+\n+  @AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  @Test\n+  public void testDataSourceWriter() throws IOException {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    String instantTime = \"001\";\n+    // init writer\n+    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n+        new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, WriteOperationType.BULK_INSERT_DATASET);\n+    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n+\n+    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n+    List<String> partitionPathsAbs = new ArrayList<>();\n+    for (String partitionPath : partitionPaths) {\n+      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n+    }\n+\n+    int size = 10 + RANDOM.nextInt(1000);\n+    int batches = 5;\n+    Dataset<Row> totalInputRows = null;\n+\n+    for (int j = 0; j < batches; j++) {\n+      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n+      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n+      writeRows(inputRows, writer);\n+      if (totalInputRows == null) {\n+        totalInputRows = inputRows;\n+      } else {\n+        totalInputRows = totalInputRows.union(inputRows);\n+      }\n+    }\n+\n+    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n+    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n+    commitMessages.add(commitMetadata);\n+    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n+    metaClient.reloadActiveTimeline();\n+    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n+    // verify output\n+    assertOutput(totalInputRows, result, instantTime);\n+    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n+  }\n+\n+  @Test\n+  public void testMultipleDataSourceWrites() throws IOException {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    int partitionCounter = 0;\n+\n+    // execute N rounds\n+    for (int i = 0; i < 5; i++) {\n+      String instantTime = \"00\" + i;\n+      // init writer\n+      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n+          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, WriteOperationType.BULK_INSERT_DATASET);\n+\n+      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n+      Dataset<Row> totalInputRows = null;\n+      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n+\n+      int size = 10 + RANDOM.nextInt(1000);\n+      int batches = 5; // one batch per partition\n+\n+      for (int j = 0; j < batches; j++) {\n+        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n+        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n+        writeRows(inputRows, writer);\n+        if (totalInputRows == null) {\n+          totalInputRows = inputRows;\n+        } else {\n+          totalInputRows = totalInputRows.union(inputRows);\n+        }\n+      }\n+\n+      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n+      commitMessages.add(commitMetadata);\n+      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n+      metaClient.reloadActiveTimeline();\n+\n+      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n+\n+      // verify output\n+      assertOutput(totalInputRows, result, instantTime);\n+      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n+    }\n+  }\n+\n+  @Test\n+  public void testLargeWrites() throws IOException {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    int partitionCounter = 0;\n+\n+    // execute N rounds\n+    for (int i = 0; i < 3; i++) {\n+      String instantTime = \"00\" + i;\n+      // init writer\n+      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n+          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, WriteOperationType.BULK_INSERT_DATASET);\n+\n+      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n+      Dataset<Row> totalInputRows = null;\n+      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n+\n+      int size = 10000 + RANDOM.nextInt(10000);\n+      int batches = 3; // one batch per partition\n+\n+      for (int j = 0; j < batches; j++) {\n+        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n+        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n+        writeRows(inputRows, writer);\n+        if (totalInputRows == null) {\n+          totalInputRows = inputRows;\n+        } else {\n+          totalInputRows = totalInputRows.union(inputRows);\n+        }\n+      }\n+\n+      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n+      commitMessages.add(commitMetadata);\n+      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n+      metaClient.reloadActiveTimeline();\n+\n+      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n+\n+      // verify output\n+      assertOutput(totalInputRows, result, instantTime);\n+      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n+    }\n+  }\n+\n+  /**\n+   * Tests that DataSourceWriter.abort() will abort the written records of interest write and commit batch1 write and abort batch2 Read of entire dataset should show only records from batch1.\n+   * commit batch1\n+   * abort batch2\n+   * verify only records from batch1 is available to read\n+   */\n+  @Test\n+  public void testAbort() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0Njk5Nw==", "bodyText": "Note to reviewer: I am yet to add tests to these new methods. Got these as part of rebase. Also, I notice few other test classes for each key generators after rebasing. Will add tests by tmrw to those new test classes.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466446997", "createdAt": "2020-08-06T14:19:16Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/test/java/org/apache/hudi/keygen/TestTimestampBasedKeyGenerator.java", "diffHunk": "@@ -116,20 +165,26 @@ public void testScalar() throws IOException {\n \n     // timezone is GMT\n     properties = getBaseKeyConfig(\"SCALAR\", \"yyyy-MM-dd hh\", \"GMT\", \"days\");\n-    HoodieKey hk5 = new TimestampBasedKeyGenerator(properties).getKey(baseRecord);\n+    TimestampBasedKeyGenerator keyGen = new TimestampBasedKeyGenerator(properties);\n+    HoodieKey hk5 = keyGen.getKey(baseRecord);\n     assertEquals(hk5.getPartitionPath(), \"2024-10-04 12\");\n+\n+    // test w/ Row\n+    baseRow = genericRecordToRow(baseRecord);\n+    keyGen.initializeRowKeyGenerator(structType, testStructName, testNamespace);\n+    assertEquals(\"2024-10-04 12\", keyGen.getPartitionPathFromRow(baseRow));\n   }\n \n   @Test\n   public void test_ExpectsMatch_SingleInputFormat_ISO8601WithMsZ_OutputTimezoneAsUTC() throws IOException {\n     baseRecord.put(\"createTime\", \"2020-04-01T13:01:33.428Z\");\n     properties = this.getBaseKeyConfig(\n-      \"DATE_STRING\",\n-      \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\",\n-      \"\",\n-      \"\",\n-      \"yyyyMMddHH\",\n-      \"GMT\");\n+        \"DATE_STRING\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0NzgwNQ==", "bodyText": "Note to reviewer: this is the test class where all key generators are tested for Row apis as well. Found new test classes for each key gen after rebasing. Yet to add tests to these new key gen test classes for Row based apis.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r466447805", "createdAt": "2020-08-06T14:20:27Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/TestDataSourceDefaults.scala", "diffHunk": "@@ -34,13 +36,28 @@ import org.scalatest.Assertions.fail\n class TestDataSourceDefaults {\n \n   val schema = SchemaTestUtil.getComplexEvolvedSchema\n+  val structType = AvroConversionUtils.convertAvroSchemaToStructType(schema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MDg0NzYz", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-457084763", "createdAt": "2020-07-28T22:56:30Z", "commit": {"oid": "3debefccc630604a1ed16974fd2ee331c5234501"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQyMjo1NjozMFrOG4ikmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMDo0Nzo0M1rOG-fPLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTkzOTg2Ng==", "bodyText": "so, this needs to be a separate class, because?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r461939866", "createdAt": "2020-07-28T22:56:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieInternalWriteStatus.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+/**\n+ * Hoodie's internal write status used in datasource implementation of bulk insert.\n+ */\n+public class HoodieInternalWriteStatus implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNTkyMg=="}, "originalCommit": {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3NTQ5OQ==", "bodyText": "one issue we need to think about is how we abstract the key generators out, so that even flink etc can use tthis? ideally we need to templatize GenericRecord, Row. this needs more thought. potentially beyond the scope of this PR", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468175499", "createdAt": "2020-08-10T20:45:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java", "diffHunk": "@@ -54,12 +51,17 @@ public String getPartitionPath(GenericRecord record) {\n   }\n \n   @Override\n-  public List<String> getRecordKeyFields() {\n-    return recordKeyFields;\n+  public List<String> getPartitionPathFields() {\n+    return new ArrayList<>();\n   }\n \n   @Override\n-  public List<String> getPartitionPathFields() {\n-    return new ArrayList<>();\n+  public String getRecordKeyFromRow(Row row) {\n+    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), getRecordKeyPositions(), true);\n+  }\n+\n+  @Override\n+  public String getPartitionPathFromRow(Row row) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3NjY4NA==", "bodyText": "we need not overload the operation type here. we can just introduce a boolean option separately.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468176684", "createdAt": "2020-08-10T20:47:43Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -129,6 +129,7 @@ object DataSourceWriteOptions {\n   val INSERT_OPERATION_OPT_VAL = \"insert\"\n   val UPSERT_OPERATION_OPT_VAL = \"upsert\"\n   val DELETE_OPERATION_OPT_VAL = \"delete\"\n+  val BULK_INSERT_DATASET_OPERATION_OPT_VAL = \"bulk_insert_dataset\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY0NzMwOTI4", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-464730928", "createdAt": "2020-08-11T03:54:33Z", "commit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwMzo1NDozM1rOG-nddA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwMzo1NDozM1rOG-nddA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODMxMTQxMg==", "bodyText": "these seem like formatting only change.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468311412", "createdAt": "2020-08-11T03:54:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -108,262 +106,280 @@ private[hudi] object HoodieSparkSqlWriter {\n           throw new HoodieException(s\"hoodie table with name $existingTableName already exist at $basePath\")\n         }\n       }\n-      val (writeStatuses, writeClient: HoodieWriteClient[HoodieRecordPayload[Nothing]]) =\n-        if (!operation.equalsIgnoreCase(DELETE_OPERATION_OPT_VAL)) {\n-          // register classes & schemas\n-          val structName = s\"${tblName}_record\"\n-          val nameSpace = s\"hoodie.${tblName}\"\n-          sparkContext.getConf.registerKryoClasses(\n-            Array(classOf[org.apache.avro.generic.GenericData],\n-              classOf[org.apache.avro.Schema]))\n-          val schema = AvroConversionUtils.convertStructTypeToAvroSchema(df.schema, structName, nameSpace)\n-          sparkContext.getConf.registerAvroSchemas(schema)\n-          log.info(s\"Registered avro schema : ${schema.toString(true)}\")\n-\n-          // Convert to RDD[HoodieRecord]\n-          val keyGenerator = DataSourceUtils.createKeyGenerator(toProperties(parameters))\n-          val genericRecords: RDD[GenericRecord] = AvroConversionUtils.createRdd(df, structName, nameSpace)\n-          val hoodieAllIncomingRecords = genericRecords.map(gr => {\n-            val orderingVal = HoodieAvroUtils.getNestedFieldVal(gr, parameters(PRECOMBINE_FIELD_OPT_KEY), false)\n-              .asInstanceOf[Comparable[_]]\n-            DataSourceUtils.createHoodieRecord(gr,\n-              orderingVal, keyGenerator.getKey(gr),\n-              parameters(PAYLOAD_CLASS_OPT_KEY))\n-          }).toJavaRDD()\n-\n-          // Handle various save modes\n-          if (mode == SaveMode.ErrorIfExists && exists) {\n-            throw new HoodieException(s\"hoodie table at $basePath already exists.\")\n-          }\n \n-          if (mode == SaveMode.Overwrite && exists) {\n-            log.warn(s\"hoodie table at $basePath already exists. Deleting existing data & overwriting with new data.\")\n-            fs.delete(basePath, true)\n-            exists = false\n-          }\n+      val (writeSuccessfulRetVal: Boolean, commitTimeRetVal: common.util.Option[String], compactionInstantRetVal: common.util.Option[String],\n+      writeClientRetVal: HoodieWriteClient[HoodieRecordPayload[Nothing]], tableConfigRetVal: HoodieTableConfig) =\n+         if (operation.equalsIgnoreCase(BULK_INSERT_DATASET_OPERATION_OPT_VAL)) {\n+        // register classes & schemas\n+        val structName = s\"${tblName}_record\"\n+        val nameSpace = s\"hoodie.${tblName}\"\n \n-          // Create the table if not present\n-          if (!exists) {\n-            //FIXME(bootstrap): bootstrapIndexClass needs to be set when bootstrap index class is integrated.\n-            val tableMetaClient = HoodieTableMetaClient.initTableTypeWithBootstrap(sparkContext.hadoopConfiguration,\n-              path.get, HoodieTableType.valueOf(tableType),\n-              tblName, \"archived\", parameters(PAYLOAD_CLASS_OPT_KEY), null, null, null)\n-            tableConfig = tableMetaClient.getTableConfig\n-          }\n+        // Handle various save modes\n+        if (mode == SaveMode.ErrorIfExists && exists) {\n+          throw new HoodieException(s\"hoodie table at $basePath already exists.\")\n+        }\n \n-          // Create a HoodieWriteClient & issue the write.\n-          val client = hoodieWriteClient.getOrElse(DataSourceUtils.createHoodieClient(jsc, schema.toString, path.get,\n-            tblName, mapAsJavaMap(parameters)\n-          )).asInstanceOf[HoodieWriteClient[HoodieRecordPayload[Nothing]]]\n+        val (success, commitTime: common.util.Option[String]) =\n+          if (mode == SaveMode.Ignore && exists) {\n+            log.warn(s\"hoodie table at $basePath already exists. Ignoring & not performing actual writes.\")\n+            (false, common.util.Option.ofNullable(instantTime))\n+          } else {\n+            if (mode == SaveMode.Overwrite && exists) {\n+              log.warn(s\"hoodie table at $basePath already exists. Deleting existing data & overwriting with new data.\")\n+              fs.delete(basePath, true)\n+              exists = false\n+            }\n \n-          if (asyncCompactionTriggerFn.isDefined &&\n-            isAsyncCompactionEnabled(client, tableConfig, parameters, jsc.hadoopConfiguration())) {\n-            asyncCompactionTriggerFn.get.apply(client)\n-          }\n+            // Create the table if not present\n+            if (!exists) {\n+              //FIXME(bootstrap): bootstrapIndexClass needs to be set when bootstrap index class is integrated.\n+              val tableMetaClient = HoodieTableMetaClient.initTableTypeWithBootstrap(sparkContext.hadoopConfiguration,\n+                path.get, HoodieTableType.valueOf(tableType),\n+                tblName, \"archived\", parameters(PAYLOAD_CLASS_OPT_KEY), null, null, null)\n+              tableConfig = tableMetaClient.getTableConfig\n+            }\n \n-          val hoodieRecords =\n-            if (parameters(INSERT_DROP_DUPS_OPT_KEY).toBoolean) {\n-              DataSourceUtils.dropDuplicates(jsc, hoodieAllIncomingRecords, mapAsJavaMap(parameters))\n+            val writeConfig = DataSourceUtils.createHoodieConfig(null, path.get, tblName,\n+              mapAsJavaMap(parameters))\n+\n+            val hoodieDF = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsert(sqlContext, writeConfig, df, structName, nameSpace)\n+            hoodieDF.write.format(\"org.apache.hudi.internal\").option(INSTANT_TIME, instantTime)\n+              .options(parameters).save()\n+            val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+            val syncHiveSucess = if (hiveSyncEnabled) {\n+              log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")\n+              val fs = FSUtils.getFs(basePath.toString, jsc.hadoopConfiguration)\n+              syncHive(basePath, fs, parameters)\n             } else {\n-              hoodieAllIncomingRecords\n+              true\n             }\n-\n-          if (hoodieRecords.isEmpty()) {\n-            log.info(\"new batch has no new records, skipping...\")\n-            (true, common.util.Option.empty())\n+            (syncHiveSucess, common.util.Option.ofNullable(instantTime))\n           }\n-          client.startCommitWithTime(instantTime)\n-          val writeStatuses = DataSourceUtils.doWriteOperation(client, hoodieRecords, instantTime, operation)\n-          (writeStatuses, client)\n-        } else {\n+        (success, commitTime, common.util.Option.of(\"\"), hoodieWriteClient.orNull, tableConfig)\n+       } else {\n+        val (writeStatuses, writeClient: HoodieWriteClient[HoodieRecordPayload[Nothing]]) =\n+          if (!operation.equalsIgnoreCase(DELETE_OPERATION_OPT_VAL)) {\n+            // register classes & schemas\n+            val structName = s\"${tblName}_record\"\n+            val nameSpace = s\"hoodie.${tblName}\"\n+            sparkContext.getConf.registerKryoClasses(\n+              Array(classOf[org.apache.avro.generic.GenericData],\n+                classOf[org.apache.avro.Schema]))\n+            val schema = AvroConversionUtils.convertStructTypeToAvroSchema(df.schema, structName, nameSpace)\n+            sparkContext.getConf.registerAvroSchemas(schema)\n+            log.info(s\"Registered avro schema : ${schema.toString(true)}\")\n+\n+            // Convert to RDD[HoodieRecord]\n+            val keyGenerator = DataSourceUtils.createKeyGenerator(HoodieWriterUtils.toProperties(parameters))\n+            val genericRecords: RDD[GenericRecord] = AvroConversionUtils.createRdd(df, structName, nameSpace)\n+            val hoodieAllIncomingRecords = genericRecords.map(gr => {\n+              val orderingVal = DataSourceUtils.getNestedFieldVal(gr, parameters(PRECOMBINE_FIELD_OPT_KEY), false)\n+                .asInstanceOf[Comparable[_]]\n+              DataSourceUtils.createHoodieRecord(gr,\n+                orderingVal, keyGenerator.getKey(gr), parameters(PAYLOAD_CLASS_OPT_KEY))\n+            }).toJavaRDD()\n+\n+            // Handle various save modes\n+            if (mode == SaveMode.ErrorIfExists && exists) {\n+              throw new HoodieException(s\"hoodie table at $basePath already exists.\")\n+            }\n+            if (mode == SaveMode.Ignore && exists) {\n+              log.warn(s\"hoodie table at $basePath already exists. Ignoring & not performing actual writes.\")\n+              (false, common.util.Option.empty())\n+            }\n+            if (mode == SaveMode.Overwrite && exists) {\n+              log.warn(s\"hoodie table at $basePath already exists. Deleting existing data & overwriting with new data.\")\n+              fs.delete(basePath, true)\n+              exists = false\n+            }\n \n-          // Handle save modes\n-          if (mode != SaveMode.Append) {\n-            throw new HoodieException(s\"Append is the only save mode applicable for $operation operation\")\n-          }\n+            // Create the table if not present\n+            if (!exists) {\n+              //FIXME(bootstrap): bootstrapIndexClass needs to be set when bootstrap index class is integrated.\n+              val tableMetaClient = HoodieTableMetaClient.initTableTypeWithBootstrap(sparkContext.hadoopConfiguration,\n+                path.get, HoodieTableType.valueOf(tableType),\n+                tblName, \"archived\", parameters(PAYLOAD_CLASS_OPT_KEY), null, null, null)\n+              tableConfig = tableMetaClient.getTableConfig\n+            }\n \n-          val structName = s\"${tblName}_record\"\n-          val nameSpace = s\"hoodie.${tblName}\"\n-          sparkContext.getConf.registerKryoClasses(\n-            Array(classOf[org.apache.avro.generic.GenericData],\n-              classOf[org.apache.avro.Schema]))\n+            // Create a HoodieWriteClient & issue the write.\n+            val client = DataSourceUtils.createHoodieClient(jsc, schema.toString, path.get, tblName,\n+              mapAsJavaMap(parameters)\n+            )\n+\n+            val hoodieRecords =\n+              if (parameters(INSERT_DROP_DUPS_OPT_KEY).toBoolean) {\n+                DataSourceUtils.dropDuplicates(jsc, hoodieAllIncomingRecords, mapAsJavaMap(parameters))\n+              } else {\n+                hoodieAllIncomingRecords\n+              }\n+\n+            if (hoodieRecords.isEmpty()) {\n+              log.info(\"new batch has no new records, skipping...\")\n+              (true, common.util.Option.empty())\n+            }\n+            client.startCommitWithTime(instantTime)\n+            val writeStatuses = DataSourceUtils.doWriteOperation(client, hoodieRecords, instantTime, operation)\n+            (writeStatuses, client)\n+          } else {\n+\n+            // Handle save modes\n+            if (mode != SaveMode.Append) {\n+              throw new HoodieException(s\"Append is the only save mode applicable for $operation operation\")\n+            }\n \n-          // Convert to RDD[HoodieKey]\n-          val keyGenerator = DataSourceUtils.createKeyGenerator(toProperties(parameters))\n-          val genericRecords: RDD[GenericRecord] = AvroConversionUtils.createRdd(df, structName, nameSpace)\n-          val hoodieKeysToDelete = genericRecords.map(gr => keyGenerator.getKey(gr)).toJavaRDD()\n+            val structName = s\"${tblName}_record\"\n+            val nameSpace = s\"hoodie.${tblName}\"\n+            sparkContext.getConf.registerKryoClasses(\n+              Array(classOf[org.apache.avro.generic.GenericData],\n+                classOf[org.apache.avro.Schema]))\n \n-          if (!exists) {\n-            throw new HoodieException(s\"hoodie table at $basePath does not exist\")\n-          }\n+            // Convert to RDD[HoodieKey]\n+            val keyGenerator = DataSourceUtils.createKeyGenerator(HoodieWriterUtils.toProperties(parameters))\n+            val genericRecords: RDD[GenericRecord] = AvroConversionUtils.createRdd(df, structName, nameSpace)\n+            val hoodieKeysToDelete = genericRecords.map(gr => keyGenerator.getKey(gr)).toJavaRDD()\n+\n+            if (!exists) {\n+              throw new HoodieException(s\"hoodie table at $basePath does not exist\")\n+            }\n \n-          // Create a HoodieWriteClient & issue the delete.\n-          val client = hoodieWriteClient.getOrElse(DataSourceUtils.createHoodieClient(jsc,\n-            Schema.create(Schema.Type.NULL).toString, path.get, tblName,\n-            mapAsJavaMap(parameters))).asInstanceOf[HoodieWriteClient[HoodieRecordPayload[Nothing]]]\n+            // Create a HoodieWriteClient & issue the delete.\n+            val client = DataSourceUtils.createHoodieClient(jsc,\n+              Schema.create(Schema.Type.NULL).toString, path.get, tblName,\n+              mapAsJavaMap(parameters)\n+            )\n \n-          if (asyncCompactionTriggerFn.isDefined &&\n-            isAsyncCompactionEnabled(client, tableConfig, parameters, jsc.hadoopConfiguration())) {\n-            asyncCompactionTriggerFn.get.apply(client)\n+            // Issue deletes\n+            client.startCommitWithTime(instantTime)\n+            val writeStatuses = DataSourceUtils.doDeleteOperation(client, hoodieKeysToDelete, instantTime)\n+            (writeStatuses, client)\n           }\n \n-          // Issue deletes\n-          client.startCommitWithTime(instantTime)\n-          val writeStatuses = DataSourceUtils.doDeleteOperation(client, hoodieKeysToDelete, instantTime)\n-          (writeStatuses, client)\n+        // Check for errors and commit the write.\n+        val (writeSuccessful, compactionInstant) =\n+          commitAndPerformPostOperations(writeStatuses, parameters, writeClient, tableConfig, instantTime, basePath,\n+            operation, jsc)\n+        (writeSuccessful, common.util.Option.ofNullable(instantTime), compactionInstant, writeClient, tableConfig)\n         }\n-\n-      // Check for errors and commit the write.\n-      val (writeSuccessful, compactionInstant) =\n-        commitAndPerformPostOperations(writeStatuses, parameters, writeClient, tableConfig, instantTime, basePath,\n-          operation, jsc)\n-      (writeSuccessful, common.util.Option.ofNullable(instantTime), compactionInstant, writeClient, tableConfig)\n+      (writeSuccessfulRetVal, commitTimeRetVal, compactionInstantRetVal, writeClientRetVal, tableConfigRetVal)\n     }\n   }\n \n-  /**\n-    * Add default options for unspecified write options keys.\n-    *\n-    * @param parameters\n-    * @return\n-    */\n-  def parametersWithWriteDefaults(parameters: Map[String, String]): Map[String, String] = {\n-    Map(OPERATION_OPT_KEY -> DEFAULT_OPERATION_OPT_VAL,\n-      TABLE_TYPE_OPT_KEY -> DEFAULT_TABLE_TYPE_OPT_VAL,\n-      PRECOMBINE_FIELD_OPT_KEY -> DEFAULT_PRECOMBINE_FIELD_OPT_VAL,\n-      PAYLOAD_CLASS_OPT_KEY -> DEFAULT_PAYLOAD_OPT_VAL,\n-      RECORDKEY_FIELD_OPT_KEY -> DEFAULT_RECORDKEY_FIELD_OPT_VAL,\n-      PARTITIONPATH_FIELD_OPT_KEY -> DEFAULT_PARTITIONPATH_FIELD_OPT_VAL,\n-      KEYGENERATOR_CLASS_OPT_KEY -> DEFAULT_KEYGENERATOR_CLASS_OPT_VAL,\n-      COMMIT_METADATA_KEYPREFIX_OPT_KEY -> DEFAULT_COMMIT_METADATA_KEYPREFIX_OPT_VAL,\n-      INSERT_DROP_DUPS_OPT_KEY -> DEFAULT_INSERT_DROP_DUPS_OPT_VAL,\n-      STREAMING_RETRY_CNT_OPT_KEY -> DEFAULT_STREAMING_RETRY_CNT_OPT_VAL,\n-      STREAMING_RETRY_INTERVAL_MS_OPT_KEY -> DEFAULT_STREAMING_RETRY_INTERVAL_MS_OPT_VAL,\n-      STREAMING_IGNORE_FAILED_BATCH_OPT_KEY -> DEFAULT_STREAMING_IGNORE_FAILED_BATCH_OPT_VAL,\n-      HIVE_SYNC_ENABLED_OPT_KEY -> DEFAULT_HIVE_SYNC_ENABLED_OPT_VAL,\n-      HIVE_DATABASE_OPT_KEY -> DEFAULT_HIVE_DATABASE_OPT_VAL,\n-      HIVE_TABLE_OPT_KEY -> DEFAULT_HIVE_TABLE_OPT_VAL,\n-      HIVE_BASE_FILE_FORMAT_OPT_KEY -> DEFAULT_HIVE_BASE_FILE_FORMAT_OPT_VAL,\n-      HIVE_USER_OPT_KEY -> DEFAULT_HIVE_USER_OPT_VAL,\n-      HIVE_PASS_OPT_KEY -> DEFAULT_HIVE_PASS_OPT_VAL,\n-      HIVE_URL_OPT_KEY -> DEFAULT_HIVE_URL_OPT_VAL,\n-      HIVE_PARTITION_FIELDS_OPT_KEY -> DEFAULT_HIVE_PARTITION_FIELDS_OPT_VAL,\n-      HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY -> DEFAULT_HIVE_PARTITION_EXTRACTOR_CLASS_OPT_VAL,\n-      HIVE_STYLE_PARTITIONING_OPT_KEY -> DEFAULT_HIVE_STYLE_PARTITIONING_OPT_VAL,\n-      HIVE_USE_JDBC_OPT_KEY -> DEFAULT_HIVE_USE_JDBC_OPT_VAL,\n-      ASYNC_COMPACT_ENABLE_KEY -> DEFAULT_ASYNC_COMPACT_ENABLE_VAL\n-    ) ++ translateStorageTypeToTableType(parameters)\n-  }\n+    private def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String]): Boolean\n \n-  def toProperties(params: Map[String, String]): TypedProperties = {\n-    val props = new TypedProperties()\n-    params.foreach(kv => props.setProperty(kv._1, kv._2))\n-    props\n-  }\n+    =\n+    {\n+      val hiveSyncConfig: HiveSyncConfig = buildSyncConfig(basePath, parameters)\n+      val hiveConf: HiveConf = new HiveConf()\n+      hiveConf.addResource(fs.getConf)\n+      new HiveSyncTool(hiveSyncConfig, hiveConf, fs).syncHoodieTable()\n+      true\n+    }\n \n-  private def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String]): Boolean = {\n-    val hiveSyncConfig: HiveSyncConfig = buildSyncConfig(basePath, parameters)\n-    val hiveConf: HiveConf = new HiveConf()\n-    hiveConf.addResource(fs.getConf)\n-    new HiveSyncTool(hiveSyncConfig, hiveConf, fs).syncHoodieTable()\n-    true\n-  }\n+    private def buildSyncConfig(basePath: Path, parameters: Map[String, String]): HiveSyncConfig\n+\n+    =\n+    {\n+      val hiveSyncConfig: HiveSyncConfig = new HiveSyncConfig()\n+      hiveSyncConfig.basePath = basePath.toString\n+      hiveSyncConfig.baseFileFormat = parameters(HIVE_BASE_FILE_FORMAT_OPT_KEY);\n+      hiveSyncConfig.usePreApacheInputFormat =\n+        parameters.get(HIVE_USE_PRE_APACHE_INPUT_FORMAT_OPT_KEY).exists(r => r.toBoolean)\n+      hiveSyncConfig.databaseName = parameters(HIVE_DATABASE_OPT_KEY)\n+      hiveSyncConfig.tableName = parameters(HIVE_TABLE_OPT_KEY)\n+      hiveSyncConfig.hiveUser = parameters(HIVE_USER_OPT_KEY)\n+      hiveSyncConfig.hivePass = parameters(HIVE_PASS_OPT_KEY)\n+      hiveSyncConfig.jdbcUrl = parameters(HIVE_URL_OPT_KEY)\n+      hiveSyncConfig.partitionFields =\n+        ListBuffer(parameters(HIVE_PARTITION_FIELDS_OPT_KEY).split(\",\").map(_.trim).filter(!_.isEmpty).toList: _*)\n+      hiveSyncConfig.partitionValueExtractorClass = parameters(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY)\n+      hiveSyncConfig.useJdbc = parameters(HIVE_USE_JDBC_OPT_KEY).toBoolean\n+      hiveSyncConfig\n+    }\n \n-  private def buildSyncConfig(basePath: Path, parameters: Map[String, String]): HiveSyncConfig = {\n-    val hiveSyncConfig: HiveSyncConfig = new HiveSyncConfig()\n-    hiveSyncConfig.basePath = basePath.toString\n-    hiveSyncConfig.baseFileFormat = parameters(HIVE_BASE_FILE_FORMAT_OPT_KEY);\n-    hiveSyncConfig.usePreApacheInputFormat =\n-      parameters.get(HIVE_USE_PRE_APACHE_INPUT_FORMAT_OPT_KEY).exists(r => r.toBoolean)\n-    hiveSyncConfig.databaseName = parameters(HIVE_DATABASE_OPT_KEY)\n-    hiveSyncConfig.tableName = parameters(HIVE_TABLE_OPT_KEY)\n-    hiveSyncConfig.hiveUser = parameters(HIVE_USER_OPT_KEY)\n-    hiveSyncConfig.hivePass = parameters(HIVE_PASS_OPT_KEY)\n-    hiveSyncConfig.jdbcUrl = parameters(HIVE_URL_OPT_KEY)\n-    hiveSyncConfig.partitionFields =\n-      ListBuffer(parameters(HIVE_PARTITION_FIELDS_OPT_KEY).split(\",\").map(_.trim).filter(!_.isEmpty).toList: _*)\n-    hiveSyncConfig.partitionValueExtractorClass = parameters(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY)\n-    hiveSyncConfig.useJdbc = parameters(HIVE_USE_JDBC_OPT_KEY).toBoolean\n-    hiveSyncConfig\n-  }\n+    private def commitAndPerformPostOperations(writeStatuses: JavaRDD[WriteStatus],\n+                                               parameters: Map[String, String],\n+                                               client: HoodieWriteClient[HoodieRecordPayload[Nothing]],\n+                                               tableConfig: HoodieTableConfig,\n+                                               instantTime: String,\n+                                               basePath: Path,\n+                                               operation: String,\n+                                               jsc: JavaSparkContext): (Boolean, common.util.Option[java.lang.String])\n+\n+    =\n+    {\n+      val errorCount = writeStatuses.rdd.filter(ws => ws.hasErrors).count()\n+      if (errorCount == 0) {\n+        log.info(\"No errors. Proceeding to commit the write.\")\n+        val metaMap = parameters.filter(kv =>\n+          kv._1.startsWith(parameters(COMMIT_METADATA_KEYPREFIX_OPT_KEY)))\n+        val commitSuccess = if (metaMap.isEmpty) {\n+          client.commit(instantTime, writeStatuses)\n+        } else {\n+          val extraMetadata: util.Map[String, String] = new util.HashMap[String, String](mapAsJavaMap(metaMap));\n+          client.commit(instantTime, writeStatuses, common.util.Option.of(extraMetadata))\n+        }\n \n-  private def commitAndPerformPostOperations(writeStatuses: JavaRDD[WriteStatus],\n-                                             parameters: Map[String, String],\n-                                             client: HoodieWriteClient[HoodieRecordPayload[Nothing]],\n-                                             tableConfig: HoodieTableConfig,\n-                                             instantTime: String,\n-                                             basePath: Path,\n-                                             operation: String,\n-                                             jsc: JavaSparkContext): (Boolean, common.util.Option[java.lang.String]) = {\n-    val errorCount = writeStatuses.rdd.filter(ws => ws.hasErrors).count()\n-    if (errorCount == 0) {\n-      log.info(\"No errors. Proceeding to commit the write.\")\n-      val metaMap = parameters.filter(kv =>\n-        kv._1.startsWith(parameters(COMMIT_METADATA_KEYPREFIX_OPT_KEY)))\n-      val commitSuccess = if (metaMap.isEmpty) {\n-        client.commit(instantTime, writeStatuses)\n-      } else {\n-        client.commit(instantTime, writeStatuses,\n-          common.util.Option.of(new util.HashMap[String, String](mapAsJavaMap(metaMap))))\n-      }\n+        if (commitSuccess) {\n+          log.info(\"Commit \" + instantTime + \" successful!\")\n+        }\n+        else {\n+          log.info(\"Commit \" + instantTime + \" failed!\")\n+        }\n \n-      if (commitSuccess) {\n-        log.info(\"Commit \" + instantTime + \" successful!\")\n-      }\n-      else {\n-        log.info(\"Commit \" + instantTime + \" failed!\")\n-      }\n+        val asyncCompactionEnabled = isAsyncCompactionEnabled(client, tableConfig, parameters, jsc.hadoopConfiguration())\n+        val compactionInstant: common.util.Option[java.lang.String] =\n+          if (asyncCompactionEnabled) {\n+            client.scheduleCompaction(common.util.Option.of(new util.HashMap[String, String](mapAsJavaMap(metaMap))))\n+          } else {\n+            common.util.Option.empty()\n+          }\n \n-      val asyncCompactionEnabled = isAsyncCompactionEnabled(client, tableConfig, parameters, jsc.hadoopConfiguration())\n-      val compactionInstant : common.util.Option[java.lang.String] =\n-      if (asyncCompactionEnabled) {\n-        client.scheduleCompaction(common.util.Option.of(new util.HashMap[String, String](mapAsJavaMap(metaMap))))\n-      } else {\n-        common.util.Option.empty()\n-      }\n+        log.info(s\"Compaction Scheduled is $compactionInstant\")\n+        val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+        val syncHiveSucess = if (hiveSyncEnabled) {\n+          log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")\n+          val fs = FSUtils.getFs(basePath.toString, jsc.hadoopConfiguration)\n+          syncHive(basePath, fs, parameters)\n+        } else {\n+          true\n+        }\n \n-      log.info(s\"Compaction Scheduled is $compactionInstant\")\n-      val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n-      val syncHiveSucess = if (hiveSyncEnabled) {\n-        log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")\n-        val fs = FSUtils.getFs(basePath.toString, jsc.hadoopConfiguration)\n-        syncHive(basePath, fs, parameters)\n+        log.info(s\"Is Async Compaction Enabled ? $asyncCompactionEnabled\")\n+        if (!asyncCompactionEnabled) {\n+          client.close()\n+        }\n+        (commitSuccess && syncHiveSucess, compactionInstant)\n       } else {\n-        true\n-      }\n-\n-      log.info(s\"Is Async Compaction Enabled ? $asyncCompactionEnabled\")\n-      if (!asyncCompactionEnabled) {\n-        client.close()\n-      }\n-      (commitSuccess && syncHiveSucess, compactionInstant)\n-    } else {\n-      log.error(s\"$operation failed with $errorCount errors :\")\n-      if (log.isTraceEnabled) {\n-        log.trace(\"Printing out the top 100 errors\")\n-        writeStatuses.rdd.filter(ws => ws.hasErrors)\n-          .take(100)\n-          .foreach(ws => {\n-            log.trace(\"Global error :\", ws.getGlobalError)\n-            if (ws.getErrors.size() > 0) {\n-              ws.getErrors.foreach(kt =>\n-                log.trace(s\"Error for key: ${kt._1}\", kt._2))\n-            }\n-          })\n+        log.error(s\"$operation failed with $errorCount errors :\")\n+        if (log.isTraceEnabled) {\n+          log.trace(\"Printing out the top 100 errors\")\n+          writeStatuses.rdd.filter(ws => ws.hasErrors)\n+            .take(100)\n+            .foreach(ws => {\n+              log.trace(\"Global error :\", ws.getGlobalError)\n+              if (ws.getErrors.size() > 0) {\n+                ws.getErrors.foreach(kt =>\n+                  log.trace(s\"Error for key: ${kt._1}\", kt._2))\n+              }\n+            })\n+        }\n+        (false, common.util.Option.empty())\n       }\n-      (false, common.util.Option.empty())\n     }\n-  }\n \n-  private def isAsyncCompactionEnabled(client: HoodieWriteClient[HoodieRecordPayload[Nothing]],\n-                                       tableConfig: HoodieTableConfig,\n-                                       parameters: Map[String, String], configuration: Configuration) : Boolean = {\n-    log.info(s\"Config.isInlineCompaction ? ${client.getConfig.isInlineCompaction}\")\n-    if (!client.getConfig.isInlineCompaction\n-      && parameters.get(ASYNC_COMPACT_ENABLE_KEY).exists(r => r.toBoolean)) {\n-      tableConfig.getTableType == HoodieTableType.MERGE_ON_READ\n-    } else {\n-      false\n+    private def isAsyncCompactionEnabled(client: HoodieWriteClient[HoodieRecordPayload[Nothing]],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 515}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY0NzU0ODQ0", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-464754844", "createdAt": "2020-08-11T05:19:26Z", "commit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwNToxOToyNlrOG-ot-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwNToxOToyNlrOG-ot-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODMzMjAyNw==", "bodyText": "this whole block is not indented at the right level. I am going to try and apply changes from this file line-by-line onto latest file on master", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468332027", "createdAt": "2020-08-11T05:19:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -108,262 +106,280 @@ private[hudi] object HoodieSparkSqlWriter {\n           throw new HoodieException(s\"hoodie table with name $existingTableName already exist at $basePath\")\n         }\n       }\n-      val (writeStatuses, writeClient: HoodieWriteClient[HoodieRecordPayload[Nothing]]) =\n-        if (!operation.equalsIgnoreCase(DELETE_OPERATION_OPT_VAL)) {\n-          // register classes & schemas\n-          val structName = s\"${tblName}_record\"\n-          val nameSpace = s\"hoodie.${tblName}\"\n-          sparkContext.getConf.registerKryoClasses(\n-            Array(classOf[org.apache.avro.generic.GenericData],\n-              classOf[org.apache.avro.Schema]))\n-          val schema = AvroConversionUtils.convertStructTypeToAvroSchema(df.schema, structName, nameSpace)\n-          sparkContext.getConf.registerAvroSchemas(schema)\n-          log.info(s\"Registered avro schema : ${schema.toString(true)}\")\n-\n-          // Convert to RDD[HoodieRecord]\n-          val keyGenerator = DataSourceUtils.createKeyGenerator(toProperties(parameters))\n-          val genericRecords: RDD[GenericRecord] = AvroConversionUtils.createRdd(df, structName, nameSpace)\n-          val hoodieAllIncomingRecords = genericRecords.map(gr => {\n-            val orderingVal = HoodieAvroUtils.getNestedFieldVal(gr, parameters(PRECOMBINE_FIELD_OPT_KEY), false)\n-              .asInstanceOf[Comparable[_]]\n-            DataSourceUtils.createHoodieRecord(gr,\n-              orderingVal, keyGenerator.getKey(gr),\n-              parameters(PAYLOAD_CLASS_OPT_KEY))\n-          }).toJavaRDD()\n-\n-          // Handle various save modes\n-          if (mode == SaveMode.ErrorIfExists && exists) {\n-            throw new HoodieException(s\"hoodie table at $basePath already exists.\")\n-          }\n \n-          if (mode == SaveMode.Overwrite && exists) {\n-            log.warn(s\"hoodie table at $basePath already exists. Deleting existing data & overwriting with new data.\")\n-            fs.delete(basePath, true)\n-            exists = false\n-          }\n+      val (writeSuccessfulRetVal: Boolean, commitTimeRetVal: common.util.Option[String], compactionInstantRetVal: common.util.Option[String],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 61}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/dacd635367b59e7d6b8de91f6b785b337dd851eb", "committedDate": "2020-08-06T11:59:01Z", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance"}, "afterCommit": {"oid": "06e969339fa52f9bd3ffb77ab26b5677143b458a", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/06e969339fa52f9bd3ffb77ab26b5677143b458a", "committedDate": "2020-08-11T06:47:48Z", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d491a32d85239b30b4321023a5001979b12c6abb", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/d491a32d85239b30b4321023a5001979b12c6abb", "committedDate": "2020-08-11T11:18:57Z", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "06e969339fa52f9bd3ffb77ab26b5677143b458a", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/06e969339fa52f9bd3ffb77ab26b5677143b458a", "committedDate": "2020-08-11T06:47:48Z", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance"}, "afterCommit": {"oid": "d491a32d85239b30b4321023a5001979b12c6abb", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/d491a32d85239b30b4321023a5001979b12c6abb", "committedDate": "2020-08-11T11:18:57Z", "message": "Bulk Insert Dataset Based Implementation using Datasource to improve performance"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1MTY0Njkx", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-465164691", "createdAt": "2020-08-11T15:10:18Z", "commit": {"oid": "866ee723613c533321d7814fb5c9bd9982e97d7e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxNToxMDoxOVrOG-8gXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxNToxMDoxOVrOG-8gXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODY1NjIyMQ==", "bodyText": "nit: can you make the 3rd arg Option.empty. when I put up the PR, I got compilation issues and hence returned empty string. I tested Option.empty locally with latest change and compilation seems to succeed.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468656221", "createdAt": "2020-08-11T15:10:19Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -105,6 +104,22 @@ private[hudi] object HoodieSparkSqlWriter {\n     } else {\n       // Handle various save modes\n       handleSaveModes(mode, basePath, tableConfig, tblName, operation, fs)\n+      // Create the table if not present\n+      if (!tableExists) {\n+        val tableMetaClient = HoodieTableMetaClient.initTableType(sparkContext.hadoopConfiguration, path.get,\n+          HoodieTableType.valueOf(tableType), tblName, \"archived\", parameters(PAYLOAD_CLASS_OPT_KEY),\n+          null.asInstanceOf[String])\n+        tableConfig = tableMetaClient.getTableConfig\n+      }\n+\n+      // short-circuit if bulk_insert via row is enabled.\n+      // scalastyle:off\n+      if (operation.equalsIgnoreCase(BULK_INSERT_DATASET_OPERATION_OPT_VAL)) {\n+        val (success, commitTime: common.util.Option[String]) = bulkInsertAsRow(sqlContext, parameters, df, tblName,\n+                                                                                basePath, path, instantTime)\n+        return (success, commitTime, common.util.Option.of(\"\"), hoodieWriteClient.orNull, tableConfig)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "866ee723613c533321d7814fb5c9bd9982e97d7e"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1MTY0OTY2", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-465164966", "createdAt": "2020-08-11T15:10:38Z", "commit": {"oid": "866ee723613c533321d7814fb5c9bd9982e97d7e"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "866ee723613c533321d7814fb5c9bd9982e97d7e", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/866ee723613c533321d7814fb5c9bd9982e97d7e", "committedDate": "2020-08-11T13:42:12Z", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures"}, "afterCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "committedDate": "2020-08-11T16:06:51Z", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1Mjc3OTU4", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-465277958", "createdAt": "2020-08-11T17:26:32Z", "commit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "state": "COMMENTED", "comments": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxNzoyNjozMlrOG_B6rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxODozODozMVrOG_EZ1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NDg3Nw==", "bodyText": "note to self: need to understand this better and see if we can simplify", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468744877", "createdAt": "2020-08-11T17:26:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0NTk1Nw==", "bodyText": "as far as I can tell, this is private and set to null by default and not assigned anywhere else. so we will never pass if (null != ..) check. I think this should be if (null ==converterFn) if the intention was lazy initialization.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468745957", "createdAt": "2020-08-11T17:28:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0OTY0MA==", "bodyText": "got this when compiling\nError:(433, 16) overloaded method value commit with alternatives:\n  (x$1: String,x$2: java.util.List[org.apache.hudi.common.model.HoodieWriteStat],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean <and>\n  (x$1: String,x$2: org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus],x$3: org.apache.hudi.common.util.Option[java.util.Map[String,String]])Boolean\n cannot be applied to (String, org.apache.spark.api.java.JavaRDD[org.apache.hudi.client.WriteStatus], org.apache.hudi.common.util.Option[java.util.HashMap[String,String]])\n        client.commit(instantTime, writeStatuses,", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468749640", "createdAt": "2020-08-11T17:34:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -95,20 +95,20 @@ public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n       Option<Map<String, String>> extraMetadata) {\n-    HoodieTableMetaClient metaClient = createMetaClient(false);\n-    return commit(instantTime, writeStatuses, extraMetadata, metaClient.getCommitActionType());\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStat(instantTime, stats, extraMetadata);\n   }\n \n-  private boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata, String actionType) {\n-\n+  // fixme(bulkinsertv2) this name is ughh\n+  public boolean commitStat(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1MDI0MA==", "bodyText": "Looks like we cannot avoid a new public API. so might as well rename", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468750240", "createdAt": "2020-08-11T17:35:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -95,20 +95,20 @@ public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses) {\n    */\n   public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n       Option<Map<String, String>> extraMetadata) {\n-    HoodieTableMetaClient metaClient = createMetaClient(false);\n-    return commit(instantTime, writeStatuses, extraMetadata, metaClient.getCommitActionType());\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStat(instantTime, stats, extraMetadata);\n   }\n \n-  private boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n-      Option<Map<String, String>> extraMetadata, String actionType) {\n-\n+  // fixme(bulkinsertv2) this name is ughh\n+  public boolean commitStat(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc0OTY0MA=="}, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1NTk2Nw==", "bodyText": "note to self : check if this is indeed correct. i was expecting us to do something like row.setNullAt(i-5)", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468755967", "createdAt": "2020-08-11T17:45:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/model/HoodieInternalRow.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.model;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Internal Row implementation for Hoodie Row. It wraps an {@link InternalRow} and keeps meta columns locally. But the {@link InternalRow}\n+ * does include the meta columns as well just that {@link HoodieInternalRow} will intercept queries for meta columns and serve from its\n+ * copy rather than fetching from {@link InternalRow}.\n+ */\n+public class HoodieInternalRow extends InternalRow {\n+\n+  private String commitTime;\n+  private String commitSeqNumber;\n+  private String recordKey;\n+  private String partitionPath;\n+  private String fileName;\n+  private InternalRow row;\n+\n+  public HoodieInternalRow(String commitTime, String commitSeqNumber, String recordKey, String partitionPath,\n+      String fileName, InternalRow row) {\n+    this.commitTime = commitTime;\n+    this.commitSeqNumber = commitSeqNumber;\n+    this.recordKey = recordKey;\n+    this.partitionPath = partitionPath;\n+    this.fileName = fileName;\n+    this.row = row;\n+  }\n+\n+  @Override\n+  public int numFields() {\n+    return row.numFields();\n+  }\n+\n+  @Override\n+  public void setNullAt(int i) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = null;\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = null;\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = null;\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = null;\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = null;\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.setNullAt(i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2MTA4NQ==", "bodyText": "rename to getMetaColumnVal", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468761085", "createdAt": "2020-08-11T17:54:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/model/HoodieInternalRow.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.model;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Internal Row implementation for Hoodie Row. It wraps an {@link InternalRow} and keeps meta columns locally. But the {@link InternalRow}\n+ * does include the meta columns as well just that {@link HoodieInternalRow} will intercept queries for meta columns and serve from its\n+ * copy rather than fetching from {@link InternalRow}.\n+ */\n+public class HoodieInternalRow extends InternalRow {\n+\n+  private String commitTime;\n+  private String commitSeqNumber;\n+  private String recordKey;\n+  private String partitionPath;\n+  private String fileName;\n+  private InternalRow row;\n+\n+  public HoodieInternalRow(String commitTime, String commitSeqNumber, String recordKey, String partitionPath,\n+      String fileName, InternalRow row) {\n+    this.commitTime = commitTime;\n+    this.commitSeqNumber = commitSeqNumber;\n+    this.recordKey = recordKey;\n+    this.partitionPath = partitionPath;\n+    this.fileName = fileName;\n+    this.row = row;\n+  }\n+\n+  @Override\n+  public int numFields() {\n+    return row.numFields();\n+  }\n+\n+  @Override\n+  public void setNullAt(int i) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = null;\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = null;\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = null;\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = null;\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = null;\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.setNullAt(i);\n+    }\n+  }\n+\n+  @Override\n+  public void update(int i, Object value) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = value.toString();\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = value.toString();\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = value.toString();\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = value.toString();\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = value.toString();\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.update(i, value);\n+    }\n+  }\n+\n+  private String getHoodieColumnVal(int ordinal) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2NjExMQ==", "bodyText": "@bvaradar  @nsivabalan why would this be null", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468766111", "createdAt": "2020-08-11T18:03:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -670,7 +670,9 @@ public Builder withPath(String basePath) {\n     }\n \n     public Builder withSchema(String schemaStr) {\n-      props.setProperty(AVRO_SCHEMA, schemaStr);\n+      if (null != schemaStr) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNjk3OQ=="}, "originalCommit": {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ==", "bodyText": "should we be hardcoding these?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468768261", "createdAt": "2020-08-11T18:06:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieRowParquetWriteSupport.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.HoodieDynamicBoundedBloomFilter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.HashMap;\n+\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_MAX_RECORD_KEY_FOOTER;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_MIN_RECORD_KEY_FOOTER;\n+\n+/**\n+ * Hoodie Write Support for directly writing Row to Parquet.\n+ */\n+public class HoodieRowParquetWriteSupport extends ParquetWriteSupport {\n+\n+  private Configuration hadoopConf;\n+  private BloomFilter bloomFilter;\n+  private String minRecordKey;\n+  private String maxRecordKey;\n+\n+  public HoodieRowParquetWriteSupport(Configuration conf, StructType structType, BloomFilter bloomFilter) {\n+    super();\n+    Configuration hadoopConf = new Configuration(conf);\n+    hadoopConf.set(\"spark.sql.parquet.writeLegacyFormat\", \"false\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2OTU5Mg==", "bodyText": "can this be just passed to the getRecordKey() methods or overload a constructor? sticking a random init() method here is not very desirable.\nOverall, this ties the KeyGenerator tightly with Spark. for e.g when we do flink, writing a key generator would require a Spark dependency for a flink job. This need more thought.\ncc @bvaradar @leesf @nsivabalan", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468769592", "createdAt": "2020-08-11T18:09:21Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/keygen/KeyGenerator.java", "diffHunk": "@@ -51,4 +53,32 @@ protected KeyGenerator(TypedProperties config) {\n     throw new UnsupportedOperationException(\"Bootstrap not supported for key generator. \"\n         + \"Please override this method in your custom key generator.\");\n   }\n+\n+  /**\n+   * Initializes {@link KeyGenerator} for {@link Row} based operations.\n+   * @param structType structype of the dataset.\n+   * @param structName struct name of the dataset.\n+   * @param recordNamespace record namespace of the dataset.\n+   */\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4Njg5Mw=="}, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MDYwMw==", "bodyText": "this is a misleading name. Need to rename this. its unclear if it refers to a hoodie dataset or a spark dataset framework.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468770603", "createdAt": "2020-08-11T18:11:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieDatasetTestUtils.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer$;\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import scala.collection.JavaConversions;\n+import scala.collection.JavaConverters;\n+\n+import static org.apache.hudi.common.testutils.FileSystemTestUtils.RANDOM;\n+\n+/**\n+ * Dataset test utils.\n+ */\n+public class HoodieDatasetTestUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MTUyNw==", "bodyText": "Need to understand why this is needed. so, we pick a different mode for the writer path I believe. We should use a config and not overload further if possible.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468771527", "createdAt": "2020-08-11T18:12:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -35,6 +35,7 @@\n   // bulk insert\n   BULK_INSERT(\"bulk_insert\"),\n   BULK_INSERT_PREPPED(\"bulk_insert_prepped\"),\n+  BULK_INSERT_DATASET(\"bulk_insert_dataset\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MjY1OQ==", "bodyText": "another general rule of thumb. we could always review our own diffs again before submitting to make sure whitespace changes are all intentional. cc @nsivabalan .", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468772659", "createdAt": "2020-08-11T18:14:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java", "diffHunk": "@@ -267,26 +258,26 @@ public static HoodieWriteClient createHoodieClient(JavaSparkContext jssc, String\n   }\n \n   public static JavaRDD<WriteStatus> doDeleteOperation(HoodieWriteClient client, JavaRDD<HoodieKey> hoodieKeys,\n-                                                       String instantTime) {\n+      String instantTime) {\n     return client.delete(hoodieKeys, instantTime);\n   }\n \n   public static HoodieRecord createHoodieRecord(GenericRecord gr, Comparable orderingVal, HoodieKey hKey,\n-                                                String payloadClass) throws IOException {\n+      String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MzM4MQ==", "bodyText": "is thre a way to avoid using positions and use names instead?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468773381", "createdAt": "2020-08-11T18:15:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Mzk5MQ==", "bodyText": "Also, this being in BuiltinKeyGenerator and not KeyGenerator is a problem and will break all the custom key generators out there when they turn on row based writing, correct? should we move this up?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468773991", "createdAt": "2020-08-11T18:17:09Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3Njc1Ng==", "bodyText": "might be good to assert this out in the constructor itself", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468776756", "createdAt": "2020-08-11T18:22:02Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/SimpleKeyGenerator.java", "diffHunk": "@@ -55,21 +51,22 @@ public SimpleKeyGenerator(TypedProperties props, String partitionPathField) {\n \n   @Override\n   public String getRecordKey(GenericRecord record) {\n-    return KeyGenUtils.getRecordKey(record, recordKeyField);\n+    return KeyGenUtils.getRecordKey(record, getRecordKeyFields().get(0));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MTE1Mg==", "bodyText": "need to look at this line-by-line again and see if its all good.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468781152", "createdAt": "2020-08-11T18:30:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -129,45 +134,54 @@ public String getPartitionPath(GenericRecord record) {\n     if (partitionVal == null) {\n       partitionVal = 1L;\n     }\n+    try {\n+      return getPartitionPath(partitionVal);\n+    } catch (Exception e) {\n+      throw new HoodieDeltaStreamerException(\"Unable to parse input partition field :\" + partitionVal, e);\n+    }\n+  }\n \n+  /**\n+   * Parse and fetch partition path based on data type.\n+   *\n+   * @param partitionVal partition path object value fetched from record/row\n+   * @return the parsed partition path based on data type\n+   * @throws ParseException on any parse exception\n+   */\n+  private String getPartitionPath(Object partitionVal) throws ParseException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0MjcxNA=="}, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4MzkwMQ==", "bodyText": "is the .get(0) really fine?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468783901", "createdAt": "2020-08-11T18:35:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -177,4 +191,26 @@ private long convertLongTimeToMillis(Long partitionVal) {\n     }\n     return MILLISECONDS.convert(partitionVal, timeUnit);\n   }\n+\n+  @Override\n+  public String getRecordKey(Row row) {\n+    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), getRecordKeyPositions(), false);\n+  }\n+\n+  @Override\n+  public String getPartitionPath(Row row) {\n+    Object fieldVal = null;\n+    Object partitionPathFieldVal =  RowKeyGeneratorHelper.getNestedFieldVal(row, getPartitionPathPositions().get(getPartitionPathFields().get(0)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4NDQxNw==", "bodyText": "this needs to be reconciled with changes in the original method. oh my.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468784417", "createdAt": "2020-08-11T18:36:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -119,21 +119,19 @@ class DefaultSource extends RelationProvider\n                               optParams: Map[String, String],\n                               df: DataFrame): BaseRelation = {\n     val parameters = HoodieSparkSqlWriter.parametersWithWriteDefaults(optParams)\n-\n     if (parameters(OPERATION_OPT_KEY).equals(BOOTSTRAP_OPERATION_OPT_VAL)) {\n       HoodieSparkSqlWriter.bootstrap(sqlContext, mode, parameters, df)\n     } else {\n       HoodieSparkSqlWriter.write(sqlContext, mode, parameters, df)\n     }\n-\n     new HoodieEmptyRelation(sqlContext, df.schema)\n   }\n \n   override def createSink(sqlContext: SQLContext,\n                           optParams: Map[String, String],\n                           partitionColumns: Seq[String],\n                           outputMode: OutputMode): Sink = {\n-    val parameters = HoodieSparkSqlWriter.parametersWithWriteDefaults(optParams)\n+    val parameters = HoodieWriterUtils.parametersWithWriteDefaults(optParams)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4NDg1Mw==", "bodyText": "these are the legit indentations. there is a PR open for scala style.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468784853", "createdAt": "2020-08-11T18:37:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -411,11 +443,11 @@ private[hudi] object HoodieSparkSqlWriter {\n \n       val asyncCompactionEnabled = isAsyncCompactionEnabled(client, tableConfig, parameters, jsc.hadoopConfiguration())\n       val compactionInstant : common.util.Option[java.lang.String] =\n-      if (asyncCompactionEnabled) {\n-        client.scheduleCompaction(common.util.Option.of(new util.HashMap[String, String](mapAsJavaMap(metaMap))))\n-      } else {\n-        common.util.Option.empty()\n-      }\n+        if (asyncCompactionEnabled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc4NTYyMg==", "bodyText": "@nsivabalan can you please file a JIRA for this?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468785622", "createdAt": "2020-08-11T18:38:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/TestDataSourceDefaults.scala", "diffHunk": "@@ -34,13 +36,28 @@ import org.scalatest.Assertions.fail\n class TestDataSourceDefaults {\n \n   val schema = SchemaTestUtil.getComplexEvolvedSchema\n+  val structType = AvroConversionUtils.convertAvroSchemaToStructType(schema)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ0NzgwNQ=="}, "originalCommit": {"oid": "dacd635367b59e7d6b8de91f6b785b337dd851eb"}, "originalPosition": 13}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c", "committedDate": "2020-08-11T16:06:51Z", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures"}, "afterCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/95a71fe33accac60ec54c0c312691e229646fa47", "committedDate": "2020-08-12T00:57:29Z", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NTU1MzU1", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-465555355", "createdAt": "2020-08-12T02:47:39Z", "commit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo0NzozOVrOG_P4cQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjo1Mzo1M1rOG_P-pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MzY4MQ==", "bodyText": "This line is different from what I see before this patch. It is an optimization, but just to be safe, we can keep it as is.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468973681", "createdAt": "2020-08-12T02:47:39Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/GlobalDeleteKeyGenerator.java", "diffHunk": "@@ -22,30 +22,27 @@\n import org.apache.hudi.common.config.TypedProperties;\n \n import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n-import java.util.stream.Collectors;\n \n /**\n- * Key generator for deletes using global indices. Global index deletes do not require partition value\n- * so this key generator avoids using partition value for generating HoodieKey.\n+ * Key generator for deletes using global indices. Global index deletes do not require partition value so this key generator avoids using partition value for generating HoodieKey.\n  */\n public class GlobalDeleteKeyGenerator extends BuiltinKeyGenerator {\n \n   private static final String EMPTY_PARTITION = \"\";\n \n-  protected final List<String> recordKeyFields;\n-\n   public GlobalDeleteKeyGenerator(TypedProperties config) {\n     super(config);\n-    this.recordKeyFields = Arrays.stream(config.getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY()).split(\",\")).map(String::trim).collect(Collectors.toList());\n+    this.recordKeyFields = Arrays.asList(config.getString(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY()).split(\",\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTI2OQ==", "bodyText": "note to reviewer: removed the outer try catch and moved it to the caller. Except that, no other code changes.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468975269", "createdAt": "2020-08-12T02:53:53Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -125,49 +130,58 @@ public TimestampBasedKeyGenerator(TypedProperties config, String partitionPathFi\n \n   @Override\n   public String getPartitionPath(GenericRecord record) {\n-    Object partitionVal = HoodieAvroUtils.getNestedFieldVal(record, partitionPathField, true);\n+    Object partitionVal = HoodieAvroUtils.getNestedFieldVal(record, getPartitionPathFields().get(0), true);\n     if (partitionVal == null) {\n       partitionVal = 1L;\n     }\n+    try {\n+      return getPartitionPath(partitionVal);\n+    } catch (Exception e) {\n+      throw new HoodieDeltaStreamerException(\"Unable to parse input partition field :\" + partitionVal, e);\n+    }\n+  }\n \n+  /**\n+   * Parse and fetch partition path based on data type.\n+   *\n+   * @param partitionVal partition path object value fetched from record/row\n+   * @return the parsed partition path based on data type\n+   * @throws ParseException on any parse exception\n+   */\n+  private String getPartitionPath(Object partitionVal) throws ParseException {\n     DateTimeFormatter partitionFormatter = DateTimeFormat.forPattern(outputDateFormat);\n     if (this.outputDateTimeZone != null) {\n       partitionFormatter = partitionFormatter.withZone(outputDateTimeZone);\n     }\n-\n-    try {\n-      long timeMs;\n-      if (partitionVal instanceof Double) {\n-        timeMs = convertLongTimeToMillis(((Double) partitionVal).longValue());\n-      } else if (partitionVal instanceof Float) {\n-        timeMs = convertLongTimeToMillis(((Float) partitionVal).longValue());\n-      } else if (partitionVal instanceof Long) {\n-        timeMs = convertLongTimeToMillis((Long) partitionVal);\n-      } else if (partitionVal instanceof CharSequence) {\n-        DateTime parsedDateTime = inputFormatter.parseDateTime(partitionVal.toString());\n-        if (this.outputDateTimeZone == null) {\n-          // Use the timezone that came off the date that was passed in, if it had one\n-          partitionFormatter = partitionFormatter.withZone(parsedDateTime.getZone());\n-        }\n-\n-        timeMs = inputFormatter.parseDateTime(partitionVal.toString()).getMillis();\n-      } else {\n-        throw new HoodieNotSupportedException(\n-            \"Unexpected type for partition field: \" + partitionVal.getClass().getName());\n+    long timeMs;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1OTY0Mjgy", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-465964282", "createdAt": "2020-08-12T14:20:49Z", "commit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNDoyMDo0OVrOG_jjog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNDoyNTozMFrOG_jxNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NjAzNA==", "bodyText": "there could be some bug here. if field is not found in structType. I fixed it in RowKeyGeneratorHelper for nested fields, but missed it here.\nsomething like\n .forEach(f ->\n        {\n          if (structType.getFieldIndex(f).isDefined()) {\n            recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n          } else {\n            throw new HoodieKeyException(\"recordKey value not found for field: \\\"\" + f + \"\\\"\");\n          }\n        });", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469296034", "createdAt": "2020-08-12T14:20:49Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NjQ0NA==", "bodyText": "same here. please note that for recordKey, we throw an exception, where as for partitionpath, we might need to return DEFAULT_PARTITION_PATH.\n     .forEach(f -> {\n              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\t            if (structType.getFieldIndex(f).isDefined()) {\n              partitionPathPositions.put(f,\n                  Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n            } else {\n              partitionPathPositions.put(f, Collections.singletonList(-1));\n            }\n          });", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469296444", "createdAt": "2020-08-12T14:21:24Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NzE1Nw==", "bodyText": "would be good to add a precondition check here that init has been called.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469297157", "createdAt": "2020-08-12T14:22:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NzIxOQ==", "bodyText": "would be good to add a precondition check here that init has been called.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469297219", "createdAt": "2020-08-12T14:22:28Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {\n+      converterFn = AvroConversionHelper.createConverterToAvro(structType, structName, recordNamespace);\n+    }\n+    GenericRecord genericRecord = (GenericRecord) converterFn.apply(row);\n+    return getKey(genericRecord).getRecordKey();\n+  }\n+\n+  /**\n+   * Fetch partition path from {@link Row}.\n+   * @param row instance of {@link Row} from which partition path is requested\n+   * @return the partition path of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getPartitionPath(Row row) {\n+    if (null != converterFn) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5ODcyNw==", "bodyText": "if you plan to add such precondition, ensure all built in key generators call it since they might override getRecordKey(Row) and getPartitionPath(Row).\n  protected void preConditionCheckForRowInit(){\n    if(!isRowInitCalled()){\n      throw new IllegalStateException(\"KeyGenerator#initializeRowKeyGenerator should have been invoked before this method \");\n    }\n  }", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469298727", "createdAt": "2020-08-12T14:24:29Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.keygen;\n+\n+import org.apache.hudi.AvroConversionHelper;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.exception.HoodieKeyException;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Function1;\n+\n+/**\n+ * Base class for all the built-in key generators. Contains methods structured for\n+ * code reuse amongst them.\n+ */\n+public abstract class BuiltinKeyGenerator extends KeyGenerator {\n+\n+  protected List<String> recordKeyFields;\n+  protected List<String> partitionPathFields;\n+\n+  private Map<String, List<Integer>> recordKeyPositions = new HashMap<>();\n+  private Map<String, List<Integer>> partitionPathPositions = new HashMap<>();\n+\n+  private transient Function1<Object, Object> converterFn = null;\n+  protected StructType structType;\n+  private String structName;\n+  private String recordNamespace;\n+\n+  protected BuiltinKeyGenerator(TypedProperties config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Generate a record Key out of provided generic record.\n+   */\n+  public abstract String getRecordKey(GenericRecord record);\n+\n+  /**\n+   * Generate a partition path out of provided generic record.\n+   */\n+  public abstract String getPartitionPath(GenericRecord record);\n+\n+  /**\n+   * Generate a Hoodie Key out of provided generic record.\n+   */\n+  public final HoodieKey getKey(GenericRecord record) {\n+    if (getRecordKeyFields() == null || getPartitionPathFields() == null) {\n+      throw new HoodieKeyException(\"Unable to find field names for record key or partition path in cfg\");\n+    }\n+    return new HoodieKey(getRecordKey(record), getPartitionPath(record));\n+  }\n+\n+  @Override\n+  public final List<String> getRecordKeyFieldNames() {\n+    // For nested columns, pick top level column name\n+    return getRecordKeyFields().stream().map(k -> {\n+      int idx = k.indexOf('.');\n+      return idx > 0 ? k.substring(0, idx) : k;\n+    }).collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n+    // parse simple feilds\n+    getRecordKeyFields().stream()\n+        .filter(f -> !(f.contains(\".\")))\n+        .forEach(f -> recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+    // parse nested fields\n+    getRecordKeyFields().stream()\n+        .filter(f -> f.contains(\".\"))\n+        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+    // parse simple fields\n+    if (getPartitionPathFields() != null) {\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              Collections.singletonList((Integer) (structType.getFieldIndex(f).get()))));\n+      // parse nested fields\n+      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+          .forEach(f -> partitionPathPositions.put(f,\n+              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+    }\n+    this.structName = structName;\n+    this.structType = structType;\n+    this.recordNamespace = recordNamespace;\n+  }\n+\n+  /**\n+   * Fetch record key from {@link Row}.\n+   * @param row instance of {@link Row} from which record key is requested.\n+   * @return the record key of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getRecordKey(Row row) {\n+    if (null != converterFn) {\n+      converterFn = AvroConversionHelper.createConverterToAvro(structType, structName, recordNamespace);\n+    }\n+    GenericRecord genericRecord = (GenericRecord) converterFn.apply(row);\n+    return getKey(genericRecord).getRecordKey();\n+  }\n+\n+  /**\n+   * Fetch partition path from {@link Row}.\n+   * @param row instance of {@link Row} from which partition path is requested\n+   * @return the partition path of interest from {@link Row}.\n+   */\n+  @Override\n+  public String getPartitionPath(Row row) {\n+    if (null != converterFn) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5NzIxOQ=="}, "originalCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTI5OTUwOQ==", "bodyText": "Can we switch this to Exception to be in sync up with GenericRecord behavior.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469299509", "createdAt": "2020-08-12T14:25:30Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/TimestampBasedKeyGenerator.java", "diffHunk": "@@ -177,4 +191,26 @@ private long convertLongTimeToMillis(Long partitionVal) {\n     }\n     return MILLISECONDS.convert(partitionVal, timeUnit);\n   }\n+\n+  @Override\n+  public String getRecordKey(Row row) {\n+    return RowKeyGeneratorHelper.getRecordKeyFromRow(row, getRecordKeyFields(), getRecordKeyPositions(), false);\n+  }\n+\n+  @Override\n+  public String getPartitionPath(Row row) {\n+    Object fieldVal = null;\n+    Object partitionPathFieldVal =  RowKeyGeneratorHelper.getNestedFieldVal(row, getPartitionPathPositions().get(getPartitionPathFields().get(0)));\n+    try {\n+      if (partitionPathFieldVal.toString().contains(DEFAULT_PARTITION_PATH) || partitionPathFieldVal.toString().contains(NULL_RECORDKEY_PLACEHOLDER)\n+          || partitionPathFieldVal.toString().contains(EMPTY_RECORDKEY_PLACEHOLDER)) {\n+        fieldVal = 1L;\n+      } else {\n+        fieldVal = partitionPathFieldVal;\n+      }\n+      return getPartitionPath(fieldVal);\n+    } catch (ParseException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "originalPosition": 142}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "06c1370f3805ba3667ea0f61f414c21787762772", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/06c1370f3805ba3667ea0f61f414c21787762772", "committedDate": "2020-08-12T15:23:44Z", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/95a71fe33accac60ec54c0c312691e229646fa47", "committedDate": "2020-08-12T00:57:29Z", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures"}, "afterCommit": {"oid": "06c1370f3805ba3667ea0f61f414c21787762772", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/06c1370f3805ba3667ea0f61f414c21787762772", "committedDate": "2020-08-12T15:23:44Z", "message": "Code review comments, cleanup, fixes, restructuring\n\n - Clean up KeyGenerator classes and fix test failures"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "383a74578defdcb82a5dddde079e65899f6d60f0", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/383a74578defdcb82a5dddde079e65899f6d60f0", "committedDate": "2020-08-12T16:06:11Z", "message": "Some fixes to key generators and adding more tests for Row apis"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NTUxNzMz", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-465551733", "createdAt": "2020-08-12T02:34:40Z", "commit": {"oid": "95a71fe33accac60ec54c0c312691e229646fa47"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwMjozNDo0MFrOG_Prmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNjo0ODo0OVrOG_p4yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MDM5NA==", "bodyText": "I think this is because row already has these metafields per se in the schema", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468970394", "createdAt": "2020-08-12T02:34:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/model/HoodieInternalRow.java", "diffHunk": "@@ -0,0 +1,243 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.model;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.unsafe.types.CalendarInterval;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Internal Row implementation for Hoodie Row. It wraps an {@link InternalRow} and keeps meta columns locally. But the {@link InternalRow}\n+ * does include the meta columns as well just that {@link HoodieInternalRow} will intercept queries for meta columns and serve from its\n+ * copy rather than fetching from {@link InternalRow}.\n+ */\n+public class HoodieInternalRow extends InternalRow {\n+\n+  private String commitTime;\n+  private String commitSeqNumber;\n+  private String recordKey;\n+  private String partitionPath;\n+  private String fileName;\n+  private InternalRow row;\n+\n+  public HoodieInternalRow(String commitTime, String commitSeqNumber, String recordKey, String partitionPath,\n+      String fileName, InternalRow row) {\n+    this.commitTime = commitTime;\n+    this.commitSeqNumber = commitSeqNumber;\n+    this.recordKey = recordKey;\n+    this.partitionPath = partitionPath;\n+    this.fileName = fileName;\n+    this.row = row;\n+  }\n+\n+  @Override\n+  public int numFields() {\n+    return row.numFields();\n+  }\n+\n+  @Override\n+  public void setNullAt(int i) {\n+    if (i < HoodieRecord.HOODIE_META_COLUMNS.size()) {\n+      switch (i) {\n+        case 0: {\n+          this.commitTime = null;\n+          break;\n+        }\n+        case 1: {\n+          this.commitSeqNumber = null;\n+          break;\n+        }\n+        case 2: {\n+          this.recordKey = null;\n+          break;\n+        }\n+        case 3: {\n+          this.partitionPath = null;\n+          break;\n+        }\n+        case 4: {\n+          this.fileName = null;\n+          break;\n+        }\n+        default: throw new IllegalArgumentException(\"Not expected\");\n+      }\n+    } else {\n+      row.setNullAt(i);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc1NTk2Nw=="}, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MDk3OQ==", "bodyText": "I did that.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468980979", "createdAt": "2020-08-12T03:17:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -670,7 +670,9 @@ public Builder withPath(String basePath) {\n     }\n \n     public Builder withSchema(String schemaStr) {\n-      props.setProperty(AVRO_SCHEMA, schemaStr);\n+      if (null != schemaStr) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODczNjk3OQ=="}, "originalCommit": {"oid": "e5d4939fc7136e846c3123d2084939f75c6fed40"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4MTQ4Mw==", "bodyText": "yes. why we are hardcoding this. any ideas @bvaradar ?", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468981483", "createdAt": "2020-08-12T03:19:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieRowParquetWriteSupport.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.HoodieDynamicBoundedBloomFilter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.HashMap;\n+\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_AVRO_BLOOM_FILTER_METADATA_KEY;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_BLOOM_FILTER_TYPE_CODE;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_MAX_RECORD_KEY_FOOTER;\n+import static org.apache.hudi.avro.HoodieAvroWriteSupport.HOODIE_MIN_RECORD_KEY_FOOTER;\n+\n+/**\n+ * Hoodie Write Support for directly writing Row to Parquet.\n+ */\n+public class HoodieRowParquetWriteSupport extends ParquetWriteSupport {\n+\n+  private Configuration hadoopConf;\n+  private BloomFilter bloomFilter;\n+  private String minRecordKey;\n+  private String maxRecordKey;\n+\n+  public HoodieRowParquetWriteSupport(Configuration conf, StructType structType, BloomFilter bloomFilter) {\n+    super();\n+    Configuration hadoopConf = new Configuration(conf);\n+    hadoopConf.set(\"spark.sql.parquet.writeLegacyFormat\", \"false\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODI2MQ=="}, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk4NTg5Mg==", "bodyText": "removing this. it was easy enough.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r468985892", "createdAt": "2020-08-12T03:37:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -35,6 +35,7 @@\n   // bulk insert\n   BULK_INSERT(\"bulk_insert\"),\n   BULK_INSERT_PREPPED(\"bulk_insert_prepped\"),\n+  BULK_INSERT_DATASET(\"bulk_insert_dataset\"),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc3MTUyNw=="}, "originalCommit": {"oid": "5e77ae315a4b168b4a0ebb5be9b04e3f3676e73c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5ODc4NA==", "bodyText": "this is publicly visible. cannot add this here.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469398784", "createdAt": "2020-08-12T16:47:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -297,8 +298,9 @@ object DataSourceWriteOptions {\n   val DEFAULT_HIVE_ASSUME_DATE_PARTITION_OPT_VAL = \"false\"\n   val DEFAULT_USE_PRE_APACHE_INPUT_FORMAT_OPT_VAL = \"false\"\n   val DEFAULT_HIVE_USE_JDBC_OPT_VAL = \"true\"\n-\n   // Async Compaction - Enabled by default for MOR\n   val ASYNC_COMPACT_ENABLE_KEY = \"hoodie.datasource.compaction.async.enable\"\n   val DEFAULT_ASYNC_COMPACT_ENABLE_VAL = \"true\"\n+  // Internal configs", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "383a74578defdcb82a5dddde079e65899f6d60f0"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5OTc1NQ==", "bodyText": "I added this from HoodieSparkSQLWriterso we can have.just one method", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469399755", "createdAt": "2020-08-12T16:48:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieWriterUtils.scala", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.DataSourceWriteOptions._\n+import org.apache.hudi.common.config.TypedProperties\n+\n+import scala.collection.JavaConversions._\n+import scala.collection.JavaConverters._\n+\n+/**\n+ * WriterUtils to assist in write path in Datasource and tests.\n+ */\n+object HoodieWriterUtils {\n+\n+  def javaParametersWithWriteDefaults(parameters: java.util.Map[String, String]): java.util.Map[String, String] = {\n+    mapAsJavaMap(parametersWithWriteDefaults(parameters.asScala.toMap))\n+  }\n+\n+  /**\n+    * Add default options for unspecified write options keys.\n+    *\n+    * @param parameters\n+    * @return\n+    */\n+  def parametersWithWriteDefaults(parameters: Map[String, String]): Map[String, String] = {\n+    Map(OPERATION_OPT_KEY -> DEFAULT_OPERATION_OPT_VAL,\n+      TABLE_TYPE_OPT_KEY -> DEFAULT_TABLE_TYPE_OPT_VAL,\n+      PRECOMBINE_FIELD_OPT_KEY -> DEFAULT_PRECOMBINE_FIELD_OPT_VAL,\n+      PAYLOAD_CLASS_OPT_KEY -> DEFAULT_PAYLOAD_OPT_VAL,\n+      RECORDKEY_FIELD_OPT_KEY -> DEFAULT_RECORDKEY_FIELD_OPT_VAL,\n+      PARTITIONPATH_FIELD_OPT_KEY -> DEFAULT_PARTITIONPATH_FIELD_OPT_VAL,\n+      KEYGENERATOR_CLASS_OPT_KEY -> DEFAULT_KEYGENERATOR_CLASS_OPT_VAL,\n+      COMMIT_METADATA_KEYPREFIX_OPT_KEY -> DEFAULT_COMMIT_METADATA_KEYPREFIX_OPT_VAL,\n+      INSERT_DROP_DUPS_OPT_KEY -> DEFAULT_INSERT_DROP_DUPS_OPT_VAL,\n+      STREAMING_RETRY_CNT_OPT_KEY -> DEFAULT_STREAMING_RETRY_CNT_OPT_VAL,\n+      STREAMING_RETRY_INTERVAL_MS_OPT_KEY -> DEFAULT_STREAMING_RETRY_INTERVAL_MS_OPT_VAL,\n+      STREAMING_IGNORE_FAILED_BATCH_OPT_KEY -> DEFAULT_STREAMING_IGNORE_FAILED_BATCH_OPT_VAL,\n+      META_SYNC_CLIENT_TOOL_CLASS -> DEFAULT_META_SYNC_CLIENT_TOOL_CLASS,\n+      HIVE_SYNC_ENABLED_OPT_KEY -> DEFAULT_HIVE_SYNC_ENABLED_OPT_VAL,\n+      META_SYNC_ENABLED_OPT_KEY -> DEFAULT_META_SYNC_ENABLED_OPT_VAL,\n+      HIVE_DATABASE_OPT_KEY -> DEFAULT_HIVE_DATABASE_OPT_VAL,\n+      HIVE_TABLE_OPT_KEY -> DEFAULT_HIVE_TABLE_OPT_VAL,\n+      HIVE_BASE_FILE_FORMAT_OPT_KEY -> DEFAULT_HIVE_BASE_FILE_FORMAT_OPT_VAL,\n+      HIVE_USER_OPT_KEY -> DEFAULT_HIVE_USER_OPT_VAL,\n+      HIVE_PASS_OPT_KEY -> DEFAULT_HIVE_PASS_OPT_VAL,\n+      HIVE_URL_OPT_KEY -> DEFAULT_HIVE_URL_OPT_VAL,\n+      HIVE_PARTITION_FIELDS_OPT_KEY -> DEFAULT_HIVE_PARTITION_FIELDS_OPT_VAL,\n+      HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY -> DEFAULT_HIVE_PARTITION_EXTRACTOR_CLASS_OPT_VAL,\n+      HIVE_STYLE_PARTITIONING_OPT_KEY -> DEFAULT_HIVE_STYLE_PARTITIONING_OPT_VAL,\n+      HIVE_USE_JDBC_OPT_KEY -> DEFAULT_HIVE_USE_JDBC_OPT_VAL,\n+      ASYNC_COMPACT_ENABLE_KEY -> DEFAULT_ASYNC_COMPACT_ENABLE_VAL", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "383a74578defdcb82a5dddde079e65899f6d60f0"}, "originalPosition": 67}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fefd4b7618aa47b8f2c3e9a28b898b5a713d88e6", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/fefd4b7618aa47b8f2c3e9a28b898b5a713d88e6", "committedDate": "2020-08-12T20:39:33Z", "message": "Cleaning up config placements, naming"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "163c9a080648409b7a644b4ae9992342d20ffa94", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/163c9a080648409b7a644b4ae9992342d20ffa94", "committedDate": "2020-08-13T02:34:11Z", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs"}, "afterCommit": {"oid": "c8745f856c4407f2eb17143237ae3a7655a43cfb", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/c8745f856c4407f2eb17143237ae3a7655a43cfb", "committedDate": "2020-08-13T04:11:08Z", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c8745f856c4407f2eb17143237ae3a7655a43cfb", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/c8745f856c4407f2eb17143237ae3a7655a43cfb", "committedDate": "2020-08-13T04:11:08Z", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs"}, "afterCommit": {"oid": "5dc8182ec308dba7ffd04ef159bd3041ede1b117", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/5dc8182ec308dba7ffd04ef159bd3041ede1b117", "committedDate": "2020-08-13T04:38:05Z", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "committedDate": "2020-08-13T05:16:14Z", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5dc8182ec308dba7ffd04ef159bd3041ede1b117", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/5dc8182ec308dba7ffd04ef159bd3041ede1b117", "committedDate": "2020-08-13T04:38:05Z", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs"}, "afterCommit": {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/865d8d6c6dbded3f4e5338ca9c9849f5369a6075", "committedDate": "2020-08-13T05:16:14Z", "message": "Clean up changes made on the public KeyGenerator class\n\n - Introduced KeyGeneratorInterface in hudi-client, moved KeyGenerator back to hudi-spark\n - Simplified the new API additions to just two new methods : getRecordKey(row), getPartitionPath(row)\n - Fixed all built-in key generators with new APIs\n - Made the field position map lazily created upon the first call to row based apis\n - Implemented native row based key generators for CustomKeyGenerator\n - Fixed all the tests, with these new APIs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2NzIzMjIz", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-466723223", "createdAt": "2020-08-13T12:42:55Z", "commit": {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMjo0Mjo1NVrOHAJxRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxMjo0Mjo1NVrOHAJxRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTkyMjExOA==", "bodyText": "may I know where is the structType being used ? AvroConversionHelper.createConverterToAvro used row.Schema() and so we may not need it. probably we should rename this to boolean positionMapInitialized.", "url": "https://github.com/apache/hudi/pull/1834#discussion_r469922118", "createdAt": "2020-08-13T12:42:55Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark/src/main/java/org/apache/hudi/keygen/BuiltinKeyGenerator.java", "diffHunk": "@@ -85,71 +84,40 @@ public final HoodieKey getKey(GenericRecord record) {\n     }).collect(Collectors.toList());\n   }\n \n-  @Override\n-  public void initializeRowKeyGenerator(StructType structType, String structName, String recordNamespace) {\n-    // parse simple feilds\n-    getRecordKeyFields().stream()\n-        .filter(f -> !(f.contains(\".\")))\n-        .forEach(f -> {\n-          if (structType.getFieldIndex(f).isDefined()) {\n-            recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n-          } else {\n-            throw new HoodieKeyException(\"recordKey value not found for field: \\\"\" + f + \"\\\"\");\n-          }\n-        });\n-    // parse nested fields\n-    getRecordKeyFields().stream()\n-        .filter(f -> f.contains(\".\"))\n-        .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n-    // parse simple fields\n-    if (getPartitionPathFields() != null) {\n-      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+  void buildFieldPositionMapIfNeeded(StructType structType) {\n+    if (this.structType == null) {\n+      // parse simple fields\n+      getRecordKeyFields().stream()\n+          .filter(f -> !(f.contains(\".\")))\n           .forEach(f -> {\n             if (structType.getFieldIndex(f).isDefined()) {\n-              partitionPathPositions.put(f,\n-                  Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n+              recordKeyPositions.put(f, Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n             } else {\n-              partitionPathPositions.put(f, Collections.singletonList(-1));\n+              throw new HoodieKeyException(\"recordKey value not found for field: \\\"\" + f + \"\\\"\");\n             }\n           });\n       // parse nested fields\n-      getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n-          .forEach(f -> partitionPathPositions.put(f,\n-              RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n-    }\n-    this.structName = structName;\n-    this.structType = structType;\n-    this.recordNamespace = recordNamespace;\n-  }\n-\n-  /**\n-   * Fetch record key from {@link Row}.\n-   *\n-   * @param row instance of {@link Row} from which record key is requested.\n-   * @return the record key of interest from {@link Row}.\n-   */\n-  @Override\n-  public String getRecordKey(Row row) {\n-    if (null == converterFn) {\n-      converterFn = AvroConversionHelper.createConverterToAvro(structType, structName, recordNamespace);\n+      getRecordKeyFields().stream()\n+          .filter(f -> f.contains(\".\"))\n+          .forEach(f -> recordKeyPositions.put(f, RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, true)));\n+      // parse simple fields\n+      if (getPartitionPathFields() != null) {\n+        getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> !(f.contains(\".\")))\n+            .forEach(f -> {\n+              if (structType.getFieldIndex(f).isDefined()) {\n+                partitionPathPositions.put(f,\n+                    Collections.singletonList((Integer) (structType.getFieldIndex(f).get())));\n+              } else {\n+                partitionPathPositions.put(f, Collections.singletonList(-1));\n+              }\n+            });\n+        // parse nested fields\n+        getPartitionPathFields().stream().filter(f -> !f.isEmpty()).filter(f -> f.contains(\".\"))\n+            .forEach(f -> partitionPathPositions.put(f,\n+                RowKeyGeneratorHelper.getNestedFieldIndices(structType, f, false)));\n+      }\n+      this.structType = structType;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075"}, "originalPosition": 127}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2NzMwNjY1", "url": "https://github.com/apache/hudi/pull/1834#pullrequestreview-466730665", "createdAt": "2020-08-13T12:52:47Z", "commit": {"oid": "865d8d6c6dbded3f4e5338ca9c9849f5369a6075"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2974, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}