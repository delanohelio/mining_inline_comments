{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAxMDYzODY2", "number": 2167, "title": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable", "bodyText": "Remove APIs in HoodieTestUtils\n\ncreateCommitFiles\ncreateDataFile\ncreateNewLogFile\ncreateCompactionRequest\n\nMigrated usages in TestCleaner#testPendingCompactions.\nAlso improved some API names in HoodieTestTable.\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-10-11T01:51:40Z", "url": "https://github.com/apache/hudi/pull/2167", "merged": true, "mergeCommit": {"oid": "c5e10d668f9366f29bdf7721f7efe4140782527b"}, "closed": true, "closedAt": "2020-10-12T06:39:11Z", "author": {"login": "xushiyan"}, "timelineItems": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdRVX7AgH2gAyNTAxMDYzODY2Ojg2M2JkNjQzZjMyZGI1NDJlMzg5NWUzMzFhNWZlYmZiNzAyYmU5ZDg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdRuHwPgFqTUwNjMyODA1Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "863bd643f32db542e3895e331a5febfb702be9d8", "author": {"user": {"login": "xushiyan", "name": "Raymond Xu"}}, "url": "https://github.com/apache/hudi/commit/863bd643f32db542e3895e331a5febfb702be9d8", "committedDate": "2020-10-11T01:48:05Z", "message": "[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable\n\nRemove APIs in `HoodieTestUtils`\n- `createCommitFiles`\n- `createDataFile`\n- `createNewLogFile`\n- `createCompactionRequest`\n\nMigrated usages in `TestCleaner#testPendingCompactions`.\n\nAlso improved some API names in `HoodieTestTable`."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MTM3NTc5", "url": "https://github.com/apache/hudi/pull/2167#pullrequestreview-506137579", "createdAt": "2020-10-11T02:02:28Z", "commit": {"oid": "863bd643f32db542e3895e331a5febfb702be9d8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMVQwMjowMjoyOFrOHfjyGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMVQwMjowMjoyOFrOHfjyGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjg1NDE3MA==", "bodyText": "@yanghua The original logic of test prep is incredibly difficult to understand. Please avoid reading it. I simply compared the output files in temp directory and made sure they were equivalent before and after the change.", "url": "https://github.com/apache/hudi/pull/2167#discussion_r502854170", "createdAt": "2020-10-11T02:02:28Z", "author": {"login": "xushiyan"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java", "diffHunk": "@@ -1098,73 +1097,82 @@ public void testCleanPreviousCorruptedCleanFiles() throws IOException {\n    * @param expNumFilesDeleted Number of files deleted\n    */\n   private void testPendingCompactions(HoodieWriteConfig config, int expNumFilesDeleted,\n-      int expNumFilesUnderCompactionDeleted, boolean retryFailure) throws IOException {\n+      int expNumFilesUnderCompactionDeleted, boolean retryFailure) throws Exception {\n     HoodieTableMetaClient metaClient =\n         HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n-    String[] instants = new String[] {\"000\", \"001\", \"003\", \"005\", \"007\", \"009\", \"011\", \"013\"};\n-    String[] compactionInstants = new String[] {\"002\", \"004\", \"006\", \"008\", \"010\"};\n-    Map<String, String> expFileIdToPendingCompaction = new HashMap<>();\n-    Map<String, String> fileIdToLatestInstantBeforeCompaction = new HashMap<>();\n-    Map<String, List<FileSlice>> compactionInstantsToFileSlices = new HashMap<>();\n-\n-    for (String instant : instants) {\n-      HoodieTestUtils.createCommitFiles(basePath, instant);\n-    }\n+    final String partition = \"2016/03/15\";\n+    Map<String, String> expFileIdToPendingCompaction = new HashMap<String, String>() {\n+      {\n+        put(\"fileId2\", \"004\");\n+        put(\"fileId3\", \"006\");\n+        put(\"fileId4\", \"008\");\n+        put(\"fileId5\", \"010\");\n+      }\n+    };\n+    Map<String, String> fileIdToLatestInstantBeforeCompaction = new HashMap<String, String>() {\n+      {\n+        put(\"fileId1\", \"000\");\n+        put(\"fileId2\", \"000\");\n+        put(\"fileId3\", \"001\");\n+        put(\"fileId4\", \"003\");\n+        put(\"fileId5\", \"005\");\n+        put(\"fileId6\", \"009\");\n+        put(\"fileId7\", \"011\");\n+      }\n+    };\n \n     // Generate 7 file-groups. First one has only one slice and no pending compaction. File Slices (2 - 5) has\n     // multiple versions with pending compaction. File Slices (6 - 7) have multiple file-slices but not under\n     // compactions\n     // FileIds 2-5 will be under compaction\n-    int maxNumFileIds = 7;\n-    String[] fileIds = new String[] {\"fileId1\", \"fileId2\", \"fileId3\", \"fileId4\", \"fileId5\", \"fileId6\", \"fileId7\"};\n-    int maxNumFileIdsForCompaction = 4;\n-    for (int i = 0; i < maxNumFileIds; i++) {\n-      final String fileId = HoodieTestUtils.createDataFile(basePath,\n-          HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, instants[0], fileIds[i]);\n-      HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, instants[0],\n-          fileId, Option.empty());\n-      HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, instants[0],\n-          fileId, Option.of(2));\n-      fileIdToLatestInstantBeforeCompaction.put(fileId, instants[0]);\n-      for (int j = 1; j <= i; j++) {\n-        if (j == i && j <= maxNumFileIdsForCompaction) {\n-          expFileIdToPendingCompaction.put(fileId, compactionInstants[j]);\n-          metaClient = HoodieTableMetaClient.reload(metaClient);\n-          HoodieTable table = HoodieSparkTable.create(config, context, metaClient);\n-          FileSlice slice =\n-              table.getSliceView().getLatestFileSlices(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)\n-                  .filter(fs -> fs.getFileId().equals(fileId)).findFirst().get();\n-          List<FileSlice> slices = new ArrayList<>();\n-          if (compactionInstantsToFileSlices.containsKey(compactionInstants[j])) {\n-            slices = compactionInstantsToFileSlices.get(compactionInstants[j]);\n-          }\n-          slices.add(slice);\n-          compactionInstantsToFileSlices.put(compactionInstants[j], slices);\n-          // Add log-files to simulate delta-commits after pending compaction\n-          HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-              compactionInstants[j], fileId, Option.empty());\n-          HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-              compactionInstants[j], fileId, Option.of(2));\n-        } else {\n-          HoodieTestUtils.createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, instants[j],\n-              fileId);\n-          HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-              instants[j], fileId, Option.empty());\n-          HoodieTestUtils.createNewLogFile(fs, basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n-              instants[j], fileId, Option.of(2));\n-          fileIdToLatestInstantBeforeCompaction.put(fileId, instants[j]);\n-        }\n-      }\n-    }\n-\n-    // Setup pending compaction plans\n-    for (String instant : compactionInstants) {\n-      List<FileSlice> fileSliceList = compactionInstantsToFileSlices.get(instant);\n-      if (null != fileSliceList) {\n-        HoodieTestUtils.createCompactionRequest(metaClient, instant, fileSliceList.stream()\n-            .map(fs -> Pair.of(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, fs)).collect(Collectors.toList()));\n-      }\n-    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "863bd643f32db542e3895e331a5febfb702be9d8"}, "originalPosition": 180}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MzI4MDU3", "url": "https://github.com/apache/hudi/pull/2167#pullrequestreview-506328057", "createdAt": "2020-10-12T06:38:03Z", "commit": {"oid": "863bd643f32db542e3895e331a5febfb702be9d8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4150, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}