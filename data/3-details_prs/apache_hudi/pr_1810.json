{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ2MzQ1ODYz", "number": 1810, "title": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nAbstract hudi-sync-common ,and migrate the hudi-hive-sync to hudi-sync-hive. hudi-sync-hive implement the hudi-sync-common .\nThen will support hudi-sync-aliyun-dla  implement the hudi-sync-common .\nThis is the RFC https://cwiki.apache.org/confluence/display/HUDI/RFC+-+17+Abstract+common+meta+sync+module+support+multiple+meta+service.\nAnd the old PR is #1716 ,which just abstract the hudi-sync-common.\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-07-08T16:13:06Z", "url": "https://github.com/apache/hudi/pull/1810", "merged": true, "mergeCommit": {"oid": "51ea27d665d8053895dd047ca85e3338b357a81d"}, "closed": true, "closedAt": "2020-08-06T04:34:56Z", "author": {"login": "lw309637554"}, "timelineItems": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABczG3oigBqjM1Mjc4NjAxMjc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc8G81cgBqjM2MjcxODU2MDI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ1MjY3ODQx", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-445267841", "createdAt": "2020-07-09T03:34:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMzozNDozNlrOGvAgkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwNDowMDoxMVrOGvA3QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NDU5Mw==", "bodyText": "Can we use TableSchemaResolver to handle all hudi schema related methods?\n\n  \n    \n      hudi/hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java\n    \n    \n         Line 338\n      in\n      2603cfb\n    \n    \n    \n    \n\n        \n          \n           public MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompactionCommitOpt) throws Exception {", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451944593", "createdAt": "2020-07-09T03:34:36Z", "author": {"login": "garyli1019"}, "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sync.common;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public abstract class AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(AbstractSyncHoodieClient.class);\n+  protected final HoodieTableMetaClient metaClient;\n+  protected HoodieTimeline activeTimeline;\n+  protected final HoodieTableType tableType;\n+  protected final FileSystem fs;\n+  private String basePath;\n+  private boolean assumeDatePartitioning;\n+\n+  public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning, FileSystem fs) {\n+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);\n+    this.tableType = metaClient.getTableType();\n+    this.basePath = basePath;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+    this.fs = fs;\n+    this.activeTimeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+  }\n+\n+  public abstract void createTable(String tableName, MessageType storageSchema,\n+                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+\n+  public abstract boolean doesTableExist(String tableName);\n+\n+  public abstract Option<String> getLastCommitTimeSynced(String tableName);\n+\n+  public abstract void updateLastCommitTimeSynced(String tableName);\n+\n+  public abstract void addPartitionsToTable(String tableName, List<String> partitionsToAdd);\n+\n+  public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n+\n+  public HoodieTimeline getActiveTimeline() {\n+    return activeTimeline;\n+  }\n+\n+  public HoodieTableType getTableType() {\n+    return tableType;\n+  }\n+\n+  public String getBasePath() {\n+    return metaClient.getBasePath();\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  public void closeQuietly(ResultSet resultSet, Statement stmt) {\n+    try {\n+      if (stmt != null) {\n+        stmt.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the statement opened \", e);\n+    }\n+\n+    try {\n+      if (resultSet != null) {\n+        resultSet.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the resultset opened \", e);\n+    }\n+  }\n+\n+  /**\n+   * Gets the schema for a hoodie table. Depending on the type of table, try to read schema from commit metadata if\n+   * present, else fallback to reading from any file written in the latest commit. We will assume that the schema has\n+   * not changed within a single atomic write.\n+   *\n+   * @return Parquet schema for this table\n+   */\n+  public MessageType getDataSchema() {\n+    try {\n+      return new TableSchemaResolver(metaClient).getTableParquetSchema();\n+    } catch (Exception e) {\n+      throw new HoodieSyncException(\"Failed to read data schema\", e);\n+    }\n+  }\n+\n+  @SuppressWarnings(\"OptionalUsedAsFieldOrParameterType\")\n+  public List<String> getPartitionsWrittenToSince(Option<String> lastCommitTimeSynced) {\n+    if (!lastCommitTimeSynced.isPresent()) {\n+      LOG.info(\"Last commit time synced is not known, listing all partitions in \" + basePath + \",FS :\" + fs);\n+      try {\n+        return FSUtils.getAllPartitionPaths(fs, basePath, assumeDatePartitioning);\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Failed to list all partitions in \" + basePath, e);\n+      }\n+    } else {\n+      LOG.info(\"Last commit time synced is \" + lastCommitTimeSynced.get() + \", Getting commits since then\");\n+\n+      HoodieTimeline timelineToSync = activeTimeline.findInstantsAfter(lastCommitTimeSynced.get(), Integer.MAX_VALUE);\n+      return timelineToSync.getInstants().map(s -> {\n+        try {\n+          return HoodieCommitMetadata.fromBytes(activeTimeline.getInstantDetails(s).get(), HoodieCommitMetadata.class);\n+        } catch (IOException e) {\n+          throw new HoodieIOException(\"Failed to get partitions written since \" + lastCommitTimeSynced, e);\n+        }\n+      }).flatMap(s -> s.getPartitionToWriteStats().keySet().stream()).distinct().collect(Collectors.toList());\n+    }\n+  }\n+\n+  private MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompactionCommitOpt) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NTUwMA==", "bodyText": "I am thinking if we can put the hudi-sync-common into hudi-common. Then we have separate modules for each query engines like hudi-sync-hive hudi-sync-dla e.t.c\nThoughts?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451945500", "createdAt": "2020-07-09T03:38:52Z", "author": {"login": "garyli1019"}, "path": "hudi-sync/hudi-hive-sync/pom.xml", "diffHunk": "@@ -43,6 +45,11 @@\n       <artifactId>hudi-hadoop-mr</artifactId>\n       <version>${project.version}</version>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.hudi</groupId>\n+      <artifactId>hudi-sync-common</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0ODM0NA==", "bodyText": "Vote for putting this into the base class because we need to ensure the schema backward compatible for all query engines.", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451948344", "createdAt": "2020-07-09T03:52:08Z", "author": {"login": "garyli1019"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -272,6 +268,7 @@ void createTable(String tableName, MessageType storageSchema, String inputFormat\n   /**\n    * Get the table schema.\n    */\n+  //???? overwrite", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0ODU3NA==", "bodyText": "nit: alphabetical order", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451948574", "createdAt": "2020-07-09T03:52:57Z", "author": {"login": "garyli1019"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/SchemaDifference.java", "diffHunk": "@@ -20,12 +20,14 @@\n \n import org.apache.parquet.schema.MessageType;\n \n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.StringJoiner;\n+import java.util.ArrayList;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0OTEzMQ==", "bodyText": "nit: order", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451949131", "createdAt": "2020-07-09T03:55:22Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -63,10 +67,11 @@\n \n import java.io.IOException;\n import java.io.Serializable;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Objects;\n+import java.util.Properties;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ==", "bodyText": "Should we change --enable-hive-sync to --enable-sync above?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451950401", "createdAt": "2020-07-09T04:00:11Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -237,6 +237,9 @@ public Operation convert(String value) throws ParameterException {\n     @Parameter(names = {\"--enable-hive-sync\"}, description = \"Enable syncing to hive\")\n     public Boolean enableHiveSync = false;\n \n+    @Parameter(names = {\"--hoodie-sync-client-tool-class\"}, description = \"Meta sync client tool, using comma to separate multi tools\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c08849c4d7226118202ca4dbeaa63634b16daf3", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/4c08849c4d7226118202ca4dbeaa63634b16daf3", "committedDate": "2020-07-15T04:17:54Z", "message": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "4c08849c4d7226118202ca4dbeaa63634b16daf3", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/4c08849c4d7226118202ca4dbeaa63634b16daf3", "committedDate": "2020-07-15T04:17:54Z", "message": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/36979503d9f4d44281136caadc51ab681e1cf056", "committedDate": "2020-07-15T09:18:54Z", "message": " [HUDI-875] Abstract support hudi-dla-sync"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/36979503d9f4d44281136caadc51ab681e1cf056", "committedDate": "2020-07-15T09:18:54Z", "message": " [HUDI-875] Abstract support hudi-dla-sync"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4ODQ4MzU3", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-448848357", "createdAt": "2020-07-15T11:02:28Z", "commit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTowMjoyOFrOGx5HLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTowMjoyOFrOGx5HLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2OTEzNQ==", "bodyText": "if user sync both hive and dla meta, the dla meta would not get synced.?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r454969135", "createdAt": "2020-07-15T11:02:28Z", "author": {"login": "leesf"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -255,6 +262,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4ODU0NDUz", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-448854453", "createdAt": "2020-07-15T11:12:23Z", "commit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMToxMjoyM1rOGx5aCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMToxMjoyM1rOGx5aCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk3Mzk2MQ==", "bodyText": "since dla meta do not support alter table properties yet, it would be simpler here", "url": "https://github.com/apache/hudi/pull/1810#discussion_r454973961", "createdAt": "2020-07-15T11:12:23Z", "author": {"login": "leesf"}, "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/DLASyncTool.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import com.beust.jcommander.JCommander;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.dla.util.Utils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.hudi.sync.common.AbstractSyncTool;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Tool to sync a hoodie table with a dla table. Either use it as a api\n+ * DLASyncTool.syncHoodieTable(DLASyncConfig) or as a command line java -cp hoodie-hive.jar DLASyncTool [args]\n+ * <p>\n+ * This utility will get the schema from the latest commit and will sync dla table schema Also this will sync the\n+ * partitions incrementally (all the partitions modified since the last commit)\n+ */\n+@SuppressWarnings(\"WeakerAccess\")\n+public class DLASyncTool extends AbstractSyncTool {\n+\n+  private static final Logger LOG = LogManager.getLogger(DLASyncTool.class);\n+  public static final String SUFFIX_SNAPSHOT_TABLE = \"_rt\";\n+  public static final String SUFFIX_READ_OPTIMIZED_TABLE = \"_ro\";\n+\n+  private final DLASyncConfig cfg;\n+  private final HoodieDLAClient hoodieDLAClient;\n+  private final String snapshotTableName;\n+  private final Option<String> roTableTableName;\n+\n+  public DLASyncTool(Properties properties, FileSystem fs) {\n+    super(properties, fs);\n+    this.hoodieDLAClient = new HoodieDLAClient(Utils.propertiesToConfig(properties), fs);\n+    this.cfg = Utils.propertiesToConfig(properties);\n+    switch (hoodieDLAClient.getTableType()) {\n+      case COPY_ON_WRITE:\n+        this.snapshotTableName = cfg.tableName;\n+        this.roTableTableName = Option.empty();\n+        break;\n+      case MERGE_ON_READ:\n+        this.snapshotTableName = cfg.tableName + SUFFIX_SNAPSHOT_TABLE;\n+        this.roTableTableName = cfg.skipROSuffix ? Option.of(cfg.tableName) :\n+            Option.of(cfg.tableName + SUFFIX_READ_OPTIMIZED_TABLE);\n+        break;\n+      default:\n+        LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+        throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+    }\n+  }\n+\n+  @Override\n+  public void syncHoodieTable() {\n+    try {\n+      switch (hoodieDLAClient.getTableType()) {\n+        case COPY_ON_WRITE:\n+          syncHoodieTable(snapshotTableName, false);\n+          break;\n+        case MERGE_ON_READ:\n+          // sync a RO table for MOR\n+          syncHoodieTable(roTableTableName.get(), false);\n+          // sync a RT table for MOR\n+          syncHoodieTable(snapshotTableName, true);\n+          break;\n+        default:\n+          LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+          throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+      }\n+    } catch (RuntimeException re) {\n+      LOG.error(\"Got runtime exception when dla syncing\", re);\n+    } finally {\n+      hoodieDLAClient.close();\n+    }\n+  }\n+\n+  private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n+    LOG.info(\"Trying to sync hoodie table \" + tableName + \" with base path \" + hoodieDLAClient.getBasePath()\n+        + \" of type \" + hoodieDLAClient.getTableType());\n+    // Check if the necessary table exists\n+    boolean tableExists = hoodieDLAClient.doesTableExist(tableName);\n+    // Get the parquet schema for this table looking at the latest commit\n+    MessageType schema = hoodieDLAClient.getDataSchema();\n+    // Sync schema if needed\n+    syncSchema(tableName, tableExists, useRealtimeInputFormat, schema);\n+\n+    LOG.info(\"Schema sync complete. Syncing partitions for \" + tableName);\n+    // Get the last time we successfully synced partitions\n+    Option<String> lastCommitTimeSynced = Option.empty();\n+    /*if (tableExists) {\n+      lastCommitTimeSynced = hoodieDLAClient.getLastCommitTimeSynced(tableName);\n+    }*/\n+    LOG.info(\"Last commit time synced was found to be \" + lastCommitTimeSynced.orElse(\"null\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4ODY0OTE0", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-448864914", "createdAt": "2020-07-15T11:30:04Z", "commit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5NDc3MTY5", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-449477169", "createdAt": "2020-07-16T03:10:00Z", "commit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQwMzoxMDowMFrOGyYfSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQwMzoxMDowMFrOGyYfSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQ4MzIxMQ==", "bodyText": "Is there any concern that not using this way to syncHive()?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r455483211", "createdAt": "2020-07-16T03:10:00Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -255,6 +262,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS\n+    }\n+    var metaSyncSuccess = true\n+    if (metaSyncEnabled) {\n+      val impls = syncClientToolClass.split(\",\")\n+      impls.foreach(impl => {\n+        val syncSuccess = impl.trim match {\n+          case DEFAULT_SYNC_CLIENT_TOOL_CLASS => {\n+            log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")\n+            val fs = FSUtils.getFs(basePath.toString, hadoopConf)\n+            syncHive(basePath, fs, parameters)\n+          }\n+          case _ => {\n+            val fs = FSUtils.getFs(basePath.toString, hadoopConf)\n+            val properties = new Properties();\n+            properties.putAll(parameters)\n+            properties.put(\"basePath\", basePath.toString)\n+            val syncHoodie = ReflectionUtils.loadClass(impl.trim, Array[Class[_]](classOf[Properties], classOf[FileSystem]), properties, fs).asInstanceOf[AbstractSyncTool]\n+            syncHoodie.syncHoodieTable()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "originalPosition": 66}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "458d7ebfff40114bb4c8536fec25038bf89deed1", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/458d7ebfff40114bb4c8536fec25038bf89deed1", "committedDate": "2020-08-01T16:05:38Z", "message": " [HUDI-875] merge master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/fcc3a9c1444f8488164a570d506abe6ab245644c", "committedDate": "2020-08-04T04:04:43Z", "message": "Merge branch 'master' into pull/1810"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNDg0Mzkw", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-460484390", "createdAt": "2020-08-04T04:17:17Z", "commit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "state": "APPROVED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNDoxNzoxN1rOG7Qe_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNToyNDowMlrOG7Riwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTI0Nw==", "bodyText": "would users sync to both?  down the line it may make sense to provide support for syncing to multiple things.\nbut even here, if we just append the HiveSync class when hiveSyncEnabled=true, we can support syncing to both Hive and dla?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464789247", "createdAt": "2020-08-04T04:17:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -255,6 +262,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2OTEzNQ=="}, "originalCommit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc5OTMxNQ==", "bodyText": "can this line be shared . the fs initialization?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464799315", "createdAt": "2020-08-04T04:57:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -261,6 +268,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS\n+    }\n+    var metaSyncSuccess = true\n+    if (metaSyncEnabled) {\n+      val impls = syncClientToolClass.split(\",\")\n+      impls.foreach(impl => {\n+        val syncSuccess = impl.trim match {\n+          case DEFAULT_SYNC_CLIENT_TOOL_CLASS => {\n+            log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")\n+            val fs = FSUtils.getFs(basePath.toString, hadoopConf)\n+            syncHive(basePath, fs, parameters)\n+          }\n+          case _ => {\n+            val fs = FSUtils.getFs(basePath.toString, hadoopConf)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc5OTQ5NQ==", "bodyText": "can we explicitly match for Hive instead of default? we may change the default for e.g and it would be an issue.", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464799495", "createdAt": "2020-08-04T04:58:09Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -261,6 +268,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS\n+    }\n+    var metaSyncSuccess = true\n+    if (metaSyncEnabled) {\n+      val impls = syncClientToolClass.split(\",\")\n+      impls.foreach(impl => {\n+        val syncSuccess = impl.trim match {\n+          case DEFAULT_SYNC_CLIENT_TOOL_CLASS => {\n+            log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNDM5Mw==", "bodyText": "I think we can name this little shorter. --sync-tool-class-list ?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464804393", "createdAt": "2020-08-04T05:16:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -237,6 +237,9 @@ public Operation convert(String value) throws ParameterException {\n     @Parameter(names = {\"--enable-hive-sync\"}, description = \"Enable syncing to hive\")\n     public Boolean enableHiveSync = false;\n \n+    @Parameter(names = {\"--hoodie-sync-client-tool-class\"}, description = \"Meta sync client tool, using comma to separate multi tools\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ=="}, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTU1OA==", "bodyText": "can we print a warning around this, so the user knows?\nhere's my take. we can change the code so that --enable-sync and --sync-tool-class-list are the main drivers out of which we derive a Set<String> denoting all the sync tool classes. if --enable-hive-sync is specified, then we simply add the hive sync tool class to this set.. rest of the code just syncs to all sync tools in this set.\nthis way, --enable-hive-sync will be just isolated to the initial command line parsing code. We can apply the same method to datasource as well, if you don't see issues  @lw309637554 @leesf wdyt?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464805558", "createdAt": "2020-08-04T05:20:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -267,9 +267,16 @@ public Operation convert(String value) throws ParameterException {\n         description = \"Should duplicate records from source be dropped/filtered out before insert/bulk-insert\")\n     public Boolean filterDupes = false;\n \n+    //will abandon in the future version, recommended use --enable-sync", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTcyMA==", "bodyText": "nit:indent", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464805720", "createdAt": "2020-08-04T05:20:44Z", "author": {"login": "vinothchandar"}, "path": "packaging/hudi-hive-sync-bundle/pom.xml", "diffHunk": "@@ -66,7 +66,8 @@\n                 <includes>\n                   <include>org.apache.hudi:hudi-common</include>\n                   <include>org.apache.hudi:hudi-hadoop-mr</include>\n-                  <include>org.apache.hudi:hudi-hive-sync</include>\n+                  <include>org.apache.hudi:hudi-sync-common</include>\n+\t\t  <include>org.apache.hudi:hudi-hive-sync</include>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ==", "bodyText": "what if both hive and meta sync are off? we would still emit metrics for meta?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464805989", "createdAt": "2020-08-04T05:21:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java", "diffHunk": "@@ -67,10 +77,15 @@ String getMetricsName(String action, String metric) {\n     return config == null ? null : String.format(\"%s.%s.%s\", tableName, action, metric);\n   }\n \n-  public void updateDeltaStreamerMetrics(long durationInNs, long hiveSyncNs) {\n+  public void updateDeltaStreamerMetrics(long durationInNs, long syncNs, boolean hiveSync) {\n     if (config.isMetricsOn()) {\n       Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"duration\"), getDurationInMs(durationInNs));\n-      Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(hiveSyncNs));\n+      if (hiveSync) {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(syncNs));\n+      } else {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"metaSyncDuration\"), getDurationInMs(syncNs));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjMzNQ==", "bodyText": "is there a way to do this by iterating over the configured sync tool classes? i.e only do it when sync is configured?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464806335", "createdAt": "2020-08-04T05:23:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -442,7 +449,8 @@ private void refreshTimeline() throws IOException {\n     long overallTimeMs = overallTimerContext != null ? overallTimerContext.stop() : 0;\n \n     // Send DeltaStreamer Metrics\n-    metrics.updateDeltaStreamerMetrics(overallTimeMs, hiveSyncTimeMs);\n+    metrics.updateDeltaStreamerMetrics(overallTimeMs, hiveSyncTimeMs, true);\n+    metrics.updateDeltaStreamerMetrics(overallTimeMs, metaSyncTimeMs, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjU5NA==", "bodyText": "should we derive the metric name from the sync tool class. i.e instead of metaSyncDuration, we do dlaSyncDuration?  that seems more usable and understandable", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464806594", "createdAt": "2020-08-04T05:24:02Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java", "diffHunk": "@@ -67,10 +77,15 @@ String getMetricsName(String action, String metric) {\n     return config == null ? null : String.format(\"%s.%s.%s\", tableName, action, metric);\n   }\n \n-  public void updateDeltaStreamerMetrics(long durationInNs, long hiveSyncNs) {\n+  public void updateDeltaStreamerMetrics(long durationInNs, long syncNs, boolean hiveSync) {\n     if (config.isMetricsOn()) {\n       Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"duration\"), getDurationInMs(durationInNs));\n-      Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(hiveSyncNs));\n+      if (hiveSync) {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(syncNs));\n+      } else {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"metaSyncDuration\"), getDurationInMs(syncNs));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ=="}, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 47}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e35037775016004364176de7f46091ce2b8ba49e", "author": {"user": {"login": "vinothchandar", "name": "vinoth chandar"}}, "url": "https://github.com/apache/hudi/commit/e35037775016004364176de7f46091ce2b8ba49e", "committedDate": "2020-08-04T05:43:53Z", "message": "Smaller CR feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwOTAxNzkz", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-460901793", "createdAt": "2020-08-04T15:02:38Z", "commit": {"oid": "e35037775016004364176de7f46091ce2b8ba49e"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNTowMjozOFrOG7klbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNTo1OTo1MVrOG7nB0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTExODU3NA==", "bodyText": "yes, when user set hiveSyncEnabled and --sync-tool-classes, sync both hive and --sync-tool-classes make sense. i will fix it", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465118574", "createdAt": "2020-08-04T15:02:38Z", "author": {"login": "lw309637554"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -255,6 +262,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2OTEzNQ=="}, "originalCommit": {"oid": "36979503d9f4d44281136caadc51ab681e1cf056"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTEzMzA5Mg==", "bodyText": "agree with you ,and  i will do it", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465133092", "createdAt": "2020-08-04T15:22:59Z", "author": {"login": "lw309637554"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -267,9 +267,16 @@ public Operation convert(String value) throws ParameterException {\n         description = \"Should duplicate records from source be dropped/filtered out before insert/bulk-insert\")\n     public Boolean filterDupes = false;\n \n+    //will abandon in the future version, recommended use --enable-sync", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTU1OA=="}, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE1NzA1Nw==", "bodyText": "i have do it , different  sync tool class have its own metrics with name of sync class", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465157057", "createdAt": "2020-08-04T15:57:35Z", "author": {"login": "lw309637554"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java", "diffHunk": "@@ -67,10 +77,15 @@ String getMetricsName(String action, String metric) {\n     return config == null ? null : String.format(\"%s.%s.%s\", tableName, action, metric);\n   }\n \n-  public void updateDeltaStreamerMetrics(long durationInNs, long hiveSyncNs) {\n+  public void updateDeltaStreamerMetrics(long durationInNs, long syncNs, boolean hiveSync) {\n     if (config.isMetricsOn()) {\n       Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"duration\"), getDurationInMs(durationInNs));\n-      Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(hiveSyncNs));\n+      if (hiveSync) {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(syncNs));\n+      } else {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"metaSyncDuration\"), getDurationInMs(syncNs));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ=="}, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE1ODYwOA==", "bodyText": "ok  , have do this in syncMeta", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465158608", "createdAt": "2020-08-04T15:59:51Z", "author": {"login": "lw309637554"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -442,7 +449,8 @@ private void refreshTimeline() throws IOException {\n     long overallTimeMs = overallTimerContext != null ? overallTimerContext.stop() : 0;\n \n     // Send DeltaStreamer Metrics\n-    metrics.updateDeltaStreamerMetrics(overallTimeMs, hiveSyncTimeMs);\n+    metrics.updateDeltaStreamerMetrics(overallTimeMs, hiveSyncTimeMs, true);\n+    metrics.updateDeltaStreamerMetrics(overallTimeMs, metaSyncTimeMs, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjMzNQ=="}, "originalCommit": {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMDQwODYy", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461040862", "createdAt": "2020-08-04T17:54:43Z", "commit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNzo1NDo0M1rOG7rRNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNzo1NzoxM1rOG7rWkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyODA4Ng==", "bodyText": "actually like META_SYNC better here. it was more meaningful. wdyt?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465228086", "createdAt": "2020-08-04T17:54:43Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -261,6 +268,44 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyODcyNg==", "bodyText": "if someone does hiveSyncEnabled == true && metaSyncEnabled == true && syncClientToolClass = org.apache.hudi.hive.HiveSyncTool, we will sync two times? can we just a set to hold the classes.", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465228726", "createdAt": "2020-08-04T17:55:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -261,6 +268,44 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters(SYNC_CLIENT_TOOL_CLASS)\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = String.format(\"%s,%s\", syncClientToolClass, \"org.apache.hudi.hive.HiveSyncTool\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyOTQ1Nw==", "bodyText": "lets do HiveSyncTool.class.getName or soemthing?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465229457", "createdAt": "2020-08-04T17:57:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -267,9 +267,16 @@ public Operation convert(String value) throws ParameterException {\n         description = \"Should duplicate records from source be dropped/filtered out before insert/bulk-insert\")\n     public Boolean filterDupes = false;\n \n+    //will abandon in the future version, recommended use --enable-sync\n     @Parameter(names = {\"--enable-hive-sync\"}, description = \"Enable syncing to hive\")\n     public Boolean enableHiveSync = false;\n \n+    @Parameter(names = {\"--enable-sync\"}, description = \"Enable syncing meta\")\n+    public Boolean enableMetaSync = false;\n+\n+    @Parameter(names = {\"--sync-tool-classes\"}, description = \"Meta sync client tool, using comma to separate multi tools\")\n+    public String syncClientToolClass = \"org.apache.hudi.hive.HiveSyncTool\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjc2MTk3", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461276197", "createdAt": "2020-08-05T01:16:41Z", "commit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToxNjo0MVrOG72uvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToxNjo0MVrOG72uvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNTg2OA==", "bodyText": "we would add a TODO here once DLA supports alter table properties.", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465415868", "createdAt": "2020-08-05T01:16:41Z", "author": {"login": "leesf"}, "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/DLASyncTool.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import com.beust.jcommander.JCommander;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.dla.util.Utils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.hudi.sync.common.AbstractSyncTool;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Tool to sync a hoodie table with a dla table. Either use it as a api\n+ * DLASyncTool.syncHoodieTable(DLASyncConfig) or as a command line java -cp hoodie-hive.jar DLASyncTool [args]\n+ * <p>\n+ * This utility will get the schema from the latest commit and will sync dla table schema Also this will sync the\n+ * partitions incrementally (all the partitions modified since the last commit)\n+ */\n+@SuppressWarnings(\"WeakerAccess\")\n+public class DLASyncTool extends AbstractSyncTool {\n+\n+  private static final Logger LOG = LogManager.getLogger(DLASyncTool.class);\n+  public static final String SUFFIX_SNAPSHOT_TABLE = \"_rt\";\n+  public static final String SUFFIX_READ_OPTIMIZED_TABLE = \"_ro\";\n+\n+  private final DLASyncConfig cfg;\n+  private final HoodieDLAClient hoodieDLAClient;\n+  private final String snapshotTableName;\n+  private final Option<String> roTableTableName;\n+\n+  public DLASyncTool(Properties properties, FileSystem fs) {\n+    super(properties, fs);\n+    this.hoodieDLAClient = new HoodieDLAClient(Utils.propertiesToConfig(properties), fs);\n+    this.cfg = Utils.propertiesToConfig(properties);\n+    switch (hoodieDLAClient.getTableType()) {\n+      case COPY_ON_WRITE:\n+        this.snapshotTableName = cfg.tableName;\n+        this.roTableTableName = Option.empty();\n+        break;\n+      case MERGE_ON_READ:\n+        this.snapshotTableName = cfg.tableName + SUFFIX_SNAPSHOT_TABLE;\n+        this.roTableTableName = cfg.skipROSuffix ? Option.of(cfg.tableName) :\n+            Option.of(cfg.tableName + SUFFIX_READ_OPTIMIZED_TABLE);\n+        break;\n+      default:\n+        LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+        throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+    }\n+  }\n+\n+  @Override\n+  public void syncHoodieTable() {\n+    try {\n+      switch (hoodieDLAClient.getTableType()) {\n+        case COPY_ON_WRITE:\n+          syncHoodieTable(snapshotTableName, false);\n+          break;\n+        case MERGE_ON_READ:\n+          // sync a RO table for MOR\n+          syncHoodieTable(roTableTableName.get(), false);\n+          // sync a RT table for MOR\n+          syncHoodieTable(snapshotTableName, true);\n+          break;\n+        default:\n+          LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+          throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+      }\n+    } catch (RuntimeException re) {\n+      LOG.error(\"Got runtime exception when dla syncing\", re);\n+    } finally {\n+      hoodieDLAClient.close();\n+    }\n+  }\n+\n+  private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n+    LOG.info(\"Trying to sync hoodie table \" + tableName + \" with base path \" + hoodieDLAClient.getBasePath()\n+        + \" of type \" + hoodieDLAClient.getTableType());\n+    // Check if the necessary table exists\n+    boolean tableExists = hoodieDLAClient.doesTableExist(tableName);\n+    // Get the parquet schema for this table looking at the latest commit\n+    MessageType schema = hoodieDLAClient.getDataSchema();\n+    // Sync schema if needed\n+    syncSchema(tableName, tableExists, useRealtimeInputFormat, schema);\n+\n+    LOG.info(\"Schema sync complete. Syncing partitions for \" + tableName);\n+    // Get the last time we successfully synced partitions\n+    Option<String> lastCommitTimeSynced = Option.empty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 121}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjc2NjE1", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461276615", "createdAt": "2020-08-05T01:18:06Z", "commit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToxODowNlrOG72wGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToxODowNlrOG72wGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNjIxNw==", "bodyText": "please use // TODO here", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465416217", "createdAt": "2020-08-05T01:18:06Z", "author": {"login": "leesf"}, "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/HoodieDLAClient.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.hive.HiveSyncConfig;\n+import org.apache.hudi.hive.HoodieHiveSyncException;\n+import org.apache.hudi.hive.PartitionValueExtractor;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.DatabaseMetaData;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class HoodieDLAClient extends AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(HoodieDLAClient.class);\n+  private static final String HOODIE_LAST_COMMIT_TIME_SYNC = \"hoodie_last_sync\";\n+  // Make sure we have the dla JDBC driver in classpath\n+  private static final String DRIVER_NAME = \"com.mysql.jdbc.Driver\";\n+  private static final String DLA_ESCAPE_CHARACTER = \"\";\n+  private static final String TBL_PROPERTIES_STR = \"TBLPROPERTIES\";\n+\n+  static {\n+    try {\n+      Class.forName(DRIVER_NAME);\n+    } catch (ClassNotFoundException e) {\n+      throw new IllegalStateException(\"Could not find \" + DRIVER_NAME + \" in classpath. \", e);\n+    }\n+  }\n+\n+  private Connection connection;\n+  private DLASyncConfig dlaConfig;\n+  private PartitionValueExtractor partitionValueExtractor;\n+\n+  public HoodieDLAClient(DLASyncConfig syncConfig, FileSystem fs) {\n+    super(syncConfig.basePath, syncConfig.assumeDatePartitioning, fs);\n+    this.dlaConfig = syncConfig;\n+    try {\n+      this.partitionValueExtractor =\n+          (PartitionValueExtractor) Class.forName(dlaConfig.partitionValueExtractorClass).newInstance();\n+    } catch (Exception e) {\n+      throw new HoodieException(\n+          \"Failed to initialize PartitionValueExtractor class \" + dlaConfig.partitionValueExtractorClass, e);\n+    }\n+    createDLAConnection();\n+  }\n+\n+  private void createDLAConnection() {\n+    if (connection == null) {\n+      try {\n+        Class.forName(DRIVER_NAME);\n+      } catch (ClassNotFoundException e) {\n+        LOG.error(\"Unable to load DLA driver class\", e);\n+        return;\n+      }\n+      try {\n+        this.connection = DriverManager.getConnection(dlaConfig.jdbcUrl, dlaConfig.dlaUser, dlaConfig.dlaPass);\n+        LOG.info(\"Successfully established DLA connection to  \" + dlaConfig.jdbcUrl);\n+      } catch (SQLException e) {\n+        throw new HoodieException(\"Cannot create dla connection \", e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void createTable(String tableName, MessageType storageSchema, String inputFormatClass, String outputFormatClass, String serdeClass) {\n+    try {\n+      String createSQLQuery = HiveSchemaUtil.generateCreateDDL(tableName, storageSchema, toHiveSyncConfig(), inputFormatClass, outputFormatClass, serdeClass);\n+      LOG.info(\"Creating table with \" + createSQLQuery);\n+      updateDLASQL(createSQLQuery);\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Failed to create table \" + tableName, e);\n+    }\n+  }\n+\n+  public Map<String, String> getTableSchema(String tableName) {\n+    if (!doesTableExist(tableName)) {\n+      throw new IllegalArgumentException(\n+          \"Failed to get schema for table \" + tableName + \" does not exist\");\n+    }\n+    Map<String, String> schema = new HashMap<>();\n+    ResultSet result = null;\n+    try {\n+      DatabaseMetaData databaseMetaData = connection.getMetaData();\n+      result = databaseMetaData.getColumns(dlaConfig.databaseName, dlaConfig.databaseName, tableName, null);\n+      while (result.next()) {\n+        String columnName = result.getString(4);\n+        String columnType = result.getString(6);\n+        if (\"DECIMAL\".equals(columnType)) {\n+          int columnSize = result.getInt(\"COLUMN_SIZE\");\n+          int decimalDigits = result.getInt(\"DECIMAL_DIGITS\");\n+          columnType += String.format(\"(%s,%s)\", columnSize, decimalDigits);\n+        }\n+        schema.put(columnName, columnType);\n+      }\n+      return schema;\n+    } catch (SQLException e) {\n+      throw new HoodieException(\"Failed to get table schema for \" + tableName, e);\n+    } finally {\n+      closeQuietly(result, null);\n+    }\n+  }\n+\n+  @Override\n+  public void addPartitionsToTable(String tableName, List<String> partitionsToAdd) {\n+    if (partitionsToAdd.isEmpty()) {\n+      LOG.info(\"No partitions to add for \" + tableName);\n+      return;\n+    }\n+    LOG.info(\"Adding partitions \" + partitionsToAdd.size() + \" to table \" + tableName);\n+    String sql = constructAddPartitions(tableName, partitionsToAdd);\n+    updateDLASQL(sql);\n+  }\n+\n+  public String constructAddPartitions(String tableName, List<String> partitions) {\n+    return constructDLAAddPartitions(tableName, partitions);\n+  }\n+\n+  String generateAbsolutePathStr(Path path) {\n+    String absolutePathStr = path.toString();\n+    if (path.toUri().getScheme() == null) {\n+      absolutePathStr = getDefaultFs() + absolutePathStr;\n+    }\n+    return absolutePathStr.endsWith(\"/\") ? absolutePathStr : absolutePathStr + \"/\";\n+  }\n+\n+  public List<String> constructChangePartitions(String tableName, List<String> partitions) {\n+    List<String> changePartitions = new ArrayList<>();\n+    String useDatabase = \"USE \" + DLA_ESCAPE_CHARACTER + dlaConfig.databaseName + DLA_ESCAPE_CHARACTER;\n+    changePartitions.add(useDatabase);\n+    String alterTable = \"ALTER TABLE \" + DLA_ESCAPE_CHARACTER + tableName + DLA_ESCAPE_CHARACTER;\n+    for (String partition : partitions) {\n+      String partitionClause = getPartitionClause(partition);\n+      Path partitionPath = FSUtils.getPartitionPath(dlaConfig.basePath, partition);\n+      String fullPartitionPathStr = generateAbsolutePathStr(partitionPath);\n+      String changePartition =\n+          alterTable + \" ADD IF NOT EXISTS PARTITION (\" + partitionClause + \") LOCATION '\" + fullPartitionPathStr + \"'\";\n+      changePartitions.add(changePartition);\n+    }\n+    return changePartitions;\n+  }\n+\n+  /**\n+   * Generate Hive Partition from partition values.\n+   *\n+   * @param partition Partition path\n+   * @return\n+   */\n+  public String getPartitionClause(String partition) {\n+    List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);\n+    ValidationUtils.checkArgument(dlaConfig.partitionFields.size() == partitionValues.size(),\n+        \"Partition key parts \" + dlaConfig.partitionFields + \" does not match with partition values \" + partitionValues\n+            + \". Check partition strategy. \");\n+    List<String> partBuilder = new ArrayList<>();\n+    for (int i = 0; i < dlaConfig.partitionFields.size(); i++) {\n+      partBuilder.add(dlaConfig.partitionFields.get(i) + \"='\" + partitionValues.get(i) + \"'\");\n+    }\n+    return partBuilder.stream().collect(Collectors.joining(\",\"));\n+  }\n+\n+  private String constructDLAAddPartitions(String tableName, List<String> partitions) {\n+    StringBuilder alterSQL = new StringBuilder(\"ALTER TABLE \");\n+    alterSQL.append(DLA_ESCAPE_CHARACTER).append(dlaConfig.databaseName)\n+        .append(DLA_ESCAPE_CHARACTER).append(\".\").append(DLA_ESCAPE_CHARACTER)\n+        .append(tableName).append(DLA_ESCAPE_CHARACTER).append(\" ADD IF NOT EXISTS \");\n+    for (String partition : partitions) {\n+      String partitionClause = getPartitionClause(partition);\n+      Path partitionPath = FSUtils.getPartitionPath(dlaConfig.basePath, partition);\n+      String fullPartitionPathStr = generateAbsolutePathStr(partitionPath);\n+      alterSQL.append(\"  PARTITION (\").append(partitionClause).append(\") LOCATION '\").append(fullPartitionPathStr)\n+          .append(\"' \");\n+    }\n+    return alterSQL.toString();\n+  }\n+\n+  private void updateDLASQL(String sql) {\n+    Statement stmt = null;\n+    try {\n+      stmt = connection.createStatement();\n+      LOG.info(\"Executing SQL \" + sql);\n+      stmt.execute(sql);\n+    } catch (SQLException e) {\n+      throw new HoodieException(\"Failed in executing SQL \" + sql, e);\n+    } finally {\n+      closeQuietly(null, stmt);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doesTableExist(String tableName) {\n+    String sql = consutructShowCreateTableSQL(tableName);\n+    Statement stmt = null;\n+    ResultSet rs = null;\n+    try {\n+      stmt = connection.createStatement();\n+      rs = stmt.executeQuery(sql);\n+    } catch (SQLException e) {\n+      return false;\n+    } finally {\n+      closeQuietly(rs, stmt);\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public Option<String> getLastCommitTimeSynced(String tableName) {\n+    String sql = consutructShowCreateTableSQL(tableName);\n+    Statement stmt = null;\n+    ResultSet rs = null;\n+    try {\n+      stmt = connection.createStatement();\n+      rs = stmt.executeQuery(sql);\n+      if (rs.next()) {\n+        String table = rs.getString(2);\n+        Map<String, String> attr = new HashMap<>();\n+        int index = table.indexOf(TBL_PROPERTIES_STR);\n+        if (index != -1) {\n+          String sub = table.substring(index + TBL_PROPERTIES_STR.length());\n+          sub = sub.replaceAll(\"\\\\(\", \"\").replaceAll(\"\\\\)\", \"\").replaceAll(\"'\", \"\");\n+          String[] str = sub.split(\",\");\n+\n+          for (int i = 0; i < str.length; i++) {\n+            String key = str[i].split(\"=\")[0].trim();\n+            String value = str[i].split(\"=\")[1].trim();\n+            attr.put(key, value);\n+          }\n+        }\n+        return Option.ofNullable(attr.getOrDefault(HOODIE_LAST_COMMIT_TIME_SYNC, null));\n+      }\n+      return Option.empty();\n+    } catch (Exception e) {\n+      throw new HoodieHiveSyncException(\"Failed to get the last commit time synced from the table\", e);\n+    } finally {\n+      closeQuietly(rs, stmt);\n+    }\n+  }\n+\n+  @Override\n+  public void updateLastCommitTimeSynced(String tableName) {\n+    // dla do not support update tblproperties, so do nothing.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 277}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjc3NTQx", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461277541", "createdAt": "2020-08-05T01:21:07Z", "commit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToyMTowOFrOG72zXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToyMTowOFrOG72zXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzA1Mw==", "bodyText": "please change to warn", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417053", "createdAt": "2020-08-05T01:21:08Z", "author": {"login": "leesf"}, "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sync.common;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public abstract class AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(AbstractSyncHoodieClient.class);\n+  protected final HoodieTableMetaClient metaClient;\n+  protected HoodieTimeline activeTimeline;\n+  protected final HoodieTableType tableType;\n+  protected final FileSystem fs;\n+  private String basePath;\n+  private boolean assumeDatePartitioning;\n+\n+  public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning, FileSystem fs) {\n+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);\n+    this.tableType = metaClient.getTableType();\n+    this.basePath = basePath;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+    this.fs = fs;\n+    this.activeTimeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+  }\n+\n+  public abstract void createTable(String tableName, MessageType storageSchema,\n+                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+\n+  public abstract boolean doesTableExist(String tableName);\n+\n+  public abstract Option<String> getLastCommitTimeSynced(String tableName);\n+\n+  public abstract void updateLastCommitTimeSynced(String tableName);\n+\n+  public abstract void addPartitionsToTable(String tableName, List<String> partitionsToAdd);\n+\n+  public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n+\n+  public abstract Map<String, String> getTableSchema(String tableName);\n+\n+  public HoodieTimeline getActiveTimeline() {\n+    return activeTimeline;\n+  }\n+\n+  public HoodieTableType getTableType() {\n+    return tableType;\n+  }\n+\n+  public String getBasePath() {\n+    return metaClient.getBasePath();\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  public void closeQuietly(ResultSet resultSet, Statement stmt) {\n+    try {\n+      if (stmt != null) {\n+        stmt.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the statement opened \", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 100}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjc3NjM4", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461277638", "createdAt": "2020-08-05T01:21:27Z", "commit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToyMToyOFrOG72zsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToyMToyOFrOG72zsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzEzNg==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417136", "createdAt": "2020-08-05T01:21:28Z", "author": {"login": "leesf"}, "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sync.common;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public abstract class AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(AbstractSyncHoodieClient.class);\n+  protected final HoodieTableMetaClient metaClient;\n+  protected HoodieTimeline activeTimeline;\n+  protected final HoodieTableType tableType;\n+  protected final FileSystem fs;\n+  private String basePath;\n+  private boolean assumeDatePartitioning;\n+\n+  public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning, FileSystem fs) {\n+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);\n+    this.tableType = metaClient.getTableType();\n+    this.basePath = basePath;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+    this.fs = fs;\n+    this.activeTimeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+  }\n+\n+  public abstract void createTable(String tableName, MessageType storageSchema,\n+                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+\n+  public abstract boolean doesTableExist(String tableName);\n+\n+  public abstract Option<String> getLastCommitTimeSynced(String tableName);\n+\n+  public abstract void updateLastCommitTimeSynced(String tableName);\n+\n+  public abstract void addPartitionsToTable(String tableName, List<String> partitionsToAdd);\n+\n+  public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n+\n+  public abstract Map<String, String> getTableSchema(String tableName);\n+\n+  public HoodieTimeline getActiveTimeline() {\n+    return activeTimeline;\n+  }\n+\n+  public HoodieTableType getTableType() {\n+    return tableType;\n+  }\n+\n+  public String getBasePath() {\n+    return metaClient.getBasePath();\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  public void closeQuietly(ResultSet resultSet, Statement stmt) {\n+    try {\n+      if (stmt != null) {\n+        stmt.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the statement opened \", e);\n+    }\n+\n+    try {\n+      if (resultSet != null) {\n+        resultSet.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the resultset opened \", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjc4MDg3", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461278087", "createdAt": "2020-08-05T01:23:04Z", "commit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToyMzowNVrOG721Qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToyMzowNVrOG721Qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzUzOQ==", "bodyText": "use HiveSyncTool.class.getName here?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417539", "createdAt": "2020-08-05T01:23:05Z", "author": {"login": "leesf"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -475,12 +480,38 @@ private String startCommit() {\n     throw lastException;\n   }\n \n-  /**\n-   * Sync to Hive.\n-   */\n-  public void syncHiveIfNeeded() {\n+  private void syncMeta(HoodieDeltaStreamerMetrics metrics) {\n+    String syncClientToolClass = cfg.syncClientToolClass;\n+    // for backward compatibility\n     if (cfg.enableHiveSync) {\n-      syncHive();\n+      cfg.enableMetaSync = true;\n+      syncClientToolClass = String.format(\"%s,%s\", cfg.syncClientToolClass, \"org.apache.hudi.hive.HiveSyncTool\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 85}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjc4ODkx", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461278891", "createdAt": "2020-08-05T01:25:46Z", "commit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToyNTo0NlrOG724fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMToyNTo0NlrOG724fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxODM2Ng==", "bodyText": "use HiveSyncTool.class.getName?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465418366", "createdAt": "2020-08-05T01:25:46Z", "author": {"login": "leesf"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -258,11 +258,14 @@ object DataSourceWriteOptions {\n     */\n   val STREAMING_IGNORE_FAILED_BATCH_OPT_KEY = \"hoodie.datasource.write.streaming.ignore.failed.batch\"\n   val DEFAULT_STREAMING_IGNORE_FAILED_BATCH_OPT_VAL = \"true\"\n+  val SYNC_CLIENT_TOOL_CLASS = \"hoodie.sync.client.tool.class\"\n+  val DEFAULT_SYNC_CLIENT_TOOL_CLASS = \"org.apache.hudi.hive.HiveSyncTool\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3"}, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3ba46533b866908feb33315fd9df058d57375cf3", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3ba46533b866908feb33315fd9df058d57375cf3", "committedDate": "2020-08-04T16:26:53Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}, "afterCommit": {"oid": "fe598ff9af58355469ca08157d7369b7150bde71", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/fe598ff9af58355469ca08157d7369b7150bde71", "committedDate": "2020-08-05T04:29:58Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMzM1MjI2", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461335226", "createdAt": "2020-08-05T04:42:04Z", "commit": {"oid": "fe598ff9af58355469ca08157d7369b7150bde71"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwNDo0MjowNFrOG758Qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwNDo0MjowNFrOG758Qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2ODQ4Mg==", "bodyText": "do we need the entire class name here? Would that not make for a long metric name? :)\nMay be have a getShortName() method for the AbstractSyncTool class and return \"hive\" and \"dla\" from them?", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465468482", "createdAt": "2020-08-05T04:42:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java", "diffHunk": "@@ -67,10 +77,15 @@ String getMetricsName(String action, String metric) {\n     return config == null ? null : String.format(\"%s.%s.%s\", tableName, action, metric);\n   }\n \n-  public void updateDeltaStreamerMetrics(long durationInNs, long hiveSyncNs) {\n+  public void updateDeltaStreamerMetrics(long durationInNs) {\n     if (config.isMetricsOn()) {\n       Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"duration\"), getDurationInMs(durationInNs));\n-      Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(hiveSyncNs));\n+    }\n+  }\n+\n+  public void updateDeltaStreamerMetaSyncMetrics(String syncClassName, long syncNs) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fe598ff9af58355469ca08157d7369b7150bde71"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMzM1NTAw", "url": "https://github.com/apache/hudi/pull/1810#pullrequestreview-461335500", "createdAt": "2020-08-05T04:43:05Z", "commit": {"oid": "fe598ff9af58355469ca08157d7369b7150bde71"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fe598ff9af58355469ca08157d7369b7150bde71", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/fe598ff9af58355469ca08157d7369b7150bde71", "committedDate": "2020-08-05T04:29:58Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}, "afterCommit": {"oid": "015ac1b69fb5443e9a24fcf84b31b21a66f020f4", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/015ac1b69fb5443e9a24fcf84b31b21a66f020f4", "committedDate": "2020-08-05T06:31:48Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "015ac1b69fb5443e9a24fcf84b31b21a66f020f4", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/015ac1b69fb5443e9a24fcf84b31b21a66f020f4", "committedDate": "2020-08-05T06:31:48Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}, "afterCommit": {"oid": "856ef5a9cbf45784932fde3574d3e8e8435f5c26", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/856ef5a9cbf45784932fde3574d3e8e8435f5c26", "committedDate": "2020-08-05T09:55:36Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3808c3253e6d71c0b7b504046be0c1e9bde969d7", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3808c3253e6d71c0b7b504046be0c1e9bde969d7", "committedDate": "2020-08-05T13:08:33Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "856ef5a9cbf45784932fde3574d3e8e8435f5c26", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/856ef5a9cbf45784932fde3574d3e8e8435f5c26", "committedDate": "2020-08-05T09:55:36Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}, "afterCommit": {"oid": "3808c3253e6d71c0b7b504046be0c1e9bde969d7", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3808c3253e6d71c0b7b504046be0c1e9bde969d7", "committedDate": "2020-08-05T13:08:33Z", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "committedDate": "2020-08-06T03:07:13Z", "message": "merge master"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5014e14bb3bdc1963ba767fa168a8514f0d84873", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/5014e14bb3bdc1963ba767fa168a8514f0d84873", "committedDate": "2020-08-05T16:31:56Z", "message": "merge master"}, "afterCommit": {"oid": "4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "committedDate": "2020-08-06T03:07:13Z", "message": "merge master"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2924, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}