{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY1MDU2MTM4", "number": 1260, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMjoxMjowN1rODZW8-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwNTozODo1MVrODZYVew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3OTE3MDUxOnYy", "diffSide": "RIGHT", "path": "docs/_docs/1_2_structure.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMjoxMjowN1rOFfsxQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxODozMTo0MFrOFgEY0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4MzY4Mw==", "bodyText": "just Query and not querying?", "url": "https://github.com/apache/hudi/pull/1260#discussion_r368783683", "createdAt": "2020-01-21T02:12:07Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/1_2_structure.md", "diffHunk": "@@ -6,16 +6,16 @@ summary: \"Hudi brings stream processing to big data, providing fresh data while\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Hudi (pronounced \u201cHoodie\u201d) ingests & manages storage of large analytical datasets over DFS ([HDFS](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) or cloud stores) and provides three logical views for query access.\n+Hudi (pronounced \u201cHoodie\u201d) ingests & manages storage of large analytical tables over DFS ([HDFS](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) or cloud stores) and provides three types of querying.\n \n- * **Read Optimized View** - Provides excellent query performance on pure columnar storage, much like plain [Parquet](https://parquet.apache.org/) tables.\n- * **Incremental View** - Provides a change stream out of the dataset to feed downstream jobs/ETLs.\n- * **Near-Real time Table** - Provides queries on real-time data, using a combination of columnar & row based storage (e.g Parquet + [Avro](http://avro.apache.org/docs/current/mr.html))\n+ * **Read Optimized querying** - Provides excellent query performance on pure columnar storage, much like plain [Parquet](https://parquet.apache.org/) tables.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE3MDY0Mg==", "bodyText": "sure", "url": "https://github.com/apache/hudi/pull/1260#discussion_r369170642", "createdAt": "2020-01-21T18:31:40Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/1_2_structure.md", "diffHunk": "@@ -6,16 +6,16 @@ summary: \"Hudi brings stream processing to big data, providing fresh data while\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Hudi (pronounced \u201cHoodie\u201d) ingests & manages storage of large analytical datasets over DFS ([HDFS](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) or cloud stores) and provides three logical views for query access.\n+Hudi (pronounced \u201cHoodie\u201d) ingests & manages storage of large analytical tables over DFS ([HDFS](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) or cloud stores) and provides three types of querying.\n \n- * **Read Optimized View** - Provides excellent query performance on pure columnar storage, much like plain [Parquet](https://parquet.apache.org/) tables.\n- * **Incremental View** - Provides a change stream out of the dataset to feed downstream jobs/ETLs.\n- * **Near-Real time Table** - Provides queries on real-time data, using a combination of columnar & row based storage (e.g Parquet + [Avro](http://avro.apache.org/docs/current/mr.html))\n+ * **Read Optimized querying** - Provides excellent query performance on pure columnar storage, much like plain [Parquet](https://parquet.apache.org/) tables.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4MzY4Mw=="}, "originalCommit": null, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3OTE3MDk4OnYy", "diffSide": "RIGHT", "path": "docs/_docs/1_3_use_cases.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMjoxMjozNlrOFfsxkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMjoxMjozNlrOFfsxkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4Mzc2MA==", "bodyText": "ah. good catch", "url": "https://github.com/apache/hudi/pull/1260#discussion_r368783760", "createdAt": "2020-01-21T02:12:36Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/1_3_use_cases.md", "diffHunk": "@@ -20,7 +20,7 @@ or [complicated handcrafted merge workflows](http://hortonworks.com/blog/four-st\n For NoSQL datastores like [Cassandra](http://cassandra.apache.org/) / [Voldemort](http://www.project-voldemort.com/voldemort/) / [HBase](https://hbase.apache.org/), even moderately big installations store billions of rows.\n It goes without saying that __full bulk loads are simply infeasible__ and more efficient approaches are needed if ingestion is to keep up with the typically high update volumes.\n \n-Even for immutable data sources like [Kafka](kafka.apache.org) , Hudi helps __enforces a minimum file size on HDFS__, which improves NameNode health by solving one of the [age old problems in Hadoop land](https://blog.cloudera.com/blog/2009/02/the-small-files-problem/) in a holistic way. This is all the more important for event streams, since typically its higher volume (eg: click streams) and if not managed well, can cause serious damage to your Hadoop cluster.\n+Even for immutable data sources like [Kafka](http://kafka.apache.org) , Hudi helps __enforces a minimum file size on HDFS__, which improves NameNode health by solving one of the [age old problems in Hadoop land](https://blog.cloudera.com/blog/2009/02/the-small-files-problem/) in a holistic way. This is all the more important for event streams, since typically its higher volume (eg: click streams) and if not managed well, can cause serious damage to your Hadoop cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3OTE3MjA3OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_1_concepts.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMjoxMzoyNlrOFfsyNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxODozMjoyM1rOFgEaFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4MzkyNQ==", "bodyText": "how do I fetch records that changed ?", "url": "https://github.com/apache/hudi/pull/1260#discussion_r368783925", "createdAt": "2020-01-21T02:13:26Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_1_concepts.md", "diffHunk": "@@ -1,37 +1,37 @@\n ---\n title: \"Concepts\"\n-keywords: hudi, design, storage, views, timeline\n+keywords: hudi, design, table, queries, timeline\n permalink: /docs/concepts.html\n summary: \"Here we introduce some basic concepts & give a broad technical overview of Hudi\"\n toc: true\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Apache Hudi (pronounced \u201cHudi\u201d) provides the following streaming primitives over datasets on DFS\n+Apache Hudi (pronounced \u201cHudi\u201d) provides the following streaming primitives over hadoop compatible storages\n \n- * Upsert                     (how do I change the dataset?)\n- * Incremental pull           (how do I fetch data that changed?)\n+ * Update/Delete Records      (how do I change records in a table?)\n+ * Change Streams             (how do I fetch data that changed?)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE3MDk2NQ==", "bodyText": "yes sure", "url": "https://github.com/apache/hudi/pull/1260#discussion_r369170965", "createdAt": "2020-01-21T18:32:23Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_1_concepts.md", "diffHunk": "@@ -1,37 +1,37 @@\n ---\n title: \"Concepts\"\n-keywords: hudi, design, storage, views, timeline\n+keywords: hudi, design, table, queries, timeline\n permalink: /docs/concepts.html\n summary: \"Here we introduce some basic concepts & give a broad technical overview of Hudi\"\n toc: true\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Apache Hudi (pronounced \u201cHudi\u201d) provides the following streaming primitives over datasets on DFS\n+Apache Hudi (pronounced \u201cHudi\u201d) provides the following streaming primitives over hadoop compatible storages\n \n- * Upsert                     (how do I change the dataset?)\n- * Incremental pull           (how do I fetch data that changed?)\n+ * Update/Delete Records      (how do I change records in a table?)\n+ * Change Streams             (how do I fetch data that changed?)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4MzkyNQ=="}, "originalCommit": null, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3OTE3MjYxOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_1_concepts.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMjoxMzo1OFrOFfsykg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxODozMjo1NVrOFgEa8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4NDAxOA==", "bodyText": "and Queries (instead of Querying)?", "url": "https://github.com/apache/hudi/pull/1260#discussion_r368784018", "createdAt": "2020-01-21T02:13:58Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_1_concepts.md", "diffHunk": "@@ -53,69 +53,70 @@ With the help of the timeline, an incremental query attempting to get all new da\n only the changed files without say scanning all the time buckets > 07:00.\n \n ## File management\n-Hudi organizes a datasets into a directory structure under a `basepath` on DFS. Dataset is broken up into partitions, which are folders containing data files for that partition,\n+Hudi organizes a table into a directory structure under a `basepath` on DFS. Table is broken up into partitions, which are folders containing data files for that partition,\n very similar to Hive tables. Each partition is uniquely identified by its `partitionpath`, which is relative to the basepath.\n \n Within each partition, files are organized into `file groups`, uniquely identified by a `file id`. Each file group contains several\n-`file slices`, where each slice contains a base columnar file (`*.parquet`) produced at a certain commit/compaction instant time,\n+`file slices`, where each slice contains a base file (`*.parquet`) produced at a certain commit/compaction instant time,\n  along with set of log files (`*.log.*`) that contain inserts/updates to the base file since the base file was produced. \n Hudi adopts a MVCC design, where compaction action merges logs and base files to produce new file slices and cleaning action gets rid of \n unused/older file slices to reclaim space on DFS. \n \n-Hudi provides efficient upserts, by mapping a given hoodie key (record key + partition path) consistently to a file group, via an indexing mechanism. \n+## Index\n+Hudi provides efficient upserts, by mapping a given hoodie key (record key + partition path) consistently to a file id, via an indexing mechanism. \n This mapping between record key and file group/file id, never changes once the first version of a record has been written to a file. In short, the \n mapped file group contains all versions of a group of records.\n \n-## Storage Types & Views\n-Hudi storage types define how data is indexed & laid out on the DFS and how the above primitives and timeline activities are implemented on top of such organization (i.e how data is written). \n-In turn, `views` define how the underlying data is exposed to the queries (i.e how data is read). \n+## Table Types & Querying", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE3MTE4NA==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/1260#discussion_r369171184", "createdAt": "2020-01-21T18:32:55Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_1_concepts.md", "diffHunk": "@@ -53,69 +53,70 @@ With the help of the timeline, an incremental query attempting to get all new da\n only the changed files without say scanning all the time buckets > 07:00.\n \n ## File management\n-Hudi organizes a datasets into a directory structure under a `basepath` on DFS. Dataset is broken up into partitions, which are folders containing data files for that partition,\n+Hudi organizes a table into a directory structure under a `basepath` on DFS. Table is broken up into partitions, which are folders containing data files for that partition,\n very similar to Hive tables. Each partition is uniquely identified by its `partitionpath`, which is relative to the basepath.\n \n Within each partition, files are organized into `file groups`, uniquely identified by a `file id`. Each file group contains several\n-`file slices`, where each slice contains a base columnar file (`*.parquet`) produced at a certain commit/compaction instant time,\n+`file slices`, where each slice contains a base file (`*.parquet`) produced at a certain commit/compaction instant time,\n  along with set of log files (`*.log.*`) that contain inserts/updates to the base file since the base file was produced. \n Hudi adopts a MVCC design, where compaction action merges logs and base files to produce new file slices and cleaning action gets rid of \n unused/older file slices to reclaim space on DFS. \n \n-Hudi provides efficient upserts, by mapping a given hoodie key (record key + partition path) consistently to a file group, via an indexing mechanism. \n+## Index\n+Hudi provides efficient upserts, by mapping a given hoodie key (record key + partition path) consistently to a file id, via an indexing mechanism. \n This mapping between record key and file group/file id, never changes once the first version of a record has been written to a file. In short, the \n mapped file group contains all versions of a group of records.\n \n-## Storage Types & Views\n-Hudi storage types define how data is indexed & laid out on the DFS and how the above primitives and timeline activities are implemented on top of such organization (i.e how data is written). \n-In turn, `views` define how the underlying data is exposed to the queries (i.e how data is read). \n+## Table Types & Querying", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4NDAxOA=="}, "originalCommit": null, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3OTE3MzgzOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_2_writing_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMjoxNTowNVrOFfszWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxOTowNToyM1rOFgFZcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4NDIxNw==", "bodyText": "lets link to the delete blog from here?", "url": "https://github.com/apache/hudi/pull/1260#discussion_r368784217", "createdAt": "2020-01-21T02:15:05Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -156,41 +157,31 @@ inputDF.write()\n \n ## Syncing to Hive\n \n-Both tools above support syncing of the dataset's latest schema to Hive metastore, such that queries can pick up new columns and partitions.\n+Both tools above support syncing of the table's latest schema to Hive metastore, such that queries can pick up new columns and partitions.\n In case, its preferable to run this from commandline or in an independent jvm, Hudi provides a `HiveSyncTool`, which can be invoked as below, \n-once you have built the hudi-hive module.\n+once you have built the hudi-hive module. Following is how we sync the above Datasource Writer written table to Hive metastore.\n+\n+```java\n+cd hudi-hive\n+./run_sync_tool.sh  --jdbc-url jdbc:hive2:\\/\\/hiveserver:10000 --user hive --pass hive --partitioned-by partition --base-path <basePath> --database default --table <tableName>\n+```\n+\n+Starting with Hudi 0.5.1 version read optimized version of merge-on-read tables are suffixed '_ro' by default. For backwards compatibility with older Hudi versions, \n+an optional HiveSyncConfig - `--skip-ro-suffix`, has been provided to turn off '_ro' suffixing if desired. Explore other hive sync options using the following command:\n \n ```java\n cd hudi-hive\n ./run_sync_tool.sh\n  [hudi-hive]$ ./run_sync_tool.sh --help\n-Usage: <main class> [options]\n-  Options:\n-  * --base-path\n-       Basepath of Hudi dataset to sync\n-  * --database\n-       name of the target database in Hive\n-    --help, -h\n-       Default: false\n-  * --jdbc-url\n-       Hive jdbc connect url\n-  * --use-jdbc\n-       Whether to use jdbc connection or hive metastore (via thrift)\n-  * --pass\n-       Hive password\n-  * --table\n-       name of the target table in Hive\n-  * --user\n-       Hive username\n ```\n \n ## Deletes \n \n-Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation. \n+Hudi supports implementing two types of deletes on data stored in Hudi tables, by enabling the user to specify a different record payload implementation. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE4NzE4NQ==", "bodyText": "will do!", "url": "https://github.com/apache/hudi/pull/1260#discussion_r369187185", "createdAt": "2020-01-21T19:05:23Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -156,41 +157,31 @@ inputDF.write()\n \n ## Syncing to Hive\n \n-Both tools above support syncing of the dataset's latest schema to Hive metastore, such that queries can pick up new columns and partitions.\n+Both tools above support syncing of the table's latest schema to Hive metastore, such that queries can pick up new columns and partitions.\n In case, its preferable to run this from commandline or in an independent jvm, Hudi provides a `HiveSyncTool`, which can be invoked as below, \n-once you have built the hudi-hive module.\n+once you have built the hudi-hive module. Following is how we sync the above Datasource Writer written table to Hive metastore.\n+\n+```java\n+cd hudi-hive\n+./run_sync_tool.sh  --jdbc-url jdbc:hive2:\\/\\/hiveserver:10000 --user hive --pass hive --partitioned-by partition --base-path <basePath> --database default --table <tableName>\n+```\n+\n+Starting with Hudi 0.5.1 version read optimized version of merge-on-read tables are suffixed '_ro' by default. For backwards compatibility with older Hudi versions, \n+an optional HiveSyncConfig - `--skip-ro-suffix`, has been provided to turn off '_ro' suffixing if desired. Explore other hive sync options using the following command:\n \n ```java\n cd hudi-hive\n ./run_sync_tool.sh\n  [hudi-hive]$ ./run_sync_tool.sh --help\n-Usage: <main class> [options]\n-  Options:\n-  * --base-path\n-       Basepath of Hudi dataset to sync\n-  * --database\n-       name of the target database in Hive\n-    --help, -h\n-       Default: false\n-  * --jdbc-url\n-       Hive jdbc connect url\n-  * --use-jdbc\n-       Whether to use jdbc connection or hive metastore (via thrift)\n-  * --pass\n-       Hive password\n-  * --table\n-       name of the target table in Hive\n-  * --user\n-       Hive username\n ```\n \n ## Deletes \n \n-Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation. \n+Hudi supports implementing two types of deletes on data stored in Hudi tables, by enabling the user to specify a different record payload implementation. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4NDIxNw=="}, "originalCommit": null, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3OTE3NTAyOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_3_querying_data.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwMjoxNjowNVrOFfs0FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxODozMzo0NVrOFgEceQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4NDQwNA==", "bodyText": "Incremental Query instead of Incremental Pull?  (again Query instead of Querying)", "url": "https://github.com/apache/hudi/pull/1260#discussion_r368784404", "createdAt": "2020-01-21T02:16:05Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -1,47 +1,52 @@\n ---\n-title: Querying Hudi Datasets\n+title: Querying Hudi Tables\n keywords: hudi, hive, spark, sql, presto\n permalink: /docs/querying_data.html\n summary: In this page, we go over how to enable SQL queries on Hudi built tables.\n toc: true\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Conceptually, Hudi stores data physically once on DFS, while providing 3 logical views on top, as explained [before](/docs/concepts.html#views). \n-Once the dataset is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n-bundle has been provided, the dataset can be queried by popular query engines like Hive, Spark and Presto.\n+Conceptually, Hudi stores data physically once on DFS, while providing 3 different ways of querying, as explained [before](/docs/concepts.html#query-types). \n+Once the table is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n+bundle has been provided, the table can be queried by popular query engines like Hive, Spark and Presto.\n \n-Specifically, there are two Hive tables named off [table name](/docs/configurations.html#TABLE_NAME_OPT_KEY) passed during write. \n-For e.g, if `table name = hudi_tbl`, then we get  \n+Specifically, following Hive tables are registered based off [table name](/docs/configurations.html#TABLE_NAME_OPT_KEY) \n+and [table type](/docs/configurations.html#TABLE_TYPE_OPT_KEY) passed during write.   \n \n- - `hudi_tbl` realizes the read optimized view of the dataset backed by `HoodieParquetInputFormat`, exposing purely columnar data.\n- - `hudi_tbl_rt` realizes the real time view of the dataset  backed by `HoodieParquetRealtimeInputFormat`, exposing merged view of base and log data.\n+If `table name = hudi_trips` and `table type = COPY_ON_WRITE`, then we get: \n+ - `hudi_trips` supports snapshot querying and incremental querying of the table backed by `HoodieParquetInputFormat`, exposing purely columnar data.\n+\n+\n+If `table name = hudi_trips` and `table type = MERGE_ON_READ`, then we get:\n+ - `hudi_trips_rt` supports snapshot querying and incremental querying (providing near-real time data) of the table  backed by `HoodieParquetRealtimeInputFormat`, exposing merged view of base and log data.\n+ - `hudi_trips_ro` supports read optimized querying of the table backed by `HoodieParquetInputFormat`, exposing purely columnar data.\n+ \n \n As discussed in the concepts section, the one key primitive needed for [incrementally processing](https://www.oreilly.com/ideas/ubers-case-for-incremental-processing-on-hadoop),\n-is `incremental pulls` (to obtain a change stream/log from a dataset). Hudi datasets can be pulled incrementally, which means you can get ALL and ONLY the updated & new rows \n+is `incremental pulls` (to obtain a change stream/log from a table). Hudi tables can be pulled incrementally, which means you can get ALL and ONLY the updated & new rows \n since a specified instant time. This, together with upserts, are particularly useful for building data pipelines where 1 or more source Hudi tables are incrementally pulled (streams/facts),\n-joined with other tables (datasets/dimensions), to [write out deltas](/docs/writing_data.html) to a target Hudi dataset. Incremental view is realized by querying one of the tables above, \n-with special configurations that indicates to query planning that only incremental data needs to be fetched out of the dataset. \n+joined with other tables (tables/dimensions), to [write out deltas](/docs/writing_data.html) to a target Hudi table. Incremental view is realized by querying one of the tables above, \n+with special configurations that indicates to query planning that only incremental data needs to be fetched out of the table. \n \n-In sections, below we will discuss in detail how to access all the 3 views on each query engine.\n+In sections, below we will discuss how to access these query types from different query engines.\n \n ## Hive\n \n-In order for Hive to recognize Hudi datasets and query correctly, the HiveServer2 needs to be provided with the `hudi-hadoop-mr-bundle-x.y.z-SNAPSHOT.jar` \n+In order for Hive to recognize Hudi tables and query correctly, the HiveServer2 needs to be provided with the `hudi-hadoop-mr-bundle-x.y.z-SNAPSHOT.jar` \n in its [aux jars path](https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html#concept_nc3_mms_lr). This will ensure the input format \n classes with its dependencies are available for query planning & execution. \n \n-### Read Optimized table\n+### Read optimized querying\n In addition to setup above, for beeline cli access, the `hive.input.format` variable needs to be set to the  fully qualified path name of the \n inputformat `org.apache.hudi.hadoop.HoodieParquetInputFormat`. For Tez, additionally the `hive.tez.input.format` needs to be set \n to `org.apache.hadoop.hive.ql.io.HiveInputFormat`\n \n-### Real time table\n+### Snapshot querying\n In addition to installing the hive bundle jar on the HiveServer2, it needs to be put on the hadoop/hive installation across the cluster, so that\n queries can pick up the custom RecordReader as well.\n \n-### Incremental Pulling\n-\n+### Incremental pulling", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE3MTU3Nw==", "bodyText": "done!", "url": "https://github.com/apache/hudi/pull/1260#discussion_r369171577", "createdAt": "2020-01-21T18:33:45Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -1,47 +1,52 @@\n ---\n-title: Querying Hudi Datasets\n+title: Querying Hudi Tables\n keywords: hudi, hive, spark, sql, presto\n permalink: /docs/querying_data.html\n summary: In this page, we go over how to enable SQL queries on Hudi built tables.\n toc: true\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Conceptually, Hudi stores data physically once on DFS, while providing 3 logical views on top, as explained [before](/docs/concepts.html#views). \n-Once the dataset is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n-bundle has been provided, the dataset can be queried by popular query engines like Hive, Spark and Presto.\n+Conceptually, Hudi stores data physically once on DFS, while providing 3 different ways of querying, as explained [before](/docs/concepts.html#query-types). \n+Once the table is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n+bundle has been provided, the table can be queried by popular query engines like Hive, Spark and Presto.\n \n-Specifically, there are two Hive tables named off [table name](/docs/configurations.html#TABLE_NAME_OPT_KEY) passed during write. \n-For e.g, if `table name = hudi_tbl`, then we get  \n+Specifically, following Hive tables are registered based off [table name](/docs/configurations.html#TABLE_NAME_OPT_KEY) \n+and [table type](/docs/configurations.html#TABLE_TYPE_OPT_KEY) passed during write.   \n \n- - `hudi_tbl` realizes the read optimized view of the dataset backed by `HoodieParquetInputFormat`, exposing purely columnar data.\n- - `hudi_tbl_rt` realizes the real time view of the dataset  backed by `HoodieParquetRealtimeInputFormat`, exposing merged view of base and log data.\n+If `table name = hudi_trips` and `table type = COPY_ON_WRITE`, then we get: \n+ - `hudi_trips` supports snapshot querying and incremental querying of the table backed by `HoodieParquetInputFormat`, exposing purely columnar data.\n+\n+\n+If `table name = hudi_trips` and `table type = MERGE_ON_READ`, then we get:\n+ - `hudi_trips_rt` supports snapshot querying and incremental querying (providing near-real time data) of the table  backed by `HoodieParquetRealtimeInputFormat`, exposing merged view of base and log data.\n+ - `hudi_trips_ro` supports read optimized querying of the table backed by `HoodieParquetInputFormat`, exposing purely columnar data.\n+ \n \n As discussed in the concepts section, the one key primitive needed for [incrementally processing](https://www.oreilly.com/ideas/ubers-case-for-incremental-processing-on-hadoop),\n-is `incremental pulls` (to obtain a change stream/log from a dataset). Hudi datasets can be pulled incrementally, which means you can get ALL and ONLY the updated & new rows \n+is `incremental pulls` (to obtain a change stream/log from a table). Hudi tables can be pulled incrementally, which means you can get ALL and ONLY the updated & new rows \n since a specified instant time. This, together with upserts, are particularly useful for building data pipelines where 1 or more source Hudi tables are incrementally pulled (streams/facts),\n-joined with other tables (datasets/dimensions), to [write out deltas](/docs/writing_data.html) to a target Hudi dataset. Incremental view is realized by querying one of the tables above, \n-with special configurations that indicates to query planning that only incremental data needs to be fetched out of the dataset. \n+joined with other tables (tables/dimensions), to [write out deltas](/docs/writing_data.html) to a target Hudi table. Incremental view is realized by querying one of the tables above, \n+with special configurations that indicates to query planning that only incremental data needs to be fetched out of the table. \n \n-In sections, below we will discuss in detail how to access all the 3 views on each query engine.\n+In sections, below we will discuss how to access these query types from different query engines.\n \n ## Hive\n \n-In order for Hive to recognize Hudi datasets and query correctly, the HiveServer2 needs to be provided with the `hudi-hadoop-mr-bundle-x.y.z-SNAPSHOT.jar` \n+In order for Hive to recognize Hudi tables and query correctly, the HiveServer2 needs to be provided with the `hudi-hadoop-mr-bundle-x.y.z-SNAPSHOT.jar` \n in its [aux jars path](https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html#concept_nc3_mms_lr). This will ensure the input format \n classes with its dependencies are available for query planning & execution. \n \n-### Read Optimized table\n+### Read optimized querying\n In addition to setup above, for beeline cli access, the `hive.input.format` variable needs to be set to the  fully qualified path name of the \n inputformat `org.apache.hudi.hadoop.HoodieParquetInputFormat`. For Tez, additionally the `hive.tez.input.format` needs to be set \n to `org.apache.hadoop.hive.ql.io.HiveInputFormat`\n \n-### Real time table\n+### Snapshot querying\n In addition to installing the hive bundle jar on the HiveServer2, it needs to be put on the hadoop/hive installation across the cluster, so that\n queries can pick up the custom RecordReader as well.\n \n-### Incremental Pulling\n-\n+### Incremental pulling", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4NDQwNA=="}, "originalCommit": null, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3OTM5NzA3OnYy", "diffSide": "RIGHT", "path": "docs/_docs/1_3_use_cases.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQwNTozODo1MVrOFfu6cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxODozNDozM1rOFgEd7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODgxODgwMw==", "bodyText": "Good catch, https://kafka.apache.org is better.", "url": "https://github.com/apache/hudi/pull/1260#discussion_r368818803", "createdAt": "2020-01-21T05:38:51Z", "author": {"login": "lamberken"}, "path": "docs/_docs/1_3_use_cases.md", "diffHunk": "@@ -20,7 +20,7 @@ or [complicated handcrafted merge workflows](http://hortonworks.com/blog/four-st\n For NoSQL datastores like [Cassandra](http://cassandra.apache.org/) / [Voldemort](http://www.project-voldemort.com/voldemort/) / [HBase](https://hbase.apache.org/), even moderately big installations store billions of rows.\n It goes without saying that __full bulk loads are simply infeasible__ and more efficient approaches are needed if ingestion is to keep up with the typically high update volumes.\n \n-Even for immutable data sources like [Kafka](kafka.apache.org) , Hudi helps __enforces a minimum file size on HDFS__, which improves NameNode health by solving one of the [age old problems in Hadoop land](https://blog.cloudera.com/blog/2009/02/the-small-files-problem/) in a holistic way. This is all the more important for event streams, since typically its higher volume (eg: click streams) and if not managed well, can cause serious damage to your Hadoop cluster.\n+Even for immutable data sources like [Kafka](http://kafka.apache.org) , Hudi helps __enforces a minimum file size on HDFS__, which improves NameNode health by solving one of the [age old problems in Hadoop land](https://blog.cloudera.com/blog/2009/02/the-small-files-problem/) in a holistic way. This is all the more important for event streams, since typically its higher volume (eg: click streams) and if not managed well, can cause serious damage to your Hadoop cluster.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE3MTk1MQ==", "bodyText": "sure", "url": "https://github.com/apache/hudi/pull/1260#discussion_r369171951", "createdAt": "2020-01-21T18:34:33Z", "author": {"login": "bhasudha"}, "path": "docs/_docs/1_3_use_cases.md", "diffHunk": "@@ -20,7 +20,7 @@ or [complicated handcrafted merge workflows](http://hortonworks.com/blog/four-st\n For NoSQL datastores like [Cassandra](http://cassandra.apache.org/) / [Voldemort](http://www.project-voldemort.com/voldemort/) / [HBase](https://hbase.apache.org/), even moderately big installations store billions of rows.\n It goes without saying that __full bulk loads are simply infeasible__ and more efficient approaches are needed if ingestion is to keep up with the typically high update volumes.\n \n-Even for immutable data sources like [Kafka](kafka.apache.org) , Hudi helps __enforces a minimum file size on HDFS__, which improves NameNode health by solving one of the [age old problems in Hadoop land](https://blog.cloudera.com/blog/2009/02/the-small-files-problem/) in a holistic way. This is all the more important for event streams, since typically its higher volume (eg: click streams) and if not managed well, can cause serious damage to your Hadoop cluster.\n+Even for immutable data sources like [Kafka](http://kafka.apache.org) , Hudi helps __enforces a minimum file size on HDFS__, which improves NameNode health by solving one of the [age old problems in Hadoop land](https://blog.cloudera.com/blog/2009/02/the-small-files-problem/) in a holistic way. This is all the more important for event streams, since typically its higher volume (eg: click streams) and if not managed well, can cause serious damage to your Hadoop cluster.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODgxODgwMw=="}, "originalCommit": null, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4992, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}