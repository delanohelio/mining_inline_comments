{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ4ODM0MzIz", "number": 1827, "reviewThreads": {"totalCount": 59, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxMDo1ODoxNFrOEPFExQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QxODoxODoxMVrOEn3Bgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0MjQ3MjM3OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxMDo1ODoxNFrOGyl2vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNjoxNDoyNFrOGzGpGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwMjIwNg==", "bodyText": "The new style or the old style, which one is right?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r455702206", "createdAt": "2020-07-16T10:58:14Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java", "diffHunk": "@@ -593,8 +592,8 @@ public String repairCompaction(\n     return output;\n   }\n \n-  private String getRenamesToBePrinted(List<RenameOpResult> res, Integer limit, String sortByField, boolean descending,\n-      boolean headerOnly, String operation) {\n+  private String getRenamesToBePrinted(List<BaseCompactionAdminClient.RenameOpResult> res, Integer limit, String sortByField, boolean descending,\n+                                       boolean headerOnly, String operation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13795f5c1223424d44244e46acb0864b93ec403e"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzOTM4NQ==", "bodyText": "Hi, @yanghua thanks for your review. I am not sure which one is right either, I will roll back these style issues just to keep as same as before.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r456239385", "createdAt": "2020-07-17T06:14:24Z", "author": {"login": "wangxianghu"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/CompactionCommand.java", "diffHunk": "@@ -593,8 +592,8 @@ public String repairCompaction(\n     return output;\n   }\n \n-  private String getRenamesToBePrinted(List<RenameOpResult> res, Integer limit, String sortByField, boolean descending,\n-      boolean headerOnly, String operation) {\n+  private String getRenamesToBePrinted(List<BaseCompactionAdminClient.RenameOpResult> res, Integer limit, String sortByField, boolean descending,\n+                                       boolean headerOnly, String operation) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwMjIwNg=="}, "originalCommit": {"oid": "13795f5c1223424d44244e46acb0864b93ec403e"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0MjQ5MTU0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxMTowNDoxMVrOGymCqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QwNjozODo1NVrOGzHJqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwNTI1OA==", "bodyText": "Why do we need to change this class?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r455705258", "createdAt": "2020-07-16T11:04:11Z", "author": {"login": "yanghua"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java", "diffHunk": "@@ -40,7 +39,7 @@\n  * HoodieParquetWriter extends the ParquetWriter to help limit the size of underlying file. Provides a way to check if\n  * the current file can take more records with the <code>canWrite()</code>\n  */\n-public class HoodieParquetWriter<T extends HoodieRecordPayload, R extends IndexedRecord>\n+public class HoodieParquetWriter<R extends IndexedRecord>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "13795f5c1223424d44244e46acb0864b93ec403e"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjI0NzcyMQ==", "bodyText": "Why do we need to change this class?\n\nThe Generic \"T\" is useless in this class, and it causes some generic problems in the abstraction, so I removed it.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r456247721", "createdAt": "2020-07-17T06:38:55Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java", "diffHunk": "@@ -40,7 +39,7 @@\n  * HoodieParquetWriter extends the ParquetWriter to help limit the size of underlying file. Provides a way to check if\n  * the current file can take more records with the <code>canWrite()</code>\n  */\n-public class HoodieParquetWriter<T extends HoodieRecordPayload, R extends IndexedRecord>\n+public class HoodieParquetWriter<R extends IndexedRecord>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTcwNTI1OA=="}, "originalCommit": {"oid": "13795f5c1223424d44244e46acb0864b93ec403e"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MTE3ODcxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwMTowNDo0MlrOGzz5Dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1MTowMVrOHNq72w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk4MDc1MQ==", "bodyText": "Just bump into this... Since this is a generic engine context, will it be better to use a generic name like engineConfig?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r456980751", "createdAt": "2020-07-20T01:04:42Z", "author": {"login": "henrywu2019"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88c6661e865945059f782880c395547c16ef1a1c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEyNzAwNA==", "bodyText": "Just bump into this... Since this is a generic engine context, will it be better to use a generic name like engineConfig?\n\nHi, henry thanks for your review. This class holds more than config stuff(your can see its child class HoodieSparkEngineContext),  maybe context is better, WDYT\uff1f", "url": "https://github.com/apache/hudi/pull/1827#discussion_r457127004", "createdAt": "2020-07-20T07:24:12Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk4MDc1MQ=="}, "originalCommit": {"oid": "88c6661e865945059f782880c395547c16ef1a1c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEyODk4OA==", "bodyText": "", "url": "https://github.com/apache/hudi/pull/1827#discussion_r457128988", "createdAt": "2020-07-20T07:27:11Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk4MDc1MQ=="}, "originalCommit": {"oid": "88c6661e865945059f782880c395547c16ef1a1c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc5ODQyNg==", "bodyText": "Oh...What I meant is at line 32 the name hadoopConf, not the class name, which implies hadoop. I bumped into this searching for Flink support from HUDI and this PR looks a big step moving in that direction. Thanks tons @Mathieu1124 and definitely @vinothchandar as well.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r457798426", "createdAt": "2020-07-21T02:26:47Z", "author": {"login": "henrywu2019"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk4MDc1MQ=="}, "originalCommit": {"oid": "88c6661e865945059f782880c395547c16ef1a1c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5Njk4Nw==", "bodyText": "I am okay leaving it as hadoopConf given that's what we wrap. leave it you both :)", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484096987", "createdAt": "2020-09-06T17:51:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njk4MDc1MQ=="}, "originalCommit": {"oid": "88c6661e865945059f782880c395547c16ef1a1c"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMTc3NjkzOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMjo0MToxOFrOHM5vfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMjo0MToxOFrOHM5vfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI5MTAwNw==", "bodyText": "nit: extra line.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r483291007", "createdAt": "2020-09-03T22:41:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java", "diffHunk": "@@ -92,8 +92,9 @@ public void init() throws IOException {\n     metaClient.getActiveTimeline().reload().getAllCommitsTimeline().filterCompletedInstants();\n \n     // archive\n-    HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, hadoopConf);\n-    archiveLog.archiveIfRequired(jsc);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3901bcbcc191319a8cd7394f0f058b6c5e28d566"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMTc4MTA3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMjo0MzoyNlrOHM5x9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMDoyODo0NFrOHOWwpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI5MTYzNg==", "bodyText": "surprised that this has so few dependencies.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r483291636", "createdAt": "2020-09-03T22:43:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/pom.xml", "diffHunk": "@@ -0,0 +1,44 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <parent>\n+    <artifactId>hudi-client</artifactId>\n+    <groupId>org.apache.hudi</groupId>\n+    <version>0.6.1-SNAPSHOT</version>\n+  </parent>\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <artifactId>hudi-client-common</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3901bcbcc191319a8cd7394f0f058b6c5e28d566"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgxNTAxNQ==", "bodyText": "have moved dependencies from hudi-client to hudi-client-common", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484815015", "createdAt": "2020-09-08T10:28:44Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/pom.xml", "diffHunk": "@@ -0,0 +1,44 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <parent>\n+    <artifactId>hudi-client</artifactId>\n+    <groupId>org.apache.hudi</groupId>\n+    <version>0.6.1-SNAPSHOT</version>\n+  </parent>\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <artifactId>hudi-client-common</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI5MTYzNg=="}, "originalCommit": {"oid": "3901bcbcc191319a8cd7394f0f058b6c5e28d566"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMTc4MTUwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/asyc/AbstractAsyncService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMjo0MzozOVrOHM5yOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMDoyODo1M1rOHOWxAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI5MTcwNQ==", "bodyText": "typo: async", "url": "https://github.com/apache/hudi/pull/1827#discussion_r483291705", "createdAt": "2020-09-03T22:43:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/asyc/AbstractAsyncService.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi.async;\n+package org.apache.hudi.asyc;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3901bcbcc191319a8cd7394f0f058b6c5e28d566"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgxNTEwNA==", "bodyText": "done", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484815104", "createdAt": "2020-09-08T10:28:53Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/asyc/AbstractAsyncService.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi.async;\n+package org.apache.hudi.asyc;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI5MTcwNQ=="}, "originalCommit": {"oid": "3901bcbcc191319a8cd7394f0f058b6c5e28d566"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzMyMDQyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNToyNTo0NFrOHNqE-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNToyNTo0NFrOHNqE-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4MjkzNg==", "bodyText": "not sure if this is right. index must be not be needed at the the write client level.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484082936", "createdAt": "2020-09-06T15:25:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -18,120 +18,195 @@\n \n package org.apache.hudi.client;\n \n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hudi.avro.model.HoodieCleanMetadata;\n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n-import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.callback.HoodieWriteCommitCallback;\n+import org.apache.hudi.callback.common.HoodieWriteCommitCallbackMessage;\n+import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.model.WriteOperationType;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.config.HoodieCompactionConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n+\n import org.apache.hudi.exception.HoodieCommitException;\n import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.exception.HoodieRestoreException;\n import org.apache.hudi.exception.HoodieRollbackException;\n import org.apache.hudi.exception.HoodieSavepointException;\n import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.metrics.HoodieMetrics;\n-import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.table.HoodieTimelineArchiveLog;\n-import org.apache.hudi.table.MarkerFiles;\n import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.table.action.HoodieWriteMetadata;\n-import org.apache.hudi.table.action.compact.CompactHelpers;\n import org.apache.hudi.table.action.savepoint.SavepointHelpers;\n-\n-import com.codahale.metrics.Timer;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.SparkConf;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.text.ParseException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n /**\n- * Hoodie Write Client helps you build tables on HDFS [insert()] and then perform efficient mutations on an HDFS\n- * table [upsert()]\n- * <p>\n- * Note that, at any given time, there can only be one Spark job performing these operations on a Hoodie table.\n+ * Abstract Write Client providing functionality for performing commit, index updates and rollback\n+ * Reused for regular write operations like upsert/insert/bulk-insert.. as well as bootstrap\n+ *\n+ * @param <T> Sub type of HoodieRecordPayload\n+ * @param <I> Type of inputs\n+ * @param <K> Type of keys\n+ * @param <O> Type of outputs\n+ * @param <P> Type of record position [Key, Option[partitionPath, fileID]] in hoodie table\n  */\n-public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T> {\n-\n+public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload, I, K, O, P> extends AbstractHoodieClient {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LogManager.getLogger(HoodieWriteClient.class);\n-  private static final String LOOKUP_STR = \"lookup\";\n-  private final boolean rollbackPending;\n-  private final transient HoodieMetrics metrics;\n-  private transient Timer.Context compactionTimer;\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieWriteClient.class);\n+\n+  protected final transient HoodieMetrics metrics;\n+  private final transient HoodieIndex<T, I, K, O, P> index;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28eecab55cb62bb602e6ed7fe1cb9d5b188a87df"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzMyMTQzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNToyNzowMVrOHNqFfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNToyNzowMVrOHNqFfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4MzA3MQ==", "bodyText": "can we move all the static members to the top, like how it was before.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484083071", "createdAt": "2020-09-06T15:27:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -18,120 +18,195 @@\n \n package org.apache.hudi.client;\n \n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hudi.avro.model.HoodieCleanMetadata;\n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n-import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.callback.HoodieWriteCommitCallback;\n+import org.apache.hudi.callback.common.HoodieWriteCommitCallbackMessage;\n+import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.model.WriteOperationType;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.config.HoodieCompactionConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n+\n import org.apache.hudi.exception.HoodieCommitException;\n import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.exception.HoodieRestoreException;\n import org.apache.hudi.exception.HoodieRollbackException;\n import org.apache.hudi.exception.HoodieSavepointException;\n import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.metrics.HoodieMetrics;\n-import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.table.HoodieTimelineArchiveLog;\n-import org.apache.hudi.table.MarkerFiles;\n import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.table.action.HoodieWriteMetadata;\n-import org.apache.hudi.table.action.compact.CompactHelpers;\n import org.apache.hudi.table.action.savepoint.SavepointHelpers;\n-\n-import com.codahale.metrics.Timer;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.SparkConf;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.text.ParseException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n /**\n- * Hoodie Write Client helps you build tables on HDFS [insert()] and then perform efficient mutations on an HDFS\n- * table [upsert()]\n- * <p>\n- * Note that, at any given time, there can only be one Spark job performing these operations on a Hoodie table.\n+ * Abstract Write Client providing functionality for performing commit, index updates and rollback\n+ * Reused for regular write operations like upsert/insert/bulk-insert.. as well as bootstrap\n+ *\n+ * @param <T> Sub type of HoodieRecordPayload\n+ * @param <I> Type of inputs\n+ * @param <K> Type of keys\n+ * @param <O> Type of outputs\n+ * @param <P> Type of record position [Key, Option[partitionPath, fileID]] in hoodie table\n  */\n-public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T> {\n-\n+public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload, I, K, O, P> extends AbstractHoodieClient {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LogManager.getLogger(HoodieWriteClient.class);\n-  private static final String LOOKUP_STR = \"lookup\";\n-  private final boolean rollbackPending;\n-  private final transient HoodieMetrics metrics;\n-  private transient Timer.Context compactionTimer;\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieWriteClient.class);\n+\n+  protected final transient HoodieMetrics metrics;\n+  private final transient HoodieIndex<T, I, K, O, P> index;\n+\n+  protected transient Timer.Context writeContext = null;\n+  private transient WriteOperationType operationType;\n+  private transient HoodieWriteCommitCallback commitCallback;\n+\n+  protected static final String LOOKUP_STR = \"lookup\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28eecab55cb62bb602e6ed7fe1cb9d5b188a87df"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzMyMjUzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNToyODo0MFrOHNqGEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNDo1OToxMFrOHX1AAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4MzIxNg==", "bodyText": "why did this constructor have to change", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484083216", "createdAt": "2020-09-06T15:28:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -18,120 +18,195 @@\n \n package org.apache.hudi.client;\n \n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hudi.avro.model.HoodieCleanMetadata;\n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n-import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.callback.HoodieWriteCommitCallback;\n+import org.apache.hudi.callback.common.HoodieWriteCommitCallbackMessage;\n+import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.model.WriteOperationType;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.config.HoodieCompactionConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n+\n import org.apache.hudi.exception.HoodieCommitException;\n import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.exception.HoodieRestoreException;\n import org.apache.hudi.exception.HoodieRollbackException;\n import org.apache.hudi.exception.HoodieSavepointException;\n import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.metrics.HoodieMetrics;\n-import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.table.HoodieTimelineArchiveLog;\n-import org.apache.hudi.table.MarkerFiles;\n import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.table.action.HoodieWriteMetadata;\n-import org.apache.hudi.table.action.compact.CompactHelpers;\n import org.apache.hudi.table.action.savepoint.SavepointHelpers;\n-\n-import com.codahale.metrics.Timer;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.SparkConf;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.text.ParseException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n /**\n- * Hoodie Write Client helps you build tables on HDFS [insert()] and then perform efficient mutations on an HDFS\n- * table [upsert()]\n- * <p>\n- * Note that, at any given time, there can only be one Spark job performing these operations on a Hoodie table.\n+ * Abstract Write Client providing functionality for performing commit, index updates and rollback\n+ * Reused for regular write operations like upsert/insert/bulk-insert.. as well as bootstrap\n+ *\n+ * @param <T> Sub type of HoodieRecordPayload\n+ * @param <I> Type of inputs\n+ * @param <K> Type of keys\n+ * @param <O> Type of outputs\n+ * @param <P> Type of record position [Key, Option[partitionPath, fileID]] in hoodie table\n  */\n-public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T> {\n-\n+public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload, I, K, O, P> extends AbstractHoodieClient {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LogManager.getLogger(HoodieWriteClient.class);\n-  private static final String LOOKUP_STR = \"lookup\";\n-  private final boolean rollbackPending;\n-  private final transient HoodieMetrics metrics;\n-  private transient Timer.Context compactionTimer;\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieWriteClient.class);\n+\n+  protected final transient HoodieMetrics metrics;\n+  private final transient HoodieIndex<T, I, K, O, P> index;\n+\n+  protected transient Timer.Context writeContext = null;\n+  private transient WriteOperationType operationType;\n+  private transient HoodieWriteCommitCallback commitCallback;\n+\n+  protected static final String LOOKUP_STR = \"lookup\";\n+  protected final boolean rollbackPending;\n+  protected transient Timer.Context compactionTimer;\n   private transient AsyncCleanerService asyncCleanerService;\n \n+  public void setOperationType(WriteOperationType operationType) {\n+    this.operationType = operationType;\n+  }\n+\n+  public WriteOperationType getOperationType() {\n+    return this.operationType;\n+  }\n+\n   /**\n    * Create a write client, without cleaning up failed/inflight commits.\n    *\n-   * @param jsc Java Spark Context\n+   * @param context      Java Spark Context\n    * @param clientConfig instance of HoodieWriteConfig\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig) {\n-    this(jsc, clientConfig, false);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    this(context, clientConfig, false);\n   }\n \n   /**\n    * Create a write client, with new hudi index.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n-    this(jsc, writeConfig, rollbackPending, HoodieIndex.createIndex(writeConfig));\n-  }\n-\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending, HoodieIndex index) {\n-    this(jsc, writeConfig, rollbackPending, index, Option.empty());\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    this(context, writeConfig, rollbackPending, Option.empty());\n   }\n \n   /**\n-   *  Create a write client, allows to specify all parameters.\n+   * Create a write client, allows to specify all parameters.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    * @param timelineService Timeline Service that runs as part of write client.\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending,\n-      HoodieIndex index, Option<EmbeddedTimelineService> timelineService) {\n-    super(jsc, index, writeConfig, timelineService);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                   Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, timelineService);\n     this.metrics = new HoodieMetrics(config, config.getTableName());\n     this.rollbackPending = rollbackPending;\n+    this.index = createIndex(writeConfig);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28eecab55cb62bb602e6ed7fe1cb9d5b188a87df"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0NzY1MQ==", "bodyText": "Understood", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494747651", "createdAt": "2020-09-25T04:59:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -18,120 +18,195 @@\n \n package org.apache.hudi.client;\n \n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hudi.avro.model.HoodieCleanMetadata;\n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n-import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.callback.HoodieWriteCommitCallback;\n+import org.apache.hudi.callback.common.HoodieWriteCommitCallbackMessage;\n+import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.model.WriteOperationType;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.config.HoodieCompactionConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n+\n import org.apache.hudi.exception.HoodieCommitException;\n import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.exception.HoodieRestoreException;\n import org.apache.hudi.exception.HoodieRollbackException;\n import org.apache.hudi.exception.HoodieSavepointException;\n import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.metrics.HoodieMetrics;\n-import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.table.HoodieTimelineArchiveLog;\n-import org.apache.hudi.table.MarkerFiles;\n import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.table.action.HoodieWriteMetadata;\n-import org.apache.hudi.table.action.compact.CompactHelpers;\n import org.apache.hudi.table.action.savepoint.SavepointHelpers;\n-\n-import com.codahale.metrics.Timer;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.SparkConf;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.text.ParseException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n /**\n- * Hoodie Write Client helps you build tables on HDFS [insert()] and then perform efficient mutations on an HDFS\n- * table [upsert()]\n- * <p>\n- * Note that, at any given time, there can only be one Spark job performing these operations on a Hoodie table.\n+ * Abstract Write Client providing functionality for performing commit, index updates and rollback\n+ * Reused for regular write operations like upsert/insert/bulk-insert.. as well as bootstrap\n+ *\n+ * @param <T> Sub type of HoodieRecordPayload\n+ * @param <I> Type of inputs\n+ * @param <K> Type of keys\n+ * @param <O> Type of outputs\n+ * @param <P> Type of record position [Key, Option[partitionPath, fileID]] in hoodie table\n  */\n-public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T> {\n-\n+public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload, I, K, O, P> extends AbstractHoodieClient {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LogManager.getLogger(HoodieWriteClient.class);\n-  private static final String LOOKUP_STR = \"lookup\";\n-  private final boolean rollbackPending;\n-  private final transient HoodieMetrics metrics;\n-  private transient Timer.Context compactionTimer;\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieWriteClient.class);\n+\n+  protected final transient HoodieMetrics metrics;\n+  private final transient HoodieIndex<T, I, K, O, P> index;\n+\n+  protected transient Timer.Context writeContext = null;\n+  private transient WriteOperationType operationType;\n+  private transient HoodieWriteCommitCallback commitCallback;\n+\n+  protected static final String LOOKUP_STR = \"lookup\";\n+  protected final boolean rollbackPending;\n+  protected transient Timer.Context compactionTimer;\n   private transient AsyncCleanerService asyncCleanerService;\n \n+  public void setOperationType(WriteOperationType operationType) {\n+    this.operationType = operationType;\n+  }\n+\n+  public WriteOperationType getOperationType() {\n+    return this.operationType;\n+  }\n+\n   /**\n    * Create a write client, without cleaning up failed/inflight commits.\n    *\n-   * @param jsc Java Spark Context\n+   * @param context      Java Spark Context\n    * @param clientConfig instance of HoodieWriteConfig\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig) {\n-    this(jsc, clientConfig, false);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    this(context, clientConfig, false);\n   }\n \n   /**\n    * Create a write client, with new hudi index.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n-    this(jsc, writeConfig, rollbackPending, HoodieIndex.createIndex(writeConfig));\n-  }\n-\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending, HoodieIndex index) {\n-    this(jsc, writeConfig, rollbackPending, index, Option.empty());\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    this(context, writeConfig, rollbackPending, Option.empty());\n   }\n \n   /**\n-   *  Create a write client, allows to specify all parameters.\n+   * Create a write client, allows to specify all parameters.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    * @param timelineService Timeline Service that runs as part of write client.\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending,\n-      HoodieIndex index, Option<EmbeddedTimelineService> timelineService) {\n-    super(jsc, index, writeConfig, timelineService);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                   Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, timelineService);\n     this.metrics = new HoodieMetrics(config, config.getTableName());\n     this.rollbackPending = rollbackPending;\n+    this.index = createIndex(writeConfig);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4MzIxNg=="}, "originalCommit": {"oid": "28eecab55cb62bb602e6ed7fe1cb9d5b188a87df"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzMyMzA2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNToyOToxOVrOHNqGSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNToyOToxOVrOHNqGSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4MzI3NA==", "bodyText": "same point, not sure if this is correct.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484083274", "createdAt": "2020-09-06T15:29:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -18,120 +18,195 @@\n \n package org.apache.hudi.client;\n \n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hudi.avro.model.HoodieCleanMetadata;\n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n-import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.callback.HoodieWriteCommitCallback;\n+import org.apache.hudi.callback.common.HoodieWriteCommitCallbackMessage;\n+import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.model.WriteOperationType;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.config.HoodieCompactionConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n+\n import org.apache.hudi.exception.HoodieCommitException;\n import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.exception.HoodieRestoreException;\n import org.apache.hudi.exception.HoodieRollbackException;\n import org.apache.hudi.exception.HoodieSavepointException;\n import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.metrics.HoodieMetrics;\n-import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.table.HoodieTimelineArchiveLog;\n-import org.apache.hudi.table.MarkerFiles;\n import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.table.action.HoodieWriteMetadata;\n-import org.apache.hudi.table.action.compact.CompactHelpers;\n import org.apache.hudi.table.action.savepoint.SavepointHelpers;\n-\n-import com.codahale.metrics.Timer;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.SparkConf;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.text.ParseException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n /**\n- * Hoodie Write Client helps you build tables on HDFS [insert()] and then perform efficient mutations on an HDFS\n- * table [upsert()]\n- * <p>\n- * Note that, at any given time, there can only be one Spark job performing these operations on a Hoodie table.\n+ * Abstract Write Client providing functionality for performing commit, index updates and rollback\n+ * Reused for regular write operations like upsert/insert/bulk-insert.. as well as bootstrap\n+ *\n+ * @param <T> Sub type of HoodieRecordPayload\n+ * @param <I> Type of inputs\n+ * @param <K> Type of keys\n+ * @param <O> Type of outputs\n+ * @param <P> Type of record position [Key, Option[partitionPath, fileID]] in hoodie table\n  */\n-public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T> {\n-\n+public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload, I, K, O, P> extends AbstractHoodieClient {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LogManager.getLogger(HoodieWriteClient.class);\n-  private static final String LOOKUP_STR = \"lookup\";\n-  private final boolean rollbackPending;\n-  private final transient HoodieMetrics metrics;\n-  private transient Timer.Context compactionTimer;\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieWriteClient.class);\n+\n+  protected final transient HoodieMetrics metrics;\n+  private final transient HoodieIndex<T, I, K, O, P> index;\n+\n+  protected transient Timer.Context writeContext = null;\n+  private transient WriteOperationType operationType;\n+  private transient HoodieWriteCommitCallback commitCallback;\n+\n+  protected static final String LOOKUP_STR = \"lookup\";\n+  protected final boolean rollbackPending;\n+  protected transient Timer.Context compactionTimer;\n   private transient AsyncCleanerService asyncCleanerService;\n \n+  public void setOperationType(WriteOperationType operationType) {\n+    this.operationType = operationType;\n+  }\n+\n+  public WriteOperationType getOperationType() {\n+    return this.operationType;\n+  }\n+\n   /**\n    * Create a write client, without cleaning up failed/inflight commits.\n    *\n-   * @param jsc Java Spark Context\n+   * @param context      Java Spark Context\n    * @param clientConfig instance of HoodieWriteConfig\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig) {\n-    this(jsc, clientConfig, false);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    this(context, clientConfig, false);\n   }\n \n   /**\n    * Create a write client, with new hudi index.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n-    this(jsc, writeConfig, rollbackPending, HoodieIndex.createIndex(writeConfig));\n-  }\n-\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending, HoodieIndex index) {\n-    this(jsc, writeConfig, rollbackPending, index, Option.empty());\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    this(context, writeConfig, rollbackPending, Option.empty());\n   }\n \n   /**\n-   *  Create a write client, allows to specify all parameters.\n+   * Create a write client, allows to specify all parameters.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    * @param timelineService Timeline Service that runs as part of write client.\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending,\n-      HoodieIndex index, Option<EmbeddedTimelineService> timelineService) {\n-    super(jsc, index, writeConfig, timelineService);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                   Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, timelineService);\n     this.metrics = new HoodieMetrics(config, config.getTableName());\n     this.rollbackPending = rollbackPending;\n+    this.index = createIndex(writeConfig);\n   }\n \n+  protected abstract HoodieIndex<T, I, K, O, P> createIndex(HoodieWriteConfig writeConfig);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28eecab55cb62bb602e6ed7fe1cb9d5b188a87df"}, "originalPosition": 162}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzMyNDQ4OnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNTozMDo1MFrOHNqG8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxMzowMDozNVrOHPGKQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4MzQ0Mg==", "bodyText": "guessing this is all moved to spark client now?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484083442", "createdAt": "2020-09-06T15:30:50Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -18,120 +18,195 @@\n \n package org.apache.hudi.client;\n \n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hudi.avro.model.HoodieCleanMetadata;\n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n-import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.callback.HoodieWriteCommitCallback;\n+import org.apache.hudi.callback.common.HoodieWriteCommitCallbackMessage;\n+import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.model.WriteOperationType;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.config.HoodieCompactionConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n+\n import org.apache.hudi.exception.HoodieCommitException;\n import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.exception.HoodieRestoreException;\n import org.apache.hudi.exception.HoodieRollbackException;\n import org.apache.hudi.exception.HoodieSavepointException;\n import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.metrics.HoodieMetrics;\n-import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.table.HoodieTimelineArchiveLog;\n-import org.apache.hudi.table.MarkerFiles;\n import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.table.action.HoodieWriteMetadata;\n-import org.apache.hudi.table.action.compact.CompactHelpers;\n import org.apache.hudi.table.action.savepoint.SavepointHelpers;\n-\n-import com.codahale.metrics.Timer;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.SparkConf;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.text.ParseException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n /**\n- * Hoodie Write Client helps you build tables on HDFS [insert()] and then perform efficient mutations on an HDFS\n- * table [upsert()]\n- * <p>\n- * Note that, at any given time, there can only be one Spark job performing these operations on a Hoodie table.\n+ * Abstract Write Client providing functionality for performing commit, index updates and rollback\n+ * Reused for regular write operations like upsert/insert/bulk-insert.. as well as bootstrap\n+ *\n+ * @param <T> Sub type of HoodieRecordPayload\n+ * @param <I> Type of inputs\n+ * @param <K> Type of keys\n+ * @param <O> Type of outputs\n+ * @param <P> Type of record position [Key, Option[partitionPath, fileID]] in hoodie table\n  */\n-public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T> {\n-\n+public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload, I, K, O, P> extends AbstractHoodieClient {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LogManager.getLogger(HoodieWriteClient.class);\n-  private static final String LOOKUP_STR = \"lookup\";\n-  private final boolean rollbackPending;\n-  private final transient HoodieMetrics metrics;\n-  private transient Timer.Context compactionTimer;\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieWriteClient.class);\n+\n+  protected final transient HoodieMetrics metrics;\n+  private final transient HoodieIndex<T, I, K, O, P> index;\n+\n+  protected transient Timer.Context writeContext = null;\n+  private transient WriteOperationType operationType;\n+  private transient HoodieWriteCommitCallback commitCallback;\n+\n+  protected static final String LOOKUP_STR = \"lookup\";\n+  protected final boolean rollbackPending;\n+  protected transient Timer.Context compactionTimer;\n   private transient AsyncCleanerService asyncCleanerService;\n \n+  public void setOperationType(WriteOperationType operationType) {\n+    this.operationType = operationType;\n+  }\n+\n+  public WriteOperationType getOperationType() {\n+    return this.operationType;\n+  }\n+\n   /**\n    * Create a write client, without cleaning up failed/inflight commits.\n    *\n-   * @param jsc Java Spark Context\n+   * @param context      Java Spark Context\n    * @param clientConfig instance of HoodieWriteConfig\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig) {\n-    this(jsc, clientConfig, false);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    this(context, clientConfig, false);\n   }\n \n   /**\n    * Create a write client, with new hudi index.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n-    this(jsc, writeConfig, rollbackPending, HoodieIndex.createIndex(writeConfig));\n-  }\n-\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending, HoodieIndex index) {\n-    this(jsc, writeConfig, rollbackPending, index, Option.empty());\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    this(context, writeConfig, rollbackPending, Option.empty());\n   }\n \n   /**\n-   *  Create a write client, allows to specify all parameters.\n+   * Create a write client, allows to specify all parameters.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    * @param timelineService Timeline Service that runs as part of write client.\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending,\n-      HoodieIndex index, Option<EmbeddedTimelineService> timelineService) {\n-    super(jsc, index, writeConfig, timelineService);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                   Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, timelineService);\n     this.metrics = new HoodieMetrics(config, config.getTableName());\n     this.rollbackPending = rollbackPending;\n+    this.index = createIndex(writeConfig);\n   }\n \n+  protected abstract HoodieIndex<T, I, K, O, P> createIndex(HoodieWriteConfig writeConfig);\n+\n   /**\n-   * Register hudi classes for Kryo serialization.\n-   *\n-   * @param conf instance of SparkConf\n-   * @return SparkConf\n+   * Commit changes performed at the given instantTime marker.\n    */\n-  public static SparkConf registerClasses(SparkConf conf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28eecab55cb62bb602e6ed7fe1cb9d5b188a87df"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTU5MTYxOA==", "bodyText": "guessing this is all moved to spark client now?\n\nyes, In SparkRDDWriteClient now", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485591618", "createdAt": "2020-09-09T13:00:35Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -18,120 +18,195 @@\n \n package org.apache.hudi.client;\n \n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hudi.avro.model.HoodieCleanMetadata;\n import org.apache.hudi.avro.model.HoodieCompactionPlan;\n import org.apache.hudi.avro.model.HoodieRestoreMetadata;\n import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n-import org.apache.hudi.client.embedded.EmbeddedTimelineService;\n+import org.apache.hudi.callback.HoodieWriteCommitCallback;\n+import org.apache.hudi.callback.common.HoodieWriteCommitCallbackMessage;\n+import org.apache.hudi.callback.util.HoodieCommitCallbackFactory;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.HoodieCommitMetadata;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.model.WriteOperationType;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.config.HoodieCompactionConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n+\n import org.apache.hudi.exception.HoodieCommitException;\n import org.apache.hudi.exception.HoodieIOException;\n import org.apache.hudi.exception.HoodieRestoreException;\n import org.apache.hudi.exception.HoodieRollbackException;\n import org.apache.hudi.exception.HoodieSavepointException;\n import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.metrics.HoodieMetrics;\n-import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.table.HoodieTimelineArchiveLog;\n-import org.apache.hudi.table.MarkerFiles;\n import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieTable;\n import org.apache.hudi.table.action.HoodieWriteMetadata;\n-import org.apache.hudi.table.action.compact.CompactHelpers;\n import org.apache.hudi.table.action.savepoint.SavepointHelpers;\n-\n-import com.codahale.metrics.Timer;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.SparkConf;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n import java.text.ParseException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n /**\n- * Hoodie Write Client helps you build tables on HDFS [insert()] and then perform efficient mutations on an HDFS\n- * table [upsert()]\n- * <p>\n- * Note that, at any given time, there can only be one Spark job performing these operations on a Hoodie table.\n+ * Abstract Write Client providing functionality for performing commit, index updates and rollback\n+ * Reused for regular write operations like upsert/insert/bulk-insert.. as well as bootstrap\n+ *\n+ * @param <T> Sub type of HoodieRecordPayload\n+ * @param <I> Type of inputs\n+ * @param <K> Type of keys\n+ * @param <O> Type of outputs\n+ * @param <P> Type of record position [Key, Option[partitionPath, fileID]] in hoodie table\n  */\n-public class HoodieWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T> {\n-\n+public abstract class AbstractHoodieWriteClient<T extends HoodieRecordPayload, I, K, O, P> extends AbstractHoodieClient {\n   private static final long serialVersionUID = 1L;\n-  private static final Logger LOG = LogManager.getLogger(HoodieWriteClient.class);\n-  private static final String LOOKUP_STR = \"lookup\";\n-  private final boolean rollbackPending;\n-  private final transient HoodieMetrics metrics;\n-  private transient Timer.Context compactionTimer;\n+  private static final Logger LOG = LogManager.getLogger(AbstractHoodieWriteClient.class);\n+\n+  protected final transient HoodieMetrics metrics;\n+  private final transient HoodieIndex<T, I, K, O, P> index;\n+\n+  protected transient Timer.Context writeContext = null;\n+  private transient WriteOperationType operationType;\n+  private transient HoodieWriteCommitCallback commitCallback;\n+\n+  protected static final String LOOKUP_STR = \"lookup\";\n+  protected final boolean rollbackPending;\n+  protected transient Timer.Context compactionTimer;\n   private transient AsyncCleanerService asyncCleanerService;\n \n+  public void setOperationType(WriteOperationType operationType) {\n+    this.operationType = operationType;\n+  }\n+\n+  public WriteOperationType getOperationType() {\n+    return this.operationType;\n+  }\n+\n   /**\n    * Create a write client, without cleaning up failed/inflight commits.\n    *\n-   * @param jsc Java Spark Context\n+   * @param context      Java Spark Context\n    * @param clientConfig instance of HoodieWriteConfig\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig) {\n-    this(jsc, clientConfig, false);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    this(context, clientConfig, false);\n   }\n \n   /**\n    * Create a write client, with new hudi index.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n-    this(jsc, writeConfig, rollbackPending, HoodieIndex.createIndex(writeConfig));\n-  }\n-\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending, HoodieIndex index) {\n-    this(jsc, writeConfig, rollbackPending, index, Option.empty());\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    this(context, writeConfig, rollbackPending, Option.empty());\n   }\n \n   /**\n-   *  Create a write client, allows to specify all parameters.\n+   * Create a write client, allows to specify all parameters.\n    *\n-   * @param jsc Java Spark Context\n-   * @param writeConfig instance of HoodieWriteConfig\n+   * @param context         HoodieEngineContext\n+   * @param writeConfig     instance of HoodieWriteConfig\n    * @param rollbackPending whether need to cleanup pending commits\n    * @param timelineService Timeline Service that runs as part of write client.\n    */\n-  public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig writeConfig, boolean rollbackPending,\n-      HoodieIndex index, Option<EmbeddedTimelineService> timelineService) {\n-    super(jsc, index, writeConfig, timelineService);\n+  public AbstractHoodieWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending,\n+                                   Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, timelineService);\n     this.metrics = new HoodieMetrics(config, config.getTableName());\n     this.rollbackPending = rollbackPending;\n+    this.index = createIndex(writeConfig);\n   }\n \n+  protected abstract HoodieIndex<T, I, K, O, P> createIndex(HoodieWriteConfig writeConfig);\n+\n   /**\n-   * Register hudi classes for Kryo serialization.\n-   *\n-   * @param conf instance of SparkConf\n-   * @return SparkConf\n+   * Commit changes performed at the given instantTime marker.\n    */\n-  public static SparkConf registerClasses(SparkConf conf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4MzQ0Mg=="}, "originalCommit": {"oid": "28eecab55cb62bb602e6ed7fe1cb9d5b188a87df"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzMzMTI3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNTozOToxN1rOHNqKRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNTozOToxN1rOHNqKRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4NDI5NQ==", "bodyText": "it would be great, if you can avoid the whitespace changes :) Have to fish for what the real changes are", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484084295", "createdAt": "2020-09-06T15:39:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -242,150 +286,93 @@ protected void rollBackInflightBootstrap() {\n    * de-duped if needed.\n    *\n    * @param preppedRecords HoodieRecords to insert\n-   * @param instantTime Instant time of the commit\n+   * @param instantTime    Instant time of the commit\n    * @return JavaRDD[WriteStatus] - RDD of WriteStatus to inspect errors and counts\n    */\n-  public JavaRDD<WriteStatus> insertPreppedRecords(JavaRDD<HoodieRecord<T>> preppedRecords, final String instantTime) {\n-    HoodieTable<T> table = getTableAndInitCtx(WriteOperationType.INSERT_PREPPED, instantTime);\n-    table.validateInsertSchema();\n-    setOperationType(WriteOperationType.INSERT_PREPPED);\n-    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n-    HoodieWriteMetadata result = table.insertPrepped(jsc,instantTime, preppedRecords);\n-    return postWrite(result, instantTime, table);\n-  }\n+  public abstract O insertPreppedRecords(I preppedRecords, final String instantTime);\n \n   /**\n    * Loads the given HoodieRecords, as inserts into the table. This is suitable for doing big bulk loads into a Hoodie\n    * table for the very first time (e.g: converting an existing table to Hoodie).\n    * <p>\n    * This implementation uses sortBy (which does range partitioning based on reservoir sampling) and attempts to control\n-   * the numbers of files with less memory compared to the {@link HoodieWriteClient#insert(JavaRDD, String)}\n+   * the numbers of files with less memory compared to the {@link AbstractHoodieWriteClient#insert(I, String)}\n    *\n-   * @param records HoodieRecords to insert\n+   * @param records     HoodieRecords to insert\n    * @param instantTime Instant time of the commit\n    * @return JavaRDD[WriteStatus] - RDD of WriteStatus to inspect errors and counts\n    */\n-  public JavaRDD<WriteStatus> bulkInsert(JavaRDD<HoodieRecord<T>> records, final String instantTime) {\n-    return bulkInsert(records, instantTime, Option.empty());\n-  }\n+  public abstract O bulkInsert(I records, final String instantTime);\n \n   /**\n    * Loads the given HoodieRecords, as inserts into the table. This is suitable for doing big bulk loads into a Hoodie\n    * table for the very first time (e.g: converting an existing table to Hoodie).\n    * <p>\n    * This implementation uses sortBy (which does range partitioning based on reservoir sampling) and attempts to control\n-   * the numbers of files with less memory compared to the {@link HoodieWriteClient#insert(JavaRDD, String)}. Optionally\n+   * the numbers of files with less memory compared to the {@link AbstractHoodieWriteClient#insert(I, String)}. Optionally\n    * it allows users to specify their own partitioner. If specified then it will be used for repartitioning records. See\n    * {@link BulkInsertPartitioner}.\n    *\n-   * @param records HoodieRecords to insert\n-   * @param instantTime Instant time of the commit\n+   * @param records                          HoodieRecords to insert\n+   * @param instantTime                      Instant time of the commit\n    * @param userDefinedBulkInsertPartitioner If specified then it will be used to partition input records before they are inserted\n-   * into hoodie.\n+   *                                         into hoodie.\n    * @return JavaRDD[WriteStatus] - RDD of WriteStatus to inspect errors and counts\n    */\n-  public JavaRDD<WriteStatus> bulkInsert(JavaRDD<HoodieRecord<T>> records, final String instantTime,\n-                                         Option<BulkInsertPartitioner> userDefinedBulkInsertPartitioner) {\n-    HoodieTable<T> table = getTableAndInitCtx(WriteOperationType.BULK_INSERT, instantTime);\n-    table.validateInsertSchema();\n-    setOperationType(WriteOperationType.BULK_INSERT);\n-    this.asyncCleanerService = AsyncCleanerService.startAsyncCleaningIfEnabled(this, instantTime);\n-    HoodieWriteMetadata result = table.bulkInsert(jsc,instantTime, records, userDefinedBulkInsertPartitioner);\n-    return postWrite(result, instantTime, table);\n-  }\n+  public abstract O bulkInsert(I records, final String instantTime,\n+                      Option<BulkInsertPartitioner<I>> userDefinedBulkInsertPartitioner);\n+\n \n   /**\n    * Loads the given HoodieRecords, as inserts into the table. This is suitable for doing big bulk loads into a Hoodie\n    * table for the very first time (e.g: converting an existing table to Hoodie). The input records should contain no\n    * duplicates if needed.\n    * <p>\n    * This implementation uses sortBy (which does range partitioning based on reservoir sampling) and attempts to control\n-   * the numbers of files with less memory compared to the {@link HoodieWriteClient#insert(JavaRDD, String)}. Optionally\n+   * the numbers of files with less memory compared to the {@link AbstractHoodieWriteClient#insert(I, String)}. Optionally\n    * it allows users to specify their own partitioner. If specified then it will be used for repartitioning records. See\n    * {@link BulkInsertPartitioner}.\n    *\n-   * @param preppedRecords HoodieRecords to insert\n-   * @param instantTime Instant time of the commit\n+   * @param preppedRecords        HoodieRecords to insert", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "28eecab55cb62bb602e6ed7fe1cb9d5b188a87df"}, "originalPosition": 434}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzMzNTcwOnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AsyncCleanerService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNTo0NDoxOVrOHNqMiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMjoxODoxOFrOHXHWDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4NDg3Mw==", "bodyText": "this method need not have moved?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484084873", "createdAt": "2020-09-06T15:44:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AsyncCleanerService.java", "diffHunk": "@@ -52,19 +52,6 @@ protected AsyncCleanerService(HoodieWriteClient<?> writeClient, String cleanInst\n     }), executor);\n   }\n \n-  public static AsyncCleanerService startAsyncCleaningIfEnabled(HoodieWriteClient writeClient,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2348f73b4a270e4d04d2f67f9d7bd9691391b569"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5OTYyOQ==", "bodyText": "this method need not have moved?\n\nIt is back now.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r493999629", "createdAt": "2020-09-24T02:18:18Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AsyncCleanerService.java", "diffHunk": "@@ -52,19 +52,6 @@ protected AsyncCleanerService(HoodieWriteClient<?> writeClient, String cleanInst\n     }), executor);\n   }\n \n-  public static AsyncCleanerService startAsyncCleaningIfEnabled(HoodieWriteClient writeClient,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA4NDg3Mw=="}, "originalCommit": {"oid": "2348f73b4a270e4d04d2f67f9d7bd9691391b569"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQzNDY1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/bootstrap/selector/BootstrapRegexModeSelector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo0ODo1MVrOHNq7CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo0ODo1MVrOHNq7CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5Njc3Ng==", "bodyText": "are these from reformatting via IDE .", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484096776", "createdAt": "2020-09-06T17:48:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/bootstrap/selector/BootstrapRegexModeSelector.java", "diffHunk": "@@ -18,17 +18,18 @@\n \n package org.apache.hudi.client.bootstrap.selector;\n \n-import java.util.List;\n-import java.util.Map;\n-import java.util.regex.Pattern;\n-import java.util.stream.Collectors;\n import org.apache.hudi.avro.model.HoodieFileStatus;\n import org.apache.hudi.client.bootstrap.BootstrapMode;\n import org.apache.hudi.common.util.collection.Pair;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n \n+import java.util.List;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQzNzc2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/BaseLazyInsertIterable.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1Mjo1M1rOHNq8iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1Mjo1M1rOHNq8iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzE2MA==", "bodyText": "In general, not sure if this class is applicable outside of Spark. but we do use it in all of the code paths. So understand that we needed to do this.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484097160", "createdAt": "2020-09-06T17:52:53Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/execution/BaseLazyInsertIterable.java", "diffHunk": "@@ -18,64 +18,47 @@\n \n package org.apache.hudi.execution;\n \n-import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.client.TaskContextSupplier;\n import org.apache.hudi.client.WriteStatus;\n import org.apache.hudi.client.utils.LazyIterableIterator;\n import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.queue.BoundedInMemoryExecutor;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.exception.HoodieException;\n-import org.apache.hudi.io.CreateHandleFactory;\n import org.apache.hudi.io.WriteHandleFactory;\n import org.apache.hudi.table.HoodieTable;\n \n-import org.apache.avro.Schema;\n-import org.apache.avro.generic.IndexedRecord;\n-\n import java.util.Iterator;\n import java.util.List;\n import java.util.function.Function;\n \n /**\n  * Lazy Iterable, that writes a stream of HoodieRecords sorted by the partitionPath, into new files.\n  */\n-public class LazyInsertIterable<T extends HoodieRecordPayload>\n+public abstract class BaseLazyInsertIterable<T extends HoodieRecordPayload>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQzOTI1OnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1NToxMlrOHNq9Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1NToxMlrOHNq9Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzM0Mw==", "bodyText": "some of these index types don't make sense without Spark Index now. actually almost all of them except may be HBaseIndex.\nSo these should all be renamed with the Spark prefix", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484097343", "createdAt": "2020-09-06T17:55:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -21,94 +21,52 @@\n import org.apache.hudi.ApiMaturityLevel;\n import org.apache.hudi.PublicAPIClass;\n import org.apache.hudi.PublicAPIMethod;\n-import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.FileSlice;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.ReflectionUtils;\n-import org.apache.hudi.common.util.StringUtils;\n-import org.apache.hudi.common.util.collection.Pair;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.exception.HoodieIndexException;\n-import org.apache.hudi.index.bloom.HoodieBloomIndex;\n-import org.apache.hudi.index.bloom.HoodieGlobalBloomIndex;\n-import org.apache.hudi.index.hbase.HBaseIndex;\n-import org.apache.hudi.index.simple.HoodieGlobalSimpleIndex;\n-import org.apache.hudi.index.simple.HoodieSimpleIndex;\n import org.apache.hudi.table.HoodieTable;\n \n-import org.apache.spark.api.java.JavaPairRDD;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n-\n import java.io.Serializable;\n \n /**\n  * Base class for different types of indexes to determine the mapping from uuid.\n  */\n @PublicAPIClass(maturity = ApiMaturityLevel.EVOLVING)\n-public abstract class HoodieIndex<T extends HoodieRecordPayload> implements Serializable {\n+public abstract class HoodieIndex<T extends HoodieRecordPayload, I, K, O, P> implements Serializable {\n \n   protected final HoodieWriteConfig config;\n \n   protected HoodieIndex(HoodieWriteConfig config) {\n     this.config = config;\n   }\n \n-  public static <T extends HoodieRecordPayload> HoodieIndex<T> createIndex(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQzOTg1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1NjowMFrOHNq9iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1NjowMFrOHNq9iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzQxNg==", "bodyText": "now I understand P better.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484097416", "createdAt": "2020-09-06T17:56:00Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -21,94 +21,52 @@\n import org.apache.hudi.ApiMaturityLevel;\n import org.apache.hudi.PublicAPIClass;\n import org.apache.hudi.PublicAPIMethod;\n-import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.FileSlice;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.ReflectionUtils;\n-import org.apache.hudi.common.util.StringUtils;\n-import org.apache.hudi.common.util.collection.Pair;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.exception.HoodieIndexException;\n-import org.apache.hudi.index.bloom.HoodieBloomIndex;\n-import org.apache.hudi.index.bloom.HoodieGlobalBloomIndex;\n-import org.apache.hudi.index.hbase.HBaseIndex;\n-import org.apache.hudi.index.simple.HoodieGlobalSimpleIndex;\n-import org.apache.hudi.index.simple.HoodieSimpleIndex;\n import org.apache.hudi.table.HoodieTable;\n \n-import org.apache.spark.api.java.JavaPairRDD;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n-\n import java.io.Serializable;\n \n /**\n  * Base class for different types of indexes to determine the mapping from uuid.\n  */\n @PublicAPIClass(maturity = ApiMaturityLevel.EVOLVING)\n-public abstract class HoodieIndex<T extends HoodieRecordPayload> implements Serializable {\n+public abstract class HoodieIndex<T extends HoodieRecordPayload, I, K, O, P> implements Serializable {\n \n   protected final HoodieWriteConfig config;\n \n   protected HoodieIndex(HoodieWriteConfig config) {\n     this.config = config;\n   }\n \n-  public static <T extends HoodieRecordPayload> HoodieIndex<T> createIndex(\n-      HoodieWriteConfig config) throws HoodieIndexException {\n-    // first use index class config to create index.\n-    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n-      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n-      if (!(instance instanceof HoodieIndex)) {\n-        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n-      }\n-      return (HoodieIndex) instance;\n-    }\n-    switch (config.getIndexType()) {\n-      case HBASE:\n-        return new HBaseIndex<>(config);\n-      case INMEMORY:\n-        return new InMemoryHashIndex<>(config);\n-      case BLOOM:\n-        return new HoodieBloomIndex<>(config);\n-      case GLOBAL_BLOOM:\n-        return new HoodieGlobalBloomIndex<>(config);\n-      case SIMPLE:\n-        return new HoodieSimpleIndex<>(config);\n-      case GLOBAL_SIMPLE:\n-        return new HoodieGlobalSimpleIndex<>(config);\n-      default:\n-        throw new HoodieIndexException(\"Index type unspecified, set \" + config.getIndexType());\n-    }\n-  }\n-\n   /**\n    * Checks if the given [Keys] exists in the hoodie table and returns [Key, Option[partitionPath, fileID]] If the\n    * optional is empty, then the key is not found.\n    */\n   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)\n-  public abstract JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(\n-      JavaRDD<HoodieKey> hoodieKeys, final JavaSparkContext jsc, HoodieTable<T> hoodieTable);\n+  public abstract P fetchRecordLocation(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQ0MTQwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1ODowNFrOHNq-TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMDoyOToyNlrOHOWyKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzYxMw==", "bodyText": "these annotations needs to moved over to a SparkHoodieIndex class? it will be hard for end developers to program against HoodieIndex directly anymore. This is a general point actually. The current public APIs should all be annotated against the Spark child classes.  wdyt?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484097613", "createdAt": "2020-09-06T17:58:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -21,94 +21,52 @@\n import org.apache.hudi.ApiMaturityLevel;\n import org.apache.hudi.PublicAPIClass;\n import org.apache.hudi.PublicAPIMethod;\n-import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.FileSlice;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.ReflectionUtils;\n-import org.apache.hudi.common.util.StringUtils;\n-import org.apache.hudi.common.util.collection.Pair;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.exception.HoodieIndexException;\n-import org.apache.hudi.index.bloom.HoodieBloomIndex;\n-import org.apache.hudi.index.bloom.HoodieGlobalBloomIndex;\n-import org.apache.hudi.index.hbase.HBaseIndex;\n-import org.apache.hudi.index.simple.HoodieGlobalSimpleIndex;\n-import org.apache.hudi.index.simple.HoodieSimpleIndex;\n import org.apache.hudi.table.HoodieTable;\n \n-import org.apache.spark.api.java.JavaPairRDD;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n-\n import java.io.Serializable;\n \n /**\n  * Base class for different types of indexes to determine the mapping from uuid.\n  */\n @PublicAPIClass(maturity = ApiMaturityLevel.EVOLVING)\n-public abstract class HoodieIndex<T extends HoodieRecordPayload> implements Serializable {\n+public abstract class HoodieIndex<T extends HoodieRecordPayload, I, K, O, P> implements Serializable {\n \n   protected final HoodieWriteConfig config;\n \n   protected HoodieIndex(HoodieWriteConfig config) {\n     this.config = config;\n   }\n \n-  public static <T extends HoodieRecordPayload> HoodieIndex<T> createIndex(\n-      HoodieWriteConfig config) throws HoodieIndexException {\n-    // first use index class config to create index.\n-    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n-      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n-      if (!(instance instanceof HoodieIndex)) {\n-        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n-      }\n-      return (HoodieIndex) instance;\n-    }\n-    switch (config.getIndexType()) {\n-      case HBASE:\n-        return new HBaseIndex<>(config);\n-      case INMEMORY:\n-        return new InMemoryHashIndex<>(config);\n-      case BLOOM:\n-        return new HoodieBloomIndex<>(config);\n-      case GLOBAL_BLOOM:\n-        return new HoodieGlobalBloomIndex<>(config);\n-      case SIMPLE:\n-        return new HoodieSimpleIndex<>(config);\n-      case GLOBAL_SIMPLE:\n-        return new HoodieGlobalSimpleIndex<>(config);\n-      default:\n-        throw new HoodieIndexException(\"Index type unspecified, set \" + config.getIndexType());\n-    }\n-  }\n-\n   /**\n    * Checks if the given [Keys] exists in the hoodie table and returns [Key, Option[partitionPath, fileID]] If the\n    * optional is empty, then the key is not found.\n    */\n   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)\n-  public abstract JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(\n-      JavaRDD<HoodieKey> hoodieKeys, final JavaSparkContext jsc, HoodieTable<T> hoodieTable);\n+  public abstract P fetchRecordLocation(\n+      K hoodieKeys, final HoodieEngineContext context, HoodieTable<T, I, K, O, P> hoodieTable);\n \n   /**\n    * Looks up the index and tags each incoming record with a location of a file that contains the row (if it is actually\n    * present).\n    */\n   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgxNTQwMg==", "bodyText": "these annotations needs to moved over to a SparkHoodieIndex class? it will be hard for end developers to program against HoodieIndex directly anymore. This is a general point actually. The current public APIs should all be annotated against the Spark child classes. wdyt?\n\ngood idea, done", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484815402", "createdAt": "2020-09-08T10:29:26Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -21,94 +21,52 @@\n import org.apache.hudi.ApiMaturityLevel;\n import org.apache.hudi.PublicAPIClass;\n import org.apache.hudi.PublicAPIMethod;\n-import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.model.FileSlice;\n import org.apache.hudi.common.model.HoodieKey;\n-import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.ReflectionUtils;\n-import org.apache.hudi.common.util.StringUtils;\n-import org.apache.hudi.common.util.collection.Pair;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.exception.HoodieIndexException;\n-import org.apache.hudi.index.bloom.HoodieBloomIndex;\n-import org.apache.hudi.index.bloom.HoodieGlobalBloomIndex;\n-import org.apache.hudi.index.hbase.HBaseIndex;\n-import org.apache.hudi.index.simple.HoodieGlobalSimpleIndex;\n-import org.apache.hudi.index.simple.HoodieSimpleIndex;\n import org.apache.hudi.table.HoodieTable;\n \n-import org.apache.spark.api.java.JavaPairRDD;\n-import org.apache.spark.api.java.JavaRDD;\n-import org.apache.spark.api.java.JavaSparkContext;\n-\n import java.io.Serializable;\n \n /**\n  * Base class for different types of indexes to determine the mapping from uuid.\n  */\n @PublicAPIClass(maturity = ApiMaturityLevel.EVOLVING)\n-public abstract class HoodieIndex<T extends HoodieRecordPayload> implements Serializable {\n+public abstract class HoodieIndex<T extends HoodieRecordPayload, I, K, O, P> implements Serializable {\n \n   protected final HoodieWriteConfig config;\n \n   protected HoodieIndex(HoodieWriteConfig config) {\n     this.config = config;\n   }\n \n-  public static <T extends HoodieRecordPayload> HoodieIndex<T> createIndex(\n-      HoodieWriteConfig config) throws HoodieIndexException {\n-    // first use index class config to create index.\n-    if (!StringUtils.isNullOrEmpty(config.getIndexClass())) {\n-      Object instance = ReflectionUtils.loadClass(config.getIndexClass(), config);\n-      if (!(instance instanceof HoodieIndex)) {\n-        throw new HoodieIndexException(config.getIndexClass() + \" is not a subclass of HoodieIndex\");\n-      }\n-      return (HoodieIndex) instance;\n-    }\n-    switch (config.getIndexType()) {\n-      case HBASE:\n-        return new HBaseIndex<>(config);\n-      case INMEMORY:\n-        return new InMemoryHashIndex<>(config);\n-      case BLOOM:\n-        return new HoodieBloomIndex<>(config);\n-      case GLOBAL_BLOOM:\n-        return new HoodieGlobalBloomIndex<>(config);\n-      case SIMPLE:\n-        return new HoodieSimpleIndex<>(config);\n-      case GLOBAL_SIMPLE:\n-        return new HoodieGlobalSimpleIndex<>(config);\n-      default:\n-        throw new HoodieIndexException(\"Index type unspecified, set \" + config.getIndexType());\n-    }\n-  }\n-\n   /**\n    * Checks if the given [Keys] exists in the hoodie table and returns [Key, Option[partitionPath, fileID]] If the\n    * optional is empty, then the key is not found.\n    */\n   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)\n-  public abstract JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(\n-      JavaRDD<HoodieKey> hoodieKeys, final JavaSparkContext jsc, HoodieTable<T> hoodieTable);\n+  public abstract P fetchRecordLocation(\n+      K hoodieKeys, final HoodieEngineContext context, HoodieTable<T, I, K, O, P> hoodieTable);\n \n   /**\n    * Looks up the index and tags each incoming record with a location of a file that contains the row (if it is actually\n    * present).\n    */\n   @PublicAPIMethod(maturity = ApiMaturityLevel.STABLE)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzYxMw=="}, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQ0MTc1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/BaseHoodieBloomIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxNzo1ODozNFrOHNq-dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzozMDo1NVrOHOdRDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzY1NA==", "bodyText": "I suggest introducing a SparkHoodieIndex base class", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484097654", "createdAt": "2020-09-06T17:58:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/BaseHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+/**\n+ * Indexing mechanism based on bloom filter. Each parquet file includes its row_key bloom filter in its metadata.\n+ */\n+public abstract class BaseHoodieBloomIndex<T extends HoodieRecordPayload, I, K, O, P> extends HoodieIndex<T, I, K, O, P> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkyMTYxMg==", "bodyText": "I suggest introducing a SparkHoodieIndex base class\n\ndone", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484921612", "createdAt": "2020-09-08T13:30:55Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/bloom/BaseHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+/**\n+ * Indexing mechanism based on bloom filter. Each parquet file includes its row_key bloom filter in its metadata.\n+ */\n+public abstract class BaseHoodieBloomIndex<T extends HoodieRecordPayload, I, K, O, P> extends HoodieIndex<T, I, K, O, P> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5NzY1NA=="}, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQ0NDE3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/hbase/BaseHoodieHBaseIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxODowMTozMlrOHNq_ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzozMToxNlrOHOdSig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5Nzk1MA==", "bodyText": "are there any code changes here, i.e logic changes?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484097950", "createdAt": "2020-09-06T18:01:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/hbase/BaseHoodieHBaseIndex.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.hbase;\n+\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieHBaseIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieDependentSystemUnavailableException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.RegionLocator;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.List;\n+\n+/**\n+ * Hoodie Index implementation backed by HBase.\n+ */\n+public abstract class BaseHoodieHBaseIndex<T extends HoodieRecordPayload, I, K, O, P> extends HoodieIndex<T, I, K, O, P> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkyMTk5NA==", "bodyText": "are there any code changes here, i.e logic changes?\n\nnothing changed", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484921994", "createdAt": "2020-09-08T13:31:16Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/index/hbase/BaseHoodieHBaseIndex.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.hbase;\n+\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieHBaseIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieDependentSystemUnavailableException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hbase.HBaseConfiguration;\n+import org.apache.hadoop.hbase.HRegionLocation;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.Connection;\n+import org.apache.hadoop.hbase.client.ConnectionFactory;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.RegionLocator;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.List;\n+\n+/**\n+ * Hoodie Index implementation backed by HBase.\n+ */\n+public abstract class BaseHoodieHBaseIndex<T extends HoodieRecordPayload, I, K, O, P> extends HoodieIndex<T, I, K, O, P> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5Nzk1MA=="}, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQ0NzQ0OnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGeneratorInterface.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxODowNTo0NlrOHNrBLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMDozMTo0N1rOHOW3Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5ODM1MA==", "bodyText": "we should make sure there are no backwards incompatible changes to the key generator interface", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484098350", "createdAt": "2020-09-06T18:05:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGeneratorInterface.java", "diffHunk": "@@ -34,8 +33,4 @@\n \n   List<String> getRecordKeyFieldNames();\n \n-  String getRecordKey(Row row);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgxNjY5MA==", "bodyText": "we should make sure there are no backwards incompatible changes to the key generator interface\n\nYes, I moved it to SparkKeyGeneratorInterface", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484816690", "createdAt": "2020-09-08T10:31:47Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/keygen/KeyGeneratorInterface.java", "diffHunk": "@@ -34,8 +33,4 @@\n \n   List<String> getRecordKeyFieldNames();\n \n-  String getRecordKey(Row row);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5ODM1MA=="}, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzQ1MTkxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseMergeHelper.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNlQxODoxMjoxNVrOHNrDTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMjoxNjoxMVrOHXHUCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5ODg5Mw==", "bodyText": "why is this no longer a mergeHandle?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484098893", "createdAt": "2020-09-06T18:12:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseMergeHelper.java", "diffHunk": "@@ -161,11 +108,11 @@ private static GenericRecord transformRecordBasedOnNewSchema(GenericDatumReader<\n   /**\n    * Consumer that dequeues records from queue and sends to Merge Handle.\n    */\n-  private static class UpdateHandler extends BoundedInMemoryQueueConsumer<GenericRecord, Void> {\n+  static class UpdateHandler extends BoundedInMemoryQueueConsumer<GenericRecord, Void> {\n \n-    private final HoodieMergeHandle upsertHandle;\n+    private final HoodieWriteHandle upsertHandle;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkyNTY4OQ==", "bodyText": "why is this no longer a mergeHandle?\n\nHoodieWriteHandle is spark-free, while HoodieMergeHandle is not. To abstract MergeHelper, the variables it holds should be spark-free too", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484925689", "createdAt": "2020-09-08T13:36:04Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseMergeHelper.java", "diffHunk": "@@ -161,11 +108,11 @@ private static GenericRecord transformRecordBasedOnNewSchema(GenericDatumReader<\n   /**\n    * Consumer that dequeues records from queue and sends to Merge Handle.\n    */\n-  private static class UpdateHandler extends BoundedInMemoryQueueConsumer<GenericRecord, Void> {\n+  static class UpdateHandler extends BoundedInMemoryQueueConsumer<GenericRecord, Void> {\n \n-    private final HoodieMergeHandle upsertHandle;\n+    private final HoodieWriteHandle upsertHandle;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5ODg5Mw=="}, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5OTExNA==", "bodyText": "why is this no longer a mergeHandle?\n\nWith parallelDo method introduced in, this change is no longer needed.  rollback already", "url": "https://github.com/apache/hudi/pull/1827#discussion_r493999114", "createdAt": "2020-09-24T02:16:11Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/BaseMergeHelper.java", "diffHunk": "@@ -161,11 +108,11 @@ private static GenericRecord transformRecordBasedOnNewSchema(GenericDatumReader<\n   /**\n    * Consumer that dequeues records from queue and sends to Merge Handle.\n    */\n-  private static class UpdateHandler extends BoundedInMemoryQueueConsumer<GenericRecord, Void> {\n+  static class UpdateHandler extends BoundedInMemoryQueueConsumer<GenericRecord, Void> {\n \n-    private final HoodieMergeHandle upsertHandle;\n+    private final HoodieWriteHandle upsertHandle;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDA5ODg5Mw=="}, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzc0Njg0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseMarkerBasedRollbackStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwMDozMDowNlrOHNtPxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwMDozMDowNlrOHNtPxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDEzNDg1Mg==", "bodyText": "These sort of classes, we should have a way to implement with just a reference to engineContext ideally. Even though we cannot implement every method in sparkContext. This is a topic for later", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484134852", "createdAt": "2020-09-07T00:30:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseMarkerBasedRollbackStrategy.java", "diffHunk": "@@ -18,63 +18,58 @@\n \n package org.apache.hudi.table.action.rollback;\n \n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.HoodieEngineContext;\n import org.apache.hudi.common.HoodieRollbackStat;\n import org.apache.hudi.common.fs.FSUtils;\n import org.apache.hudi.common.model.HoodieLogFile;\n-import org.apache.hudi.common.model.IOType;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.table.log.HoodieLogFormat;\n import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n import org.apache.hudi.common.table.timeline.HoodieInstant;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.exception.HoodieIOException;\n-import org.apache.hudi.exception.HoodieRollbackException;\n import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.table.MarkerFiles;\n \n-import org.apache.hadoop.fs.Path;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n-import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n \n-import scala.Tuple2;\n-\n /**\n  * Performs rollback using marker files generated during the write..\n  */\n-public class MarkerBasedRollbackStrategy implements BaseRollbackActionExecutor.RollbackStrategy {\n+public abstract class BaseMarkerBasedRollbackStrategy<T extends HoodieRecordPayload, I, K, O, P> implements BaseRollbackActionExecutor.RollbackStrategy {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyNzc0OTIwOnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseMarkerBasedRollbackStrategy.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwMDozMjoxOFrOHNtQ4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QwMDozMjoxOFrOHNtQ4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDEzNTEzNw==", "bodyText": "would a parallelDo(func, parallelism) method in HoodieEngineContext help us avoid a lot of base/child class duplication of logic like this?\nMost of clean, compact, rollback, restore etc can be implemented this way. Most of them just take a list, parallelize it, and execute some function, collect results and get the objects back", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484135137", "createdAt": "2020-09-07T00:32:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/BaseMarkerBasedRollbackStrategy.java", "diffHunk": "@@ -132,32 +127,4 @@ private HoodieRollbackStat undoAppend(String appendBaseFilePath, HoodieInstant i\n         .build();\n   }\n \n-  @Override\n-  public List<HoodieRollbackStat> execute(HoodieInstant instantToRollback) {\n-    try {\n-      MarkerFiles markerFiles = new MarkerFiles(table, instantToRollback.getTimestamp());\n-      List<String> markerFilePaths = markerFiles.allMarkerFilePaths();\n-      int parallelism = Math.max(Math.min(markerFilePaths.size(), config.getRollbackParallelism()), 1);\n-      return jsc.parallelize(markerFilePaths, parallelism)\n-          .map(markerFilePath -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac3339704c703741f9ff50f2d96019cef2d2c72b"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDI2ODg0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjozMzo0NFrOHOEh8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMDo0NDozNVrOHOXRqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxNjMzOQ==", "bodyText": "the MOR equivalent method got moved I guess", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484516339", "createdAt": "2020-09-07T16:33:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class RollbackUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(RollbackUtils.class);\n+\n+  static Map<HoodieLogBlock.HeaderMetadataType, String> generateHeader(String instantToRollback, String rollbackInstantTime) {\n+    // generate metadata\n+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>(3);\n+    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, rollbackInstantTime);\n+    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, instantToRollback);\n+    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n+        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n+    return header;\n+  }\n+\n+  /**\n+   * Helper to merge 2 rollback-stats for a given partition.\n+   *\n+   * @param stat1 HoodieRollbackStat\n+   * @param stat2 HoodieRollbackStat\n+   * @return Merged HoodieRollbackStat\n+   */\n+  static HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRollbackStat stat2) {\n+    ValidationUtils.checkArgument(stat1.getPartitionPath().equals(stat2.getPartitionPath()));\n+    final List<String> successDeleteFiles = new ArrayList<>();\n+    final List<String> failedDeleteFiles = new ArrayList<>();\n+    final Map<FileStatus, Long> commandBlocksCount = new HashMap<>();\n+    final List<FileStatus> filesToRollback = new ArrayList<>();\n+    Option.ofNullable(stat1.getSuccessDeleteFiles()).ifPresent(successDeleteFiles::addAll);\n+    Option.ofNullable(stat2.getSuccessDeleteFiles()).ifPresent(successDeleteFiles::addAll);\n+    Option.ofNullable(stat1.getFailedDeleteFiles()).ifPresent(failedDeleteFiles::addAll);\n+    Option.ofNullable(stat2.getFailedDeleteFiles()).ifPresent(failedDeleteFiles::addAll);\n+    Option.ofNullable(stat1.getCommandBlocksCount()).ifPresent(commandBlocksCount::putAll);\n+    Option.ofNullable(stat2.getCommandBlocksCount()).ifPresent(commandBlocksCount::putAll);\n+    return new HoodieRollbackStat(stat1.getPartitionPath(), successDeleteFiles, failedDeleteFiles, commandBlocksCount);\n+  }\n+\n+  /**\n+   * Generate all rollback requests that needs rolling back this action without actually performing rollback for COW table type.\n+   * @param fs instance of {@link FileSystem} to use.\n+   * @param basePath base path of interest.\n+   * @param shouldAssumeDatePartitioning {@code true} if date partitioning should be assumed. {@code false} otherwise.\n+   * @return {@link List} of {@link ListingBasedRollbackRequest}s thus collected.\n+   */\n+  public static List<ListingBasedRollbackRequest> generateRollbackRequestsByListingCOW(FileSystem fs, String basePath, boolean shouldAssumeDatePartitioning) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgyMzQ2NQ==", "bodyText": "the MOR equivalent method got moved I guess\n\nYes,  MOR equivalent method moved to SparkRollbackUtils.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484823465", "createdAt": "2020-09-08T10:44:35Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/rollback/RollbackUtils.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.rollback;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import org.apache.hudi.common.HoodieRollbackStat;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.table.log.block.HoodieCommandBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class RollbackUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(RollbackUtils.class);\n+\n+  static Map<HoodieLogBlock.HeaderMetadataType, String> generateHeader(String instantToRollback, String rollbackInstantTime) {\n+    // generate metadata\n+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>(3);\n+    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, rollbackInstantTime);\n+    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, instantToRollback);\n+    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n+        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n+    return header;\n+  }\n+\n+  /**\n+   * Helper to merge 2 rollback-stats for a given partition.\n+   *\n+   * @param stat1 HoodieRollbackStat\n+   * @param stat2 HoodieRollbackStat\n+   * @return Merged HoodieRollbackStat\n+   */\n+  static HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRollbackStat stat2) {\n+    ValidationUtils.checkArgument(stat1.getPartitionPath().equals(stat2.getPartitionPath()));\n+    final List<String> successDeleteFiles = new ArrayList<>();\n+    final List<String> failedDeleteFiles = new ArrayList<>();\n+    final Map<FileStatus, Long> commandBlocksCount = new HashMap<>();\n+    final List<FileStatus> filesToRollback = new ArrayList<>();\n+    Option.ofNullable(stat1.getSuccessDeleteFiles()).ifPresent(successDeleteFiles::addAll);\n+    Option.ofNullable(stat2.getSuccessDeleteFiles()).ifPresent(successDeleteFiles::addAll);\n+    Option.ofNullable(stat1.getFailedDeleteFiles()).ifPresent(failedDeleteFiles::addAll);\n+    Option.ofNullable(stat2.getFailedDeleteFiles()).ifPresent(failedDeleteFiles::addAll);\n+    Option.ofNullable(stat1.getCommandBlocksCount()).ifPresent(commandBlocksCount::putAll);\n+    Option.ofNullable(stat2.getCommandBlocksCount()).ifPresent(commandBlocksCount::putAll);\n+    return new HoodieRollbackStat(stat1.getPartitionPath(), successDeleteFiles, failedDeleteFiles, commandBlocksCount);\n+  }\n+\n+  /**\n+   * Generate all rollback requests that needs rolling back this action without actually performing rollback for COW table type.\n+   * @param fs instance of {@link FileSystem} to use.\n+   * @param basePath base path of interest.\n+   * @param shouldAssumeDatePartitioning {@code true} if date partitioning should be assumed. {@code false} otherwise.\n+   * @return {@link List} of {@link ListingBasedRollbackRequest}s thus collected.\n+   */\n+  public static List<ListingBasedRollbackRequest> generateRollbackRequestsByListingCOW(FileSystem fs, String basePath, boolean shouldAssumeDatePartitioning) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxNjMzOQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDI3MzQ4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/async/HoodieSparkAsyncCompactService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjozNTo1OVrOHOEkhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjozNTo1OVrOHOEkhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxNjk5Ng==", "bodyText": "might make sense to move the COMPACT_POOL_NAME also to the child class", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484516996", "createdAt": "2020-09-07T16:35:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/async/HoodieSparkAsyncCompactService.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import org.apache.hudi.asyc.BaseAsyncCompactService;\n+import org.apache.hudi.client.AbstractHoodieWriteClient;\n+import org.apache.hudi.client.BaseCompactor;\n+import org.apache.hudi.client.HoodieSparkCompactor;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.stream.IntStream;\n+\n+public class HoodieSparkAsyncCompactService extends BaseAsyncCompactService {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkAsyncCompactService.class);\n+\n+  private transient JavaSparkContext jssc;\n+  public HoodieSparkAsyncCompactService(HoodieEngineContext context, AbstractHoodieWriteClient client) {\n+    super(context, client);\n+    this.jssc = HoodieSparkEngineContext.getSparkContext(context);\n+  }\n+\n+  public HoodieSparkAsyncCompactService(HoodieEngineContext context, AbstractHoodieWriteClient client, boolean runInDaemonMode) {\n+    super(context, client, runInDaemonMode);\n+    this.jssc = HoodieSparkEngineContext.getSparkContext(context);\n+  }\n+\n+  @Override\n+  protected BaseCompactor createCompactor(AbstractHoodieWriteClient client) {\n+    return new HoodieSparkCompactor(client);\n+  }\n+\n+  @Override\n+  protected Pair<CompletableFuture, ExecutorService> startService() {\n+    ExecutorService executor = Executors.newFixedThreadPool(maxConcurrentCompaction,\n+        r -> {\n+          Thread t = new Thread(r, \"async_compact_thread\");\n+          t.setDaemon(isRunInDaemonMode());\n+          return t;\n+        });\n+    return Pair.of(CompletableFuture.allOf(IntStream.range(0, maxConcurrentCompaction).mapToObj(i -> CompletableFuture.supplyAsync(() -> {\n+      try {\n+        // Set Compactor Pool Name for allowing users to prioritize compaction\n+        LOG.info(\"Setting Spark Pool name for compaction to \" + COMPACT_POOL_NAME);\n+        jssc.setLocalProperty(\"spark.scheduler.pool\", COMPACT_POOL_NAME);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDI4ODk4OnYy", "diffSide": "RIGHT", "path": "style/checkstyle.xml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjo0Mzo1M1rOHOEtFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMDo1NDo1NVrOHOXl7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxOTE5MA==", "bodyText": "why is this necessary for this PR?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484519190", "createdAt": "2020-09-07T16:43:53Z", "author": {"login": "vinothchandar"}, "path": "style/checkstyle.xml", "diffHunk": "@@ -62,7 +62,7 @@\n             <property name=\"allowNonPrintableEscapes\" value=\"true\"/>\n         </module>\n         <module name=\"LineLength\">\n-            <property name=\"max\" value=\"200\"/>\n+            <property name=\"max\" value=\"500\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzODE0MA==", "bodyText": "let's discuss this in a separate PR? 500 is a really large threshold", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484538140", "createdAt": "2020-09-07T18:11:09Z", "author": {"login": "vinothchandar"}, "path": "style/checkstyle.xml", "diffHunk": "@@ -62,7 +62,7 @@\n             <property name=\"allowNonPrintableEscapes\" value=\"true\"/>\n         </module>\n         <module name=\"LineLength\">\n-            <property name=\"max\" value=\"200\"/>\n+            <property name=\"max\" value=\"500\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxOTE5MA=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgyODY1Mg==", "bodyText": "let's discuss this in a separate PR? 500 is a really large threshold\n\nvery sorry to say this ...\nAccording to the current abstraction\uff0csome declaration of the method is longer than 200.\nsuch as org.apache.hudi.table.action.compact.HoodieSparkMergeOnReadTableCompactor#generateCompactionPlan\nIt is 357 characters long", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484828652", "createdAt": "2020-09-08T10:54:55Z", "author": {"login": "wangxianghu"}, "path": "style/checkstyle.xml", "diffHunk": "@@ -62,7 +62,7 @@\n             <property name=\"allowNonPrintableEscapes\" value=\"true\"/>\n         </module>\n         <module name=\"LineLength\">\n-            <property name=\"max\" value=\"200\"/>\n+            <property name=\"max\" value=\"500\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxOTE5MA=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDI5NDgxOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjo0Njo0OVrOHOEwTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjo0Njo0OVrOHOEwTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMDAxMw==", "bodyText": "wondering if this renaming will have any impact on deserializing older plans. cc @bvaradar to confirm", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484520013", "createdAt": "2020-09-07T16:46:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactionAdminTool.java", "diffHunk": "@@ -60,38 +60,38 @@ public static void main(String[] args) throws Exception {\n    */\n   public void run(JavaSparkContext jsc) throws Exception {\n     HoodieTableMetaClient metaClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.basePath);\n-    try (CompactionAdminClient admin = new CompactionAdminClient(jsc, cfg.basePath)) {\n+    try (HoodieSparkCompactionAdminClient admin = new HoodieSparkCompactionAdminClient(new HoodieSparkEngineContext(jsc), cfg.basePath)) {\n       final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n       if (cfg.outputPath != null && fs.exists(new Path(cfg.outputPath))) {\n         throw new IllegalStateException(\"Output File Path already exists\");\n       }\n       switch (cfg.operation) {\n         case VALIDATE:\n-          List<ValidationOpResult> res =\n+          List<BaseCompactionAdminClient.ValidationOpResult> res =\n               admin.validateCompactionPlan(metaClient, cfg.compactionInstantTime, cfg.parallelism);\n           if (cfg.printOutput) {\n             printOperationResult(\"Result of Validation Operation :\", res);\n           }\n           serializeOperationResult(fs, res);\n           break;\n         case UNSCHEDULE_FILE:\n-          List<RenameOpResult> r = admin.unscheduleCompactionFileId(\n+          List<BaseCompactionAdminClient.RenameOpResult> r = admin.unscheduleCompactionFileId(\n               new HoodieFileGroupId(cfg.partitionPath, cfg.fileId), cfg.skipValidation, cfg.dryRun);\n           if (cfg.printOutput) {\n             System.out.println(r);\n           }\n           serializeOperationResult(fs, r);\n           break;\n         case UNSCHEDULE_PLAN:\n-          List<RenameOpResult> r2 = admin.unscheduleCompactionPlan(cfg.compactionInstantTime, cfg.skipValidation,\n+          List<BaseCompactionAdminClient.RenameOpResult> r2 = admin.unscheduleCompactionPlan(cfg.compactionInstantTime, cfg.skipValidation,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDI5Nzk2OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjo0ODo0MVrOHOEyKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMDo1NToxNFrOHOXmlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMDQ5MA==", "bodyText": "nit: no space before , and space after , metaClient", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484520490", "createdAt": "2020-09-07T16:48:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -64,8 +64,7 @@ class IncrementalRelation(val sqlContext: SQLContext,\n     throw new HoodieException(\"Incremental view not implemented yet, for merge-on-read tables\")\n   }\n   // TODO : Figure out a valid HoodieWriteConfig\n-  private val hoodieTable = HoodieTable.create(metaClient, HoodieWriteConfig.newBuilder().withPath(basePath).build(),\n-    sqlContext.sparkContext.hadoopConfiguration)\n+  private val hoodieTable = HoodieSparkTable.create(HoodieWriteConfig.newBuilder().withPath(basePath).build(), new HoodieSparkEngineContext(new JavaSparkContext(sqlContext.sparkContext)) ,metaClient)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgyODgyMw==", "bodyText": "nit: no space before , and space after , metaClient\n\ndone", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484828823", "createdAt": "2020-09-08T10:55:14Z", "author": {"login": "wangxianghu"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -64,8 +64,7 @@ class IncrementalRelation(val sqlContext: SQLContext,\n     throw new HoodieException(\"Incremental view not implemented yet, for merge-on-read tables\")\n   }\n   // TODO : Figure out a valid HoodieWriteConfig\n-  private val hoodieTable = HoodieTable.create(metaClient, HoodieWriteConfig.newBuilder().withPath(basePath).build(),\n-    sqlContext.sparkContext.hadoopConfiguration)\n+  private val hoodieTable = HoodieSparkTable.create(HoodieWriteConfig.newBuilder().withPath(basePath).build(), new HoodieSparkEngineContext(new JavaSparkContext(sqlContext.sparkContext)) ,metaClient)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMDQ5MA=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDMwMDk4OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/java/org/apache/hudi/bootstrap/SparkParquetBootstrapDataProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjo1MDoxNFrOHOEz4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzozOTozMFrOHOdqdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMDkyOQ==", "bodyText": "rename: generateInputRecords", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484520929", "createdAt": "2020-09-07T16:50:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/java/org/apache/hudi/bootstrap/SparkParquetBootstrapDataProvider.java", "diffHunk": "@@ -43,18 +43,18 @@\n /**\n  * Spark Data frame based bootstrap input provider.\n  */\n-public class SparkParquetBootstrapDataProvider extends FullRecordBootstrapDataProvider {\n+public class SparkParquetBootstrapDataProvider extends FullRecordBootstrapDataProvider<JavaRDD<HoodieRecord>> {\n \n   private final transient SparkSession sparkSession;\n \n   public SparkParquetBootstrapDataProvider(TypedProperties props,\n-                                           JavaSparkContext jsc) {\n-    super(props, jsc);\n-    this.sparkSession = SparkSession.builder().config(jsc.getConf()).getOrCreate();\n+                                           HoodieSparkEngineContext context) {\n+    super(props, context);\n+    this.sparkSession = SparkSession.builder().config(context.getJavaSparkContext().getConf()).getOrCreate();\n   }\n \n   @Override\n-  public JavaRDD<HoodieRecord> generateInputRecordRDD(String tableName, String sourceBasePath,\n+  public JavaRDD<HoodieRecord> generateInputRecord(String tableName, String sourceBasePath,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkyODExNw==", "bodyText": "rename: generateInputRecords\n\ndone", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484928117", "createdAt": "2020-09-08T13:39:30Z", "author": {"login": "wangxianghu"}, "path": "hudi-spark/src/main/java/org/apache/hudi/bootstrap/SparkParquetBootstrapDataProvider.java", "diffHunk": "@@ -43,18 +43,18 @@\n /**\n  * Spark Data frame based bootstrap input provider.\n  */\n-public class SparkParquetBootstrapDataProvider extends FullRecordBootstrapDataProvider {\n+public class SparkParquetBootstrapDataProvider extends FullRecordBootstrapDataProvider<JavaRDD<HoodieRecord>> {\n \n   private final transient SparkSession sparkSession;\n \n   public SparkParquetBootstrapDataProvider(TypedProperties props,\n-                                           JavaSparkContext jsc) {\n-    super(props, jsc);\n-    this.sparkSession = SparkSession.builder().config(jsc.getConf()).getOrCreate();\n+                                           HoodieSparkEngineContext context) {\n+    super(props, context);\n+    this.sparkSession = SparkSession.builder().config(context.getJavaSparkContext().getConf()).getOrCreate();\n   }\n \n   @Override\n-  public JavaRDD<HoodieRecord> generateInputRecordRDD(String tableName, String sourceBasePath,\n+  public JavaRDD<HoodieRecord> generateInputRecord(String tableName, String sourceBasePath,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUyMDkyOQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDQyNzU1OnYy", "diffSide": "RIGHT", "path": "hudi-client/pom.xml", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxODoxMzoxNlrOHOF4hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xM1QyMDo1MToxOFrOHQ_RUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzODUwMQ==", "bodyText": "should we limit scala to just the spark module?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484538501", "createdAt": "2020-09-07T18:13:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/pom.xml", "diffHunk": "@@ -68,6 +107,12 @@\n   </build>\n \n   <dependencies>\n+    <!-- Scala -->\n+    <dependency>\n+      <groupId>org.scala-lang</groupId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDgzMjgyNA==", "bodyText": "should we limit scala to just the spark module?\n\nYes, it is better. can we do it in another PR?\nbecause although some classes have nothing to do with spark, it used scala.Tuple2, so scala is still needed.\nwe can replace it with 'org.apache.hudi.common.util.collection.Pair'\nWDYT?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484832824", "createdAt": "2020-09-08T11:03:16Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/pom.xml", "diffHunk": "@@ -68,6 +107,12 @@\n   </build>\n \n   <dependencies>\n+    <!-- Scala -->\n+    <dependency>\n+      <groupId>org.scala-lang</groupId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzODUwMQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzU3NTg4OA==", "bodyText": "fair. let me take a closer look", "url": "https://github.com/apache/hudi/pull/1827#discussion_r487575888", "createdAt": "2020-09-13T20:51:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/pom.xml", "diffHunk": "@@ -68,6 +107,12 @@\n   </build>\n \n   <dependencies>\n+    <!-- Scala -->\n+    <dependency>\n+      <groupId>org.scala-lang</groupId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzODUwMQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDQyOTkxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxODoxNTowNlrOHOF5vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzo0Mzo0OFrOHOd2WQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzODgxNQ==", "bodyText": "why was this change required?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484538815", "createdAt": "2020-09-07T18:15:06Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java", "diffHunk": "@@ -81,7 +82,9 @@\n    */\n   public static SparkConf getSparkConfForTest(String appName) {\n     SparkConf sparkConf = new SparkConf().setAppName(appName)\n-        .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").setMaster(\"local[8]\");\n+        .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+        .set(\"spark.driver.host\",\"localhost\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMTE2MQ==", "bodyText": "why was this change required?\n\nI have rolled back this.\nThe unit test is not runnable in my local yesterday, but ok now... weird", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484931161", "createdAt": "2020-09-08T13:43:48Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java", "diffHunk": "@@ -81,7 +82,9 @@\n    */\n   public static SparkConf getSparkConfForTest(String appName) {\n     SparkConf sparkConf = new SparkConf().setAppName(appName)\n-        .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").setMaster(\"local[8]\");\n+        .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+        .set(\"spark.driver.host\",\"localhost\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUzODgxNQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDg3NjIyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMTowMTo0MVrOHOJjHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzo0NDozMVrOHOd4ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU5ODU1OQ==", "bodyText": "need to ensure the ordering of closing resources is the same as before/", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484598559", "createdAt": "2020-09-08T01:01:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -716,32 +674,97 @@ private void rollbackPendingCommits() {\n    * @param compactionInstantTime Compaction Instant Time\n    * @return RDD of Write Status\n    */\n-  private JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n-    HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n-    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n-    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n-    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n-      rollbackInflightCompaction(inflightInstant, table);\n-      table.getMetaClient().reloadActiveTimeline();\n-    }\n-    compactionTimer = metrics.getCompactionCtx();\n-    HoodieWriteMetadata compactionMetadata = table.compact(jsc, compactionInstantTime);\n-    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n-    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n-      completeCompaction(compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n-    }\n-    return statuses;\n-  }\n+  protected abstract O compact(String compactionInstantTime, boolean shouldComplete);\n \n   /**\n    * Performs a compaction operation on a table, serially before or after an insert/upsert action.\n    */\n-  private Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n+  protected Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n     Option<String> compactionInstantTimeOpt = scheduleCompaction(extraMetadata);\n     compactionInstantTimeOpt.ifPresent(compactionInstantTime -> {\n       // inline compaction should auto commit as the user is never given control\n       compact(compactionInstantTime, true);\n     });\n     return compactionInstantTimeOpt;\n   }\n+\n+  /**\n+   * Finalize Write operation.\n+   *\n+   * @param table       HoodieTable\n+   * @param instantTime Instant Time\n+   * @param stats       Hoodie Write Stat\n+   */\n+  protected void finalizeWrite(HoodieTable<T, I, K, O, P> table, String instantTime, List<HoodieWriteStat> stats) {\n+    try {\n+      final Timer.Context finalizeCtx = metrics.getFinalizeCtx();\n+      table.finalizeWrite(context, instantTime, stats);\n+      if (finalizeCtx != null) {\n+        Option<Long> durationInMs = Option.of(metrics.getDurationInMs(finalizeCtx.stop()));\n+        durationInMs.ifPresent(duration -> {\n+          LOG.info(\"Finalize write elapsed time (milliseconds): \" + duration);\n+          metrics.updateFinalizeWriteMetrics(duration, stats.size());\n+        });\n+      }\n+    } catch (HoodieIOException ioe) {\n+      throw new HoodieCommitException(\"Failed to complete commit \" + instantTime + \" due to finalize errors.\", ioe);\n+    }\n+  }\n+\n+  public HoodieMetrics getMetrics() {\n+    return metrics;\n+  }\n+\n+  public HoodieIndex<T, I, K, O, P> getIndex() {\n+    return index;\n+  }\n+\n+  /**\n+   * Get HoodieTable and init {@link Timer.Context}.\n+   *\n+   * @param operationType write operation type\n+   * @param instantTime   current inflight instant time\n+   * @return HoodieTable\n+   */\n+  protected abstract HoodieTable<T, I, K, O, P> getTableAndInitCtx(WriteOperationType operationType, String instantTime);\n+\n+  /**\n+   * Sets write schema from last instant since deletes may not have schema set in the config.\n+   */\n+  protected void setWriteSchemaForDeletes(HoodieTableMetaClient metaClient) {\n+    try {\n+      HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();\n+      Option<HoodieInstant> lastInstant =\n+          activeTimeline.filterCompletedInstants().filter(s -> s.getAction().equals(metaClient.getCommitActionType()))\n+              .lastInstant();\n+      if (lastInstant.isPresent()) {\n+        HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(\n+            activeTimeline.getInstantDetails(lastInstant.get()).get(), HoodieCommitMetadata.class);\n+        if (commitMetadata.getExtraMetadata().containsKey(HoodieCommitMetadata.SCHEMA_KEY)) {\n+          config.setSchema(commitMetadata.getExtraMetadata().get(HoodieCommitMetadata.SCHEMA_KEY));\n+        } else {\n+          throw new HoodieIOException(\"Latest commit does not have any schema in commit metadata\");\n+        }\n+      } else {\n+        throw new HoodieIOException(\"Deletes issued without any prior commits\");\n+      }\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"IOException thrown while reading last commit metadata\", e);\n+    }\n+  }\n+\n+  public abstract AsyncCleanerService startAsyncCleaningIfEnabled(AbstractHoodieWriteClient<T, I, K, O, P> client, String instantTime);\n+\n+  @Override\n+  public void close() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 909}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMTcwNw==", "bodyText": "need to ensure the ordering of closing resources is the same as before/\n\nYes, they are the same.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484931707", "createdAt": "2020-09-08T13:44:31Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -716,32 +674,97 @@ private void rollbackPendingCommits() {\n    * @param compactionInstantTime Compaction Instant Time\n    * @return RDD of Write Status\n    */\n-  private JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n-    HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n-    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n-    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n-    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n-      rollbackInflightCompaction(inflightInstant, table);\n-      table.getMetaClient().reloadActiveTimeline();\n-    }\n-    compactionTimer = metrics.getCompactionCtx();\n-    HoodieWriteMetadata compactionMetadata = table.compact(jsc, compactionInstantTime);\n-    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n-    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n-      completeCompaction(compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n-    }\n-    return statuses;\n-  }\n+  protected abstract O compact(String compactionInstantTime, boolean shouldComplete);\n \n   /**\n    * Performs a compaction operation on a table, serially before or after an insert/upsert action.\n    */\n-  private Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n+  protected Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n     Option<String> compactionInstantTimeOpt = scheduleCompaction(extraMetadata);\n     compactionInstantTimeOpt.ifPresent(compactionInstantTime -> {\n       // inline compaction should auto commit as the user is never given control\n       compact(compactionInstantTime, true);\n     });\n     return compactionInstantTimeOpt;\n   }\n+\n+  /**\n+   * Finalize Write operation.\n+   *\n+   * @param table       HoodieTable\n+   * @param instantTime Instant Time\n+   * @param stats       Hoodie Write Stat\n+   */\n+  protected void finalizeWrite(HoodieTable<T, I, K, O, P> table, String instantTime, List<HoodieWriteStat> stats) {\n+    try {\n+      final Timer.Context finalizeCtx = metrics.getFinalizeCtx();\n+      table.finalizeWrite(context, instantTime, stats);\n+      if (finalizeCtx != null) {\n+        Option<Long> durationInMs = Option.of(metrics.getDurationInMs(finalizeCtx.stop()));\n+        durationInMs.ifPresent(duration -> {\n+          LOG.info(\"Finalize write elapsed time (milliseconds): \" + duration);\n+          metrics.updateFinalizeWriteMetrics(duration, stats.size());\n+        });\n+      }\n+    } catch (HoodieIOException ioe) {\n+      throw new HoodieCommitException(\"Failed to complete commit \" + instantTime + \" due to finalize errors.\", ioe);\n+    }\n+  }\n+\n+  public HoodieMetrics getMetrics() {\n+    return metrics;\n+  }\n+\n+  public HoodieIndex<T, I, K, O, P> getIndex() {\n+    return index;\n+  }\n+\n+  /**\n+   * Get HoodieTable and init {@link Timer.Context}.\n+   *\n+   * @param operationType write operation type\n+   * @param instantTime   current inflight instant time\n+   * @return HoodieTable\n+   */\n+  protected abstract HoodieTable<T, I, K, O, P> getTableAndInitCtx(WriteOperationType operationType, String instantTime);\n+\n+  /**\n+   * Sets write schema from last instant since deletes may not have schema set in the config.\n+   */\n+  protected void setWriteSchemaForDeletes(HoodieTableMetaClient metaClient) {\n+    try {\n+      HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();\n+      Option<HoodieInstant> lastInstant =\n+          activeTimeline.filterCompletedInstants().filter(s -> s.getAction().equals(metaClient.getCommitActionType()))\n+              .lastInstant();\n+      if (lastInstant.isPresent()) {\n+        HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(\n+            activeTimeline.getInstantDetails(lastInstant.get()).get(), HoodieCommitMetadata.class);\n+        if (commitMetadata.getExtraMetadata().containsKey(HoodieCommitMetadata.SCHEMA_KEY)) {\n+          config.setSchema(commitMetadata.getExtraMetadata().get(HoodieCommitMetadata.SCHEMA_KEY));\n+        } else {\n+          throw new HoodieIOException(\"Latest commit does not have any schema in commit metadata\");\n+        }\n+      } else {\n+        throw new HoodieIOException(\"Deletes issued without any prior commits\");\n+      }\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"IOException thrown while reading last commit metadata\", e);\n+    }\n+  }\n+\n+  public abstract AsyncCleanerService startAsyncCleaningIfEnabled(AbstractHoodieWriteClient<T, I, K, O, P> client, String instantTime);\n+\n+  @Override\n+  public void close() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU5ODU1OQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 909}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDg4MTQwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMTowNTo1NVrOHOJl6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzo0Mzo1OVrOHOd29Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU5OTI3NQ==", "bodyText": "Let's name this SparkRDDWriteClient ?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484599275", "createdAt": "2020-09-08T01:05:55Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.client.embedded.SparkEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieSparkIndexFactory;\n+import org.apache.hudi.table.BaseHoodieTimelineArchiveLog;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieSparkTimelineArchiveLog;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.SparkMarkerFiles;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.compact.SparkCompactHelpers;\n+import org.apache.hudi.table.upgrade.SparkUpgradeDowngrade;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HoodieSparkWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMTMxNw==", "bodyText": "Let's name this SparkRDDWriteClient ?\n\ndone", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484931317", "createdAt": "2020-09-08T13:43:59Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.client.embedded.SparkEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieSparkIndexFactory;\n+import org.apache.hudi.table.BaseHoodieTimelineArchiveLog;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieSparkTimelineArchiveLog;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.SparkMarkerFiles;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.compact.SparkCompactHelpers;\n+import org.apache.hudi.table.upgrade.SparkUpgradeDowngrade;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HoodieSparkWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDU5OTI3NQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDg5MjA5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToxMzo0M1rOHOJrpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzo0NjoxMlrOHOd9lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMDc0Mg==", "bodyText": "why are we not hanging onto the returned object?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484600742", "createdAt": "2020-09-08T01:13:43Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.client.embedded.SparkEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieSparkIndexFactory;\n+import org.apache.hudi.table.BaseHoodieTimelineArchiveLog;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieSparkTimelineArchiveLog;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.SparkMarkerFiles;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.compact.SparkCompactHelpers;\n+import org.apache.hudi.table.upgrade.SparkUpgradeDowngrade;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HoodieSparkWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T,\n+    JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkWriteClient.class);\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending, Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Register hudi classes for Kryo serialization.\n+   *\n+   * @param conf instance of SparkConf\n+   * @return SparkConf\n+   */\n+  public static SparkConf registerClasses(SparkConf conf) {\n+    conf.registerKryoClasses(new Class[]{HoodieWriteConfig.class, HoodieRecord.class, HoodieKey.class});\n+    return conf;\n+  }\n+\n+  @Override\n+  protected HoodieIndex<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createIndex(HoodieWriteConfig writeConfig) {\n+    return HoodieSparkIndexFactory.createIndex(config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata) {\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStats(instantTime, stats, extraMetadata);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieSparkTable.create(config, context);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> filterExists(JavaRDD<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieTable table = HoodieSparkTable.create(config, context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    JavaRDD<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.filter(v1 -> !v1.isCurrentLocationKnown());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS);\n+    table.bootstrap(context, extraMetadata);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> upsert(JavaRDD<HoodieRecord<T>> records, String instantTime) {\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    startAsyncCleaningIfEnabled(this, instantTime);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMzAxMw==", "bodyText": "why are we not hanging onto the returned object?\n\nmy bad. done", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484933013", "createdAt": "2020-09-08T13:46:12Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.client.embedded.SparkEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieSparkIndexFactory;\n+import org.apache.hudi.table.BaseHoodieTimelineArchiveLog;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieSparkTimelineArchiveLog;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.SparkMarkerFiles;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.compact.SparkCompactHelpers;\n+import org.apache.hudi.table.upgrade.SparkUpgradeDowngrade;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HoodieSparkWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T,\n+    JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkWriteClient.class);\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending, Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Register hudi classes for Kryo serialization.\n+   *\n+   * @param conf instance of SparkConf\n+   * @return SparkConf\n+   */\n+  public static SparkConf registerClasses(SparkConf conf) {\n+    conf.registerKryoClasses(new Class[]{HoodieWriteConfig.class, HoodieRecord.class, HoodieKey.class});\n+    return conf;\n+  }\n+\n+  @Override\n+  protected HoodieIndex<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createIndex(HoodieWriteConfig writeConfig) {\n+    return HoodieSparkIndexFactory.createIndex(config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata) {\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStats(instantTime, stats, extraMetadata);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieSparkTable.create(config, context);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> filterExists(JavaRDD<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieTable table = HoodieSparkTable.create(config, context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    JavaRDD<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.filter(v1 -> !v1.isCurrentLocationKnown());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS);\n+    table.bootstrap(context, extraMetadata);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> upsert(JavaRDD<HoodieRecord<T>> records, String instantTime) {\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    startAsyncCleaningIfEnabled(this, instantTime);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMDc0Mg=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDg5MjYxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToxNDowMVrOHOJr5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToxNDowMVrOHOJr5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMDgwNw==", "bodyText": "same here and everywhere else.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484600807", "createdAt": "2020-09-08T01:14:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieSparkWriteClient.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import com.codahale.metrics.Timer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.client.embedded.SparkEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTableVersion;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieSparkIndexFactory;\n+import org.apache.hudi.table.BaseHoodieTimelineArchiveLog;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieSparkTimelineArchiveLog;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.SparkMarkerFiles;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.compact.SparkCompactHelpers;\n+import org.apache.hudi.table.upgrade.SparkUpgradeDowngrade;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HoodieSparkWriteClient<T extends HoodieRecordPayload> extends AbstractHoodieWriteClient<T,\n+    JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkWriteClient.class);\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig clientConfig) {\n+    super(context, clientConfig);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending) {\n+    super(context, writeConfig, rollbackPending);\n+  }\n+\n+  public HoodieSparkWriteClient(HoodieEngineContext context, HoodieWriteConfig writeConfig, boolean rollbackPending, Option<BaseEmbeddedTimelineService> timelineService) {\n+    super(context, writeConfig, rollbackPending, timelineService);\n+  }\n+\n+  /**\n+   * Register hudi classes for Kryo serialization.\n+   *\n+   * @param conf instance of SparkConf\n+   * @return SparkConf\n+   */\n+  public static SparkConf registerClasses(SparkConf conf) {\n+    conf.registerKryoClasses(new Class[]{HoodieWriteConfig.class, HoodieRecord.class, HoodieKey.class});\n+    return conf;\n+  }\n+\n+  @Override\n+  protected HoodieIndex<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createIndex(HoodieWriteConfig writeConfig) {\n+    return HoodieSparkIndexFactory.createIndex(config);\n+  }\n+\n+  @Override\n+  public boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses, Option<Map<String, String>> extraMetadata) {\n+    List<HoodieWriteStat> stats = writeStatuses.map(WriteStatus::getStat).collect();\n+    return commitStats(instantTime, stats, extraMetadata);\n+  }\n+\n+  @Override\n+  protected HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> createTable(HoodieWriteConfig config, Configuration hadoopConf) {\n+    return HoodieSparkTable.create(config, context);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> filterExists(JavaRDD<HoodieRecord<T>> hoodieRecords) {\n+    // Create a Hoodie table which encapsulated the commits and files visible\n+    HoodieTable table = HoodieSparkTable.create(config, context);\n+    Timer.Context indexTimer = metrics.getIndexCtx();\n+    JavaRDD<HoodieRecord<T>> recordsWithLocation = getIndex().tagLocation(hoodieRecords, context, table);\n+    metrics.updateIndexMetrics(LOOKUP_STR, metrics.getDurationInMs(indexTimer == null ? 0L : indexTimer.stop()));\n+    return recordsWithLocation.filter(v1 -> !v1.isCurrentLocationKnown());\n+  }\n+\n+  /**\n+   * Main API to run bootstrap to hudi.\n+   */\n+  @Override\n+  public void bootstrap(Option<Map<String, String>> extraMetadata) {\n+    if (rollbackPending) {\n+      rollBackInflightBootstrap();\n+    }\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS);\n+    table.bootstrap(context, extraMetadata);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> upsert(JavaRDD<HoodieRecord<T>> records, String instantTime) {\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT);\n+    startAsyncCleaningIfEnabled(this, instantTime);\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> result = table.upsert(context, instantTime, records);\n+    if (result.getIndexLookupDuration().isPresent()) {\n+      metrics.updateIndexMetrics(LOOKUP_STR, result.getIndexLookupDuration().get().toMillis());\n+    }\n+    return postWrite(result, instantTime, table);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> upsertPreppedRecords(JavaRDD<HoodieRecord<T>> preppedRecords, String instantTime) {\n+    HoodieSparkTable table = (HoodieSparkTable) getTableAndInitCtx(WriteOperationType.UPSERT_PREPPED, instantTime);\n+    table.validateUpsertSchema();\n+    setOperationType(WriteOperationType.UPSERT_PREPPED);\n+    startAsyncCleaningIfEnabled(this, instantTime);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDg5ODQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/embedded/SparkEmbeddedTimelineService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToxODoxMFrOHOJvHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNTozNzozNlrOHOjC5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMTYzMQ==", "bodyText": "I think we can eliminate the need for breaking this up into spark vs non-spark, by just passing in the host. This class does not make much sense being broken up.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484601631", "createdAt": "2020-09-08T01:18:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/embedded/SparkEmbeddedTimelineService.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.embedded;\n+\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+\n+/**\n+ * Spark implementation of Timeline Service.\n+ */\n+public class SparkEmbeddedTimelineService extends BaseEmbeddedTimelineService {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkEmbeddedTimelineService.class);\n+\n+  public SparkEmbeddedTimelineService(HoodieEngineContext context, FileSystemViewStorageConfig config) {\n+    super(context, config);\n+  }\n+\n+  @Override\n+  public void setHostAddrFromContext(HoodieEngineContext context) {\n+    SparkConf sparkConf = HoodieSparkEngineContext.getSparkContext(context).getConf();\n+    String hostAddr = sparkConf.get(\"spark.driver.host\", null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAxNjI5NA==", "bodyText": "I think we can eliminate the need for breaking this up into spark vs non-spark, by just passing in the host. This class does not make much sense being broken up.\n\ndone, add hoodie.embed.timeline.server.host to HoodieWriteConfig,  it can be obtained via method getEmbeddedServerHost()\nThis is not the same as before(acquired from sparkConf). users who enabled the embedded timeline service should config this hostaddr additionally.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485016294", "createdAt": "2020-09-08T15:37:36Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/embedded/SparkEmbeddedTimelineService.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.embedded;\n+\n+import org.apache.hudi.client.embebbed.BaseEmbeddedTimelineService;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+\n+/**\n+ * Spark implementation of Timeline Service.\n+ */\n+public class SparkEmbeddedTimelineService extends BaseEmbeddedTimelineService {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkEmbeddedTimelineService.class);\n+\n+  public SparkEmbeddedTimelineService(HoodieEngineContext context, FileSystemViewStorageConfig config) {\n+    super(context, config);\n+  }\n+\n+  @Override\n+  public void setHostAddrFromContext(HoodieEngineContext context) {\n+    SparkConf sparkConf = HoodieSparkEngineContext.getSparkContext(context).getConf();\n+    String hostAddr = sparkConf.get(\"spark.driver.host\", null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMTYzMQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDkwNTE1OnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/HoodieSparkHBaseIndex.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToyMzoxMlrOHOJywg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToyMzoxMlrOHOJywg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMjU2Mg==", "bodyText": "note to self: make sure these methods are now in the base class", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484602562", "createdAt": "2020-09-08T01:23:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/hbase/HoodieSparkHBaseIndex.java", "diffHunk": "@@ -18,169 +18,60 @@\n \n package org.apache.hudi.index.hbase;\n \n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.BufferedMutator;\n+import org.apache.hadoop.hbase.client.Delete;\n+import org.apache.hadoop.hbase.client.Get;\n+import org.apache.hadoop.hbase.client.HTable;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.hudi.client.WriteStatus;\n import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.HoodieSparkEngineContext;\n import org.apache.hudi.common.model.HoodieKey;\n import org.apache.hudi.common.model.HoodieRecord;\n import org.apache.hudi.common.model.HoodieRecordLocation;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.common.table.timeline.HoodieTimeline;\n import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.ReflectionUtils;\n import org.apache.hudi.common.util.collection.Pair;\n-import org.apache.hudi.config.HoodieHBaseIndexConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.hudi.exception.HoodieDependentSystemUnavailableException;\n import org.apache.hudi.exception.HoodieIndexException;\n-import org.apache.hudi.index.HoodieIndex;\n import org.apache.hudi.table.HoodieTable;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hbase.HBaseConfiguration;\n-import org.apache.hadoop.hbase.HRegionLocation;\n-import org.apache.hadoop.hbase.TableName;\n-import org.apache.hadoop.hbase.client.BufferedMutator;\n-import org.apache.hadoop.hbase.client.Connection;\n-import org.apache.hadoop.hbase.client.ConnectionFactory;\n-import org.apache.hadoop.hbase.client.Delete;\n-import org.apache.hadoop.hbase.client.Get;\n-import org.apache.hadoop.hbase.client.HTable;\n-import org.apache.hadoop.hbase.client.Mutation;\n-import org.apache.hadoop.hbase.client.Put;\n-import org.apache.hadoop.hbase.client.RegionLocator;\n-import org.apache.hadoop.hbase.client.Result;\n-import org.apache.hadoop.hbase.util.Bytes;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n import org.apache.spark.SparkConf;\n import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.api.java.function.Function2;\n+import scala.Tuple2;\n \n import java.io.IOException;\n-import java.io.Serializable;\n import java.util.ArrayList;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n \n-import scala.Tuple2;\n+public class HoodieSparkHBaseIndex<T extends HoodieRecordPayload> extends BaseHoodieHBaseIndex<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n \n-/**\n- * Hoodie Index implementation backed by HBase.\n- */\n-public class HBaseIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+  private static final Logger LOG = LogManager.getLogger(HoodieSparkHBaseIndex.class);\n \n   public static final String DEFAULT_SPARK_EXECUTOR_INSTANCES_CONFIG_NAME = \"spark.executor.instances\";\n   public static final String DEFAULT_SPARK_DYNAMIC_ALLOCATION_ENABLED_CONFIG_NAME = \"spark.dynamicAllocation.enabled\";\n   public static final String DEFAULT_SPARK_DYNAMIC_ALLOCATION_MAX_EXECUTORS_CONFIG_NAME =\n       \"spark.dynamicAllocation.maxExecutors\";\n \n-  private static final byte[] SYSTEM_COLUMN_FAMILY = Bytes.toBytes(\"_s\");\n-  private static final byte[] COMMIT_TS_COLUMN = Bytes.toBytes(\"commit_ts\");\n-  private static final byte[] FILE_NAME_COLUMN = Bytes.toBytes(\"file_name\");\n-  private static final byte[] PARTITION_PATH_COLUMN = Bytes.toBytes(\"partition_path\");\n-  private static final int SLEEP_TIME_MILLISECONDS = 100;\n-\n-  private static final Logger LOG = LogManager.getLogger(HBaseIndex.class);\n-  private static Connection hbaseConnection = null;\n-  private HBaseIndexQPSResourceAllocator hBaseIndexQPSResourceAllocator = null;\n-  private float qpsFraction;\n-  private int maxQpsPerRegionServer;\n-  /**\n-   * multiPutBatchSize will be computed and re-set in updateLocation if\n-   * {@link HoodieHBaseIndexConfig#HBASE_PUT_BATCH_SIZE_AUTO_COMPUTE_PROP} is set to true.\n-   */\n-  private Integer multiPutBatchSize;\n-  private Integer numRegionServersForTable;\n-  private final String tableName;\n-  private HBasePutBatchSizeCalculator putBatchSizeCalculator;\n-\n-  public HBaseIndex(HoodieWriteConfig config) {\n+  public HoodieSparkHBaseIndex(HoodieWriteConfig config) {\n     super(config);\n-    this.tableName = config.getHbaseTableName();\n-    addShutDownHook();\n-    init(config);\n-  }\n-\n-  private void init(HoodieWriteConfig config) {\n-    this.multiPutBatchSize = config.getHbaseIndexGetBatchSize();\n-    this.qpsFraction = config.getHbaseIndexQPSFraction();\n-    this.maxQpsPerRegionServer = config.getHbaseIndexMaxQPSPerRegionServer();\n-    this.putBatchSizeCalculator = new HBasePutBatchSizeCalculator();\n-    this.hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);\n-  }\n-\n-  public HBaseIndexQPSResourceAllocator createQPSResourceAllocator(HoodieWriteConfig config) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDkxMDAxOnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/simple/HoodieSparkGlobalSimpleIndex.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToyNjoxNlrOHOJ1Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToyNjoxNlrOHOJ1Zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMzIzOA==", "bodyText": "note to self: make sure these methods are in the base class now", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484603238", "createdAt": "2020-09-08T01:26:16Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/index/simple/HoodieSparkGlobalSimpleIndex.java", "diffHunk": "@@ -71,43 +75,14 @@ public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n    * @return {@link JavaRDD} of records with record locations set\n    */\n   protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> inputRecordRDD, JavaSparkContext jsc,\n-                                                         HoodieTable<T> hoodieTable) {\n+                                                         HoodieTable hoodieTable) {\n \n     JavaPairRDD<String, HoodieRecord<T>> keyedInputRecordRDD = inputRecordRDD.mapToPair(entry -> new Tuple2<>(entry.getRecordKey(), entry));\n     JavaPairRDD<HoodieKey, HoodieRecordLocation> allRecordLocationsInTable = fetchAllRecordLocations(jsc, hoodieTable,\n         config.getGlobalSimpleIndexParallelism());\n     return getTaggedRecords(keyedInputRecordRDD, allRecordLocationsInTable);\n   }\n \n-  /**\n-   * Fetch record locations for passed in {@link HoodieKey}s.\n-   *\n-   * @param jsc         instance of {@link JavaSparkContext} to use\n-   * @param hoodieTable instance of {@link HoodieTable} of interest\n-   * @param parallelism parallelism to use\n-   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n-   */\n-  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchAllRecordLocations(JavaSparkContext jsc,\n-                                                                                 HoodieTable hoodieTable,\n-                                                                                 int parallelism) {\n-    List<Pair<String, HoodieBaseFile>> latestBaseFiles = getAllBaseFilesInTable(jsc, hoodieTable);\n-    return fetchRecordLocations(jsc, hoodieTable, parallelism, latestBaseFiles);\n-  }\n-\n-  /**\n-   * Load all files for all partitions as <Partition, filename> pair RDD.\n-   */\n-  protected List<Pair<String, HoodieBaseFile>> getAllBaseFilesInTable(final JavaSparkContext jsc, final HoodieTable hoodieTable) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDkxNDA3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/HoodieSparkMergeHandle.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMToyOToxMFrOHOJ3mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNTo1Mjo1NlrOHOjrFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMzgwMA==", "bodyText": "at the MergeHandle level, we need not introduce any notion of RDDs. the io package should be free of spark already. All we need to do is to pass in the taskContextSupplier correctly? This is a large outstanding issue we need to resolve", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484603800", "createdAt": "2020-09-08T01:29:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/HoodieSparkMergeHandle.java", "diffHunk": "@@ -54,9 +60,9 @@\n import java.util.Set;\n \n @SuppressWarnings(\"Duplicates\")\n-public class HoodieMergeHandle<T extends HoodieRecordPayload> extends HoodieWriteHandle<T> {\n+public class HoodieSparkMergeHandle<T extends HoodieRecordPayload> extends HoodieWriteHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNjU4MQ==", "bodyText": "at the MergeHandle level, we need not introduce any notion of RDDs. the io package should be free of spark already. All we need to do is to pass in the taskContextSupplier correctly? This is a large outstanding issue we need to resolve\n\nActually not yet. #1756 added support for rollbacks using marker files, and MarkerFiles is spark related.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485026581", "createdAt": "2020-09-08T15:52:56Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/HoodieSparkMergeHandle.java", "diffHunk": "@@ -54,9 +60,9 @@\n import java.util.Set;\n \n @SuppressWarnings(\"Duplicates\")\n-public class HoodieMergeHandle<T extends HoodieRecordPayload> extends HoodieWriteHandle<T> {\n+public class HoodieSparkMergeHandle<T extends HoodieRecordPayload> extends HoodieWriteHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwMzgwMA=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDkxNTUzOnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/HoodieSparkMergeHandle.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMTozMDowOFrOHOJ4aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNTo1NDoyN1rOHOju_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDAxMQ==", "bodyText": "please refrain from moving methods around within the file. it makes life hard during review :(", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484604011", "createdAt": "2020-09-08T01:30:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/HoodieSparkMergeHandle.java", "diffHunk": "@@ -71,34 +77,25 @@\n   protected boolean useWriterSchema;\n   private HoodieBaseFile baseFileToMerge;\n \n-  public HoodieMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n-       Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, SparkTaskContextSupplier sparkTaskContextSupplier) {\n-    super(config, instantTime, partitionPath, fileId, hoodieTable, sparkTaskContextSupplier);\n+  public HoodieSparkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable hoodieTable,\n+                                Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, TaskContextSupplier taskContextSupplier) {\n+    super(config, instantTime, partitionPath, fileId, hoodieTable, taskContextSupplier);\n     init(fileId, recordItr);\n     init(fileId, partitionPath, hoodieTable.getBaseFileOnlyView().getLatestBaseFile(partitionPath, fileId).get());\n   }\n \n   /**\n    * Called by compactor code path.\n    */\n-  public HoodieMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n-      Map<String, HoodieRecord<T>> keyToNewRecords, String partitionPath, String fileId,\n-      HoodieBaseFile dataFileToBeMerged, SparkTaskContextSupplier sparkTaskContextSupplier) {\n-    super(config, instantTime, partitionPath, fileId, hoodieTable, sparkTaskContextSupplier);\n+  public HoodieSparkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable hoodieTable,\n+                                Map<String, HoodieRecord<T>> keyToNewRecords, String partitionPath, String fileId,\n+                                HoodieBaseFile dataFileToBeMerged, TaskContextSupplier taskContextSupplier) {\n+    super(config, instantTime, partitionPath, fileId, hoodieTable, taskContextSupplier);\n     this.keyToNewRecords = keyToNewRecords;\n     this.useWriterSchema = true;\n     init(fileId, this.partitionPath, dataFileToBeMerged);\n   }\n \n-  @Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAyNzU4Mw==", "bodyText": "please refrain from moving methods around within the file. it makes life hard during review :(\n\nsorry for the inconvenient, let me see what I can do to avoid this :)", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485027583", "createdAt": "2020-09-08T15:54:27Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/HoodieSparkMergeHandle.java", "diffHunk": "@@ -71,34 +77,25 @@\n   protected boolean useWriterSchema;\n   private HoodieBaseFile baseFileToMerge;\n \n-  public HoodieMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n-       Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, SparkTaskContextSupplier sparkTaskContextSupplier) {\n-    super(config, instantTime, partitionPath, fileId, hoodieTable, sparkTaskContextSupplier);\n+  public HoodieSparkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable hoodieTable,\n+                                Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId, TaskContextSupplier taskContextSupplier) {\n+    super(config, instantTime, partitionPath, fileId, hoodieTable, taskContextSupplier);\n     init(fileId, recordItr);\n     init(fileId, partitionPath, hoodieTable.getBaseFileOnlyView().getLatestBaseFile(partitionPath, fileId).get());\n   }\n \n   /**\n    * Called by compactor code path.\n    */\n-  public HoodieMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T> hoodieTable,\n-      Map<String, HoodieRecord<T>> keyToNewRecords, String partitionPath, String fileId,\n-      HoodieBaseFile dataFileToBeMerged, SparkTaskContextSupplier sparkTaskContextSupplier) {\n-    super(config, instantTime, partitionPath, fileId, hoodieTable, sparkTaskContextSupplier);\n+  public HoodieSparkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable hoodieTable,\n+                                Map<String, HoodieRecord<T>> keyToNewRecords, String partitionPath, String fileId,\n+                                HoodieBaseFile dataFileToBeMerged, TaskContextSupplier taskContextSupplier) {\n+    super(config, instantTime, partitionPath, fileId, hoodieTable, taskContextSupplier);\n     this.keyToNewRecords = keyToNewRecords;\n     this.useWriterSchema = true;\n     init(fileId, this.partitionPath, dataFileToBeMerged);\n   }\n \n-  @Override", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDAxMQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDkxNzcwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/SparkAppendHandleFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMTozMToxNFrOHOJ5hA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMjoyMjozNlrOHXHaLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDI5Mg==", "bodyText": "same here. we need to make sure these factory methods don't have spark vs non-spark versions", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484604292", "createdAt": "2020-09-08T01:31:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/SparkAppendHandleFactory.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * Factory to create {@link HoodieSparkAppendHandle}.\n+ */\n+public class SparkAppendHandleFactory<T extends HoodieRecordPayload> extends WriteHandleFactory<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDAwMDY4Nw==", "bodyText": "same here. we need to make sure these factory methods don't have spark vs non-spark versions\n\ndone", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494000687", "createdAt": "2020-09-24T02:22:36Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/SparkAppendHandleFactory.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * Factory to create {@link HoodieSparkAppendHandle}.\n+ */\n+public class SparkAppendHandleFactory<T extends HoodieRecordPayload> extends WriteHandleFactory<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDI5Mg=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDkxODIzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/SparkCreateHandleFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMTozMTo0MVrOHOJ50Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzo0NzoxNVrOHOeAew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDM2OQ==", "bodyText": "same. is there a way to not make these spark specific", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484604369", "createdAt": "2020-09-08T01:31:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/SparkCreateHandleFactory.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+public class SparkCreateHandleFactory<T extends HoodieRecordPayload> extends WriteHandleFactory<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n+\n+  @Override\n+  public HoodieSparkCreateHandle create(final HoodieWriteConfig hoodieConfig,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDkzMzc1NQ==", "bodyText": "same. is there a way to not make these spark specific\n\nI'll give a try", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484933755", "createdAt": "2020-09-08T13:47:15Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/io/SparkCreateHandleFactory.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+public class SparkCreateHandleFactory<T extends HoodieRecordPayload> extends WriteHandleFactory<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\n+\n+  @Override\n+  public HoodieSparkCreateHandle create(final HoodieWriteConfig hoodieConfig,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNDM2OQ=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDkyNDM2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkWorkloadProfile.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMTozNjoyNlrOHOJ9ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNjo0Mzo1MFrOHOlm1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTI5MA==", "bodyText": "we can actually try and keep this generic and just pass in what we need from taggedRecords to constructor instead of the entire thing", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484605290", "createdAt": "2020-09-08T01:36:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkWorkloadProfile.java", "diffHunk": "@@ -22,49 +22,22 @@\n import org.apache.hudi.common.model.HoodieRecordLocation;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.util.Option;\n-\n import org.apache.spark.api.java.JavaRDD;\n+import scala.Tuple2;\n \n-import java.io.Serializable;\n-import java.util.HashMap;\n import java.util.Map;\n-import java.util.Set;\n-\n-import scala.Tuple2;\n \n /**\n- * Information about incoming records for upsert/insert obtained either via sampling or introspecting the data fully.\n- * <p>\n- * TODO(vc): Think about obtaining this directly from index.tagLocation\n+ * Spark implementation of {@link BaseWorkloadProfile}.\n+ * @param <T>\n  */\n-public class WorkloadProfile<T extends HoodieRecordPayload> implements Serializable {\n-\n-  /**\n-   * Input workload.\n-   */\n-  private final JavaRDD<HoodieRecord<T>> taggedRecords;\n-\n-  /**\n-   * Computed workload profile.\n-   */\n-  private final HashMap<String, WorkloadStat> partitionPathStatMap;\n-\n-  /**\n-   * Global workloadStat.\n-   */\n-  private final WorkloadStat globalStat;\n-\n-  public WorkloadProfile(JavaRDD<HoodieRecord<T>> taggedRecords) {\n-    this.taggedRecords = taggedRecords;\n-    this.partitionPathStatMap = new HashMap<>();\n-    this.globalStat = new WorkloadStat();\n-    buildProfile();\n+public class SparkWorkloadProfile<T extends HoodieRecordPayload> extends BaseWorkloadProfile<JavaRDD<HoodieRecord<T>>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTA1ODI2MQ==", "bodyText": "we can actually try and keep this generic and just pass in what we need from taggedRecords to constructor instead of the entire thing\n\ndone", "url": "https://github.com/apache/hudi/pull/1827#discussion_r485058261", "createdAt": "2020-09-08T16:43:50Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkWorkloadProfile.java", "diffHunk": "@@ -22,49 +22,22 @@\n import org.apache.hudi.common.model.HoodieRecordLocation;\n import org.apache.hudi.common.model.HoodieRecordPayload;\n import org.apache.hudi.common.util.Option;\n-\n import org.apache.spark.api.java.JavaRDD;\n+import scala.Tuple2;\n \n-import java.io.Serializable;\n-import java.util.HashMap;\n import java.util.Map;\n-import java.util.Set;\n-\n-import scala.Tuple2;\n \n /**\n- * Information about incoming records for upsert/insert obtained either via sampling or introspecting the data fully.\n- * <p>\n- * TODO(vc): Think about obtaining this directly from index.tagLocation\n+ * Spark implementation of {@link BaseWorkloadProfile}.\n+ * @param <T>\n  */\n-public class WorkloadProfile<T extends HoodieRecordPayload> implements Serializable {\n-\n-  /**\n-   * Input workload.\n-   */\n-  private final JavaRDD<HoodieRecord<T>> taggedRecords;\n-\n-  /**\n-   * Computed workload profile.\n-   */\n-  private final HashMap<String, WorkloadStat> partitionPathStatMap;\n-\n-  /**\n-   * Global workloadStat.\n-   */\n-  private final WorkloadStat globalStat;\n-\n-  public WorkloadProfile(JavaRDD<HoodieRecord<T>> taggedRecords) {\n-    this.taggedRecords = taggedRecords;\n-    this.partitionPathStatMap = new HashMap<>();\n-    this.globalStat = new WorkloadStat();\n-    buildProfile();\n+public class SparkWorkloadProfile<T extends HoodieRecordPayload> extends BaseWorkloadProfile<JavaRDD<HoodieRecord<T>>> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTI5MA=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDkyNTkyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwMTozNzozMFrOHOJ-TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxMzo1NTo0OFrOHOeZqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTUxNw==", "bodyText": "hmmm? why do we return null here", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484605517", "createdAt": "2020-09-08T01:37:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "diffHunk": "@@ -77,34 +81,44 @@\n import org.apache.parquet.hadoop.ParquetReader;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n-public class BootstrapCommitActionExecutor<T extends HoodieRecordPayload<T>>\n-    extends BaseCommitActionExecutor<T, HoodieBootstrapWriteMetadata> {\n+public class SparkBootstrapCommitActionExecutor<T extends HoodieRecordPayload>\n+    extends BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>, HoodieBootstrapWriteMetadata> {\n \n-  private static final Logger LOG = LogManager.getLogger(BootstrapCommitActionExecutor.class);\n+  private static final Logger LOG = LogManager.getLogger(SparkBootstrapCommitActionExecutor.class);\n   protected String bootstrapSchema = null;\n   private transient FileSystem bootstrapSourceFileSystem;\n \n-  public BootstrapCommitActionExecutor(JavaSparkContext jsc, HoodieWriteConfig config, HoodieTable<?> table,\n-      Option<Map<String, String>> extraMetadata) {\n-    super(jsc, new HoodieWriteConfig.Builder().withProps(config.getProps())\n-        .withAutoCommit(true).withWriteStatusClass(BootstrapWriteStatus.class)\n-        .withBulkInsertParallelism(config.getBootstrapParallelism())\n-        .build(), table, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS, WriteOperationType.BOOTSTRAP,\n+  public SparkBootstrapCommitActionExecutor(HoodieSparkEngineContext context,\n+                                            HoodieWriteConfig config,\n+                                            HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> table,\n+                                            Option<Map<String, String>> extraMetadata) {\n+    super(context, new HoodieWriteConfig.Builder().withProps(config.getProps())\n+            .withAutoCommit(true).withWriteStatusClass(BootstrapWriteStatus.class)\n+            .withBulkInsertParallelism(config.getBootstrapParallelism())\n+            .build(), table, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS, WriteOperationType.BOOTSTRAP,\n         extraMetadata);\n     bootstrapSourceFileSystem = FSUtils.getFs(config.getBootstrapSourceBasePath(), hadoopConf);\n   }\n \n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDk0MDIwMw==", "bodyText": "hmmm? why do we return null here\n\nBootstrapCommitActionExecutor dose not need this method actually, inherited from its parent class.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r484940203", "createdAt": "2020-09-08T13:55:48Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/bootstrap/SparkBootstrapCommitActionExecutor.java", "diffHunk": "@@ -77,34 +81,44 @@\n import org.apache.parquet.hadoop.ParquetReader;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.schema.MessageType;\n-import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaPairRDD;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n \n import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n \n-public class BootstrapCommitActionExecutor<T extends HoodieRecordPayload<T>>\n-    extends BaseCommitActionExecutor<T, HoodieBootstrapWriteMetadata> {\n+public class SparkBootstrapCommitActionExecutor<T extends HoodieRecordPayload>\n+    extends BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>, HoodieBootstrapWriteMetadata> {\n \n-  private static final Logger LOG = LogManager.getLogger(BootstrapCommitActionExecutor.class);\n+  private static final Logger LOG = LogManager.getLogger(SparkBootstrapCommitActionExecutor.class);\n   protected String bootstrapSchema = null;\n   private transient FileSystem bootstrapSourceFileSystem;\n \n-  public BootstrapCommitActionExecutor(JavaSparkContext jsc, HoodieWriteConfig config, HoodieTable<?> table,\n-      Option<Map<String, String>> extraMetadata) {\n-    super(jsc, new HoodieWriteConfig.Builder().withProps(config.getProps())\n-        .withAutoCommit(true).withWriteStatusClass(BootstrapWriteStatus.class)\n-        .withBulkInsertParallelism(config.getBootstrapParallelism())\n-        .build(), table, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS, WriteOperationType.BOOTSTRAP,\n+  public SparkBootstrapCommitActionExecutor(HoodieSparkEngineContext context,\n+                                            HoodieWriteConfig config,\n+                                            HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> table,\n+                                            Option<Map<String, String>> extraMetadata) {\n+    super(context, new HoodieWriteConfig.Builder().withProps(config.getProps())\n+            .withAutoCommit(true).withWriteStatusClass(BootstrapWriteStatus.class)\n+            .withBulkInsertParallelism(config.getBootstrapParallelism())\n+            .build(), table, HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS, WriteOperationType.BOOTSTRAP,\n         extraMetadata);\n     bootstrapSourceFileSystem = FSUtils.getFs(config.getBootstrapSourceBasePath(), hadoopConf);\n   }\n \n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDYwNTUxNw=="}, "originalCommit": {"oid": "7aedc0bdc1565d47b044487aab04b659167bd086"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4MDU4OTExOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "isResolved": false, "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxODoyNjozNFrOHVdRzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QwMToyMjoxN1rOHWRtmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw==", "bodyText": "I think we should leave this abstract and let the engines implement this?  even for Java. Its better to have a HoodieJavaEngineContext. From what I can see, this is not overridden in HoodieSparkEngineContext and thus we lose the parallel execution that we currently have with Spark with this change.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492261837", "createdAt": "2020-09-21T18:26:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2NDM1Mg==", "bodyText": "Also these APIs should take in a parallelism parameter, no?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492264352", "createdAt": "2020-09-21T18:31:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzNDQwNQ==", "bodyText": "I think we should leave this abstract and let the engines implement this? even for Java. Its better to have a HoodieJavaEngineContext. From what I can see, this is not overridden in HoodieSparkEngineContext and thus we lose the parallel execution that we currently have with Spark with this change.\n\nas we discussed before, parallelDo model need a function as input parameter, Unfortunately, different engines need different type function, its hard to align them in an abstract parallelDo method. so we agreed to use  java.util.function.Function as the unified input function. in this way, there is no need to distinguish spark and flink, no need to make it abstract and the parallelism is not needed too. its just java, can be implemented directly.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492434405", "createdAt": "2020-09-22T01:41:42Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQ0OTU0Mw==", "bodyText": "@wangxianghu functionality wise, you are correct. it can be implemented just using Java. but, we do parallelization of different pieces of code e.g deletion of files in parallel using spark for a reason. It significantly speeds these up, for large tables.\nAll I am saying is to implement the HoodieSparkEngineContext#map like below\n public <I, O> List<O> map(List<I> data, Function<I, O> func, int parallelism) {\n    return javaSparkContext.parallelize(data, parallelism).map(func).collect();\n }\n\nsimilarly for the other two methods. I don't see any issues with this. do you?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492449543", "createdAt": "2020-09-22T02:57:24Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQ1MzI5OA==", "bodyText": "@wangxianghu functionality wise, you are correct. it can be implemented just using Java. but, we do parallelization of different pieces of code e.g deletion of files in parallel using spark for a reason. It significantly speeds these up, for large tables.\nAll I am saying is to implement the HoodieSparkEngineContext#map like below\n public <I, O> List<O> map(List<I> data, Function<I, O> func, int parallelism) {\n    return javaSparkContext.parallelize(data, parallelism).map(func).collect();\n }\n\nsimilarly for the other two methods. I don't see any issues with this. do you?\n\nI know what you mean.\nwhat I am saying is that the func in HoodieSparkEngineContext#map and HoodieEngineContext#map is not the same type.\nfor HoodieEngineContext#map it is java.util.function.Function,\nfor HoodieSparkEngineContext#map  it is org.apache.spark.api.java.function.Function.\nHoodieSparkEngineContext#map can not override from HoodieEngineContext#map", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492453298", "createdAt": "2020-09-22T03:14:48Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQ2MDcxOQ==", "bodyText": "Is it possible to take a java.util.function.Function and then within HoodieSparkEngineContext#map wrap that into a org.apache.spark.api.java.function.Function ?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492460719", "createdAt": "2020-09-22T03:52:43Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQ2ODU0MA==", "bodyText": "Is it possible to take a java.util.function.Function and then within HoodieSparkEngineContext#map wrap that into a org.apache.spark.api.java.function.Function ?\n\nlet me try", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492468540", "createdAt": "2020-09-22T04:31:10Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQ3NDEwOA==", "bodyText": "In Spark, there is a functional interface defined like this\npackage org.apache.spark.api.java.function;\n\nimport java.io.Serializable;\n\n/**\n * Base interface for functions whose return types do not create special RDDs. PairFunction and\n * DoubleFunction are handled separately, to allow PairRDDs and DoubleRDDs to be constructed\n * when mapping RDDs of other types.\n */\n@FunctionalInterface\npublic interface Function<T1, R> extends Serializable {\n  R call(T1 v1) throws Exception;\n}", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492474108", "createdAt": "2020-09-22T04:57:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQ3NDQ5NQ==", "bodyText": "when the use passes in a regular lambda, into rdd.map(), this is what it gets converted into", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492474495", "createdAt": "2020-09-22T04:59:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQ5MzczOQ==", "bodyText": "when the use passes in a regular lambda, into rdd.map(), this is what it gets converted into\n\nThe serializable issue can be solved by introducing a seriableFuncition to replace java.util.function.Function\npublic interface SerializableFunction<I, O> extends Serializable {\n  O call(I v1) throws Exception;\n}\n\nHoodieEngineContext can be\npublic abstract class HoodieEngineContext {\n  public abstract  <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism) ;\n}\n\nHoodieSparkEngineContext can be\npublic class HoodieSparkEngineContext extends HoodieEngineContext {\n  private static JavaSparkContext jsc;\n\n  // tmp\n  static {\n    SparkConf conf = new SparkConf()\n        .setMaster(\"local[4]\")\n        .set(\"spark.driver.host\",\"localhost\")\n        .setAppName(\"HoodieSparkEngineContext\");\n\n    jsc = new JavaSparkContext(conf);\n  }\n  \n  @Override\n  public <I, O> List<O> map(List<I> data, SerializableFunction<I, O> func, int parallelism) {\n    return jsc.parallelize(data, parallelism).map(func::call).collect();\n  }\n}\n\nthis works :)", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492493739", "createdAt": "2020-09-22T06:14:07Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg3MjUwNA==", "bodyText": "@wangxianghu This is awesome. Hopefully this can reduce the amount of code you need to write for Flink significantly. TestMarkerFiles seems to pass, so guess the serialization etc is working as expected.\nWe can go ahead with doing more files in this approach and remerge the base/child classes back as much as possible. cc @leesf @yanghua  as well in case they have more things to add.\ncc @bvaradar as well as FYI", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492872504", "createdAt": "2020-09-22T16:26:31Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzEyMDkyMg==", "bodyText": "@wangxianghu This is awesome. Hopefully this can reduce the amount of code you need to write for Flink significantly. TestMarkerFiles seems to pass, so guess the serialization etc is working as expected.\nWe can go ahead with doing more files in this approach and remerge the base/child classes back as much as possible. cc @leesf @yanghua as well in case they have more things to add.\ncc @bvaradar as well as FYI\n\nYes, it also reduce tons of code in the refactoring. I'm working on it ,hope to finish it today or tomorrow", "url": "https://github.com/apache/hudi/pull/1827#discussion_r493120922", "createdAt": "2020-09-23T01:22:17Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/common/HoodieEngineContext.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.TaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Base class contains the context information needed by the engine at runtime. It will be extended by different\n+ * engine implementation if needed.\n+ */\n+public class HoodieEngineContext {\n+  /**\n+   * A wrapped hadoop configuration which can be serialized.\n+   */\n+  private SerializableConfiguration hadoopConf;\n+\n+  private TaskContextSupplier taskContextSupplier;\n+\n+  public HoodieEngineContext(SerializableConfiguration hadoopConf, TaskContextSupplier taskContextSupplier) {\n+    this.hadoopConf = hadoopConf;\n+    this.taskContextSupplier = taskContextSupplier;\n+  }\n+\n+  public SerializableConfiguration getHadoopConf() {\n+    return hadoopConf;\n+  }\n+\n+  public TaskContextSupplier getTaskContextSupplier() {\n+    return taskContextSupplier;\n+  }\n+\n+  public <I, O> List<O> map(List<I> data, Function<I, O> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2MTgzNw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4MDU5NjQ5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/common/HoodieSparkEngineContext.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxODoyODozNlrOHVdWNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMjoxMzozMVrOHXHRag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2Mjk2NA==", "bodyText": "can we implement versiosn of map, flatMap, forEach here which use javaSparkContext.parallelize() ? It would be good to keep this PR free of any changes in terms of whether we are executing the deletes/lists in parallel or in serial.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492262964", "createdAt": "2020-09-21T18:28:36Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/common/HoodieSparkEngineContext.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SQLContext;\n+\n+/**\n+ * A Spark engine implementation of HoodieEngineContext.\n+ */\n+public class HoodieSparkEngineContext extends HoodieEngineContext {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5ODQ0Mg==", "bodyText": "can we implement versiosn of map, flatMap, forEach here which use javaSparkContext.parallelize() ? It would be good to keep this PR free of any changes in terms of whether we are executing the deletes/lists in parallel or in serial.\n\ndone", "url": "https://github.com/apache/hudi/pull/1827#discussion_r493998442", "createdAt": "2020-09-24T02:13:31Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/common/HoodieSparkEngineContext.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SQLContext;\n+\n+/**\n+ * A Spark engine implementation of HoodieEngineContext.\n+ */\n+public class HoodieSparkEngineContext extends HoodieEngineContext {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2Mjk2NA=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4MDYwODcyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkMarkerFiles.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxODozMjowNFrOHVdd1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxODozMjowNFrOHVdd1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2NDkxOQ==", "bodyText": "we are not using parallelism here. This will lead to a perf regression w.r.t master.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492264919", "createdAt": "2020-09-21T18:32:04Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkMarkerFiles.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.hudi.bifunction.wrapper.ThrowingFunction;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.IOType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.bifunction.wrapper.BiFunctionWrapper.throwingConsumerWrapper;\n+import static org.apache.hudi.bifunction.wrapper.BiFunctionWrapper.throwingFlatMapWrapper;\n+\n+public class SparkMarkerFiles extends BaseMarkerFiles {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkMarkerFiles.class);\n+\n+  public SparkMarkerFiles(HoodieTable table, String instantTime) {\n+    super(table, instantTime);\n+  }\n+\n+  public SparkMarkerFiles(FileSystem fs, String basePath, String markerFolderPath, String instantTime) {\n+    super(fs, basePath, markerFolderPath, instantTime);\n+  }\n+\n+  @Override\n+  public boolean deleteMarkerDir(HoodieEngineContext context, int parallelism) {\n+    try {\n+      if (fs.exists(markerDirPath)) {\n+        FileStatus[] fileStatuses = fs.listStatus(markerDirPath);\n+        List<String> markerDirSubPaths = Arrays.stream(fileStatuses)\n+            .map(fileStatus -> fileStatus.getPath().toString())\n+            .collect(Collectors.toList());\n+\n+        if (markerDirSubPaths.size() > 0) {\n+          SerializableConfiguration conf = new SerializableConfiguration(fs.getConf());\n+          context.foreach(markerDirSubPaths, throwingConsumerWrapper(subPathStr -> {\n+            Path subPath = new Path(subPathStr);\n+            FileSystem fileSystem = subPath.getFileSystem(conf.get());\n+            fileSystem.delete(subPath, true);\n+          }));\n+        }\n+\n+        boolean result = fs.delete(markerDirPath, true);\n+        LOG.info(\"Removing marker directory at \" + markerDirPath);\n+        return result;\n+      }\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(ioe.getMessage(), ioe);\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Set<String> createdAndMergedDataPaths(HoodieEngineContext context, int parallelism) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4MDYxMDMxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkMarkerFiles.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxODozMjozM1rOHVde2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwMTo0NzoxMlrOHVn4Sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2NTE3Nw==", "bodyText": "Given this file is now free of Spark, we dont have the need of breaking these into base and child classes right.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492265177", "createdAt": "2020-09-21T18:32:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkMarkerFiles.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.hudi.bifunction.wrapper.ThrowingFunction;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.IOType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.bifunction.wrapper.BiFunctionWrapper.throwingConsumerWrapper;\n+import static org.apache.hudi.bifunction.wrapper.BiFunctionWrapper.throwingFlatMapWrapper;\n+\n+public class SparkMarkerFiles extends BaseMarkerFiles {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzNTUzMQ==", "bodyText": "Given this file is now free of Spark, we dont have the need of breaking these into base and child classes right.\n\nYes, this is an example to show you the bi function, if you agree with this implementation, I'll rollback them in one class", "url": "https://github.com/apache/hudi/pull/1827#discussion_r492435531", "createdAt": "2020-09-22T01:47:12Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/SparkMarkerFiles.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table;\n+\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.hudi.bifunction.wrapper.ThrowingFunction;\n+import org.apache.hudi.common.HoodieEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.model.IOType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.bifunction.wrapper.BiFunctionWrapper.throwingConsumerWrapper;\n+import static org.apache.hudi.bifunction.wrapper.BiFunctionWrapper.throwingFlatMapWrapper;\n+\n+public class SparkMarkerFiles extends BaseMarkerFiles {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI2NTE3Nw=="}, "originalCommit": {"oid": "2d1f2124db067f1379fd342b94b4fe1775ace663"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTU2OTUwOnYy", "diffSide": "RIGHT", "path": "hudi-cli/pom.xml", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1NTo1NVrOHXGE5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzowMjowNFrOHbsu8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODg1Mw==", "bodyText": "Actually, I am thinking hudi-spark-client and hudi-client-spark, which is better? Given there is a module named hudi-client-common. wdyt? @vinothchandar", "url": "https://github.com/apache/hudi/pull/1827#discussion_r493978853", "createdAt": "2020-09-24T00:55:55Z", "author": {"login": "yanghua"}, "path": "hudi-cli/pom.xml", "diffHunk": "@@ -148,7 +148,14 @@\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hudi</groupId>\n-      <artifactId>hudi-client</artifactId>\n+      <artifactId>hudi-client-common</artifactId>\n+      <version>${project.version}</version>\n+      <scope>test</scope>\n+      <type>test-jar</type>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hudi</groupId>\n+      <artifactId>hudi-spark-client</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cab19a136418580dd69fc794787b63b07b372d08"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3NDM2NQ==", "bodyText": "cc @wangxianghu probably hudi-client-spark is easier on the eyes?", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494074365", "createdAt": "2020-09-24T06:45:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-cli/pom.xml", "diffHunk": "@@ -148,7 +148,14 @@\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hudi</groupId>\n-      <artifactId>hudi-client</artifactId>\n+      <artifactId>hudi-client-common</artifactId>\n+      <version>${project.version}</version>\n+      <scope>test</scope>\n+      <type>test-jar</type>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hudi</groupId>\n+      <artifactId>hudi-spark-client</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODg1Mw=="}, "originalCommit": {"oid": "cab19a136418580dd69fc794787b63b07b372d08"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEwMDc5Nw==", "bodyText": "cc @wangxianghu probably hudi-client-spark is easier on the eyes?\n\nI am ok with both of them :)  let's rename it to hudi-client-spark", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494100797", "createdAt": "2020-09-24T07:36:31Z", "author": {"login": "wangxianghu"}, "path": "hudi-cli/pom.xml", "diffHunk": "@@ -148,7 +148,14 @@\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hudi</groupId>\n-      <artifactId>hudi-client</artifactId>\n+      <artifactId>hudi-client-common</artifactId>\n+      <version>${project.version}</version>\n+      <scope>test</scope>\n+      <type>test-jar</type>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hudi</groupId>\n+      <artifactId>hudi-spark-client</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODg1Mw=="}, "originalCommit": {"oid": "cab19a136418580dd69fc794787b63b07b372d08"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE4ODk4Ng==", "bodyText": "So, will we keep hudi-spark-client? We have had a hudi-spark module. IMHO, this naming may not seem so clear.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r498188986", "createdAt": "2020-10-01T12:00:26Z", "author": {"login": "yanghua"}, "path": "hudi-cli/pom.xml", "diffHunk": "@@ -148,7 +148,14 @@\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hudi</groupId>\n-      <artifactId>hudi-client</artifactId>\n+      <artifactId>hudi-client-common</artifactId>\n+      <version>${project.version}</version>\n+      <scope>test</scope>\n+      <type>test-jar</type>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hudi</groupId>\n+      <artifactId>hudi-spark-client</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODg1Mw=="}, "originalCommit": {"oid": "cab19a136418580dd69fc794787b63b07b372d08"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgwNjUxNA==", "bodyText": "So, will we keep hudi-spark-client? We have had a hudi-spark module. IMHO, this naming may not seem so clear.\n\n@vinothchandar @yanghua I have filed a pr to rename hudi-spark-client to hudi-client-spark : #2139\nplease take a look when free", "url": "https://github.com/apache/hudi/pull/1827#discussion_r498806514", "createdAt": "2020-10-02T13:02:04Z", "author": {"login": "wangxianghu"}, "path": "hudi-cli/pom.xml", "diffHunk": "@@ -148,7 +148,14 @@\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hudi</groupId>\n-      <artifactId>hudi-client</artifactId>\n+      <artifactId>hudi-client-common</artifactId>\n+      <version>${project.version}</version>\n+      <scope>test</scope>\n+      <type>test-jar</type>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.hudi</groupId>\n+      <artifactId>hudi-spark-client</artifactId>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3ODg1Mw=="}, "originalCommit": {"oid": "cab19a136418580dd69fc794787b63b07b372d08"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTY4OTM0OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMjowNjowNFrOHXHKYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNjo0NDowOFrOHXL4Ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5NjY0Mg==", "bodyText": "Here, I used org.apache.hudi.common.util.SizeEstimator#sizeEstimate to replace org.apache.spark.util.SizeEstimator#estimate is it ok?  @vinothchandar", "url": "https://github.com/apache/hudi/pull/1827#discussion_r493996642", "createdAt": "2020-09-24T02:06:04Z", "author": {"login": "wangxianghu"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -134,7 +138,7 @@ private void init(HoodieRecord record) {\n       writeStatus.setPartitionPath(partitionPath);\n       writeStatus.getStat().setPartitionPath(partitionPath);\n       writeStatus.getStat().setFileId(fileId);\n-      averageRecordSize = SizeEstimator.estimate(record);\n+      averageRecordSize = sizeEstimator.sizeEstimate(record);\n       try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "131aa883ec95f2cef0f9d4493ac4549770f59e8b"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3Mzg3NQ==", "bodyText": "Should be okay.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494073875", "createdAt": "2020-09-24T06:44:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -134,7 +138,7 @@ private void init(HoodieRecord record) {\n       writeStatus.setPartitionPath(partitionPath);\n       writeStatus.getStat().setPartitionPath(partitionPath);\n       writeStatus.getStat().setFileId(fileId);\n-      averageRecordSize = SizeEstimator.estimate(record);\n+      averageRecordSize = sizeEstimator.sizeEstimate(record);\n       try {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5NjY0Mg=="}, "originalCommit": {"oid": "131aa883ec95f2cef0f9d4493ac4549770f59e8b"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NTg0NTkzOnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMjozNzozM1rOHXu8SA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwMTo0MzoyMVrOHYsHkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY0ODM5Mg==", "bodyText": "can we replace more of the code to direclty just use HoodieTable instead. Need to examine cases that need an explicit HoodieSparkTable", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494648392", "createdAt": "2020-09-24T22:37:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java", "diffHunk": "@@ -92,8 +93,9 @@ public void init() throws IOException {\n     metaClient.getActiveTimeline().reload().getAllCommitsTimeline().filterCompletedInstants();\n \n     // archive\n-    HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, hadoopConf);\n-    archiveLog.archiveIfRequired(jsc);\n+    HoodieSparkTable table = HoodieSparkTable.create(cfg, context, metaClient);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY1MDcwNQ==", "bodyText": "can we replace more of the code to direclty just use HoodieTable instead. Need to examine cases that need an explicit HoodieSparkTable\n\nyes, make sense", "url": "https://github.com/apache/hudi/pull/1827#discussion_r495650705", "createdAt": "2020-09-28T01:43:21Z", "author": {"login": "wangxianghu"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestArchivedCommitsCommand.java", "diffHunk": "@@ -92,8 +93,9 @@ public void init() throws IOException {\n     metaClient.getActiveTimeline().reload().getAllCommitsTimeline().filterCompletedInstants();\n \n     // archive\n-    HoodieTimelineArchiveLog archiveLog = new HoodieTimelineArchiveLog(cfg, hadoopConf);\n-    archiveLog.archiveIfRequired(jsc);\n+    HoodieSparkTable table = HoodieSparkTable.create(cfg, context, metaClient);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY0ODM5Mg=="}, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NTg0ODk4OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRollbacksCommand.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMjozODo1NFrOHXu-GQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMjozODo1NFrOHXu-GQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY0ODg1Nw==", "bodyText": "just like this, we should try to  use the abstract class as much as we can", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494648857", "createdAt": "2020-09-24T22:38:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRollbacksCommand.java", "diffHunk": "@@ -88,7 +88,7 @@ public void init() throws IOException {\n     HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(tablePath)\n         .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n \n-    try (HoodieWriteClient client = getHoodieWriteClient(config)) {\n+    try (AbstractHoodieWriteClient client = getHoodieWriteClient(config)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NTg1NTYzOnYy", "diffSide": "LEFT", "path": "hudi-client/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMjo0MTo0N1rOHXvB8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQyMjo0MTo0N1rOHXvB8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY0OTg0MQ==", "bodyText": "Need to ensure there are no side effects in the pom due to this. i.e something that can affect bundles so forth.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494649841", "createdAt": "2020-09-24T22:41:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/pom.xml", "diffHunk": "@@ -24,294 +24,14 @@\n   <modelVersion>4.0.0</modelVersion>\n \n   <artifactId>hudi-client</artifactId>\n-  <packaging>jar</packaging>\n+  <packaging>pom</packaging>\n \n   <properties>\n     <main.basedir>${project.parent.basedir}</main.basedir>\n   </properties>\n \n-  <build>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NjUxMTE4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNDo1MjozOFrOHX06VA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNDo1MjozOFrOHX06VA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0NjE5Ng==", "bodyText": "this was actually same. fixing it", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494746196", "createdAt": "2020-09-25T04:52:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -716,32 +669,95 @@ private void rollbackPendingCommits() {\n    * @param compactionInstantTime Compaction Instant Time\n    * @return RDD of Write Status\n    */\n-  private JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n-    HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n-    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n-    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n-    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n-      rollbackInflightCompaction(inflightInstant, table);\n-      table.getMetaClient().reloadActiveTimeline();\n-    }\n-    compactionTimer = metrics.getCompactionCtx();\n-    HoodieWriteMetadata compactionMetadata = table.compact(jsc, compactionInstantTime);\n-    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n-    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n-      completeCompaction(compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n-    }\n-    return statuses;\n-  }\n+  protected abstract O compact(String compactionInstantTime, boolean shouldComplete);\n \n   /**\n    * Performs a compaction operation on a table, serially before or after an insert/upsert action.\n    */\n-  private Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n+  protected Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n     Option<String> compactionInstantTimeOpt = scheduleCompaction(extraMetadata);\n     compactionInstantTimeOpt.ifPresent(compactionInstantTime -> {\n       // inline compaction should auto commit as the user is never given control\n       compact(compactionInstantTime, true);\n     });\n     return compactionInstantTimeOpt;\n   }\n+\n+  /**\n+   * Finalize Write operation.\n+   *\n+   * @param table HoodieTable\n+   * @param instantTime Instant Time\n+   * @param stats Hoodie Write Stat\n+   */\n+  protected void finalizeWrite(HoodieTable<T, I, K, O, P> table, String instantTime, List<HoodieWriteStat> stats) {\n+    try {\n+      final Timer.Context finalizeCtx = metrics.getFinalizeCtx();\n+      table.finalizeWrite(context, instantTime, stats);\n+      if (finalizeCtx != null) {\n+        Option<Long> durationInMs = Option.of(metrics.getDurationInMs(finalizeCtx.stop()));\n+        durationInMs.ifPresent(duration -> {\n+          LOG.info(\"Finalize write elapsed time (milliseconds): \" + duration);\n+          metrics.updateFinalizeWriteMetrics(duration, stats.size());\n+        });\n+      }\n+    } catch (HoodieIOException ioe) {\n+      throw new HoodieCommitException(\"Failed to complete commit \" + instantTime + \" due to finalize errors.\", ioe);\n+    }\n+  }\n+\n+  public HoodieMetrics getMetrics() {\n+    return metrics;\n+  }\n+\n+  public HoodieIndex<T, I, K, O, P> getIndex() {\n+    return index;\n+  }\n+\n+  /**\n+   * Get HoodieTable and init {@link Timer.Context}.\n+   *\n+   * @param operationType write operation type\n+   * @param instantTime current inflight instant time\n+   * @return HoodieTable\n+   */\n+  protected abstract HoodieTable<T, I, K, O, P> getTableAndInitCtx(WriteOperationType operationType, String instantTime);\n+\n+  /**\n+   * Sets write schema from last instant since deletes may not have schema set in the config.\n+   */\n+  protected void setWriteSchemaForDeletes(HoodieTableMetaClient metaClient) {\n+    try {\n+      HoodieActiveTimeline activeTimeline = metaClient.getActiveTimeline();\n+      Option<HoodieInstant> lastInstant =\n+          activeTimeline.filterCompletedInstants().filter(s -> s.getAction().equals(metaClient.getCommitActionType()))\n+              .lastInstant();\n+      if (lastInstant.isPresent()) {\n+        HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(\n+            activeTimeline.getInstantDetails(lastInstant.get()).get(), HoodieCommitMetadata.class);\n+        if (commitMetadata.getExtraMetadata().containsKey(HoodieCommitMetadata.SCHEMA_KEY)) {\n+          config.setSchema(commitMetadata.getExtraMetadata().get(HoodieCommitMetadata.SCHEMA_KEY));\n+        } else {\n+          throw new HoodieIOException(\"Latest commit does not have any schema in commit metadata\");\n+        }\n+      } else {\n+        throw new HoodieIOException(\"Deletes issued without any prior commits\");\n+      }\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"IOException thrown while reading last commit metadata\", e);\n+    }\n+  }\n+\n+  @Override\n+  public void close() {\n+    // Stop timeline-server if running\n+    super.close();\n+    // Calling this here releases any resources used by your index, so make sure to finish any related operations\n+    // before this point\n+    this.index.close();\n+\n+    // release AsyncCleanerService\n+    AsyncCleanerService.forceShutdown(asyncCleanerService);\n+    asyncCleanerService = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 839}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NjUyMDMyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNDo1ODozMFrOHX0_fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNDo1ODozMFrOHX0_fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc0NzUxOQ==", "bodyText": "this needs to be removed. but not the issue for this PR to be bothered about may be", "url": "https://github.com/apache/hudi/pull/1827#discussion_r494747519", "createdAt": "2020-09-25T04:58:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -716,32 +669,95 @@ private void rollbackPendingCommits() {\n    * @param compactionInstantTime Compaction Instant Time\n    * @return RDD of Write Status\n    */\n-  private JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n-    HoodieTable<T> table = HoodieTable.create(config, hadoopConf);\n-    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n-    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n-    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n-      rollbackInflightCompaction(inflightInstant, table);\n-      table.getMetaClient().reloadActiveTimeline();\n-    }\n-    compactionTimer = metrics.getCompactionCtx();\n-    HoodieWriteMetadata compactionMetadata = table.compact(jsc, compactionInstantTime);\n-    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n-    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n-      completeCompaction(compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n-    }\n-    return statuses;\n-  }\n+  protected abstract O compact(String compactionInstantTime, boolean shouldComplete);\n \n   /**\n    * Performs a compaction operation on a table, serially before or after an insert/upsert action.\n    */\n-  private Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n+  protected Option<String> inlineCompact(Option<Map<String, String>> extraMetadata) {\n     Option<String> compactionInstantTimeOpt = scheduleCompaction(extraMetadata);\n     compactionInstantTimeOpt.ifPresent(compactionInstantTime -> {\n       // inline compaction should auto commit as the user is never given control\n       compact(compactionInstantTime, true);\n     });\n     return compactionInstantTimeOpt;\n   }\n+\n+  /**\n+   * Finalize Write operation.\n+   *\n+   * @param table HoodieTable\n+   * @param instantTime Instant Time\n+   * @param stats Hoodie Write Stat\n+   */\n+  protected void finalizeWrite(HoodieTable<T, I, K, O, P> table, String instantTime, List<HoodieWriteStat> stats) {\n+    try {\n+      final Timer.Context finalizeCtx = metrics.getFinalizeCtx();\n+      table.finalizeWrite(context, instantTime, stats);\n+      if (finalizeCtx != null) {\n+        Option<Long> durationInMs = Option.of(metrics.getDurationInMs(finalizeCtx.stop()));\n+        durationInMs.ifPresent(duration -> {\n+          LOG.info(\"Finalize write elapsed time (milliseconds): \" + duration);\n+          metrics.updateFinalizeWriteMetrics(duration, stats.size());\n+        });\n+      }\n+    } catch (HoodieIOException ioe) {\n+      throw new HoodieCommitException(\"Failed to complete commit \" + instantTime + \" due to finalize errors.\", ioe);\n+    }\n+  }\n+\n+  public HoodieMetrics getMetrics() {\n+    return metrics;\n+  }\n+\n+  public HoodieIndex<T, I, K, O, P> getIndex() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 791}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTY1MDcxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQyMToxNDowMVrOHYSwxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQyMToxNDowMVrOHYSwxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIzNTI3MQ==", "bodyText": "this cannot be configurable. yarn/k8s will decide the actual driver host. changing it to how it was before", "url": "https://github.com/apache/hudi/pull/1827#discussion_r495235271", "createdAt": "2020-09-25T21:14:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -100,6 +99,7 @@\n \n   public static final String EMBEDDED_TIMELINE_SERVER_ENABLED = \"hoodie.embed.timeline.server\";\n   public static final String DEFAULT_EMBEDDED_TIMELINE_SERVER_ENABLED = \"true\";\n+  public static final String EMBEDDED_TIMELINE_SERVER_HOST = \"hoodie.embed.timeline.server.host\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwMjMxMTE3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/pom.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QxODoxNDoyMFrOHYo_3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QxODoxNDoyMFrOHYo_3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU5OTU4Mw==", "bodyText": "we are missing the dependency we had on hbase-client and hbase-server here. Will punt for now, as it will get picked up from hudi-common.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r495599583", "createdAt": "2020-09-27T18:14:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/pom.xml", "diffHunk": "@@ -0,0 +1,264 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwMjMxNDI2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QxODoxODoxMVrOHYpBRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QxODoxODoxMVrOHYpBRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU5OTk0MA==", "bodyText": "this is a problem. it changes behavior and needs to be reworked.", "url": "https://github.com/apache/hudi/pull/1827#discussion_r495599940", "createdAt": "2020-09-27T18:18:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -149,7 +148,7 @@ private void init(String fileId, String partitionPath, HoodieBaseFile baseFileTo\n   private void init(String fileId, Iterator<HoodieRecord<T>> newRecordsItr) {\n     try {\n       // Load the new records in a map\n-      long memoryForMerge = SparkConfigUtils.getMaxMemoryPerPartitionMerge(config.getProps());\n+      long memoryForMerge = config.getMaxMemoryPerPartitionMerge();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c24a47c9eae815337d7b7ad42695958a3afa9e3c"}, "originalPosition": 61}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4563, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}