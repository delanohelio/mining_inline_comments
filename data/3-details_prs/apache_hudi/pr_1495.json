{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAwNjE1MTg4", "number": 1495, "title": "[HUDI-770] Organize upsert/insert API implementation under a single package", "bodyText": "[HUDI-770] Organize upsert/insert API implementation under a single package\n\nCreated ActionExecutor for performing upsert/insert.\n\nThere is inherent complexity with supporting auto commit vs non-auto commit. Once that is resolved, we will be able to remove much of the code in HoodieWriteClient.", "createdAt": "2020-04-08T04:00:07Z", "url": "https://github.com/apache/hudi/pull/1495", "merged": true, "mergeCommit": {"oid": "17bf9303423821483f17785f8536b7bfed45e42e"}, "closed": true, "closedAt": "2020-04-13T06:11:01Z", "author": {"login": "bvaradar"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcV1QImgFqTM5MDQ4NzU3OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcXHvLAABqjMyMjU5NTAzODc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwNDg3NTc5", "url": "https://github.com/apache/hudi/pull/1495#pullrequestreview-390487579", "createdAt": "2020-04-09T04:45:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNDo0NTo0OVrOGDJbjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwNTowMDo1NVrOGDJqdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1MzQyMQ==", "bodyText": "IMO we should just mirror the same APIs upsert, insert on the table.. ingest is confusing, since it also implies that we are reading from some source, whihc we are not..", "url": "https://github.com/apache/hudi/pull/1495#discussion_r405953421", "createdAt": "2020-04-09T04:45:49Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -455,49 +454,15 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   }\n \n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String instantTime,\n-      HoodieTable<T> hoodieTable, final boolean isUpsert) {\n-\n-    // Cache the tagged records, so we don't end up computing both\n-    // TODO: Consistent contract in HoodieWriteClient regarding preppedRecord storage level handling\n-    if (preppedRecords.getStorageLevel() == StorageLevel.NONE()) {\n-      preppedRecords.persist(StorageLevel.MEMORY_AND_DISK_SER());\n-    } else {\n-      LOG.info(\"RDD PreppedRecords was persisted at: \" + preppedRecords.getStorageLevel());\n-    }\n-\n-    WorkloadProfile profile = null;\n-    if (hoodieTable.isWorkloadProfileNeeded()) {\n-      profile = new WorkloadProfile(preppedRecords);\n-      LOG.info(\"Workload profile :\" + profile);\n-      saveWorkloadProfileMetadataToInflight(profile, hoodieTable, instantTime);\n-    }\n-\n-    // partition using the insert partitioner\n-    final Partitioner partitioner = getPartitioner(hoodieTable, isUpsert, profile);\n-    JavaRDD<HoodieRecord<T>> partitionedRecords = partition(preppedRecords, partitioner);\n-    JavaRDD<WriteStatus> writeStatusRDD = partitionedRecords.mapPartitionsWithIndex((partition, recordItr) -> {\n-      if (isUpsert) {\n-        return hoodieTable.handleUpsertPartition(instantTime, partition, recordItr, partitioner);\n-      } else {\n-        return hoodieTable.handleInsertPartition(instantTime, partition, recordItr, partitioner);\n-      }\n-    }, true).flatMap(List::iterator);\n-\n-    return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, instantTime);\n-  }\n-\n-  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile) {\n-    if (isUpsert) {\n-      return table.getUpsertPartitioner(profile, jsc);\n-    } else {\n-      return table.getInsertPartitioner(profile, jsc);\n+      HoodieTable<T> hoodieTable) {\n+    CommitActionResult result = hoodieTable.ingest(jsc, preppedRecords, instantTime, getOperationType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1NDA2MQ==", "bodyText": "break this into two methods?  one to do the post commit and one to emit?", "url": "https://github.com/apache/hudi/pull/1495#discussion_r405954061", "createdAt": "2020-04-09T04:48:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -165,6 +165,19 @@ private boolean commit(String instantTime, JavaRDD<WriteStatus> writeStatuses,\n       activeTimeline.saveAsComplete(new HoodieInstant(true, actionType, instantTime),\n           Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n \n+      doPostCommitAndEmitCommitMetrics(instantTime, metadata, extraMetadata, actionType);\n+\n+      LOG.info(\"Committed \" + instantTime);\n+    } catch (IOException e) {\n+      throw new HoodieCommitException(\"Failed to complete commit \" + config.getBasePath() + \" at time \" + instantTime,\n+          e);\n+    }\n+    return true;\n+  }\n+\n+  void doPostCommitAndEmitCommitMetrics(String instantTime, HoodieCommitMetadata metadata,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1NDI3Ng==", "bodyText": "and can we return HoodieCommitMetadata instead? I guess the issue is we have to pass this to the caller (deltastreamer or datasource) to decide whether to commit or not?", "url": "https://github.com/apache/hudi/pull/1495#discussion_r405954276", "createdAt": "2020-04-09T04:49:08Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -455,49 +454,15 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   }\n \n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String instantTime,\n-      HoodieTable<T> hoodieTable, final boolean isUpsert) {\n-\n-    // Cache the tagged records, so we don't end up computing both\n-    // TODO: Consistent contract in HoodieWriteClient regarding preppedRecord storage level handling\n-    if (preppedRecords.getStorageLevel() == StorageLevel.NONE()) {\n-      preppedRecords.persist(StorageLevel.MEMORY_AND_DISK_SER());\n-    } else {\n-      LOG.info(\"RDD PreppedRecords was persisted at: \" + preppedRecords.getStorageLevel());\n-    }\n-\n-    WorkloadProfile profile = null;\n-    if (hoodieTable.isWorkloadProfileNeeded()) {\n-      profile = new WorkloadProfile(preppedRecords);\n-      LOG.info(\"Workload profile :\" + profile);\n-      saveWorkloadProfileMetadataToInflight(profile, hoodieTable, instantTime);\n-    }\n-\n-    // partition using the insert partitioner\n-    final Partitioner partitioner = getPartitioner(hoodieTable, isUpsert, profile);\n-    JavaRDD<HoodieRecord<T>> partitionedRecords = partition(preppedRecords, partitioner);\n-    JavaRDD<WriteStatus> writeStatusRDD = partitionedRecords.mapPartitionsWithIndex((partition, recordItr) -> {\n-      if (isUpsert) {\n-        return hoodieTable.handleUpsertPartition(instantTime, partition, recordItr, partitioner);\n-      } else {\n-        return hoodieTable.handleInsertPartition(instantTime, partition, recordItr, partitioner);\n-      }\n-    }, true).flatMap(List::iterator);\n-\n-    return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, instantTime);\n-  }\n-\n-  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile) {\n-    if (isUpsert) {\n-      return table.getUpsertPartitioner(profile, jsc);\n-    } else {\n-      return table.getInsertPartitioner(profile, jsc);\n+      HoodieTable<T> hoodieTable) {\n+    CommitActionResult result = hoodieTable.ingest(jsc, preppedRecords, instantTime, getOperationType());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1NDkxNw==", "bodyText": "rename to just BaseCommitActionExecutor?", "url": "https://github.com/apache/hudi/pull/1495#discussion_r405954917", "createdAt": "2020-04-09T04:51:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/AbstractBaseCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieCommitException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.BaseActionExecutor;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+public abstract class AbstractBaseCommitActionExecutor<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1NTYxMA==", "bodyText": "if tagging the index is outside this action, then updating should be moved out as well?", "url": "https://github.com/apache/hudi/pull/1495#discussion_r405955610", "createdAt": "2020-04-09T04:54:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionResult.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import java.util.List;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.time.Duration;\n+\n+/**\n+ * Contains metadata, write-statuses and latency times corresponding to a commit/delta-commit action.\n+ */\n+public class CommitActionResult {\n+\n+  private JavaRDD<WriteStatus> writeStatuses;\n+  private Duration indexUpdateDuration;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1NjQ0Mg==", "bodyText": "is nt this really debt? can we get rid of this boolean and just replace with a helper method that takes in the operation type and returns true/false?", "url": "https://github.com/apache/hudi/pull/1495#discussion_r405956442", "createdAt": "2020-04-09T04:57:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -27,23 +27,25 @@\n  */\n public enum WriteOperationType {\n   // directly insert\n-  INSERT(\"insert\"),\n-  INSERT_PREPPED(\"insert_prepped\"),\n+  INSERT(\"insert\", false),\n+  INSERT_PREPPED(\"insert_prepped\", false),\n   // update and insert\n-  UPSERT(\"upsert\"),\n-  UPSERT_PREPPED(\"upsert_prepped\"),\n+  UPSERT(\"upsert\", true),\n+  UPSERT_PREPPED(\"upsert_prepped\", true),\n   // bulk insert\n-  BULK_INSERT(\"bulk_insert\"),\n-  BULK_INSERT_PREPPED(\"bulk_insert_prepped\"),\n+  BULK_INSERT(\"bulk_insert\", false),\n+  BULK_INSERT_PREPPED(\"bulk_insert_prepped\", false),\n   // delete\n-  DELETE(\"delete\"),\n+  DELETE(\"delete\", true),\n   // used for old version\n-  UNKNOWN(\"unknown\");\n+  UNKNOWN(\"unknown\", false);\n \n   private final String value;\n+  private final boolean isUpsert;\n \n-  WriteOperationType(String value) {\n+  WriteOperationType(String value, boolean isUpsert) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1NjYwMw==", "bodyText": "Like this method, can be just a static method that returns true/false", "url": "https://github.com/apache/hudi/pull/1495#discussion_r405956603", "createdAt": "2020-04-09T04:58:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -69,4 +71,8 @@ public static WriteOperationType fromValue(String value) {\n         throw new HoodieException(\"Invalid value of Type.\");\n     }\n   }\n+\n+  public boolean isUpsert() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1NzIzOQ==", "bodyText": "Ideally we are able to return HoodieCommitMetadata... but since we can't, may be still rename this to HoodieWriteMetadata to stay consistent with the other objects we are returning now. (RollbackMetadata, RestoreMetadata etc)?", "url": "https://github.com/apache/hudi/pull/1495#discussion_r405957239", "createdAt": "2020-04-09T05:00:55Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionResult.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import java.util.List;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.util.Option;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.time.Duration;\n+\n+/**\n+ * Contains metadata, write-statuses and latency times corresponding to a commit/delta-commit action.\n+ */\n+public class CommitActionResult {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxOTA4NzQ2", "url": "https://github.com/apache/hudi/pull/1495#pullrequestreview-391908746", "createdAt": "2020-04-13T00:30:46Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QwMDozMDo0NlrOGEZ6bA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QwMDo0Mzo0NVrOGEZ_-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3MjA0NA==", "bodyText": "This name is very misleading.. Just WriteHelper is even better.. this also does all the indexing per se.", "url": "https://github.com/apache/hudi/pull/1495#discussion_r407272044", "createdAt": "2020-04-13T00:30:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/DedupeWriteHelper.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import scala.Tuple2;\n+\n+public class DedupeWriteHelper<T extends HoodieRecordPayload<T>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3MjMwNw==", "bodyText": "anything that will produce a deltacommit needs to be in a deltacommit package instead?  We should avoid overloading commit loosely at this level.. ? wdyt?", "url": "https://github.com/apache/hudi/pull/1495#discussion_r407272307", "createdAt": "2020-04-13T00:33:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/MergeOnReadBulkInsertExecutor.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3MjU0NQ==", "bodyText": "classes renamed as MergeOnReadDeltaCommitActionExecutor?  (or we can drop the MergeOnRead even since delta commits only apply to MOR and the forking happens clearly above at teh HoodieTable level)", "url": "https://github.com/apache/hudi/pull/1495#discussion_r407272545", "createdAt": "2020-04-13T00:35:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/MergeOnReadCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.execution.MergeOnReadLazyInsertIterable;\n+import org.apache.hudi.io.HoodieAppendHandle;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.Partitioner;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+public abstract class MergeOnReadCommitActionExecutor<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3Mjc0Ng==", "bodyText": "consistent naming of classes ending with ActionExecutor ? In fact, I wonder if it should be DeleteDeltaCommitActionExecutor .. (same for other classes)... That clearly shows that this is a Delta commit in the scope of deletes..\nSimilarly DeleteCommitActionExecutor and so on for cow classes.. They are now shorter and IMO more precise.", "url": "https://github.com/apache/hudi/pull/1495#discussion_r407272746", "createdAt": "2020-04-13T00:37:14Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/MergeOnReadDeleteExecutor.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+public class MergeOnReadDeleteExecutor<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3MzQ2Nw==", "bodyText": "rename isChangingRecords()  or isChangeOperation() ?", "url": "https://github.com/apache/hudi/pull/1495#discussion_r407273467", "createdAt": "2020-04-13T00:43:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java", "diffHunk": "@@ -69,4 +69,8 @@ public static WriteOperationType fromValue(String value) {\n         throw new HoodieException(\"Invalid value of Type.\");\n     }\n   }\n+\n+  public static boolean isUpsert(WriteOperationType operationType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a3b61298b7396834331422c27438c3d87ba2e35", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/8a3b61298b7396834331422c27438c3d87ba2e35", "committedDate": "2020-04-13T05:06:06Z", "message": "[HUDI-770] Organize upsert/insert API implementation under a single package"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "8a3b61298b7396834331422c27438c3d87ba2e35", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/8a3b61298b7396834331422c27438c3d87ba2e35", "committedDate": "2020-04-13T05:06:06Z", "message": "[HUDI-770] Organize upsert/insert API implementation under a single package"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3359, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}