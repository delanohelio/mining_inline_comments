{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc2OTE2Mzkz", "number": 411, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQwMTo0OToyMlrODg6iUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQwMTo0OToyMlrODg6iUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1ODQwMDgxOnYy", "diffSide": "RIGHT", "path": "ethereum/api/src/main/java/org/hyperledger/besu/ethereum/api/query/TransactionLogBloomCacher.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQwMTo0OToyMlrOFrYnZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxNToxOTo0OFrOFrsU1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTAzNjM5MQ==", "bodyText": "This while loop near the chain head on EthScheduler seems to be approximating a listener. The whole logs caching endeavor reminds me of pruning in that way. Where there's something that needs to be done in bulk (marking the whole state trie of one special block <-> doing all the caching for previous segments) and one that is done with a listener (marking new nodes from new blocks with a listener on the state storage <-> random access writing the log blooms for recent blocks). Could a similar approach be useful here?", "url": "https://github.com/hyperledger/besu/pull/411#discussion_r381036391", "createdAt": "2020-02-19T01:49:22Z", "author": {"login": "RatanRSur"}, "path": "ethereum/api/src/main/java/org/hyperledger/besu/ethereum/api/query/TransactionLogBloomCacher.java", "diffHunk": "@@ -128,27 +132,63 @@ private void fillCacheFile(\n     }\n   }\n \n-  public void cacheLogsBloomForBlockHeader(\n+  void cacheLogsBloomForBlockHeader(\n       final BlockHeader blockHeader,\n       final Optional<File> reusedCacheFile,\n       final boolean ensureChecks) {\n     try {\n+      if (cachingStatus.cachingCount.incrementAndGet() != 1) {\n+        return;\n+      }\n       final long blockNumber = blockHeader.getNumber();\n       LOG.debug(\"Caching logs bloom for block {}.\", \"0x\" + Long.toHexString(blockNumber));\n       if (ensureChecks) {\n         ensurePreviousSegmentsArePresent(blockNumber);\n       }\n       final File cacheFile = reusedCacheFile.orElse(calculateCacheFileName(blockNumber, cacheDir));\n-      if (!cacheFile.exists()) {\n-        Files.createFile(cacheFile.toPath());\n+      if (cacheFile.exists()) {\n+        cacheSingleBlock(blockHeader, cacheFile);\n+      } else {\n+        scheduler.scheduleComputationTask(this::populateLatestSegment);\n+      }\n+    } catch (final IOException e) {\n+      LOG.error(\"Unhandled caching exception.\", e);\n+    } finally {\n+      cachingStatus.cachingCount.decrementAndGet();\n+    }\n+  }\n+\n+  private void cacheSingleBlock(final BlockHeader blockHeader, final File cacheFile)\n+      throws IOException {\n+    try (final RandomAccessFile writer = new RandomAccessFile(cacheFile, \"rw\")) {\n+      final long offset = (blockHeader.getNumber() % BLOCKS_PER_BLOOM_CACHE) * BLOOM_BITS_LENGTH;\n+      writer.seek(offset);\n+      writer.write(ensureBloomBitsAreCorrectLength(blockHeader.getLogsBloom().toArray()));\n+    }\n+  }\n+\n+  private boolean populateLatestSegment() {\n+    try {\n+      long blockNumber = blockchain.getChainHeadBlockNumber();\n+      final File currentFile = calculateCacheFileName(CURRENT, cacheDir);\n+      final long segmentNumber = blockNumber / BLOCKS_PER_BLOOM_CACHE;\n+      try (final OutputStream out = new FileOutputStream(currentFile)) {\n+        fillCacheFile(segmentNumber * BLOCKS_PER_BLOOM_CACHE, blockNumber, out);\n       }\n-      try (RandomAccessFile writer = new RandomAccessFile(cacheFile, \"rw\")) {\n-        final long offset = (blockNumber / BLOCKS_PER_BLOOM_CACHE) * BLOOM_BITS_LENGTH;\n-        writer.seek(offset);\n-        writer.write(ensureBloomBitsAreCorrectLength(blockHeader.getLogsBloom().toArray()));\n+      while (blockNumber <= blockchain.getChainHeadBlockNumber()\n+          && (blockNumber % BLOCKS_PER_BLOOM_CACHE != 0)) {\n+        cacheSingleBlock(blockchain.getBlockHeader(blockNumber).orElseThrow(), currentFile);\n+        blockNumber++;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcd6fcfe714b96c1e674316250a5ac25f7f6ae4b"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTM1OTMxNw==", "bodyText": "This is catching up for the case where more blocks could have shown up in the interim while caching the current segment.  We could do it with a listener but I am concerned that the readability would further suffer, since we would need to put the newly arrived blocks to the side while we finish up caching the segment and then while we pop off a listened object yet another one could show up.  This class already is a tad bit hard to read and making a do this while we are behind keeps that logic isolated to one method.", "url": "https://github.com/hyperledger/besu/pull/411#discussion_r381359317", "createdAt": "2020-02-19T15:19:48Z", "author": {"login": "shemnon"}, "path": "ethereum/api/src/main/java/org/hyperledger/besu/ethereum/api/query/TransactionLogBloomCacher.java", "diffHunk": "@@ -128,27 +132,63 @@ private void fillCacheFile(\n     }\n   }\n \n-  public void cacheLogsBloomForBlockHeader(\n+  void cacheLogsBloomForBlockHeader(\n       final BlockHeader blockHeader,\n       final Optional<File> reusedCacheFile,\n       final boolean ensureChecks) {\n     try {\n+      if (cachingStatus.cachingCount.incrementAndGet() != 1) {\n+        return;\n+      }\n       final long blockNumber = blockHeader.getNumber();\n       LOG.debug(\"Caching logs bloom for block {}.\", \"0x\" + Long.toHexString(blockNumber));\n       if (ensureChecks) {\n         ensurePreviousSegmentsArePresent(blockNumber);\n       }\n       final File cacheFile = reusedCacheFile.orElse(calculateCacheFileName(blockNumber, cacheDir));\n-      if (!cacheFile.exists()) {\n-        Files.createFile(cacheFile.toPath());\n+      if (cacheFile.exists()) {\n+        cacheSingleBlock(blockHeader, cacheFile);\n+      } else {\n+        scheduler.scheduleComputationTask(this::populateLatestSegment);\n+      }\n+    } catch (final IOException e) {\n+      LOG.error(\"Unhandled caching exception.\", e);\n+    } finally {\n+      cachingStatus.cachingCount.decrementAndGet();\n+    }\n+  }\n+\n+  private void cacheSingleBlock(final BlockHeader blockHeader, final File cacheFile)\n+      throws IOException {\n+    try (final RandomAccessFile writer = new RandomAccessFile(cacheFile, \"rw\")) {\n+      final long offset = (blockHeader.getNumber() % BLOCKS_PER_BLOOM_CACHE) * BLOOM_BITS_LENGTH;\n+      writer.seek(offset);\n+      writer.write(ensureBloomBitsAreCorrectLength(blockHeader.getLogsBloom().toArray()));\n+    }\n+  }\n+\n+  private boolean populateLatestSegment() {\n+    try {\n+      long blockNumber = blockchain.getChainHeadBlockNumber();\n+      final File currentFile = calculateCacheFileName(CURRENT, cacheDir);\n+      final long segmentNumber = blockNumber / BLOCKS_PER_BLOOM_CACHE;\n+      try (final OutputStream out = new FileOutputStream(currentFile)) {\n+        fillCacheFile(segmentNumber * BLOCKS_PER_BLOOM_CACHE, blockNumber, out);\n       }\n-      try (RandomAccessFile writer = new RandomAccessFile(cacheFile, \"rw\")) {\n-        final long offset = (blockNumber / BLOCKS_PER_BLOOM_CACHE) * BLOOM_BITS_LENGTH;\n-        writer.seek(offset);\n-        writer.write(ensureBloomBitsAreCorrectLength(blockHeader.getLogsBloom().toArray()));\n+      while (blockNumber <= blockchain.getChainHeadBlockNumber()\n+          && (blockNumber % BLOCKS_PER_BLOOM_CACHE != 0)) {\n+        cacheSingleBlock(blockchain.getBlockHeader(blockNumber).orElseThrow(), currentFile);\n+        blockNumber++;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTAzNjM5MQ=="}, "originalCommit": {"oid": "fcd6fcfe714b96c1e674316250a5ac25f7f6ae4b"}, "originalPosition": 128}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1105, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}