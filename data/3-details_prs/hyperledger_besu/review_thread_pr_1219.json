{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ5MDM2NDkw", "number": 1219, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNjoxNzo0M1rOEwVSNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDoyOToyN1rOExXZcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MTE1ODI4OnYy", "diffSide": "RIGHT", "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNjoxNzo0M1rOHl0sFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNjoxNzo0M1rOHl0sFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQyMjYxNA==", "bodyText": "Experimentally determined a while ago. Since the rocks db default lock timeout is 1000ms, I aimed for a flush size that was as big as possible that completes in under 500ms", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r509422614", "createdAt": "2020-10-21T16:17:43Z", "author": {"login": "RatanRSur"}, "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "diffHunk": "@@ -29,23 +29,31 @@\n import java.util.Collections;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n import java.util.function.Function;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Stopwatch;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.apache.tuweni.bytes.Bytes;\n import org.apache.tuweni.bytes.Bytes32;\n \n public class MarkSweepPruner {\n \n-  private static final int DEFAULT_OPS_PER_TRANSACTION = 1000;\n   private static final Logger LOG = LogManager.getLogger();\n   private static final byte[] IN_USE = Bytes.of(1).toArrayUnsafe();\n \n+  private static final int DEFAULT_OPS_PER_TRANSACTION = 50_000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08adbb3a470c6509f6bf5ddbde8ed559e09f8f57"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MjI2MDUwOnYy", "diffSide": "RIGHT", "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOTozMToxNlrOHmAJkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxOTozMToxNlrOHmAJkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTYxMDM4Ng==", "bodyText": "I see the metrics bug here.", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r509610386", "createdAt": "2020-10-21T19:31:16Z", "author": {"login": "RatanRSur"}, "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "diffHunk": "@@ -199,52 +253,57 @@ private boolean isMarked(final byte[] key) {\n         Function.identity());\n   }\n \n-  private void processAccountState(final Bytes value) {\n+  private void processAccountState(final Bytes value, final ExecutorService executorService) {\n     final StateTrieAccountValue accountValue = StateTrieAccountValue.readFrom(RLP.input(value));\n     markNode(accountValue.getCodeHash());\n \n     createStorageTrie(accountValue.getStorageRoot())\n-        .visitAll(storageNode -> markNode(storageNode.getHash()));\n+        .visitAll(storageNode -> markNode(storageNode.getHash()), executorService);\n   }\n \n   @VisibleForTesting\n   void markNode(final Bytes32 hash) {\n-    markedNodesCounter.inc();\n-    markLock.lock();\n-    try {\n-      pendingMarks.add(hash);\n-      maybeFlushPendingMarks();\n-    } finally {\n-      markLock.unlock();\n-    }\n+    markThenMaybeFlush(() -> pendingMarks.add(hash));\n   }\n \n   private void markNodes(final Collection<Bytes32> nodeHashes) {\n-    markedNodesCounter.inc(nodeHashes.size());\n-    markLock.lock();\n+    markThenMaybeFlush(() -> pendingMarks.addAll(nodeHashes));\n+  }\n+\n+  private void markThenMaybeFlush(final Runnable nodeMarker) {\n+    // We use the read lock here because pendingMarks is threadsafe and we want to allow all the\n+    // marking threads access simultaneously.\n+    final Lock addLock = pendingMarkLock.readLock();\n+    addLock.lock();\n     try {\n-      pendingMarks.addAll(nodeHashes);\n-      maybeFlushPendingMarks();\n+      nodeMarker.run();\n     } finally {\n-      markLock.unlock();\n+      addLock.unlock();\n     }\n-  }\n+    markedNodesCounter.inc();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08adbb3a470c6509f6bf5ddbde8ed559e09f8f57"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NTg4MDE2OnYy", "diffSide": "RIGHT", "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/StoredMerklePatriciaTrie.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNDowMzowNVrOHmjZZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNDowMzowNVrOHmjZZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE4Nzg3OQ==", "bodyText": "(nit) Do you need this?", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r510187879", "createdAt": "2020-10-22T14:03:05Z", "author": {"login": "mbaxter"}, "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/StoredMerklePatriciaTrie.java", "diffHunk": "@@ -33,6 +38,9 @@\n  * @param <V> The type of values stored by this trie.\n  */\n public class StoredMerklePatriciaTrie<K extends Bytes, V> implements MerklePatriciaTrie<K, V> {\n+  @SuppressWarnings(\"UnusedVariable\")\n+  private static final Logger LOG = LogManager.getLogger();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bd2b55b6d5160b8df65af1685a013503d67d3670"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NjIyOTMwOnYy", "diffSide": "RIGHT", "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToxNDoxN1rOHmm1Xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNjoyMTowNlrOHnUzAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI0NDE5MA==", "bodyText": "Curious about this - is it really very advantageous to bundle up changes into large transactions?  Versus just picking a reasonable size and committing more frequently?  I also wonder if the node were under heavy load if its possible we might hit the timeout.", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r510244190", "createdAt": "2020-10-22T15:14:17Z", "author": {"login": "mbaxter"}, "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "diffHunk": "@@ -29,23 +29,31 @@\n import java.util.Collections;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n import java.util.function.Function;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Stopwatch;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.apache.tuweni.bytes.Bytes;\n import org.apache.tuweni.bytes.Bytes32;\n \n public class MarkSweepPruner {\n \n-  private static final int DEFAULT_OPS_PER_TRANSACTION = 1000;\n   private static final Logger LOG = LogManager.getLogger();\n   private static final byte[] IN_USE = Bytes.of(1).toArrayUnsafe();\n \n+  private static final int DEFAULT_OPS_PER_TRANSACTION = 50_000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bd2b55b6d5160b8df65af1685a013503d67d3670"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk5NzI0OA==", "bodyText": "Sounds reasonable to me, I'll lower it to 10k. Let me know if you think it should go lower.", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r510997248", "createdAt": "2020-10-23T16:21:06Z", "author": {"login": "RatanRSur"}, "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "diffHunk": "@@ -29,23 +29,31 @@\n import java.util.Collections;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.LinkedBlockingDeque;\n+import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n import java.util.function.Function;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Stopwatch;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.apache.tuweni.bytes.Bytes;\n import org.apache.tuweni.bytes.Bytes32;\n \n public class MarkSweepPruner {\n \n-  private static final int DEFAULT_OPS_PER_TRANSACTION = 1000;\n   private static final Logger LOG = LogManager.getLogger();\n   private static final byte[] IN_USE = Bytes.of(1).toArrayUnsafe();\n \n+  private static final int DEFAULT_OPS_PER_TRANSACTION = 50_000;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI0NDE5MA=="}, "originalCommit": {"oid": "bd2b55b6d5160b8df65af1685a013503d67d3670"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NjI1OTMyOnYy", "diffSide": "RIGHT", "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNToyMDozNVrOHmnH9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxNTo0Nzo1NFrOHmoYmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI0ODk0OA==", "bodyText": "Seems a bit suboptimal to dedicate a thread to just sit and wait for all of this processing to finish.   But may not be worth the rework right now since testing takes so much time.", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r510248948", "createdAt": "2020-10-22T15:20:35Z", "author": {"login": "mbaxter"}, "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "diffHunk": "@@ -115,20 +125,64 @@ public void prepare() {\n     nodeAddedListenerId = worldStateStorage.addNodeAddedListener(this::markNodes);\n   }\n \n+  /**\n+   * This is a parallel mark implementation.\n+   *\n+   * <p>The parallel task production is by sub-trie, so calling `visitAll` on a root node will\n+   * eventually spawn up to 16 tasks (for a hexary trie).\n+   *\n+   * <p>If we marked each sub-trie in its own thread, with no common queue of tasks, our mark speed\n+   * would be limited by the sub-trie with the maximum number of nodes. In practice for the Ethereum\n+   * mainnet, we see a large imbalance in sub-trie size so without a common task pool the time in\n+   * which there is only 1 thread left marking its big sub-trie would be substantial.\n+   *\n+   * <p>If we were to leave all threads to produce mark tasks before starting to mark, we would run\n+   * out of memory quickly.\n+   *\n+   * <p>If we were to have a constant number of threads producing the mark threads with the others\n+   * consuming them, we would have to optimize the production/consumption balance.\n+   *\n+   * <p>To get the best of both worlds, the marking executor has a {@link\n+   * ThreadPoolExecutor.CallerRunsPolicy} which causes the producing tasks to essentially consume\n+   * their own mark task immediately when the thread pool is full. The resulting behavior is threads\n+   * that mark their own sub-trie until they finish that sub-trie, at which point they switch to\n+   * marking the sub-trie tasks produced by another thread.\n+   *\n+   * @param rootHash The root hash of the whole state trie. Roots of storage tries will be\n+   *     discovered though traversal.\n+   */\n   public void mark(final Hash rootHash) {\n     markOperationCounter.inc();\n     markStopwatch.start();\n+    final ExecutorService markingExecutorService =\n+        new ThreadPoolExecutor(\n+            0,\n+            MAX_MARKING_THREAD_POOL_SIZE,\n+            5L,\n+            TimeUnit.SECONDS,\n+            new LinkedBlockingDeque<>(16),\n+            new ThreadFactoryBuilder()\n+                .setDaemon(true)\n+                .setPriority(Thread.MIN_PRIORITY)\n+                .setNameFormat(this.getClass().getSimpleName() + \"-%d\")\n+                .build(),\n+            new ThreadPoolExecutor.CallerRunsPolicy());\n     createStateTrie(rootHash)\n         .visitAll(\n             node -> {\n-              if (Thread.interrupted()) {\n-                // Since we don't expect to abort marking ourselves,\n-                // our abort process consists only of handling interrupts\n-                throw new RuntimeException(\"Interrupted while marking\");\n-              }\n               markNode(node.getHash());\n-              node.getValue().ifPresent(this::processAccountState);\n-            });\n+              node.getValue()\n+                  .ifPresent(value -> processAccountState(value, markingExecutorService));\n+            },\n+            markingExecutorService)\n+        .join() /* This will block on all the marking tasks to be _produced_ but doesn't guarantee that the marking tasks have been completed. */;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bd2b55b6d5160b8df65af1685a013503d67d3670"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI2OTU5NQ==", "bodyText": "Trying to rework to be non-blocking now", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r510269595", "createdAt": "2020-10-22T15:47:54Z", "author": {"login": "RatanRSur"}, "path": "ethereum/core/src/main/java/org/hyperledger/besu/ethereum/worldstate/MarkSweepPruner.java", "diffHunk": "@@ -115,20 +125,64 @@ public void prepare() {\n     nodeAddedListenerId = worldStateStorage.addNodeAddedListener(this::markNodes);\n   }\n \n+  /**\n+   * This is a parallel mark implementation.\n+   *\n+   * <p>The parallel task production is by sub-trie, so calling `visitAll` on a root node will\n+   * eventually spawn up to 16 tasks (for a hexary trie).\n+   *\n+   * <p>If we marked each sub-trie in its own thread, with no common queue of tasks, our mark speed\n+   * would be limited by the sub-trie with the maximum number of nodes. In practice for the Ethereum\n+   * mainnet, we see a large imbalance in sub-trie size so without a common task pool the time in\n+   * which there is only 1 thread left marking its big sub-trie would be substantial.\n+   *\n+   * <p>If we were to leave all threads to produce mark tasks before starting to mark, we would run\n+   * out of memory quickly.\n+   *\n+   * <p>If we were to have a constant number of threads producing the mark threads with the others\n+   * consuming them, we would have to optimize the production/consumption balance.\n+   *\n+   * <p>To get the best of both worlds, the marking executor has a {@link\n+   * ThreadPoolExecutor.CallerRunsPolicy} which causes the producing tasks to essentially consume\n+   * their own mark task immediately when the thread pool is full. The resulting behavior is threads\n+   * that mark their own sub-trie until they finish that sub-trie, at which point they switch to\n+   * marking the sub-trie tasks produced by another thread.\n+   *\n+   * @param rootHash The root hash of the whole state trie. Roots of storage tries will be\n+   *     discovered though traversal.\n+   */\n   public void mark(final Hash rootHash) {\n     markOperationCounter.inc();\n     markStopwatch.start();\n+    final ExecutorService markingExecutorService =\n+        new ThreadPoolExecutor(\n+            0,\n+            MAX_MARKING_THREAD_POOL_SIZE,\n+            5L,\n+            TimeUnit.SECONDS,\n+            new LinkedBlockingDeque<>(16),\n+            new ThreadFactoryBuilder()\n+                .setDaemon(true)\n+                .setPriority(Thread.MIN_PRIORITY)\n+                .setNameFormat(this.getClass().getSimpleName() + \"-%d\")\n+                .build(),\n+            new ThreadPoolExecutor.CallerRunsPolicy());\n     createStateTrie(rootHash)\n         .visitAll(\n             node -> {\n-              if (Thread.interrupted()) {\n-                // Since we don't expect to abort marking ourselves,\n-                // our abort process consists only of handling interrupts\n-                throw new RuntimeException(\"Interrupted while marking\");\n-              }\n               markNode(node.getHash());\n-              node.getValue().ifPresent(this::processAccountState);\n-            });\n+              node.getValue()\n+                  .ifPresent(value -> processAccountState(value, markingExecutorService));\n+            },\n+            markingExecutorService)\n+        .join() /* This will block on all the marking tasks to be _produced_ but doesn't guarantee that the marking tasks have been completed. */;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI0ODk0OA=="}, "originalCommit": {"oid": "bd2b55b6d5160b8df65af1685a013503d67d3670"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMDcwNzA2OnYy", "diffSide": "RIGHT", "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/SimpleMerklePatriciaTrie.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNDo1OTo1N1rOHnRnfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNDo1OTo1N1rOHnRnfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk0NTE0OQ==", "bodyText": "It seems unexpected that the root node would be processed in the calling thread - this should probably happen in the executor.", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r510945149", "createdAt": "2020-10-23T14:59:57Z", "author": {"login": "mbaxter"}, "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/SimpleMerklePatriciaTrie.java", "diffHunk": "@@ -99,8 +101,22 @@ public void commit(final NodeUpdater nodeUpdater) {\n   }\n \n   @Override\n-  public void visitAll(final Consumer<Node<V>> visitor) {\n-    root.accept(new AllNodesVisitor<>(visitor));\n+  public void visitAll(final Consumer<Node<V>> nodeConsumer) {\n+    root.accept(new AllNodesVisitor<>(nodeConsumer));\n+  }\n+\n+  @Override\n+  public CompletableFuture<Void> visitAll(\n+      final Consumer<Node<V>> nodeConsumer, final ExecutorService executorService) {\n+    nodeConsumer.accept(root);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5700395e0a9ae5fc29c01eb920747d1885539c68"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMDcxODgwOnYy", "diffSide": "RIGHT", "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/StoredMerklePatriciaTrie.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNTowMjoyMlrOHnRuSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNTowMjoyMlrOHnRuSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk0Njg5MA==", "bodyText": "Same comment here - the root node should be processed in the executor.", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r510946890", "createdAt": "2020-10-23T15:02:22Z", "author": {"login": "mbaxter"}, "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/StoredMerklePatriciaTrie.java", "diffHunk": "@@ -125,8 +133,23 @@ public void commit(final NodeUpdater nodeUpdater) {\n   }\n \n   @Override\n-  public void visitAll(final Consumer<Node<V>> visitor) {\n-    root.accept(new AllNodesVisitor<>(visitor));\n+  public void visitAll(final Consumer<Node<V>> nodeConsumer) {\n+    root.accept(new AllNodesVisitor<>(nodeConsumer));\n+  }\n+\n+  @Override\n+  public CompletableFuture<Void> visitAll(\n+      final Consumer<Node<V>> nodeConsumer, final ExecutorService executorService) {\n+    nodeConsumer.accept(root);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5700395e0a9ae5fc29c01eb920747d1885539c68"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMDczMjA1OnYy", "diffSide": "RIGHT", "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/SimpleMerklePatriciaTrie.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNTowNTozN1rOHnR2mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNjozNDozNVrOHnVP-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk0OTAxNg==", "bodyText": "Alternatively, I'd be tempted to just expose getRoot() on the Trie interface, and move this logic into the Pruner...", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r510949016", "createdAt": "2020-10-23T15:05:37Z", "author": {"login": "mbaxter"}, "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/SimpleMerklePatriciaTrie.java", "diffHunk": "@@ -99,8 +101,22 @@ public void commit(final NodeUpdater nodeUpdater) {\n   }\n \n   @Override\n-  public void visitAll(final Consumer<Node<V>> visitor) {\n-    root.accept(new AllNodesVisitor<>(visitor));\n+  public void visitAll(final Consumer<Node<V>> nodeConsumer) {\n+    root.accept(new AllNodesVisitor<>(nodeConsumer));\n+  }\n+\n+  @Override\n+  public CompletableFuture<Void> visitAll(\n+      final Consumer<Node<V>> nodeConsumer, final ExecutorService executorService) {\n+    nodeConsumer.accept(root);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5700395e0a9ae5fc29c01eb920747d1885539c68"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTAwNDY2NA==", "bodyText": "I kinda like it being on the Trie because we're adding other traversal methods there. It becomes one place to look various types of trie iteration. Not sure if I understood what you were suggesting so does that make sense?", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r511004664", "createdAt": "2020-10-23T16:34:35Z", "author": {"login": "RatanRSur"}, "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/SimpleMerklePatriciaTrie.java", "diffHunk": "@@ -99,8 +101,22 @@ public void commit(final NodeUpdater nodeUpdater) {\n   }\n \n   @Override\n-  public void visitAll(final Consumer<Node<V>> visitor) {\n-    root.accept(new AllNodesVisitor<>(visitor));\n+  public void visitAll(final Consumer<Node<V>> nodeConsumer) {\n+    root.accept(new AllNodesVisitor<>(nodeConsumer));\n+  }\n+\n+  @Override\n+  public CompletableFuture<Void> visitAll(\n+      final Consumer<Node<V>> nodeConsumer, final ExecutorService executorService) {\n+    nodeConsumer.accept(root);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk0OTAxNg=="}, "originalCommit": {"oid": "5700395e0a9ae5fc29c01eb920747d1885539c68"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwMTk5MDI0OnYy", "diffSide": "RIGHT", "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/SimpleMerklePatriciaTrie.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMDoyOToyN1rOHndj0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDoyMzo0OFrOHoR6og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MDgxNg==", "bodyText": "For this change, I think you need to bump up your queue size to 17 in the Pruner.  This is part of why I suggested moving this logic to the Pruner - it feels a bit tightly coupled in terms of how the traversal is happening and the required queue size, etc.", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r511140816", "createdAt": "2020-10-23T20:29:27Z", "author": {"login": "mbaxter"}, "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/SimpleMerklePatriciaTrie.java", "diffHunk": "@@ -108,14 +110,17 @@ public void visitAll(final Consumer<Node<V>> nodeConsumer) {\n   @Override\n   public CompletableFuture<Void> visitAll(\n       final Consumer<Node<V>> nodeConsumer, final ExecutorService executorService) {\n-    nodeConsumer.accept(root);\n     return CompletableFuture.allOf(\n-        root.getChildren().stream()\n-            .map(\n-                rootChild ->\n-                    CompletableFuture.runAsync(\n-                        () -> rootChild.accept(new AllNodesVisitor<>(nodeConsumer)),\n-                        executorService))\n+        Stream.concat(\n+                Stream.of(\n+                    CompletableFuture.runAsync(() -> nodeConsumer.accept(root), executorService)),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5fb2372059538870abe1130fc4749d19364b0038"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5ODYyNg==", "bodyText": "Maybe this is wrong but in my mental model, the 16 never mattered too much. It could very well be 2, or 7, or whatever. The reason being that the main thing we're expecting to do is to start marking sub-tries almost immediately through the CallerRuns policy.", "url": "https://github.com/hyperledger/besu/pull/1219#discussion_r511998626", "createdAt": "2020-10-26T14:23:48Z", "author": {"login": "RatanRSur"}, "path": "ethereum/trie/src/main/java/org/hyperledger/besu/ethereum/trie/SimpleMerklePatriciaTrie.java", "diffHunk": "@@ -108,14 +110,17 @@ public void visitAll(final Consumer<Node<V>> nodeConsumer) {\n   @Override\n   public CompletableFuture<Void> visitAll(\n       final Consumer<Node<V>> nodeConsumer, final ExecutorService executorService) {\n-    nodeConsumer.accept(root);\n     return CompletableFuture.allOf(\n-        root.getChildren().stream()\n-            .map(\n-                rootChild ->\n-                    CompletableFuture.runAsync(\n-                        () -> rootChild.accept(new AllNodesVisitor<>(nodeConsumer)),\n-                        executorService))\n+        Stream.concat(\n+                Stream.of(\n+                    CompletableFuture.runAsync(() -> nodeConsumer.accept(root), executorService)),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE0MDgxNg=="}, "originalCommit": {"oid": "5fb2372059538870abe1130fc4749d19364b0038"}, "originalPosition": 30}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 860, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}