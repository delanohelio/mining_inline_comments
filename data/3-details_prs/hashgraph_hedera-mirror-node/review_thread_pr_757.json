{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE4MTkxMTU3", "number": 757, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOTowNzowMFrOD8nOpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNTozNzozMFrOD9EGGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODgzODc2OnYy", "diffSide": "LEFT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/Downloader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOTowNzowMFrOGVqayA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOTowNzowMFrOGVqayA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM2ODI2NA==", "bodyText": "chain verification logic was duplicated in two places  - here and in RecordFileParser.\nMoved it to Utility.verifyHashChain() to be shared.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425368264", "createdAt": "2020-05-14T19:07:00Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/Downloader.java", "diffHunk": "@@ -386,32 +379,6 @@ private void verifySigsAndDownloadDataFiles(Multimap<String, FileStreamSignature\n         }\n     }\n \n-    /**\n-     * Verifies that prevFileHash in given {@code file} matches that in application repository.\n-     *\n-     * @throws Exception\n-     */\n-    protected boolean verifyHashChain(File file) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODg0OTMzOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOToxMDoyMVrOGVqhyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOToxMDoyMVrOGVqhyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM3MDA1OA==", "bodyText": "This is the crux of parsing record file only once.\nRather than getPrevFileHash() and getDataFileHash() parsing file twice, this does everything in single parse.\nEven better, it's shared with RecordFileParser rather than having duplicate parsing logic.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425370058", "createdAt": "2020-05-14T19:10:21Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -62,18 +63,21 @@ protected ApplicationStatusCode getLastValidDownloadedFileHashKey() {\n         return ApplicationStatusCode.LAST_VALID_DOWNLOADED_RECORD_FILE_HASH;\n     }\n \n+    /**\n+     * Checks that hash of data file matches the verified hash. Then checks that data file is next in line based on\n+     * previous file hash.\n+     */\n     @Override\n-    protected ApplicationStatusCode getBypassHashKey() {\n-        return ApplicationStatusCode.RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER;\n-    }\n-\n-    @Override\n-    protected String getPrevFileHash(String filePath) {\n-        return RecordFileParser.readPrevFileHash(filePath);\n-    }\n-\n-    @Override\n-    protected byte[] getDataFileHash(String fileName) {\n-        return Utility.getRecordFileHash(fileName);\n+    protected boolean verifyDataFile(String filePath, byte[] verifiedHash) {\n+        RecordFile recordFile =  Utility.parseRecordFile(filePath, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODg1MTE5OnYy", "diffSide": "LEFT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOToxMDo1NFrOGVqi_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOToxMDo1NFrOGVqi_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM3MDM2NA==", "bodyText": "This was being used in downloader only, and it's in RFP file :-/", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425370364", "createdAt": "2020-05-14T19:10:54Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -98,101 +92,27 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n                         \"processed the transaction\");\n     }\n \n-    /**\n-     * Given a service record name, read its prevFileHash\n-     *\n-     * @param fileName the name of record file to read\n-     * @return return previous file hash's Hex String\n-     */\n-    public static String readPrevFileHash(String fileName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODg1NTY0OnYy", "diffSide": "LEFT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOToxMjozMVrOGVql7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOToxMjozMVrOGVql7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM3MTExOA==", "bodyText": "Removed this parsing logic which was almost same as in Utility.parseRecordFile()\nAlso RFP was reading file twice, once in this logic, second time in String thisFileHash = Hex.encodeHexString(Utility.getRecordFileHash(fileName)) below,", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425371118", "createdAt": "2020-05-14T19:12:31Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -98,101 +92,27 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n                         \"processed the transaction\");\n     }\n \n-    /**\n-     * Given a service record name, read its prevFileHash\n-     *\n-     * @param fileName the name of record file to read\n-     * @return return previous file hash's Hex String\n-     */\n-    public static String readPrevFileHash(String fileName) {\n-        File file = new File(fileName);\n-        if (file.exists() == false) {\n-            log.warn(\"File does not exist {}\", fileName);\n-            return null;\n-        }\n-        byte[] prevFileHash = new byte[48];\n-        try (DataInputStream dis = new DataInputStream(new FileInputStream(file))) {\n-            // record_format_version\n-            dis.readInt();\n-            // version\n-            dis.readInt();\n-\n-            byte typeDelimiter = dis.readByte();\n-\n-            if (typeDelimiter == FileDelimiter.RECORD_TYPE_PREV_HASH) {\n-                dis.read(prevFileHash);\n-                String hexString = Hex.encodeHexString(prevFileHash);\n-                log.trace(\"Read previous file hash {} for file {}\", hexString, fileName);\n-                return hexString;\n-            } else {\n-                log.error(\"Expecting previous file hash, but found file delimiter {} for file {}\", typeDelimiter,\n-                        fileName);\n-            }\n-        } catch (Exception e) {\n-            log.error(\"Error reading previous file hash {}\", fileName, e);\n-        }\n-\n-        return null;\n-    }\n-\n     /**\n      * Given a service record name, read and parse and return as a list of service record pair\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) throws IOException {\n-        Stopwatch stopwatch = Stopwatch.createStarted();\n-        long loadStart = Instant.now().getEpochSecond();\n+    public void loadRecordFile(StreamFileData streamFileData) {\n+        Instant startTime = Instant.now();\n         recordStreamFileListener.onStart(streamFileData);\n-        String fileName = streamFileData.getFilename();\n-        String actualPrevFileHash = \"\";\n-        long counter = 0;\n-        Integer recordFileVersion = 0;\n-        Boolean success = false;\n-\n-        try (DataInputStream dis = new DataInputStream(streamFileData.getInputStream())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODkzNTIwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloaderTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOTozNzo1MlrOGVrYqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNDo1ODoxM1rOGZaQnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM4NDEwNQ==", "bodyText": "Unused", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425384105", "createdAt": "2020-05-14T19:37:52Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloaderTest.java", "diffHunk": "@@ -47,6 +47,8 @@\n \n @ExtendWith(MockitoExtension.class)\n public class RecordFileDownloaderTest extends AbstractDownloaderTest {\n+    private static final String FILE1_HASH =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTI5NzgyMA==", "bodyText": "I fixed in one of my PRs", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r429297820", "createdAt": "2020-05-22T14:58:13Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloaderTest.java", "diffHunk": "@@ -47,6 +47,8 @@\n \n @ExtendWith(MockitoExtension.class)\n public class RecordFileDownloaderTest extends AbstractDownloaderTest {\n+    private static final String FILE1_HASH =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM4NDEwNQ=="}, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0ODkzOTM2OnYy", "diffSide": "RIGHT", "path": "docs/configuration.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQxOTozOToxMFrOGVrbTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMzo1ODo0MlrOGcOAlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM4NDc4MA==", "bodyText": "verifyHashAfter. Also should be sorted to be at the bottom.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425384780", "createdAt": "2020-05-14T19:39:10Z", "author": {"login": "steven-sheehy"}, "path": "docs/configuration.md", "diffHunk": "@@ -22,6 +22,7 @@ value, it is recommended to only populate overridden properties in the custom `a\n | Name                                                                 | Default                 | Description                                                                                    |\n | -------------------------------------------------------------------- | ----------------------- | ---------------------------------------------------------------------------------------------- |\n | `hedera.mirror.importer.dataPath`                                    | ./data                  | The data directory used to store downloaded files and other application state                  |\n+| `hedera.mirror.importer.verfiyHashAfter`                             | \"\"                      | Streams in which files are linked using hashes (prevHash) to ensure file ordering, the check would be skipped until after (and not including) this value. Format: YYYY-MM-DDTHH:MM:SS.fffffffffZ |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0MjgzOQ==", "bodyText": "done.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432242839", "createdAt": "2020-05-29T03:58:42Z", "author": {"login": "apeksharma"}, "path": "docs/configuration.md", "diffHunk": "@@ -22,6 +22,7 @@ value, it is recommended to only populate overridden properties in the custom `a\n | Name                                                                 | Default                 | Description                                                                                    |\n | -------------------------------------------------------------------- | ----------------------- | ---------------------------------------------------------------------------------------------- |\n | `hedera.mirror.importer.dataPath`                                    | ./data                  | The data directory used to store downloaded files and other application state                  |\n+| `hedera.mirror.importer.verfiyHashAfter`                             | \"\"                      | Streams in which files are linked using hashes (prevHash) to ensure file ordering, the check would be skipped until after (and not including) this value. Format: YYYY-MM-DDTHH:MM:SS.fffffffffZ |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTM4NDc4MA=="}, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0OTQ2NTU1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/util/Utility.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMjozOTo0MVrOGVwnpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwMjowNToyNFrOGV0Cww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2OTg2MA==", "bodyText": "q: Is there UT coverage for this? I'm not seeing it here", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425469860", "createdAt": "2020-05-14T22:39:41Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/util/Utility.java", "diffHunk": "@@ -450,10 +455,28 @@ String protobufKeyToHexIfEd25519OrNull(@Nullable byte[] protobufKey)\n      * Generates a TransactionID object\n      *\n      * @param payerAccountId the AccountID of the transaction payer account\n-     * @return\n      */\n     public static TransactionID getTransactionId(AccountID payerAccountId) {\n         Timestamp validStart = Utility.instantToTimestamp(Instant.now());\n         return TransactionID.newBuilder().setAccountID(payerAccountId).setTransactionValidStart(validStart).build();\n     }\n+\n+    public static boolean verifyHashChain(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyNTk1NQ==", "bodyText": "added the tests.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425525955", "createdAt": "2020-05-15T02:05:24Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/util/Utility.java", "diffHunk": "@@ -450,10 +455,28 @@ String protobufKeyToHexIfEd25519OrNull(@Nullable byte[] protobufKey)\n      * Generates a TransactionID object\n      *\n      * @param payerAccountId the AccountID of the transaction payer account\n-     * @return\n      */\n     public static TransactionID getTransactionId(AccountID payerAccountId) {\n         Timestamp validStart = Utility.instantToTimestamp(Instant.now());\n         return TransactionID.newBuilder().setAccountID(payerAccountId).setTransactionValidStart(validStart).build();\n     }\n+\n+    public static boolean verifyHashChain(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ2OTg2MA=="}, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0OTQ3MTA4OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/balance/AccountBalancesDownloader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMjo0MjoxOFrOGVwq-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwMjowNDo1MVrOGV0CFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ3MDcxMg==", "bodyText": "You should add a basic test coverage for this", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425470712", "createdAt": "2020-05-14T22:42:18Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/balance/AccountBalancesDownloader.java", "diffHunk": "@@ -53,8 +54,8 @@ public void download() {\n     }\n \n     @Override\n-    protected boolean verifyHashChain(File file) {\n-        return true;\n+    protected boolean verifyDataFile(String filePath, byte[] verifiedHash) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyNTc4Mg==", "bodyText": "not public function, so shouldn't be tested in unit tests.\nAlso, gets covered by other tests.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425525782", "createdAt": "2020-05-15T02:04:51Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/balance/AccountBalancesDownloader.java", "diffHunk": "@@ -53,8 +54,8 @@ public void download() {\n     }\n \n     @Override\n-    protected boolean verifyHashChain(File file) {\n-        return true;\n+    protected boolean verifyDataFile(String filePath, byte[] verifiedHash) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ3MDcxMg=="}, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0OTQ3MjQxOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNFQyMjo0Mjo1OFrOGVwrxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQwMjowNTowMVrOGV0CWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ3MDkxOQ==", "bodyText": "Similar this should have a test for this method.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425470919", "createdAt": "2020-05-14T22:42:58Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -62,18 +63,21 @@ protected ApplicationStatusCode getLastValidDownloadedFileHashKey() {\n         return ApplicationStatusCode.LAST_VALID_DOWNLOADED_RECORD_FILE_HASH;\n     }\n \n+    /**\n+     * Checks that hash of data file matches the verified hash. Then checks that data file is next in line based on\n+     * previous file hash.\n+     */\n     @Override\n-    protected ApplicationStatusCode getBypassHashKey() {\n-        return ApplicationStatusCode.RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER;\n-    }\n-\n-    @Override\n-    protected String getPrevFileHash(String filePath) {\n-        return RecordFileParser.readPrevFileHash(filePath);\n-    }\n-\n-    @Override\n-    protected byte[] getDataFileHash(String fileName) {\n-        return Utility.getRecordFileHash(fileName);\n+    protected boolean verifyDataFile(String filePath, byte[] verifiedHash) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTUyNTg1MA==", "bodyText": "same as above.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425525850", "createdAt": "2020-05-15T02:05:01Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -62,18 +63,21 @@ protected ApplicationStatusCode getLastValidDownloadedFileHashKey() {\n         return ApplicationStatusCode.LAST_VALID_DOWNLOADED_RECORD_FILE_HASH;\n     }\n \n+    /**\n+     * Checks that hash of data file matches the verified hash. Then checks that data file is next in line based on\n+     * previous file hash.\n+     */\n     @Override\n-    protected ApplicationStatusCode getBypassHashKey() {\n-        return ApplicationStatusCode.RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER;\n-    }\n-\n-    @Override\n-    protected String getPrevFileHash(String filePath) {\n-        return RecordFileParser.readPrevFileHash(filePath);\n-    }\n-\n-    @Override\n-    protected byte[] getDataFileHash(String fileName) {\n-        return Utility.getRecordFileHash(fileName);\n+    protected boolean verifyDataFile(String filePath, byte[] verifiedHash) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTQ3MDkxOQ=="}, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MjQzNjYzOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/util/Utility.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNzoyNzo1MlrOGWNq5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQxNzoyNzo1MlrOGWNq5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTk0NTgyOA==", "bodyText": "Would be better to move this default into the class itself to avoid NPE in more places.\nedit: did in my PR", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r425945828", "createdAt": "2020-05-15T17:27:52Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/util/Utility.java", "diffHunk": "@@ -125,80 +128,82 @@\n      * @param filename file name\n      * @return byte array of hash value of null if calculating has failed\n      */\n-    public static byte[] getRecordFileHash(String filename) {\n-        byte[] readFileHash = new byte[48];\n+    public static RecordFile parseRecordFile(String filename, boolean parseRecordItems) {\n+        RecordFile recordFile = new RecordFile();\n+        recordFile.setName(filename);\n+        recordFile.setRecordItems(new ArrayList<>());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c869cb373e79c34ddec13628d6a77db6e585a102"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzEyODI0OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/resources/db/migration/V1.24__remove_bypass_state.sql", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMTozNDoyN1rOGWUfMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMTozNDoyN1rOGWUfMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA1NzUyMA==", "bodyText": "The standard is to name migrations with semver, so this is missing patch version of 0. I've addressed in my PR, just FYI.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426057520", "createdAt": "2020-05-15T21:34:27Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/resources/db/migration/V1.24__remove_bypass_state.sql", "diffHunk": "@@ -0,0 +1 @@\n+delete from t_application_status where status_code = 'RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER';", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzIwMDAyOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/Downloader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMjoxMDozMVrOGWVKXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDo1NjozOVrOGcOxlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA2ODU3Mg==", "bodyText": "Do we need to update troubleshooting doc and ops with this change?", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426068572", "createdAt": "2020-05-15T22:10:31Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/Downloader.java", "diffHunk": "@@ -365,11 +359,10 @@ private void verifySigsAndDownloadDataFiles(Multimap<String, FileStreamSignature\n                                         .updateStatusValue(getLastValidDownloadedFileKey(), destination.getName());\n                                 valid = true;\n                                 break;\n-                            }\n                         }\n-                    } else if (signedDataFile != null) {\n-                        log.warn(\"Hash doesn't match the hash contained in valid signature file. Will try to download\" +\n-                                \" a file with same timestamp from other nodes and check the Hash: {}\", signedDataFile);\n+                    } else {\n+                        log.warn(\"Verification of data file failed. Will try to download a file with same timestamp \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1NTM4MA==", "bodyText": "Thanks for bringing to attention. Not needed for this, but updated troubleshooting doc with other messages.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432255380", "createdAt": "2020-05-29T04:56:39Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/Downloader.java", "diffHunk": "@@ -365,11 +359,10 @@ private void verifySigsAndDownloadDataFiles(Multimap<String, FileStreamSignature\n                                         .updateStatusValue(getLastValidDownloadedFileKey(), destination.getName());\n                                 valid = true;\n                                 break;\n-                            }\n                         }\n-                    } else if (signedDataFile != null) {\n-                        log.warn(\"Hash doesn't match the hash contained in valid signature file. Will try to download\" +\n-                                \" a file with same timestamp from other nodes and check the Hash: {}\", signedDataFile);\n+                    } else {\n+                        log.warn(\"Verification of data file failed. Will try to download a file with same timestamp \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA2ODU3Mg=="}, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzIyODE1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/util/UtilityTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNVQyMjoyNjo0OVrOGWVbdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDo0NDo0NFrOGcOnOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3Mjk1MQ==", "bodyText": "nit: The testName actually make this harder to understand, but of course provides value at runtime. Perhaps a // @formatter:off/on might help here to make them each on a single line?", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426072951", "createdAt": "2020-05-15T22:26:49Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/util/UtilityTest.java", "diffHunk": "@@ -214,4 +214,27 @@ void convertTimestampToNanosIllegalInput() {\n             Utility.timeStampInNanos(timestamp);\n         });\n     }\n+\n+    @ParameterizedTest(name = \"verifyHashChain {5}\")\n+    @CsvSource({\n+            \"'', '', '', 2000-01-01T10_00_00.000000.rcd, true, passes if both hashes are empty\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MjczMA==", "bodyText": "done.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432252730", "createdAt": "2020-05-29T04:44:44Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/util/UtilityTest.java", "diffHunk": "@@ -214,4 +214,27 @@ void convertTimestampToNanosIllegalInput() {\n             Utility.timeStampInNanos(timestamp);\n         });\n     }\n+\n+    @ParameterizedTest(name = \"verifyHashChain {5}\")\n+    @CsvSource({\n+            \"'', '', '', 2000-01-01T10_00_00.000000.rcd, true, passes if both hashes are empty\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA3Mjk1MQ=="}, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzUzODI5OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNDozMTo1NlrOGWYIiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDozODo0OVrOGcOiXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExNzI1Nw==", "bodyText": "Should pass in a File object so you don't need to re-load the file just to get its filename.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426117257", "createdAt": "2020-05-16T04:31:56Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -62,18 +63,21 @@ protected ApplicationStatusCode getLastValidDownloadedFileHashKey() {\n         return ApplicationStatusCode.LAST_VALID_DOWNLOADED_RECORD_FILE_HASH;\n     }\n \n+    /**\n+     * Checks that hash of data file matches the verified hash. Then checks that data file is next in line based on\n+     * previous file hash.\n+     */\n     @Override\n-    protected ApplicationStatusCode getBypassHashKey() {\n-        return ApplicationStatusCode.RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER;\n-    }\n-\n-    @Override\n-    protected String getPrevFileHash(String filePath) {\n-        return RecordFileParser.readPrevFileHash(filePath);\n-    }\n-\n-    @Override\n-    protected byte[] getDataFileHash(String fileName) {\n-        return Utility.getRecordFileHash(fileName);\n+    protected boolean verifyDataFile(String filePath, byte[] verifiedHash) {\n+        RecordFile recordFile =  Utility.parseRecordFile(filePath, false);\n+        if (!recordFile.getFileHash().contentEquals(Hex.encodeHexString(verifiedHash))) {\n+            return false;\n+        }\n+        String fileName = Utility.getFileName(filePath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MTQ4Nw==", "bodyText": "done.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432251487", "createdAt": "2020-05-29T04:38:49Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -62,18 +63,21 @@ protected ApplicationStatusCode getLastValidDownloadedFileHashKey() {\n         return ApplicationStatusCode.LAST_VALID_DOWNLOADED_RECORD_FILE_HASH;\n     }\n \n+    /**\n+     * Checks that hash of data file matches the verified hash. Then checks that data file is next in line based on\n+     * previous file hash.\n+     */\n     @Override\n-    protected ApplicationStatusCode getBypassHashKey() {\n-        return ApplicationStatusCode.RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER;\n-    }\n-\n-    @Override\n-    protected String getPrevFileHash(String filePath) {\n-        return RecordFileParser.readPrevFileHash(filePath);\n-    }\n-\n-    @Override\n-    protected byte[] getDataFileHash(String fileName) {\n-        return Utility.getRecordFileHash(fileName);\n+    protected boolean verifyDataFile(String filePath, byte[] verifiedHash) {\n+        RecordFile recordFile =  Utility.parseRecordFile(filePath, false);\n+        if (!recordFile.getFileHash().contentEquals(Hex.encodeHexString(verifiedHash))) {\n+            return false;\n+        }\n+        String fileName = Utility.getFileName(filePath);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExNzI1Nw=="}, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzUzOTMxOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNDozNDoyM1rOGWYJEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNDozNDoyM1rOGWYJEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExNzM5NQ==", "bodyText": "This log doesn't make sense here as this method is dealing with verification, not downloading. Also, there's already a log for \"finished downloading {}\" in PendingDownload. We can remove this.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426117395", "createdAt": "2020-05-16T04:34:23Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -62,18 +63,21 @@ protected ApplicationStatusCode getLastValidDownloadedFileHashKey() {\n         return ApplicationStatusCode.LAST_VALID_DOWNLOADED_RECORD_FILE_HASH;\n     }\n \n+    /**\n+     * Checks that hash of data file matches the verified hash. Then checks that data file is next in line based on\n+     * previous file hash.\n+     */\n     @Override\n-    protected ApplicationStatusCode getBypassHashKey() {\n-        return ApplicationStatusCode.RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER;\n-    }\n-\n-    @Override\n-    protected String getPrevFileHash(String filePath) {\n-        return RecordFileParser.readPrevFileHash(filePath);\n-    }\n-\n-    @Override\n-    protected byte[] getDataFileHash(String fileName) {\n-        return Utility.getRecordFileHash(fileName);\n+    protected boolean verifyDataFile(String filePath, byte[] verifiedHash) {\n+        RecordFile recordFile =  Utility.parseRecordFile(filePath, false);\n+        if (!recordFile.getFileHash().contentEquals(Hex.encodeHexString(verifiedHash))) {\n+            return false;\n+        }\n+        String fileName = Utility.getFileName(filePath);\n+        log.debug(\"Downloaded data file {} corresponding to verified hash\", fileName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzU0NTk0OnYy", "diffSide": "RIGHT", "path": "docs/configuration.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNDo0ODoyMFrOGWYMYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDozMDoyN1rOGcObxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExODI0MQ==", "bodyText": "YYYY-MM-DDTHH:MM:SS.fffffffffZ does not match rcd filename format like 2019-08-30T18_10_00.419072Z.rcd. I would recommend the property be parsed as an Instant or other date type and the filename be converted to the same before comparison to ensure there's no silent failures if user typed in the time in the wrong format.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426118241", "createdAt": "2020-05-16T04:48:20Z", "author": {"login": "steven-sheehy"}, "path": "docs/configuration.md", "diffHunk": "@@ -22,6 +22,7 @@ value, it is recommended to only populate overridden properties in the custom `a\n | Name                                                                 | Default                 | Description                                                                                    |\n | -------------------------------------------------------------------- | ----------------------- | ---------------------------------------------------------------------------------------------- |\n | `hedera.mirror.importer.dataPath`                                    | ./data                  | The data directory used to store downloaded files and other application state                  |\n+| `hedera.mirror.importer.verfiyHashAfter`                             | \"\"                      | Streams in which files are linked using hashes (prevHash) to ensure file ordering, the check would be skipped until after (and not including) this value. Format: YYYY-MM-DDTHH:MM:SS.fffffffffZ |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI0OTc5Nw==", "bodyText": "done.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432249797", "createdAt": "2020-05-29T04:30:27Z", "author": {"login": "apeksharma"}, "path": "docs/configuration.md", "diffHunk": "@@ -22,6 +22,7 @@ value, it is recommended to only populate overridden properties in the custom `a\n | Name                                                                 | Default                 | Description                                                                                    |\n | -------------------------------------------------------------------- | ----------------------- | ---------------------------------------------------------------------------------------------- |\n | `hedera.mirror.importer.dataPath`                                    | ./data                  | The data directory used to store downloaded files and other application state                  |\n+| `hedera.mirror.importer.verfiyHashAfter`                             | \"\"                      | Streams in which files are linked using hashes (prevHash) to ensure file ordering, the check would be skipped until after (and not including) this value. Format: YYYY-MM-DDTHH:MM:SS.fffffffffZ |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExODI0MQ=="}, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzU0ODQ2OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/util/UtilityTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNDo1Mzo1OFrOGWYNrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDo0NToyMVrOGcOn0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExODU3Mw==", "bodyText": "This test fails if you remove the .rcd extension, as the docs suggest you populate the value. Would recommend removing .rcd from all tests here.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426118573", "createdAt": "2020-05-16T04:53:58Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/util/UtilityTest.java", "diffHunk": "@@ -214,4 +214,27 @@ void convertTimestampToNanosIllegalInput() {\n             Utility.timeStampInNanos(timestamp);\n         });\n     }\n+\n+    @ParameterizedTest(name = \"verifyHashChain {5}\")\n+    @CsvSource({\n+            \"'', '', '', 2000-01-01T10_00_00.000000.rcd, true, passes if both hashes are empty\",\n+            \"xx, '', '', 2000-01-01T10_00_00.000000.rcd, true, \" +\n+                    \" passes if hash mismatch and expected hash is empty\", // starting stream in middle\n+            \"'', xx, '', 2000-01-01T10_00_00.000000.rcd, false,\" +\n+                    \" fails if hash mismatch and actual hash is empty\", // bad db state\n+            \"xx, yy, '', 2000-01-01T10_00_00.000000.rcd, false, fails if hash mismatch and hashes are non-empty\",\n+            \"xx, yy, 2000-01-02T10_00_00.000001.rcd, 2000-01-01T10_00_00.000000.rcd, true,\" +\n+                    \" passes if hash mismatch but verifyHashAfter is after filename\",\n+            \"xx, yy, 2000-01-01T10_00_00.000000.rcd, 2000-01-01T10_00_00.000000.rcd, true,\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1Mjg4MQ==", "bodyText": "should not be a problem now.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432252881", "createdAt": "2020-05-29T04:45:21Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/util/UtilityTest.java", "diffHunk": "@@ -214,4 +214,27 @@ void convertTimestampToNanosIllegalInput() {\n             Utility.timeStampInNanos(timestamp);\n         });\n     }\n+\n+    @ParameterizedTest(name = \"verifyHashChain {5}\")\n+    @CsvSource({\n+            \"'', '', '', 2000-01-01T10_00_00.000000.rcd, true, passes if both hashes are empty\",\n+            \"xx, '', '', 2000-01-01T10_00_00.000000.rcd, true, \" +\n+                    \" passes if hash mismatch and expected hash is empty\", // starting stream in middle\n+            \"'', xx, '', 2000-01-01T10_00_00.000000.rcd, false,\" +\n+                    \" fails if hash mismatch and actual hash is empty\", // bad db state\n+            \"xx, yy, '', 2000-01-01T10_00_00.000000.rcd, false, fails if hash mismatch and hashes are non-empty\",\n+            \"xx, yy, 2000-01-02T10_00_00.000001.rcd, 2000-01-01T10_00_00.000000.rcd, true,\" +\n+                    \" passes if hash mismatch but verifyHashAfter is after filename\",\n+            \"xx, yy, 2000-01-01T10_00_00.000000.rcd, 2000-01-01T10_00_00.000000.rcd, true,\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExODU3Mw=="}, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzU2MTkxOnYy", "diffSide": "LEFT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/Downloader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNToyMzoyNlrOGWYUkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDozNTo1MFrOGcOf3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMDMzOA==", "bodyText": "Utility.hashIsEmpty(prevFileHash) is not carried over to the new code. To be honest I'm not sure if that's good or bad. The old behavior of just accepting a stream reset file seems suspect. This needs to be considered carefully.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426120338", "createdAt": "2020-05-16T05:23:26Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/Downloader.java", "diffHunk": "@@ -386,32 +379,6 @@ private void verifySigsAndDownloadDataFiles(Multimap<String, FileStreamSignature\n         }\n     }\n \n-    /**\n-     * Verifies that prevFileHash in given {@code file} matches that in application repository.\n-     *\n-     * @throws Exception\n-     */\n-    protected boolean verifyHashChain(File file) {\n-        String filePath = file.getAbsolutePath();\n-        String lastValidFileHash = applicationStatusRepository.findByStatusCode(getLastValidDownloadedFileHashKey());\n-        String bypassMismatch = applicationStatusRepository.findByStatusCode(getBypassHashKey());\n-        String prevFileHash = getPrevFileHash(filePath);\n-\n-        if (prevFileHash == null) {\n-            log.warn(\"Does not contain valid previous file hash: {}\", filePath);\n-            return false;\n-        }\n-\n-        if (StringUtils.isBlank(lastValidFileHash) || lastValidFileHash.equals(prevFileHash) ||\n-                Utility.hashIsEmpty(prevFileHash) || bypassMismatch.compareTo(file.getName()) > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MDg0Ng==", "bodyText": "prev hash in a file cannot be empty unless its first file in the stream. In which case, database should be empty too and both hashes (equal to empty) will match.\nWe should not skip hash check if prevFileHash is empty.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432250846", "createdAt": "2020-05-29T04:35:50Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/Downloader.java", "diffHunk": "@@ -386,32 +379,6 @@ private void verifySigsAndDownloadDataFiles(Multimap<String, FileStreamSignature\n         }\n     }\n \n-    /**\n-     * Verifies that prevFileHash in given {@code file} matches that in application repository.\n-     *\n-     * @throws Exception\n-     */\n-    protected boolean verifyHashChain(File file) {\n-        String filePath = file.getAbsolutePath();\n-        String lastValidFileHash = applicationStatusRepository.findByStatusCode(getLastValidDownloadedFileHashKey());\n-        String bypassMismatch = applicationStatusRepository.findByStatusCode(getBypassHashKey());\n-        String prevFileHash = getPrevFileHash(filePath);\n-\n-        if (prevFileHash == null) {\n-            log.warn(\"Does not contain valid previous file hash: {}\", filePath);\n-            return false;\n-        }\n-\n-        if (StringUtils.isBlank(lastValidFileHash) || lastValidFileHash.equals(prevFileHash) ||\n-                Utility.hashIsEmpty(prevFileHash) || bypassMismatch.compareTo(file.getName()) > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMDMzOA=="}, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzU2NTYzOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/util/Utility.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNTozMjoxOFrOGWYWiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDo1MzoxMVrOGcOutw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMDg0MQ==", "bodyText": "I have a huge concern with this new approach to loading all record items into memory at once instead of processing them one by one. If we process 10K TPS * 5s per file * 6144 bytes per transaction that's 293MB loaded into memory in this loop. At 100K TPS it would be crazy. Even if we consider a more average of 500 bytes per transaction that's still 25MB causing a lot of memory churn.\nI like the code de-duplication done here, but it needs to be done in a streaming fashion without loading everything at once and then processing. For downloader, there's the parseRecordItems flag so it's not a problem. For parser, he doesn't need to read the whole file first to verify anything, he just needs the previous hash. Perhaps a callback passed in or returning a file reading Iterator instead of a list.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426120841", "createdAt": "2020-05-16T05:32:18Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/util/Utility.java", "diffHunk": "@@ -125,80 +128,82 @@\n      * @param filename file name\n      * @return byte array of hash value of null if calculating has failed\n      */\n-    public static byte[] getRecordFileHash(String filename) {\n-        byte[] readFileHash = new byte[48];\n+    public static RecordFile parseRecordFile(String filename, boolean parseRecordItems) {\n+        RecordFile recordFile = new RecordFile();\n+        recordFile.setName(filename);\n+        recordFile.setRecordItems(new ArrayList<>());\n \n         try (DataInputStream dis = new DataInputStream(new FileInputStream(filename))) {\n             MessageDigest md = MessageDigest.getInstance(FileDelimiter.HASH_ALGORITHM);\n             MessageDigest mdForContent = MessageDigest.getInstance(FileDelimiter.HASH_ALGORITHM);\n \n-            int record_format_version = dis.readInt();\n+            int recordFormatVersion = dis.readInt();\n             int version = dis.readInt();\n+            log.info(\"Loading record format version {} from record file: {}\", recordFormatVersion, filename);\n+            recordFile.setRecordFormatVersion(recordFormatVersion);\n \n-            md.update(Utility.integerToBytes(record_format_version));\n+            md.update(Utility.integerToBytes(recordFormatVersion));\n             md.update(Utility.integerToBytes(version));\n \n-            log.debug(\"Calculating hash for version {} record file: {}\", record_format_version, filename);\n+            log.debug(\"Calculating hash for version {} record file: {}\", recordFormatVersion, filename);\n \n             while (dis.available() != 0) {\n-\n                 byte typeDelimiter = dis.readByte();\n-\n                 switch (typeDelimiter) {\n                     case FileDelimiter.RECORD_TYPE_PREV_HASH:\n                         md.update(typeDelimiter);\n+                        byte[] readFileHash = new byte[48];\n                         dis.read(readFileHash);\n+                        recordFile.setPreviousHash(Hex.encodeHexString(readFileHash));\n                         md.update(readFileHash);\n                         break;\n-                    case FileDelimiter.RECORD_TYPE_RECORD:\n \n-                        int byteLength = dis.readInt();\n-                        byte[] rawBytes = new byte[byteLength];\n-                        dis.readFully(rawBytes);\n-                        if (record_format_version >= FileDelimiter.RECORD_FORMAT_VERSION) {\n-                            mdForContent.update(typeDelimiter);\n-                            mdForContent.update(Utility.integerToBytes(byteLength));\n-                            mdForContent.update(rawBytes);\n-                        } else {\n-                            md.update(typeDelimiter);\n-                            md.update(Utility.integerToBytes(byteLength));\n-                            md.update(rawBytes);\n+                    case FileDelimiter.RECORD_TYPE_RECORD:\n+                        MessageDigest messageDigest = md;\n+                        if (recordFormatVersion >= FileDelimiter.RECORD_FORMAT_VERSION) {\n+                            messageDigest = mdForContent;\n                         }\n-\n+                        // Read Transaction\n+                        int byteLength = dis.readInt();\n+                        byte[] transactionRawBytes = new byte[byteLength];\n+                        dis.readFully(transactionRawBytes);\n+                        messageDigest.update(typeDelimiter);\n+                        messageDigest.update(Utility.integerToBytes(byteLength));\n+                        messageDigest.update(transactionRawBytes);\n+                        // Read TransactionRecord\n                         byteLength = dis.readInt();\n-                        rawBytes = new byte[byteLength];\n-                        dis.readFully(rawBytes);\n-\n-                        if (record_format_version >= FileDelimiter.RECORD_FORMAT_VERSION) {\n-                            mdForContent.update(Utility.integerToBytes(byteLength));\n-                            mdForContent.update(rawBytes);\n-                        } else {\n-                            md.update(Utility.integerToBytes(byteLength));\n-                            md.update(rawBytes);\n+                        byte[] recordRawBytes = new byte[byteLength];\n+                        dis.readFully(recordRawBytes);\n+                        messageDigest.update(Utility.integerToBytes(byteLength));\n+                        messageDigest.update(recordRawBytes);\n+                        if (parseRecordItems) {\n+                            recordFile.getRecordItems().add(new RecordItem(transactionRawBytes, recordRawBytes));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1NDY0Nw==", "bodyText": "I have given it a good thought in first iteration itself. With 10k tps and 6k txns msg size totaling to 300MB, it's nothing worth worry about. Java GC will have no problem at such small scale.\nIt's worth nothing that we want to remove FS dependency and all things will live in memory eventually.\nAn importer with few gigs of mem would have no issues even then.\nNo need of optimization here. Keep it simple.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432254647", "createdAt": "2020-05-29T04:53:11Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/util/Utility.java", "diffHunk": "@@ -125,80 +128,82 @@\n      * @param filename file name\n      * @return byte array of hash value of null if calculating has failed\n      */\n-    public static byte[] getRecordFileHash(String filename) {\n-        byte[] readFileHash = new byte[48];\n+    public static RecordFile parseRecordFile(String filename, boolean parseRecordItems) {\n+        RecordFile recordFile = new RecordFile();\n+        recordFile.setName(filename);\n+        recordFile.setRecordItems(new ArrayList<>());\n \n         try (DataInputStream dis = new DataInputStream(new FileInputStream(filename))) {\n             MessageDigest md = MessageDigest.getInstance(FileDelimiter.HASH_ALGORITHM);\n             MessageDigest mdForContent = MessageDigest.getInstance(FileDelimiter.HASH_ALGORITHM);\n \n-            int record_format_version = dis.readInt();\n+            int recordFormatVersion = dis.readInt();\n             int version = dis.readInt();\n+            log.info(\"Loading record format version {} from record file: {}\", recordFormatVersion, filename);\n+            recordFile.setRecordFormatVersion(recordFormatVersion);\n \n-            md.update(Utility.integerToBytes(record_format_version));\n+            md.update(Utility.integerToBytes(recordFormatVersion));\n             md.update(Utility.integerToBytes(version));\n \n-            log.debug(\"Calculating hash for version {} record file: {}\", record_format_version, filename);\n+            log.debug(\"Calculating hash for version {} record file: {}\", recordFormatVersion, filename);\n \n             while (dis.available() != 0) {\n-\n                 byte typeDelimiter = dis.readByte();\n-\n                 switch (typeDelimiter) {\n                     case FileDelimiter.RECORD_TYPE_PREV_HASH:\n                         md.update(typeDelimiter);\n+                        byte[] readFileHash = new byte[48];\n                         dis.read(readFileHash);\n+                        recordFile.setPreviousHash(Hex.encodeHexString(readFileHash));\n                         md.update(readFileHash);\n                         break;\n-                    case FileDelimiter.RECORD_TYPE_RECORD:\n \n-                        int byteLength = dis.readInt();\n-                        byte[] rawBytes = new byte[byteLength];\n-                        dis.readFully(rawBytes);\n-                        if (record_format_version >= FileDelimiter.RECORD_FORMAT_VERSION) {\n-                            mdForContent.update(typeDelimiter);\n-                            mdForContent.update(Utility.integerToBytes(byteLength));\n-                            mdForContent.update(rawBytes);\n-                        } else {\n-                            md.update(typeDelimiter);\n-                            md.update(Utility.integerToBytes(byteLength));\n-                            md.update(rawBytes);\n+                    case FileDelimiter.RECORD_TYPE_RECORD:\n+                        MessageDigest messageDigest = md;\n+                        if (recordFormatVersion >= FileDelimiter.RECORD_FORMAT_VERSION) {\n+                            messageDigest = mdForContent;\n                         }\n-\n+                        // Read Transaction\n+                        int byteLength = dis.readInt();\n+                        byte[] transactionRawBytes = new byte[byteLength];\n+                        dis.readFully(transactionRawBytes);\n+                        messageDigest.update(typeDelimiter);\n+                        messageDigest.update(Utility.integerToBytes(byteLength));\n+                        messageDigest.update(transactionRawBytes);\n+                        // Read TransactionRecord\n                         byteLength = dis.readInt();\n-                        rawBytes = new byte[byteLength];\n-                        dis.readFully(rawBytes);\n-\n-                        if (record_format_version >= FileDelimiter.RECORD_FORMAT_VERSION) {\n-                            mdForContent.update(Utility.integerToBytes(byteLength));\n-                            mdForContent.update(rawBytes);\n-                        } else {\n-                            md.update(Utility.integerToBytes(byteLength));\n-                            md.update(rawBytes);\n+                        byte[] recordRawBytes = new byte[byteLength];\n+                        dis.readFully(recordRawBytes);\n+                        messageDigest.update(Utility.integerToBytes(byteLength));\n+                        messageDigest.update(recordRawBytes);\n+                        if (parseRecordItems) {\n+                            recordFile.getRecordItems().add(new RecordItem(transactionRawBytes, recordRawBytes));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMDg0MQ=="}, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzU2ODI1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNTozNzozMFrOGWYXyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNDo0MTowNlrOGcOkXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTE2MA==", "bodyText": "This will print hash mismatch errors twice in the log: one for Utility.verifyHashChain() and one for catch.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r426121160", "createdAt": "2020-05-16T05:37:30Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -98,101 +92,27 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n                         \"processed the transaction\");\n     }\n \n-    /**\n-     * Given a service record name, read its prevFileHash\n-     *\n-     * @param fileName the name of record file to read\n-     * @return return previous file hash's Hex String\n-     */\n-    public static String readPrevFileHash(String fileName) {\n-        File file = new File(fileName);\n-        if (file.exists() == false) {\n-            log.warn(\"File does not exist {}\", fileName);\n-            return null;\n-        }\n-        byte[] prevFileHash = new byte[48];\n-        try (DataInputStream dis = new DataInputStream(new FileInputStream(file))) {\n-            // record_format_version\n-            dis.readInt();\n-            // version\n-            dis.readInt();\n-\n-            byte typeDelimiter = dis.readByte();\n-\n-            if (typeDelimiter == FileDelimiter.RECORD_TYPE_PREV_HASH) {\n-                dis.read(prevFileHash);\n-                String hexString = Hex.encodeHexString(prevFileHash);\n-                log.trace(\"Read previous file hash {} for file {}\", hexString, fileName);\n-                return hexString;\n-            } else {\n-                log.error(\"Expecting previous file hash, but found file delimiter {} for file {}\", typeDelimiter,\n-                        fileName);\n-            }\n-        } catch (Exception e) {\n-            log.error(\"Error reading previous file hash {}\", fileName, e);\n-        }\n-\n-        return null;\n-    }\n-\n     /**\n      * Given a service record name, read and parse and return as a list of service record pair\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) throws IOException {\n-        Stopwatch stopwatch = Stopwatch.createStarted();\n-        long loadStart = Instant.now().getEpochSecond();\n+    public void loadRecordFile(StreamFileData streamFileData) {\n+        Instant startTime = Instant.now();\n         recordStreamFileListener.onStart(streamFileData);\n-        String fileName = streamFileData.getFilename();\n-        String actualPrevFileHash = \"\";\n-        long counter = 0;\n-        Integer recordFileVersion = 0;\n-        Boolean success = false;\n-\n-        try (DataInputStream dis = new DataInputStream(streamFileData.getInputStream())) {\n-            recordFileVersion = dis.readInt();\n-            int version = dis.readInt();\n-            log.info(\"Loading version {} record file: {}\", recordFileVersion, fileName);\n-            while (dis.available() != 0) {\n-                byte typeDelimiter = dis.readByte();\n-\n-                switch (typeDelimiter) {\n-                    case FileDelimiter.RECORD_TYPE_PREV_HASH:\n-                        byte[] readFileHash = new byte[48];\n-                        dis.read(readFileHash);\n-                        actualPrevFileHash = Hex.encodeHexString(readFileHash);\n-                        String expectedPrevFileHash = applicationStatusRepository.findByStatusCode(\n-                                ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH);\n-                        if (Utility.hashIsEmpty(expectedPrevFileHash)) {\n-                            log.error(\"Previous file hash not available\");\n-                            expectedPrevFileHash = actualPrevFileHash;\n-                        }\n-                        log.trace(\"actual file hash = {}, expected file hash = {}\", actualPrevFileHash,\n-                                expectedPrevFileHash);\n-                        if (!actualPrevFileHash.contentEquals(expectedPrevFileHash)) {\n-                            if (applicationStatusRepository\n-                                    .findByStatusCode(ApplicationStatusCode.RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER)\n-                                    .compareTo(Utility.getFileName(fileName)) < 0) {\n-                                // last file for which mismatch is allowed is in the past\n-                                throw new ParserException(String.format(\n-                                        \"Hash mismatch for file %s. Expected = %s, Actual = %s\",\n-                                        fileName, expectedPrevFileHash, actualPrevFileHash));\n-                            }\n-                        }\n-                        break;\n-                    case FileDelimiter.RECORD_TYPE_RECORD:\n+        RecordFile recordFile = Utility.parseRecordFile(streamFileData.getFilename(), true);\n+        String fileName = Utility.getFileName(streamFileData.getFilename());\n+        recordFile.setLoadStart(startTime.getEpochSecond());\n+        int counter = 0;\n+        boolean success = false;\n+        try {\n+            if (!Utility.verifyHashChain(recordFile.getPreviousHash(),\n+                    applicationStatusRepository.findByStatusCode(ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH),\n+                    parserProperties.getMirrorProperties().getVerifyHashAfter(), fileName)) {\n+                throw new ParserException(\"Hash mismatch for file \" + fileName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI1MTk5OA==", "bodyText": "The actual log is fn is detailed. This is just exception message.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/757#discussion_r432251998", "createdAt": "2020-05-29T04:41:06Z", "author": {"login": "apeksharma"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -98,101 +92,27 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n                         \"processed the transaction\");\n     }\n \n-    /**\n-     * Given a service record name, read its prevFileHash\n-     *\n-     * @param fileName the name of record file to read\n-     * @return return previous file hash's Hex String\n-     */\n-    public static String readPrevFileHash(String fileName) {\n-        File file = new File(fileName);\n-        if (file.exists() == false) {\n-            log.warn(\"File does not exist {}\", fileName);\n-            return null;\n-        }\n-        byte[] prevFileHash = new byte[48];\n-        try (DataInputStream dis = new DataInputStream(new FileInputStream(file))) {\n-            // record_format_version\n-            dis.readInt();\n-            // version\n-            dis.readInt();\n-\n-            byte typeDelimiter = dis.readByte();\n-\n-            if (typeDelimiter == FileDelimiter.RECORD_TYPE_PREV_HASH) {\n-                dis.read(prevFileHash);\n-                String hexString = Hex.encodeHexString(prevFileHash);\n-                log.trace(\"Read previous file hash {} for file {}\", hexString, fileName);\n-                return hexString;\n-            } else {\n-                log.error(\"Expecting previous file hash, but found file delimiter {} for file {}\", typeDelimiter,\n-                        fileName);\n-            }\n-        } catch (Exception e) {\n-            log.error(\"Error reading previous file hash {}\", fileName, e);\n-        }\n-\n-        return null;\n-    }\n-\n     /**\n      * Given a service record name, read and parse and return as a list of service record pair\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) throws IOException {\n-        Stopwatch stopwatch = Stopwatch.createStarted();\n-        long loadStart = Instant.now().getEpochSecond();\n+    public void loadRecordFile(StreamFileData streamFileData) {\n+        Instant startTime = Instant.now();\n         recordStreamFileListener.onStart(streamFileData);\n-        String fileName = streamFileData.getFilename();\n-        String actualPrevFileHash = \"\";\n-        long counter = 0;\n-        Integer recordFileVersion = 0;\n-        Boolean success = false;\n-\n-        try (DataInputStream dis = new DataInputStream(streamFileData.getInputStream())) {\n-            recordFileVersion = dis.readInt();\n-            int version = dis.readInt();\n-            log.info(\"Loading version {} record file: {}\", recordFileVersion, fileName);\n-            while (dis.available() != 0) {\n-                byte typeDelimiter = dis.readByte();\n-\n-                switch (typeDelimiter) {\n-                    case FileDelimiter.RECORD_TYPE_PREV_HASH:\n-                        byte[] readFileHash = new byte[48];\n-                        dis.read(readFileHash);\n-                        actualPrevFileHash = Hex.encodeHexString(readFileHash);\n-                        String expectedPrevFileHash = applicationStatusRepository.findByStatusCode(\n-                                ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH);\n-                        if (Utility.hashIsEmpty(expectedPrevFileHash)) {\n-                            log.error(\"Previous file hash not available\");\n-                            expectedPrevFileHash = actualPrevFileHash;\n-                        }\n-                        log.trace(\"actual file hash = {}, expected file hash = {}\", actualPrevFileHash,\n-                                expectedPrevFileHash);\n-                        if (!actualPrevFileHash.contentEquals(expectedPrevFileHash)) {\n-                            if (applicationStatusRepository\n-                                    .findByStatusCode(ApplicationStatusCode.RECORD_HASH_MISMATCH_BYPASS_UNTIL_AFTER)\n-                                    .compareTo(Utility.getFileName(fileName)) < 0) {\n-                                // last file for which mismatch is allowed is in the past\n-                                throw new ParserException(String.format(\n-                                        \"Hash mismatch for file %s. Expected = %s, Actual = %s\",\n-                                        fileName, expectedPrevFileHash, actualPrevFileHash));\n-                            }\n-                        }\n-                        break;\n-                    case FileDelimiter.RECORD_TYPE_RECORD:\n+        RecordFile recordFile = Utility.parseRecordFile(streamFileData.getFilename(), true);\n+        String fileName = Utility.getFileName(streamFileData.getFilename());\n+        recordFile.setLoadStart(startTime.getEpochSecond());\n+        int counter = 0;\n+        boolean success = false;\n+        try {\n+            if (!Utility.verifyHashChain(recordFile.getPreviousHash(),\n+                    applicationStatusRepository.findByStatusCode(ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH),\n+                    parserProperties.getMirrorProperties().getVerifyHashAfter(), fileName)) {\n+                throw new ParserException(\"Hash mismatch for file \" + fileName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyMTE2MA=="}, "originalCommit": {"oid": "827fce5f27c0599e5949216b0958760b1985718e"}, "originalPosition": 134}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1038, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}