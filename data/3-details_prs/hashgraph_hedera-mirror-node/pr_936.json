{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MzQ3MjU2", "number": 936, "title": "Moved RecordFileParser file system logic to RecordFilePoller", "bodyText": "Detailed description:\nRecordFileParser contains both file system logic to poll for rcd files from file system as well as controller logic to parse said files and persist in DB.\nThis makes for a complex file.\nIn the case of #899 it's difficult to separate and ensure proper rollback of data persistence in cases like Shutdown.\nThis change:\n\nAdd FilePoller interface with the poll() function\nAdd RecordFilePoller that implements FilePoller and contains all the logic to poll the file system for new valid rcd files\nUpdate FileParser signature to support StreamFileData implementations for parsing\nUpdate RecordFileParser, renaming loadRecordFile() -> parse(StreamFileData ...) therefore implementing FileParser.\nAlso makes parse() @Transactional to ensure rollback should shutdown or other event require persisted transactions from an rcd file to be reverted.\nUpdated broken tests and moves poll() tests into new RecordFilePollerTest\nMoved record_file and application_status updates post file process into the OnEnd so as to pick up the transactional logic\nWrapped cache managers to be able to support transactional operations\nAdded tests to verify above recovery scenarios\n\nWhich issue(s) this PR fixes:\nFixes #899\nFixes #568\nSpecial notes for your reviewer:\nVerified above logic running local parser against testnet bucket. Simulating error cases for mid batch, post batch,  record_file and application status errors and viewed the parser recover.\nThere does seem to be some further configuration that needs to be done to ensure spring date spa repositories and hibernate transaction management logic are always in sync. This is confused by the fact that we sue repositories, prepared statements and CopyManager to update the db.\nFuture items including using repositories only and utilizing improved ingestion options such as timescaleDb etc will simplify the code and reduce the complication.\nAs such we can wait on those or address the transaction management in another ticket.\nChecklist\n\n Documentation added\n Tests updated", "createdAt": "2020-08-07T02:02:21Z", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936", "merged": true, "mergeCommit": {"oid": "0ebf5ce7665af79a3a3e2498b60bd5159a139a7d"}, "closed": true, "closedAt": "2020-08-24T17:20:44Z", "author": {"login": "Nana-EC"}, "timelineItems": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc8m208gFqTQ2MzM4MjA2OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdCFSk7AFqTQ3MzY2ODYwMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzMzgyMDY5", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#pullrequestreview-463382069", "createdAt": "2020-08-07T14:57:55Z", "commit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNDo1Nzo1NVrOG9dLEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjoxNzowN1rOG9f3hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NDI5MA==", "bodyText": "This sort can be removed since it's already sorted above.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467094290", "createdAt": "2020-08-07T14:57:55Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)\n+                        .collect(Collectors.toList());\n+\n+                if (fullPaths != null && fullPaths.size() != 0) {\n+                    log.trace(\"Processing record files: {}\", fullPaths);\n+                    loadRecordFiles(fullPaths);\n+                } else {\n+                    log.debug(\"No files to parse\");\n+                }\n+            } else {\n+                log.error(\"Input parameter is not a folder: {}\", path);\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Error parsing files\", e);\n+        }\n+    }\n+\n+    /**\n+     * read and parse a list of record files\n+     *\n+     * @throws Exception\n+     */\n+    private void loadRecordFiles(List<String> filePaths) {\n+        Collections.sort(filePaths);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NjAwNQ==", "bodyText": "This can return null. We should return here if so.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467096005", "createdAt": "2020-08-07T15:00:50Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NjY4NA==", "bodyText": "It would be more efficient to not loop over the files twice: once here and once again in loadRecordFiles(). Better to just add the valid path inside the loop in loadRecordFiles.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467096684", "createdAt": "2020-08-07T15:02:00Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5OTY4MQ==", "bodyText": "Can we change to use validPath.resolve(s) here so it's Windows compatible?", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467099681", "createdAt": "2020-08-07T15:07:22Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEwNzgwMQ==", "bodyText": "As order is very important here and this approach does not verify that, we should use InOrder mock to verify files are processed in order.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467107801", "createdAt": "2020-08-07T15:21:38Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFilePollerTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoInteractions;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.List;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Mock;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+\n+import com.hedera.mirror.importer.FileCopier;\n+import com.hedera.mirror.importer.MirrorProperties;\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamType;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+\n+@ExtendWith(MockitoExtension.class)\n+public class RecordFilePollerTest {\n+\n+    @TempDir\n+    Path dataPath;\n+    @Mock\n+    RecordFileParser recordFileParser;\n+    private FileCopier fileCopier;\n+    private RecordFilePoller recordFilePoller;\n+    private RecordParserProperties parserProperties;\n+\n+    private File file1;\n+    private File file2;\n+    private static final long FILE1_CONSENSUS_START = 1567188600419072000L;\n+    private static final int NUM_TXNS_FILE_1 = 19;\n+    private static final int NUM_TXNS_FILE_2 = 15;\n+    private static RecordFile recordFile1;\n+    private static RecordFile recordFile2;\n+\n+    @BeforeEach\n+    void before() {\n+        var mirrorProperties = new MirrorProperties();\n+        mirrorProperties.setDataPath(dataPath);\n+        parserProperties = new RecordParserProperties(mirrorProperties);\n+        parserProperties.setKeepFiles(false);\n+        parserProperties.init();\n+        recordFilePoller = new RecordFilePoller(parserProperties, recordFileParser);\n+        StreamType streamType = StreamType.RECORD;\n+        fileCopier = FileCopier\n+                .create(Path.of(getClass().getClassLoader().getResource(\"data\").getPath()), dataPath)\n+                .from(streamType.getPath(), \"v2\", \"record0.0.3\")\n+                .filterFiles(\"*.rcd\")\n+                .to(streamType.getPath(), streamType.getValid());\n+        file1 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_00.419072Z.rcd\").toFile();\n+        file2 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_05.249678Z.rcd\").toFile();\n+        recordFile1 = new RecordFile(1567188600419072000L, 1567188604906443001L, null, file1.getName(), 0L, 0L,\n+                \"591558e059bd1629ee386c4e35a6875b4c67a096718f5d225772a651042715189414df7db5588495efb2a85dc4a0ffda\",\n+                \"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", 2);\n+\n+        recordFile2 = new RecordFile(1567188605249678000L, 1567188609705382001L, null, file2.getName(), 0L, 0L,\n+                \"5ed51baeff204eb6a2a68b76bbaadcb9b6e7074676c1746b99681d075bef009e8d57699baaa6342feec4e83726582d36\",\n+                recordFile1.getFileHash(), 2);\n+    }\n+\n+    @Test\n+    void poll() throws Exception {\n+        // given\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void pollAndKeepFiles() throws Exception {\n+        // given\n+        parserProperties.setKeepFiles(true);\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void noFiles() throws Exception {\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertParsedFiles();\n+        verifyNoInteractions(recordFileParser);\n+    }\n+\n+    // Asserts that recordFileParser.parse is called wth exactly the given fileNames.\n+    private void assertParse(String... fileNames) {\n+        ArgumentCaptor<StreamFileData> captor = ArgumentCaptor.forClass(StreamFileData.class);\n+        verify(recordFileParser, times(fileNames.length)).parse(captor.capture());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExMjMzMA==", "bodyText": "This exception is already logged in poller, so this log can be removed.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467112330", "createdAt": "2020-08-07T15:29:29Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -165,6 +155,10 @@ public void loadRecordFile(StreamFileData streamFileData) {\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (Exception ex) {\n+            log.warn(\"Failed to parse {}\", streamFileData.getFilename(), ex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMjI5Ng==", "bodyText": "This seems to leak details of the parser. I think it makes more sense to catch DuplicateFileException in the parser and ignore it there.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467122296", "createdAt": "2020-08-07T15:47:30Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)\n+                        .collect(Collectors.toList());\n+\n+                if (fullPaths != null && fullPaths.size() != 0) {\n+                    log.trace(\"Processing record files: {}\", fullPaths);\n+                    loadRecordFiles(fullPaths);\n+                } else {\n+                    log.debug(\"No files to parse\");\n+                }\n+            } else {\n+                log.error(\"Input parameter is not a folder: {}\", path);\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Error parsing files\", e);\n+        }\n+    }\n+\n+    /**\n+     * read and parse a list of record files\n+     *\n+     * @throws Exception\n+     */\n+    private void loadRecordFiles(List<String> filePaths) {\n+        Collections.sort(filePaths);\n+        for (String filePath : filePaths) {\n+            if (ShutdownHelper.isStopping()) {\n+                return;\n+            }\n+\n+            File file = new File(filePath);\n+\n+            try (InputStream fileInputStream = new FileInputStream(file)) {\n+                recordFileParser.parse(new StreamFileData(filePath, fileInputStream));\n+\n+                if (parserProperties.isKeepFiles()) {\n+                    Utility.archiveFile(file, parserProperties.getParsedPath());\n+                } else {\n+                    FileUtils.deleteQuietly(file);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.warn(\"File does not exist {}\", filePath);\n+                return;\n+            } catch (Exception e) {\n+                log.error(\"Error parsing file {}\", filePath, e);\n+                if (!(e instanceof DuplicateFileException)) { // if DuplicateFileException, continue with other", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzA1Mw==", "bodyText": "There's a problem here. In SqlEntityListener we manually manage the commit and that can cause issues. From connection.setAutoCommit():\nIf this method is called during a transaction and the auto-commit mode is changed, the transaction is committed. If setAutoCommit is called and the auto-commit mode is not changed, the call is a no-op.\n\nAlso, I'm not sure the impact of explicitly calling commit on a connection with an outer transaction in progress. It would be better to remove setAutoCommit and not call commit/rollback manually. This annotation should handle it for us. But please verify with tests.\nI'm not sure about the connection and statements in SqlEntityListener. I'm guessing we still need to manually close those but since they're proxied they won't actually close until the outer transaction completes.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467133053", "createdAt": "2020-08-07T16:07:01Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -135,11 +123,13 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n     }\n \n     /**\n-     * Given a service record name, read and parse and return as a list of service record pair\n+     * Given a stream file data representing an rcd file from the service parse record items and persist changes\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) {\n+    @Override\n+    @Transactional", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzkxMQ==", "bodyText": "Can remove rollback changes comment, considering previous change.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467133911", "createdAt": "2020-08-07T16:08:36Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -165,6 +155,10 @@ public void loadRecordFile(StreamFileData streamFileData) {\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (Exception ex) {\n+            log.warn(\"Failed to parse {}\", streamFileData.getFilename(), ex);\n+            recordStreamFileListener.onError(); // rollback changes", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzNDUxNg==", "bodyText": "Unused", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467134516", "createdAt": "2020-08-07T16:09:40Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODA0Ng==", "bodyText": "I don't think it's possible to truly test database rollback with mocks. Since it's probably tricky to convert this to not use mocks, we can probably create a separate integration test file just for the rollback test.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467138046", "createdAt": "2020-08-07T16:16:28Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();\n+        doThrow(ParserSQLException.class).when(applicationStatusRepository).updateStatusValue(any(), any());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODA3MQ==", "bodyText": "There should be nothing processed, right? This test is suspect.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467138071", "createdAt": "2020-08-07T16:16:30Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();\n+        doThrow(ParserSQLException.class).when(applicationStatusRepository).updateStatusValue(any(), any());\n+\n+        // when\n+        Assertions.assertThrows(ParserSQLException.class, () -> {\n+            recordFileParser.parse(streamFileData2);\n+        });\n+\n+        // then\n+        assertProcessedFile(streamFileData2, recordFile2, NUM_TXNS_FILE_2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODQzNw==", "bodyText": "Please add a test for not a directory. Would also be good to check code coverage report and see if any other paths not tested.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467138437", "createdAt": "2020-08-07T16:17:07Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFilePollerTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoInteractions;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.List;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Mock;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+\n+import com.hedera.mirror.importer.FileCopier;\n+import com.hedera.mirror.importer.MirrorProperties;\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamType;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+\n+@ExtendWith(MockitoExtension.class)\n+public class RecordFilePollerTest {\n+\n+    @TempDir\n+    Path dataPath;\n+    @Mock\n+    RecordFileParser recordFileParser;\n+    private FileCopier fileCopier;\n+    private RecordFilePoller recordFilePoller;\n+    private RecordParserProperties parserProperties;\n+\n+    private File file1;\n+    private File file2;\n+    private static final long FILE1_CONSENSUS_START = 1567188600419072000L;\n+    private static final int NUM_TXNS_FILE_1 = 19;\n+    private static final int NUM_TXNS_FILE_2 = 15;\n+    private static RecordFile recordFile1;\n+    private static RecordFile recordFile2;\n+\n+    @BeforeEach\n+    void before() {\n+        var mirrorProperties = new MirrorProperties();\n+        mirrorProperties.setDataPath(dataPath);\n+        parserProperties = new RecordParserProperties(mirrorProperties);\n+        parserProperties.setKeepFiles(false);\n+        parserProperties.init();\n+        recordFilePoller = new RecordFilePoller(parserProperties, recordFileParser);\n+        StreamType streamType = StreamType.RECORD;\n+        fileCopier = FileCopier\n+                .create(Path.of(getClass().getClassLoader().getResource(\"data\").getPath()), dataPath)\n+                .from(streamType.getPath(), \"v2\", \"record0.0.3\")\n+                .filterFiles(\"*.rcd\")\n+                .to(streamType.getPath(), streamType.getValid());\n+        file1 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_00.419072Z.rcd\").toFile();\n+        file2 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_05.249678Z.rcd\").toFile();\n+        recordFile1 = new RecordFile(1567188600419072000L, 1567188604906443001L, null, file1.getName(), 0L, 0L,\n+                \"591558e059bd1629ee386c4e35a6875b4c67a096718f5d225772a651042715189414df7db5588495efb2a85dc4a0ffda\",\n+                \"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", 2);\n+\n+        recordFile2 = new RecordFile(1567188605249678000L, 1567188609705382001L, null, file2.getName(), 0L, 0L,\n+                \"5ed51baeff204eb6a2a68b76bbaadcb9b6e7074676c1746b99681d075bef009e8d57699baaa6342feec4e83726582d36\",\n+                recordFile1.getFileHash(), 2);\n+    }\n+\n+    @Test\n+    void poll() throws Exception {\n+        // given\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void pollAndKeepFiles() throws Exception {\n+        // given\n+        parserProperties.setKeepFiles(true);\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void noFiles() throws Exception {\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertParsedFiles();\n+        verifyNoInteractions(recordFileParser);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 126}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY0NjIyMDM4", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#pullrequestreview-464622038", "createdAt": "2020-08-10T22:09:24Z", "commit": {"oid": "d83065ef5dd67f75f00548058af9bd067ab89a0b"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMjowOToyNFrOG-hgQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMzoxOTo1MlrOG-i9bQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxMzgyNw==", "bodyText": "This is missing a return statement. We don't want to continue to the next file if a parsing error on current file.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r468213827", "createdAt": "2020-08-10T22:09:24Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,110 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                if (files == null || files.length == 0) {\n+                    log.debug(\"No files to parse in directory {}\", file.getPath());\n+                    return;\n+                }\n+\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                log.trace(\"Processing record files: {}\", files);\n+                loadRecordFiles(files);\n+            } else {\n+                log.error(\"Input parameter is not a folder: {}\", path);\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Error parsing files\", e);\n+        }\n+    }\n+\n+    /**\n+     * read and parse a list of record files\n+     *\n+     * @throws Exception\n+     */\n+    private void loadRecordFiles(String[] filePaths) {\n+        Path validPath = parserProperties.getValidPath();\n+        for (String filePath : filePaths) {\n+            if (ShutdownHelper.isStopping()) {\n+                return;\n+            }\n+\n+            // get file from full path\n+            File file = validPath.resolve(filePath).toFile();\n+\n+            try (InputStream fileInputStream = new FileInputStream(file)) {\n+                recordFileParser.parse(new StreamFileData(file.getAbsolutePath(), fileInputStream));\n+\n+                if (parserProperties.isKeepFiles()) {\n+                    Utility.archiveFile(file, parserProperties.getParsedPath());\n+                } else {\n+                    FileUtils.deleteQuietly(file);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.warn(\"File does not exist {}\", filePath);\n+                return;\n+            } catch (Exception e) {\n+                log.error(String.format(\"Error parsing file %s\", filePath), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d83065ef5dd67f75f00548058af9bd067ab89a0b"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIzNjk1MQ==", "bodyText": "Should remove rollbackFor. I don't see a reason to exclude Error.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r468236951", "createdAt": "2020-08-10T23:17:20Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -135,11 +124,13 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n     }\n \n     /**\n-     * Given a service record name, read and parse and return as a list of service record pair\n+     * Given a stream file data representing an rcd file from the service parse record items and persist changes\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) {\n+    @Override\n+    @Transactional(rollbackFor = Exception.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d83065ef5dd67f75f00548058af9bd067ab89a0b"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIzNzY3Nw==", "bodyText": "I think it's fine to just log this instead of rethrow. Also, because we want to ensure cleanup() is always called.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r468237677", "createdAt": "2020-08-10T23:19:52Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -198,9 +184,11 @@ private void executeBatch(PreparedStatement ps, String entity) {\n \n     private void closeConnectionAndStatements() {\n         try {\n-            sqlInsertEntityId.close();\n-            sqlNotifyTopicMessage.close();\n-            connection.close();\n+            if (connection != null) {\n+                sqlInsertEntityId.close();\n+                sqlNotifyTopicMessage.close();\n+                connection.close();\n+            }\n         } catch (SQLException e) {\n             throw new ParserSQLException(\"Error closing connection\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d83065ef5dd67f75f00548058af9bd067ab89a0b"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NzM3NjU4", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#pullrequestreview-469737658", "createdAt": "2020-08-18T18:57:20Z", "commit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxODo1NzoyMFrOHCh0VQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNTowOToxOFrOHDL75A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxMzI2OQ==", "bodyText": "I believe moving this into SqlEntityListener breaks PubSub as it doesn't update the last processed hash.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r472413269", "createdAt": "2020-08-18T18:57:20Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -160,16 +149,19 @@ public void loadRecordFile(StreamFileData streamFileData) {\n             recordFile.setLoadStart(startTime.getEpochSecond());\n             recordFile.setLoadEnd(Instant.now().getEpochSecond());\n             recordStreamFileListener.onEnd(recordFile);\n-            applicationStatusRepository.updateStatusValue(\n-                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxNjk1Ng==", "bodyText": "nit: Would prefer parameters be named the same as the fields as most of the other fields are.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r472416956", "createdAt": "2020-08-18T19:04:26Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -95,13 +105,20 @@\n     private PreparedStatement sqlNotifyTopicMessage;\n     private Connection connection;\n     private long batchCount;\n+    private RecordFile partialRecordFile;\n \n     public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n                              RecordFileRepository recordFileRepository, MeterRegistry meterRegistry,\n-                             @Qualifier(CacheConfiguration.NEVER_EXPIRE_LARGE) CacheManager cacheManager) {\n+                             @Qualifier(CacheConfiguration.NEVER_EXPIRE_LARGE) CacheManager cacheManager,\n+                             ApplicationStatusRepository applicationStatusRepository,\n+                             TransactionRepository transactionRepository,\n+                             RecordParserProperties parserProps) {\n         this.dataSource = dataSource;\n+        this.applicationStatusRepository = applicationStatusRepository;\n         this.recordFileRepository = recordFileRepository;\n+        this.transactionRepository = transactionRepository;\n         sqlProperties = properties;\n+        parserProperties = parserProps;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQyNTk0OQ==", "bodyText": "Turns out record_file.name has a unique constraint so this can't happen. Remove", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r472425949", "createdAt": "2020-08-18T19:20:42Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -160,16 +149,19 @@ public void loadRecordFile(StreamFileData streamFileData) {\n             recordFile.setLoadStart(startTime.getEpochSecond());\n             recordFile.setLoadEnd(Instant.now().getEpochSecond());\n             recordStreamFileListener.onEnd(recordFile);\n-            applicationStatusRepository.updateStatusValue(\n-                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (DuplicateFileException ex) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjUxMDU0Ng==", "bodyText": "name is unique so this repository method can only ever return one entry. Should be adjusted to return single optional and handle method be adjusted.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r472510546", "createdAt": "2020-08-18T21:43:59Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -120,14 +137,18 @@ public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n         liveHashes = new ArrayList<>();\n         entityIds = new HashSet<>();\n         topicMessages = new ArrayList<>();\n+        partialRecordFile = null;\n     }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = FilenameUtils.getName(streamFileData.getFilename());\n \n-        if (recordFileRepository.findByName(fileName).size() > 0) {\n-            throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n+        List<RecordFile> matchingRecords = recordFileRepository.findByName(fileName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEwMDAyMA==", "bodyText": "Filesystem is case sensitive so comparison should be as well.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r473100020", "createdAt": "2020-08-19T15:04:41Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -314,6 +339,65 @@ private void notifyTopicMessages() {\n         executeBatch(sqlNotifyTopicMessage, \"topic notifications\");\n     }\n \n+    /**\n+     * Handle purely duplicate file in addition to faulty transaction management between spring repositories and manual\n+     * java sql operations. Check whether the given file was partially processed and matches the last record file\n+     * processed where the record_file table was updated but application_status table LAST_PROCESSED_RECORD_HASH was not\n+     * updated. If so update application_status table LAST_PROCESSED_RECORD_HASH appropriately with correct hash. Also\n+     * handle case where transaction data was rolled bach but not record_file entry\n+     *\n+     * @param recordFile record file object that was partially processed file\n+     */\n+    private void handleDuplicateOrPartiallyProcessedRecordFile(RecordFile recordFile) {\n+        // handle faulty transaction management between spring repositories and manual java sql operations\n+        DuplicateFileException duplicateFileException = new DuplicateFileException(\"File already exists in the \" +\n+                \"database: \" + recordFile.getName());\n+        RecordFile latestRecordFile = recordFileRepository.findTopByOrderByConsensusEndDesc().get();\n+        String lastRecordFileHash =\n+                applicationStatusRepository.findByStatusCode(ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH);\n+\n+        // check if latest record file is a match processed file by comparing name and hash\n+        if (latestRecordFile.getName().equalsIgnoreCase(recordFile.getName()) &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEwMzMzMg==", "bodyText": "I don't see any point to this method. It only recovers from the scenario when record file saves but application status does not. This scenario can be easily handled by putting a @Transactional on onEnd() to ensure they're committed together. There shouldn't be any problem with that approach since they're both repositories.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r473103332", "createdAt": "2020-08-19T15:09:18Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -314,6 +339,65 @@ private void notifyTopicMessages() {\n         executeBatch(sqlNotifyTopicMessage, \"topic notifications\");\n     }\n \n+    /**\n+     * Handle purely duplicate file in addition to faulty transaction management between spring repositories and manual\n+     * java sql operations. Check whether the given file was partially processed and matches the last record file\n+     * processed where the record_file table was updated but application_status table LAST_PROCESSED_RECORD_HASH was not\n+     * updated. If so update application_status table LAST_PROCESSED_RECORD_HASH appropriately with correct hash. Also\n+     * handle case where transaction data was rolled bach but not record_file entry\n+     *\n+     * @param recordFile record file object that was partially processed file\n+     */\n+    private void handleDuplicateOrPartiallyProcessedRecordFile(RecordFile recordFile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 137}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5bb96f354bf15cbf85e8f6e4364f78d9eaa604e0", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/5bb96f354bf15cbf85e8f6e4364f78d9eaa604e0", "committedDate": "2020-08-21T07:11:32Z", "message": "Moved RecordFileParser file system logic to RecordFilePoller\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b23462bdfa1fc2dd0371e85e5153ad447f8a828", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/3b23462bdfa1fc2dd0371e85e5153ad447f8a828", "committedDate": "2020-08-21T07:11:32Z", "message": "Removed transactional from SqlEntityListener.OnEnd\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ba305984f72c15a7c9551fbc3c82369f518b883", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/2ba305984f72c15a7c9551fbc3c82369f518b883", "committedDate": "2020-08-21T07:11:33Z", "message": "Addressed feedback if poller and parser\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f345762e97fe2b4a66de19759ceb96eacc11885", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/1f345762e97fe2b4a66de19759ceb96eacc11885", "committedDate": "2020-08-21T07:11:33Z", "message": "Cleaned up a bit\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72492b10ad6b2e5e0fb0b9f2c18f738433dff64a", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/72492b10ad6b2e5e0fb0b9f2c18f738433dff64a", "committedDate": "2020-08-21T07:11:33Z", "message": "Added poller tests\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e52bb6ce52d20fab8c3b45955376252c24fe4cd", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/1e52bb6ce52d20fab8c3b45955376252c24fe4cd", "committedDate": "2020-08-21T07:11:33Z", "message": "Addressed additional feedback\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba7113b865a5b82f3cf74e3c8c06f6979e29dc86", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/ba7113b865a5b82f3cf74e3c8c06f6979e29dc86", "committedDate": "2020-08-21T07:11:33Z", "message": "Reverted to autocommit false, fixed rollback test\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f751b76b7302fa4d8b274c54c191ac2bb9d151b", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/2f751b76b7302fa4d8b274c54c191ac2bb9d151b", "committedDate": "2020-08-21T07:11:33Z", "message": "Add logic to handle and recover from transaction management issues\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "06cadc07aaf127073e2d82d093dc4605b1db342a", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/06cadc07aaf127073e2d82d093dc4605b1db342a", "committedDate": "2020-08-21T07:11:33Z", "message": "Increased hikari max pool size andrenamed some tests\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b2939cc5c9779e6d415c9d1e674f37356378ab8", "author": {"user": {"login": "steven-sheehy", "name": "Steven Sheehy"}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/2b2939cc5c9779e6d415c9d1e674f37356378ab8", "committedDate": "2020-08-21T07:11:33Z", "message": "Use @Transactional\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e962d8dd5fe0a06ae248ba04f604c44e24be85ea", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/e962d8dd5fe0a06ae248ba04f604c44e24be85ea", "committedDate": "2020-08-21T07:11:33Z", "message": "Add release connection, convert entity persistence to use repositories and fixed tests\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cab3a93ee3e294ad55a39b4f1501658c574f035d", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/cab3a93ee3e294ad55a39b4f1501658c574f035d", "committedDate": "2020-08-21T07:16:13Z", "message": "Moved RecordFileParser file system logic to RecordFilePoller\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4cc3eb744b66c38142331083132933c04c42c6e", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/b4cc3eb744b66c38142331083132933c04c42c6e", "committedDate": "2020-08-21T07:16:14Z", "message": "Removed transactional from SqlEntityListener.OnEnd\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e969c08a4d361fcc7d24aa828f08a0033611ed2b", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/e969c08a4d361fcc7d24aa828f08a0033611ed2b", "committedDate": "2020-08-21T07:21:28Z", "message": "Addressed feedback if poller and parser\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a43a5910249d6663de2bd74846727eb02670ff0e", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/a43a5910249d6663de2bd74846727eb02670ff0e", "committedDate": "2020-08-21T07:22:13Z", "message": "Cleaned up a bit\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a396e69ac30e7a9aa88358fde0acce5ba73e25f2", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/a396e69ac30e7a9aa88358fde0acce5ba73e25f2", "committedDate": "2020-08-21T07:22:30Z", "message": "Addressed additional feedback\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "674faa4bcf2cccf21461e998fc75b46eea872ee9", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/674faa4bcf2cccf21461e998fc75b46eea872ee9", "committedDate": "2020-08-21T07:23:37Z", "message": "Reverted to autocommit false, fixed rollback test\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee307fc75600692c03bbb6379b6e6d8ec296ad41", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/ee307fc75600692c03bbb6379b6e6d8ec296ad41", "committedDate": "2020-08-21T07:25:08Z", "message": "Add logic to handle and recover from transaction management issues\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18c528c8a2e88b244409a5792d3151d2858da0d5", "author": {"user": {"login": "steven-sheehy", "name": "Steven Sheehy"}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/18c528c8a2e88b244409a5792d3151d2858da0d5", "committedDate": "2020-08-21T07:27:06Z", "message": "Use @Transactional\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5ec87ea6fd0637f301a66ffb3712ca3ff1d761a8", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/5ec87ea6fd0637f301a66ffb3712ca3ff1d761a8", "committedDate": "2020-08-21T07:10:50Z", "message": "Fix merge conflicts post rebase\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}, "afterCommit": {"oid": "18c528c8a2e88b244409a5792d3151d2858da0d5", "author": {"user": {"login": "steven-sheehy", "name": "Steven Sheehy"}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/18c528c8a2e88b244409a5792d3151d2858da0d5", "committedDate": "2020-08-21T07:27:06Z", "message": "Use @Transactional\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8f6e32df0462a0038977243d94254f453359b232", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/8f6e32df0462a0038977243d94254f453359b232", "committedDate": "2020-08-21T15:35:16Z", "message": "Removed duplicate file check from RecordParser and reverted back to HashSet for entityIds in SqlENtityListener\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/93ad6221c56a7010f711163484f0bd61c42f4054", "committedDate": "2020-08-21T15:38:50Z", "message": "Removed extra cache manager storage\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyNjYzODA2", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#pullrequestreview-472663806", "createdAt": "2020-08-21T17:24:47Z", "commit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNzoyNDo0N1rOHE1UCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQyMDo0MjowOVrOHE8x1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgyOTgzMg==", "bodyText": "This is not quite correct. If we inserted 0.0.2 during a previous file then get a rollback error next file it would be unnecessarily removed even though it exists in db. Please wrap entityCache with transactionAwareCacheDecorator instead.\nOr remove entityCache and entityIds and put @Cacheable(cacheManager = CacheConfiguration.NEVER_EXPIRE_LARGE) on entityRepository.insertEntityId().", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474829832", "createdAt": "2020-08-21T17:24:47Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgzNTIzNw==", "bodyText": "I think we should always clear entityIds and release connection. The connection doesn't really get released, it just gets a reference decremented since it's wrapped in an outer connection.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474835237", "createdAt": "2020-08-21T17:36:02Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));\n+        }\n+\n+        closeQuietly(sqlNotifyTopicMessage);\n+        sqlNotifyTopicMessage = null;\n         batchCount = 0;\n         contractResults.clear();\n         cryptoTransfers.clear();\n-        entityIds.clear();\n         fileData.clear();\n         liveHashes.clear();\n         nonFeeTransfers.clear();\n         topicMessages.clear();\n         transactions.clear();\n+\n+        if (releaseConnection) {\n+            entityIds.clear();\n+            DataSourceUtils.releaseConnection(connection, dataSource);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk1MjE1MA==", "bodyText": "This breaks encapsulation of SqlEntityListener since now it relies on EntityRecordItemListener to populate the same cache for this to work. So if SqlEntityListener is used outside of this scope, say from a test, it would not be functionally correct.\nBetter option would be to add a @CachePut to entityRepository.save() that updates the entity id cache. You can use @Caching to group.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474952150", "createdAt": "2020-08-21T20:42:09Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -241,22 +240,7 @@ public void onTransaction(Transaction transaction) throws ImporterException {\n \n     @Override\n     public void onEntityId(EntityId entityId) throws ImporterException {\n-        // add entities not found in cache to list of entities to be persisted\n-        if (entityCache.get(entityId.getId()) != null) {\n-            return;\n-        }\n-\n-        try {\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ID.ordinal(), entityId.getId());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_SHARD.ordinal(), entityId.getShardNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_REALM.ordinal(), entityId.getRealmNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_NUM.ordinal(), entityId.getEntityNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.TYPE.ordinal(), entityId.getType());\n-            sqlInsertEntityId.addBatch();\n-            entityIds.add(entityId);\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+        entityIds.add(entityId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 223}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3eedeade2e3496808da55fc270a1dc5619a4a4f5", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/3eedeade2e3496808da55fc270a1dc5619a4a4f5", "committedDate": "2020-08-22T22:33:50Z", "message": "Moved cache management back to SqlEntityListener and utilized TransactionAwareCacheDecorator to account for rollbacks\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30af6fa95cf88d42cf2393bb8e73576501308910", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/30af6fa95cf88d42cf2393bb8e73576501308910", "committedDate": "2020-08-22T22:39:32Z", "message": "Removed unused recordfilerepository method\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f77fdb95bbb25fc658f21b1d5919af8a8c7049c1", "author": {"user": {"login": "Nana-EC", "name": null}}, "url": "https://github.com/hashgraph/hedera-mirror-node/commit/f77fdb95bbb25fc658f21b1d5919af8a8c7049c1", "committedDate": "2020-08-24T16:01:11Z", "message": "Wrapped cachemanagers in TransactionAwareCacheManagerProxy\n\nSigned-off-by: Nana-EC <56320167+Nana-EC@users.noreply.github.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczNjY4NjAw", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#pullrequestreview-473668600", "createdAt": "2020-08-24T16:34:54Z", "commit": {"oid": "f77fdb95bbb25fc658f21b1d5919af8a8c7049c1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3033, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}