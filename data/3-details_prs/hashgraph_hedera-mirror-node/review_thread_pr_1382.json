{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQzNzU4ODY5", "number": 1382, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNjo1MzoyNVrOFIOhow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMToxNTozMFrOFJn1pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTcwOTE1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/exception/RecordFileReaderException.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNjo1MzoyNVrOIKB42A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNjo1MzoyNVrOIKB42A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4NzYwOA==", "bodyText": "Might be better to make this and its subclass stream agnostic for reuse with events v5 and new balance file. Maybe StreamFileReaderException and InvalidStreamFileException.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547387608", "createdAt": "2020-12-22T16:53:25Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/exception/RecordFileReaderException.java", "diffHunk": "@@ -0,0 +1,34 @@\n+package com.hedera.mirror.importer.exception;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+public class RecordFileReaderException extends ImporterException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTcxNzcwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "isResolved": false, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNjo1NTo1MVrOIKB9vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMDo1NDoyMVrOIOAu9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA==", "bodyText": "It would be better for RecordFileReader to be responsible for opening the BufferedInputStream as it already does for DataInputStream. Otherwise this duplicates code and leaks details of RecordFileReader with every client having to know that it needs a BufferedInputStream because internally it's doing a lot of small reads. StreamFileData should just store an InputStream.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547388860", "createdAt": "2020-12-22T16:55:51Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwNTcxNw==", "bodyText": "The reason for enforcing BufferedInputStream instead of the generic InputStream is RecordFileReader (most likely all stream file readers) needs a InputStream which supports mark and reset so the composite reader can peek the version, rewind it back, and then choose which implementation to parse the stream file.\nAn InputStream may or may not support mark/reset depending on the concrete subclass.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547505717", "createdAt": "2020-12-22T21:01:21Z", "author": {"login": "xin-hedera"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwODk1MQ==", "bodyText": "Isn't that even more of a reason for CompositeRecordFileReader to create the BufferedInputStream instead of relying it be passed in? Technically someone could extend BufferedInputStream and re-implement mark/reset to do nothing.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547508951", "createdAt": "2020-12-22T21:10:15Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUxMDkxNQ==", "bodyText": "yes exactly!", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547510915", "createdAt": "2020-12-22T21:15:58Z", "author": {"login": "xin-hedera"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUxOTA0OQ==", "bodyText": "I've gone back and forth on this myself, the actual file parsers need the BufferedInputStream created by the Composite in order to actually make use of the reset.  That can be passed as just an InputStream to respect the Interface method, but then the various implementations need to either know that the Composite has passed a BufferedInputStream without stating it, or you're going to end up wrapping a BufferedInputStream in another BufferedInputStream at some point.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547519049", "createdAt": "2020-12-22T21:38:05Z", "author": {"login": "ijungmann"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNDYwMA==", "bodyText": "I think some coupling between composite and versioned readers is fine. It's coupling between reader and its clients you want to avoid. Double buffering shouldn't really be a concern and should be caught by code reviews.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547524600", "createdAt": "2020-12-22T21:53:35Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1MDk0MA==", "bodyText": "I'll push the changes and we can review and discuss. I don't feel it's bad for the Composite to pass a DataInputStream(BufferedInputStream(InputStream)) to the concrete versioned reader. On the other side, if we choose BufferedInputStream over InputStream for StreamFileData the code looks cleaner and the requirement is made explicitly.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547550940", "createdAt": "2020-12-22T23:12:13Z", "author": {"login": "xin-hedera"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3MTQ0Mw==", "bodyText": "I think BufferedInputStream is the better option.\n\nmake it explicit in the contract that the readers need a buffered input stream for two reasons\n\nperformance, for when the source of the stream is a disk file and small size reads will be much faster\nmark/reset support, since the composite reader needs to peek at the version then rewind the read position\n\n\nthe implementations will not have the chance to make the mistake of not wrapping a InputStream with a BufferedInputStream\n\nRegarding the concern of a subclass of BufferedInputStream being passed which does not support mark/reset, it's the subclass which violates the contract of BufferedInputStream.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r548071443", "createdAt": "2020-12-23T17:29:40Z", "author": {"login": "xin-hedera"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTU2Mjk5Ng==", "bodyText": "Just as as followup, I ended up going with the InputStream in my related PR and allowing for a little bit of coupling between the readers, as suggested by Steven.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r551562996", "createdAt": "2021-01-04T20:54:21Z", "author": {"login": "ijungmann"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM4ODg2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc2MTAxOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzowOToyN1rOIKCX1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzowOToyN1rOIKCX1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5NTU0Mg==", "bodyText": "nit: Add method comments, brief overview of what to expect from the header", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547395542", "createdAt": "2020-12-22T17:09:27Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "diffHunk": "@@ -0,0 +1,154 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.google.common.primitives.Ints;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.commons.io.FilenameUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.InvalidRecordFileException;\n+import com.hedera.mirror.importer.exception.RecordFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@RequiredArgsConstructor\n+public abstract class AbstractRecordFileReader implements RecordFileReader {\n+\n+    protected static final String HASH_ALGORITHM = \"SHA-384\";\n+    protected static final int HASH_SIZE = 48; // 48-byte SHA-384 hash\n+    protected static final byte RECORD_FILE_MARKER_PREV_HASH = 1;\n+    protected static final byte RECORD_FILE_MARKER_RECORD = 2;\n+\n+    protected final Logger log = LogManager.getLogger(getClass());\n+    private final int readerVersion;\n+\n+    @Override\n+    public RecordFile read(@NonNull StreamFileData streamFileData, Consumer<RecordItem> itemConsumer) {\n+        try (DataInputStream dis = new DataInputStream(streamFileData.getBufferedInputStream())) {\n+            RecordFile recordFile = new RecordFile();\n+            RecordFileDigest digest = getRecordFileDigest();\n+\n+            recordFile.setName(FilenameUtils.getName(streamFileData.getFilename()));\n+            readHeader(dis, digest, recordFile);\n+            readBody(dis, digest, itemConsumer, recordFile);\n+            recordFile.setFileHash(Hex.encodeHexString(digest.digest()));\n+\n+            return recordFile;\n+        } catch (RecordFileReaderException e) {\n+            throw e;\n+        }  catch (Exception e) {\n+            throw new RecordFileReaderException(\"Error reading record file \" + streamFileData.getFilename(), e);\n+        }\n+    }\n+\n+    protected abstract RecordFileDigest getRecordFileDigest();\n+\n+    private void readHeader(DataInputStream dis, RecordFileDigest digest, RecordFile recordFile) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTc2MTQ4OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzowOTozOFrOIKCYKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzowOTozOFrOIKCYKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzM5NTYyNw==", "bodyText": "nit: Add method comments, brief overview of what to expect from the body", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547395627", "createdAt": "2020-12-22T17:09:38Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "diffHunk": "@@ -0,0 +1,154 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.google.common.primitives.Ints;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.commons.io.FilenameUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.InvalidRecordFileException;\n+import com.hedera.mirror.importer.exception.RecordFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@RequiredArgsConstructor\n+public abstract class AbstractRecordFileReader implements RecordFileReader {\n+\n+    protected static final String HASH_ALGORITHM = \"SHA-384\";\n+    protected static final int HASH_SIZE = 48; // 48-byte SHA-384 hash\n+    protected static final byte RECORD_FILE_MARKER_PREV_HASH = 1;\n+    protected static final byte RECORD_FILE_MARKER_RECORD = 2;\n+\n+    protected final Logger log = LogManager.getLogger(getClass());\n+    private final int readerVersion;\n+\n+    @Override\n+    public RecordFile read(@NonNull StreamFileData streamFileData, Consumer<RecordItem> itemConsumer) {\n+        try (DataInputStream dis = new DataInputStream(streamFileData.getBufferedInputStream())) {\n+            RecordFile recordFile = new RecordFile();\n+            RecordFileDigest digest = getRecordFileDigest();\n+\n+            recordFile.setName(FilenameUtils.getName(streamFileData.getFilename()));\n+            readHeader(dis, digest, recordFile);\n+            readBody(dis, digest, itemConsumer, recordFile);\n+            recordFile.setFileHash(Hex.encodeHexString(digest.digest()));\n+\n+            return recordFile;\n+        } catch (RecordFileReaderException e) {\n+            throw e;\n+        }  catch (Exception e) {\n+            throw new RecordFileReaderException(\"Error reading record file \" + streamFileData.getFilename(), e);\n+        }\n+    }\n+\n+    protected abstract RecordFileDigest getRecordFileDigest();\n+\n+    private void readHeader(DataInputStream dis, RecordFileDigest digest, RecordFile recordFile) throws IOException {\n+        // record file version\n+        int version = dis.readInt();\n+        checkField(version, readerVersion, \"record file version\", recordFile.getName());\n+\n+        int hapiVersion = dis.readInt();\n+\n+        // previous record file hash\n+        byte marker = dis.readByte();\n+        checkField(marker, RECORD_FILE_MARKER_PREV_HASH, \"RECORD_FILE_MARKER_PREV_HASH\", recordFile.getName());\n+        byte[] prevHash = dis.readNBytes(HASH_SIZE);\n+        checkField(prevHash.length, HASH_SIZE, \"previous hash size\", recordFile.getName());\n+\n+        digest.updateHeader(Ints.toByteArray(version));\n+        digest.updateHeader(Ints.toByteArray(hapiVersion));\n+        digest.updateHeader(marker);\n+        digest.updateHeader(prevHash);\n+\n+        recordFile.setRecordFormatVersion(version);\n+        recordFile.setPreviousHash(Hex.encodeHexString(prevHash));\n+    }\n+\n+    private void readBody(DataInputStream dis, RecordFileDigest digest, Consumer<RecordItem> itemConsumer,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTgwODE5OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoyMzo1NlrOIKC0CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQyMTo1NDoyNVrOIKKREw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwMjc2MA==", "bodyText": "Think it might be better to create a method in Utility that opens the input stream quietly instead of leaking IOExceptions everywhere:\npublic static InputStream openQuietly(File file) {\n  try {\n    return new FileInputStream(file);\n  } catch (IOException e) {\n    throw new FileOperationException(\"Unable to open file \" + file.getPath(), e);\n  }\n}", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547402760", "createdAt": "2020-12-22T17:23:56Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -71,7 +78,11 @@ public void download() {\n      */\n     @Override\n     protected StreamFile readStreamFile(File file) {\n-        return Utility.parseRecordFile(file.getPath(), null);\n+        try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {\n+            return recordFileReader.read(new StreamFileData(file.getAbsolutePath(), bis), null);\n+        } catch (IOException e) {\n+            throw new FileOperationException(\"Unable to open record file \" + file.getPath(), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUxMzA3MQ==", "bodyText": "this is definitely better. however, still have to bubble up / catch the IOException thrown during auto-close...", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547513071", "createdAt": "2020-12-22T21:21:36Z", "author": {"login": "xin-hedera"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -71,7 +78,11 @@ public void download() {\n      */\n     @Override\n     protected StreamFile readStreamFile(File file) {\n-        return Utility.parseRecordFile(file.getPath(), null);\n+        try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {\n+            return recordFileReader.read(new StreamFileData(file.getAbsolutePath(), bis), null);\n+        } catch (IOException e) {\n+            throw new FileOperationException(\"Unable to open record file \" + file.getPath(), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwMjc2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUyNDg4Mw==", "bodyText": "Which is why I noted that we can make reader be responsible for closing and clients shouldn't use try with resources.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547524883", "createdAt": "2020-12-22T21:54:25Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/downloader/record/RecordFileDownloader.java", "diffHunk": "@@ -71,7 +78,11 @@ public void download() {\n      */\n     @Override\n     protected StreamFile readStreamFile(File file) {\n-        return Utility.parseRecordFile(file.getPath(), null);\n+        try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {\n+            return recordFileReader.read(new StreamFileData(file.getAbsolutePath(), bis), null);\n+        } catch (IOException e) {\n+            throw new FileOperationException(\"Unable to open record file \" + file.getPath(), e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwMjc2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTgxNzI3OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoyNjozN1rOIKC5Nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzoyNjozN1rOIKC5Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwNDA4Nw==", "bodyText": "Maybe we should add another default method to RecordFileReader that only has StreamFileData and internally calls the overloaded method with null? e.g. recordFileReader.read(streamFileData)", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547404087", "createdAt": "2020-12-22T17:26:37Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/migration/V1_32_0__Missing_StreamFile_Record.java", "diffHunk": "@@ -108,7 +114,10 @@ private StreamFile readStreamFile(File file, StreamType streamType) {\n                     .name(file.getName())\n                     .build();\n         } else if (streamType == StreamType.RECORD) {\n-            streamFile = Utility.parseRecordFile(file.getPath(), null);\n+            try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(file))) {\n+                StreamFileData streamFileData = new StreamFileData(file.getAbsolutePath(), bis);\n+                streamFile = recordFileReader.read(streamFileData, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTg0Mjk0OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/domain/StreamFileData.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzozNDo1M1rOIKDIvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QwNDozNzoxN1rOIKQ-mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwODA2MA==", "bodyText": "q: I see the BufferedInputStream creation repeated in a few places. Do you think it's worth centralizing that here by making this non final and a LazyGetter.\nSo you'd create a new StreamFileData object passing in a File instead of fileName, it could then set the fileName in the constructor and then on demand getting could return bufferedInputStream", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547408060", "createdAt": "2020-12-22T17:34:53Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/domain/StreamFileData.java", "diffHunk": "@@ -20,11 +20,11 @@\n  * \u200d\n  */\n \n-import java.io.InputStream;\n+import java.io.BufferedInputStream;\n import lombok.Value;\n \n @Value\n public class StreamFileData {\n     private final String filename;\n-    private final InputStream inputStream;\n+    private final BufferedInputStream bufferedInputStream;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyMDQ0MA==", "bodyText": "I don't see the benefit doing so.\nIn all places StreamFileData is used, data is always read from it after BufferedInputStream is created so lazygetter just defers the creation.\nOn the other side, LazyGetter does complicate resource management, currently it's taken care of by the try-with-resources statement.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547420440", "createdAt": "2020-12-22T18:00:23Z", "author": {"login": "xin-hedera"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/domain/StreamFileData.java", "diffHunk": "@@ -20,11 +20,11 @@\n  * \u200d\n  */\n \n-import java.io.InputStream;\n+import java.io.BufferedInputStream;\n import lombok.Value;\n \n @Value\n public class StreamFileData {\n     private final String filename;\n-    private final InputStream inputStream;\n+    private final BufferedInputStream bufferedInputStream;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwODA2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzYzNDg0MA==", "bodyText": "Sounds good", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547634840", "createdAt": "2020-12-23T04:37:17Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/domain/StreamFileData.java", "diffHunk": "@@ -20,11 +20,11 @@\n  * \u200d\n  */\n \n-import java.io.InputStream;\n+import java.io.BufferedInputStream;\n import lombok.Value;\n \n @Value\n public class StreamFileData {\n     private final String filename;\n-    private final InputStream inputStream;\n+    private final BufferedInputStream bufferedInputStream;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQwODA2MA=="}, "originalCommit": {"oid": "84dd1ae99eedb9e2e86b49e5c35af8eeb437724f"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTkxNzUyOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo1ODoxNFrOIKD1Vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo1ODoxNFrOIKD1Vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxOTQ3OQ==", "bodyText": "String description should be regular English like the above record file version. It's also redundant to say \"record file\" again within the RecordFileReader. \"RECORD_FILE_MARKER_PREV_HASH\" -> \"previous hash marker\".\nSimilar note for other check fields.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547419479", "createdAt": "2020-12-22T17:58:14Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "diffHunk": "@@ -0,0 +1,155 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.google.common.primitives.Ints;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.commons.io.FilenameUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.ImporterException;\n+import com.hedera.mirror.importer.exception.InvalidRecordFileException;\n+import com.hedera.mirror.importer.exception.RecordFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@RequiredArgsConstructor\n+public abstract class AbstractRecordFileReader implements RecordFileReader {\n+\n+    protected static final String HASH_ALGORITHM = \"SHA-384\";\n+    protected static final int HASH_SIZE = 48; // 48-byte SHA-384 hash\n+    protected static final byte RECORD_FILE_MARKER_PREV_HASH = 1;\n+    protected static final byte RECORD_FILE_MARKER_RECORD = 2;\n+\n+    protected final Logger log = LogManager.getLogger(getClass());\n+    private final int readerVersion;\n+\n+    @Override\n+    public RecordFile read(@NonNull StreamFileData streamFileData, Consumer<RecordItem> itemConsumer) {\n+        try (DataInputStream dis = new DataInputStream(streamFileData.getBufferedInputStream())) {\n+            RecordFile recordFile = new RecordFile();\n+            RecordFileDigest digest = getRecordFileDigest();\n+\n+            recordFile.setName(FilenameUtils.getName(streamFileData.getFilename()));\n+            readHeader(dis, digest, recordFile);\n+            readBody(dis, digest, itemConsumer, recordFile);\n+            recordFile.setFileHash(Hex.encodeHexString(digest.digest()));\n+\n+            return recordFile;\n+        } catch (ImporterException e) {\n+            throw e;\n+        }  catch (Exception e) {\n+            throw new RecordFileReaderException(\"Error reading record file \" + streamFileData.getFilename(), e);\n+        }\n+    }\n+\n+    protected abstract RecordFileDigest getRecordFileDigest();\n+\n+    private void readHeader(DataInputStream dis, RecordFileDigest digest, RecordFile recordFile) throws IOException {\n+        // record file version\n+        int version = dis.readInt();\n+        checkField(version, readerVersion, \"record file version\", recordFile.getName());\n+\n+        int hapiVersion = dis.readInt();\n+\n+        // previous record file hash\n+        byte marker = dis.readByte();\n+        checkField(marker, RECORD_FILE_MARKER_PREV_HASH, \"RECORD_FILE_MARKER_PREV_HASH\", recordFile.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "944ae01598e3bfcc5b986a4eeb6f12be98d68db6"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MTkzMTQ1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODowMzoyMlrOIKD99g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODowMzoyMlrOIKD99g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQyMTY4Ng==", "bodyText": "Can drop the RECORD_FILE_ prefix since it's now within a record file specific class.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547421686", "createdAt": "2020-12-22T18:03:22Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "diffHunk": "@@ -0,0 +1,155 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.google.common.primitives.Ints;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.commons.io.FilenameUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.ImporterException;\n+import com.hedera.mirror.importer.exception.InvalidRecordFileException;\n+import com.hedera.mirror.importer.exception.RecordFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@RequiredArgsConstructor\n+public abstract class AbstractRecordFileReader implements RecordFileReader {\n+\n+    protected static final String HASH_ALGORITHM = \"SHA-384\";\n+    protected static final int HASH_SIZE = 48; // 48-byte SHA-384 hash\n+    protected static final byte RECORD_FILE_MARKER_PREV_HASH = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "944ae01598e3bfcc5b986a4eeb6f12be98d68db6"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjA0MzE1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/reader/record/RecordFileReaderTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNDozNVrOIKFDrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNDozNVrOIKFDrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQzOTUzMw==", "bodyText": "Use Path instead of format for Windows compatability.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547439533", "createdAt": "2020-12-22T18:34:35Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/reader/record/RecordFileReaderTest.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+import java.io.BufferedInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.commons.io.FileUtils;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.DynamicTest;\n+import org.junit.jupiter.api.TestFactory;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+\n+import com.hedera.mirror.importer.FileCopier;\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.InvalidRecordFileException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@ExtendWith(MockitoExtension.class)\n+abstract class RecordFileReaderTest {\n+    private final static String pathPrefix = \"data/recordstreams\";\n+\n+    protected FileCopier fileCopier;\n+    protected RecordFileReader recordFileReader;\n+    protected List<RecordFile> allRecordFiles;\n+\n+    @TempDir\n+    Path dataPath;\n+\n+    private static void corruptFile(Path p) {\n+        try {\n+            File file = p.toFile();\n+            if (file.isFile()) {\n+                FileUtils.writeStringToFile(file, \"corrupt\", \"UTF-8\", true);\n+            }\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static void truncateFile(Path p) {\n+        try {\n+            File file = p.toFile();\n+            if (file.isFile()) {\n+                FileChannel outChan = new FileOutputStream(file, true).getChannel();\n+                if (outChan.size() <= 48) {\n+                    outChan.truncate(outChan.size() / 2);\n+                } else {\n+                    outChan.truncate(48);\n+                }\n+                outChan.close();\n+            }\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    @BeforeEach\n+    void setup() {\n+        fileCopier = FileCopier\n+                .create(Path.of(getClass().getClassLoader().getResource(pathPrefix).getPath()), dataPath);\n+        recordFileReader = getRecordFileReader();\n+\n+        RecordFile recordFileV1_1 = new RecordFile(1561990380317763000L, 1561990399074934000L, null,\n+                \"2019-07-01T14:13:00.317763Z.rcd\", null, null,\n+                \"333d6940254659533fd6b939033e59c57fe8f4ff78375d1e687c032918aa0b7b8179c7fd403754274a8c91e0b6c0195a\",\n+                \"f423447a3d5a531a07426070e511555283daae063706242590949116f717a0524e4dd18f9d64e66c73982d475401db04\",\n+                null, 15L, 1);\n+        RecordFile recordFileV1_2 = new RecordFile(1561991340302068000L, 1561991353226225001L, null,\n+                \"2019-07-01T14:29:00.302068Z.rcd\", null, null,\n+                \"1faf198f8fdbefa59bde191f214d73acdc4f5c0f434677a7edf9591b129e21aea90a5b3119d2802cee522e7be6bc8830\",\n+                recordFileV1_1.getFileHash(), null, 69L, 1);\n+        RecordFile recordFileV2_1 = new RecordFile(1567188600419072000L, 1567188604906443001L, null,\n+                \"2019-08-30T18_10_00.419072Z.rcd\", null, null,\n+                \"591558e059bd1629ee386c4e35a6875b4c67a096718f5d225772a651042715189414df7db5588495efb2a85dc4a0ffda\",\n+                \"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\",\n+                null, 19L, 2);\n+        RecordFile recordFileV2_2 = new RecordFile(1567188605249678000L, 1567188609705382001L, null,\n+                \"2019-08-30T18_10_05.249678Z.rcd\", null, null,\n+                \"5ed51baeff204eb6a2a68b76bbaadcb9b6e7074676c1746b99681d075bef009e8d57699baaa6342feec4e83726582d36\",\n+                recordFileV2_1.getFileHash(), null, 15L, 2);\n+        allRecordFiles = List.of(recordFileV1_1, recordFileV1_2, recordFileV2_1, recordFileV2_2);\n+    }\n+\n+    @TestFactory\n+    Stream<DynamicTest> readValidFileWithConsumer() {\n+        String template = \"read valid version %d file %s\";\n+\n+        return DynamicTest.stream(\n+                getFilteredFiles(false),\n+                (recordFile) -> String.format(template, recordFile.getRecordFormatVersion(), recordFile.getName()),\n+                (recordFile) -> {\n+                    String filename = recordFile.getName();\n+                    Consumer<RecordItem> itemConsumer = mock(Consumer.class);\n+\n+                    // given\n+                    fileCopier.from(getSubPath(recordFile.getRecordFormatVersion())).filterFiles(filename).copy();\n+                    File inputFile = fileCopier.getTo().resolve(filename).toFile();\n+\n+                    // when\n+                    RecordFile actual;\n+                    try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(inputFile))) {\n+                        StreamFileData streamFileData = new StreamFileData(inputFile.getAbsolutePath(), bis);\n+                        actual = recordFileReader.read(streamFileData, itemConsumer);\n+                    }\n+\n+                    // then\n+                    assertThat(actual).isEqualTo(recordFile);\n+                    ArgumentCaptor<RecordItem> captor = ArgumentCaptor.forClass(RecordItem.class);\n+                    verify(itemConsumer, times(recordFile.getCount().intValue())).accept(captor.capture());\n+                    RecordItem[] recordItems = captor.getAllValues().toArray(new RecordItem[0]);\n+                    assertThat(recordItems[0].getConsensusTimestamp()).isEqualTo(recordFile.getConsensusStart());\n+                    assertThat(recordItems[recordItems.length - 1].getConsensusTimestamp()).isEqualTo(recordFile.getConsensusEnd());\n+                });\n+    }\n+\n+    @TestFactory\n+    Stream<DynamicTest> readValidFileWithoutConsumer() {\n+        String template = \"read valid version %d file %s\";\n+\n+        return DynamicTest.stream(\n+                getFilteredFiles(false),\n+                (recordFile) -> String.format(template, recordFile.getRecordFormatVersion(), recordFile.getName()),\n+                (recordFile) -> {\n+                    String filename = recordFile.getName();\n+\n+                    // given\n+                    fileCopier.from(getSubPath(recordFile.getRecordFormatVersion())).filterFiles(filename).copy();\n+                    File inputFile = fileCopier.getTo().resolve(filename).toFile();\n+\n+                    // when\n+                    RecordFile actual;\n+                    try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(inputFile))) {\n+                        StreamFileData streamFileData = new StreamFileData(inputFile.getAbsolutePath(), bis);\n+                        actual = recordFileReader.read(streamFileData, null);\n+                    }\n+\n+                    // then\n+                    assertThat(actual).isEqualTo(recordFile);\n+                });\n+    }\n+\n+    @TestFactory\n+    Stream<DynamicTest> readInvalidFileWithGarbageAppended() {\n+        String template = \"read corrupted version %d file %s\";\n+\n+        return DynamicTest.stream(\n+                getFilteredFiles(false),\n+                (recordFile) -> String.format(template, recordFile.getRecordFormatVersion(), recordFile.getName()),\n+                (recordFile) -> {\n+                    String filename = recordFile.getName();\n+\n+                    // given\n+                    fileCopier.from(getSubPath(recordFile.getRecordFormatVersion())).filterFiles(filename).copy();\n+                    File inputFile = fileCopier.getTo().resolve(filename).toFile();\n+                    Files.walk(dataPath).forEach(RecordFileReaderTest::corruptFile);\n+\n+                    // when\n+                    try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(inputFile))) {\n+                        StreamFileData streamFileData = new StreamFileData(inputFile.getAbsolutePath(), bis);\n+                        Assertions.assertThrows(InvalidRecordFileException.class,\n+                                () -> recordFileReader.read(streamFileData, null));\n+                    }\n+                });\n+    }\n+\n+    @TestFactory\n+    Stream<DynamicTest> readInvalidFileWithDataTruncated() {\n+        String template = \"read incomplete version %d file %s\";\n+\n+        return DynamicTest.stream(\n+                getFilteredFiles(false),\n+                (recordFile) -> String.format(template, recordFile.getRecordFormatVersion(), recordFile.getName()),\n+                (recordFile) -> {\n+                    String filename = recordFile.getName();\n+\n+                    // given\n+                    fileCopier.from(getSubPath(recordFile.getRecordFormatVersion())).filterFiles(filename).copy();\n+                    File inputFile = fileCopier.getTo().resolve(filename).toFile();\n+                    Files.walk(dataPath).forEach(RecordFileReaderTest::truncateFile);\n+\n+                    // when\n+                    try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(inputFile))) {\n+                        StreamFileData streamFileData = new StreamFileData(inputFile.getAbsolutePath(), bis);\n+                        Assertions.assertThrows(InvalidRecordFileException.class,\n+                                () -> recordFileReader.read(streamFileData, null));\n+                    }\n+                });\n+    }\n+\n+    protected Iterator<RecordFile> getFilteredFiles(boolean negate) {\n+        return allRecordFiles.stream()\n+                .filter((recordFile) -> negate ^ filterFile(recordFile.getRecordFormatVersion()))\n+                .collect(Collectors.toList())\n+                .iterator();\n+    }\n+\n+    protected String getSubPath(int version) {\n+        return String.format(\"v%d/record0.0.3\", version);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "944ae01598e3bfcc5b986a4eeb6f12be98d68db6"}, "originalPosition": 236}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjA0Nzg2OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/reader/record/RecordFileReaderTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNjoxNFrOIKFGbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODozNjoxNFrOIKFGbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0MDIzNg==", "bodyText": "Use Path", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547440236", "createdAt": "2020-12-22T18:36:14Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/reader/record/RecordFileReaderTest.java", "diffHunk": "@@ -0,0 +1,242 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+import java.io.BufferedInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.commons.io.FileUtils;\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.DynamicTest;\n+import org.junit.jupiter.api.TestFactory;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+\n+import com.hedera.mirror.importer.FileCopier;\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.InvalidRecordFileException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@ExtendWith(MockitoExtension.class)\n+abstract class RecordFileReaderTest {\n+    private final static String pathPrefix = \"data/recordstreams\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "944ae01598e3bfcc5b986a4eeb6f12be98d68db6"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ0MjA3MDk0OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/CompositeRecordFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo0NDoyOFrOIKFUHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxODo0NDoyOFrOIKFUHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ0Mzc0MQ==", "bodyText": "Since the passed in InputStream will be closed already by this CompositeRecordFileReader, we can make it part of the RecordFileReader contract/documentation that it will be responsible for closing and clients don't need to use a separate try with resources.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r547443741", "createdAt": "2020-12-22T18:44:28Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/CompositeRecordFileReader.java", "diffHunk": "@@ -0,0 +1,75 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import javax.inject.Named;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.springframework.context.annotation.Primary;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.InvalidRecordFileException;\n+import com.hedera.mirror.importer.exception.RecordFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@Named\n+@Primary\n+@Log4j2\n+@RequiredArgsConstructor\n+public class CompositeRecordFileReader implements RecordFileReader {\n+\n+    private final RecordFileReaderImplV1 version1Reader;\n+    private final RecordFileReaderImplV2 version2Reader;\n+\n+    @Override\n+    public RecordFile read(@NonNull StreamFileData streamFileData, Consumer<RecordItem> itemConsumer) {\n+        try (DataInputStream dis = new DataInputStream(streamFileData.getBufferedInputStream())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "944ae01598e3bfcc5b986a4eeb6f12be98d68db6"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjI2NTgwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMDozMjo1OFrOIMBgaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxNjowMjoyN1rOIMSlJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ3ODUwNA==", "bodyText": "This isn't the \"previous hash size\", it's the \"previous hash\" itself.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r549478504", "createdAt": "2020-12-28T20:32:58Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "diffHunk": "@@ -0,0 +1,178 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.google.common.primitives.Ints;\n+import java.io.BufferedInputStream;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.commons.io.FilenameUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.ImporterException;\n+import com.hedera.mirror.importer.exception.InvalidStreamFileException;\n+import com.hedera.mirror.importer.exception.StreamFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@RequiredArgsConstructor\n+public abstract class AbstractRecordFileReader implements RecordFileReader {\n+\n+    protected static final String HASH_ALGORITHM = \"SHA-384\";\n+    protected static final int HASH_SIZE = 48; // 48-byte SHA-384 hash\n+    protected static final byte PREV_HASH_MARKER = 1;\n+    protected static final byte RECORD_MARKER = 2;\n+\n+    protected final Logger log = LogManager.getLogger(getClass());\n+    private final int readerVersion;\n+\n+    @Override\n+    public RecordFile read(@NonNull StreamFileData streamFileData, Consumer<RecordItem> itemConsumer) {\n+        try (DataInputStream dis = new DataInputStream(new BufferedInputStream(streamFileData.getInputStream()))) {\n+            RecordFile recordFile = new RecordFile();\n+            RecordFileDigest digest = getRecordFileDigest();\n+\n+            recordFile.setName(FilenameUtils.getName(streamFileData.getFilename()));\n+            readHeader(dis, digest, recordFile);\n+            readBody(dis, digest, itemConsumer, recordFile);\n+            recordFile.setFileHash(Hex.encodeHexString(digest.digest()));\n+\n+            return recordFile;\n+        } catch (ImporterException e) {\n+            throw e;\n+        }  catch (Exception e) {\n+            throw new StreamFileReaderException(\"Error reading record file \" + streamFileData.getFilename(), e);\n+        }\n+    }\n+\n+    protected abstract RecordFileDigest getRecordFileDigest();\n+\n+    /**\n+     * Reads the record file header, updates the message digest with data from the header, and sets corresponding\n+     * {@link RecordFile} fields. {@code dis} should point at the beginning of the stream. The header should contain\n+     * file version, HAPI version, and the previous file hash.\n+     *\n+     * @param dis the {@link DataInputStream} of the record file\n+     * @param digest the {@link RecordFileDigest} to update the digest with\n+     * @param recordFile the {@link RecordFile} object\n+     * @throws IOException\n+     */\n+    private void readHeader(DataInputStream dis, RecordFileDigest digest, RecordFile recordFile) throws IOException {\n+        // record file version\n+        int version = dis.readInt();\n+        checkField(version, readerVersion, \"record file version\", recordFile.getName());\n+\n+        int hapiVersion = dis.readInt();\n+\n+        // previous record file hash\n+        byte marker = dis.readByte();\n+        checkField(marker, PREV_HASH_MARKER, \"previous hash marker\", recordFile.getName());\n+        byte[] prevHash = dis.readNBytes(HASH_SIZE);\n+        checkField(prevHash.length, HASH_SIZE, \"previous hash size\", recordFile.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6115e66dad9cd1b40777d8c5a77c22d3387b06f2"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc1ODI0Nw==", "bodyText": "Nevermind, \"previous hash size\" looks fine after looking at this again.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r549758247", "createdAt": "2020-12-29T16:02:27Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "diffHunk": "@@ -0,0 +1,178 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.google.common.primitives.Ints;\n+import java.io.BufferedInputStream;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.commons.io.FilenameUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.ImporterException;\n+import com.hedera.mirror.importer.exception.InvalidStreamFileException;\n+import com.hedera.mirror.importer.exception.StreamFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@RequiredArgsConstructor\n+public abstract class AbstractRecordFileReader implements RecordFileReader {\n+\n+    protected static final String HASH_ALGORITHM = \"SHA-384\";\n+    protected static final int HASH_SIZE = 48; // 48-byte SHA-384 hash\n+    protected static final byte PREV_HASH_MARKER = 1;\n+    protected static final byte RECORD_MARKER = 2;\n+\n+    protected final Logger log = LogManager.getLogger(getClass());\n+    private final int readerVersion;\n+\n+    @Override\n+    public RecordFile read(@NonNull StreamFileData streamFileData, Consumer<RecordItem> itemConsumer) {\n+        try (DataInputStream dis = new DataInputStream(new BufferedInputStream(streamFileData.getInputStream()))) {\n+            RecordFile recordFile = new RecordFile();\n+            RecordFileDigest digest = getRecordFileDigest();\n+\n+            recordFile.setName(FilenameUtils.getName(streamFileData.getFilename()));\n+            readHeader(dis, digest, recordFile);\n+            readBody(dis, digest, itemConsumer, recordFile);\n+            recordFile.setFileHash(Hex.encodeHexString(digest.digest()));\n+\n+            return recordFile;\n+        } catch (ImporterException e) {\n+            throw e;\n+        }  catch (Exception e) {\n+            throw new StreamFileReaderException(\"Error reading record file \" + streamFileData.getFilename(), e);\n+        }\n+    }\n+\n+    protected abstract RecordFileDigest getRecordFileDigest();\n+\n+    /**\n+     * Reads the record file header, updates the message digest with data from the header, and sets corresponding\n+     * {@link RecordFile} fields. {@code dis} should point at the beginning of the stream. The header should contain\n+     * file version, HAPI version, and the previous file hash.\n+     *\n+     * @param dis the {@link DataInputStream} of the record file\n+     * @param digest the {@link RecordFileDigest} to update the digest with\n+     * @param recordFile the {@link RecordFile} object\n+     * @throws IOException\n+     */\n+    private void readHeader(DataInputStream dis, RecordFileDigest digest, RecordFile recordFile) throws IOException {\n+        // record file version\n+        int version = dis.readInt();\n+        checkField(version, readerVersion, \"record file version\", recordFile.getName());\n+\n+        int hapiVersion = dis.readInt();\n+\n+        // previous record file hash\n+        byte marker = dis.readByte();\n+        checkField(marker, PREV_HASH_MARKER, \"previous hash marker\", recordFile.getName());\n+        byte[] prevHash = dis.readNBytes(HASH_SIZE);\n+        checkField(prevHash.length, HASH_SIZE, \"previous hash size\", recordFile.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ3ODUwNA=="}, "originalCommit": {"oid": "6115e66dad9cd1b40777d8c5a77c22d3387b06f2"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjI5OTQyOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMDo1MTo1M1rOIMBzIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNTo0Mzo0MVrOIN2dLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ4MzI5Nw==", "bodyText": "Could use readNBytes to be consistent with hash reading and collapse last 3 lines to 1.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r549483297", "createdAt": "2020-12-28T20:51:53Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "diffHunk": "@@ -0,0 +1,178 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.google.common.primitives.Ints;\n+import java.io.BufferedInputStream;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.commons.io.FilenameUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.ImporterException;\n+import com.hedera.mirror.importer.exception.InvalidStreamFileException;\n+import com.hedera.mirror.importer.exception.StreamFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@RequiredArgsConstructor\n+public abstract class AbstractRecordFileReader implements RecordFileReader {\n+\n+    protected static final String HASH_ALGORITHM = \"SHA-384\";\n+    protected static final int HASH_SIZE = 48; // 48-byte SHA-384 hash\n+    protected static final byte PREV_HASH_MARKER = 1;\n+    protected static final byte RECORD_MARKER = 2;\n+\n+    protected final Logger log = LogManager.getLogger(getClass());\n+    private final int readerVersion;\n+\n+    @Override\n+    public RecordFile read(@NonNull StreamFileData streamFileData, Consumer<RecordItem> itemConsumer) {\n+        try (DataInputStream dis = new DataInputStream(new BufferedInputStream(streamFileData.getInputStream()))) {\n+            RecordFile recordFile = new RecordFile();\n+            RecordFileDigest digest = getRecordFileDigest();\n+\n+            recordFile.setName(FilenameUtils.getName(streamFileData.getFilename()));\n+            readHeader(dis, digest, recordFile);\n+            readBody(dis, digest, itemConsumer, recordFile);\n+            recordFile.setFileHash(Hex.encodeHexString(digest.digest()));\n+\n+            return recordFile;\n+        } catch (ImporterException e) {\n+            throw e;\n+        }  catch (Exception e) {\n+            throw new StreamFileReaderException(\"Error reading record file \" + streamFileData.getFilename(), e);\n+        }\n+    }\n+\n+    protected abstract RecordFileDigest getRecordFileDigest();\n+\n+    /**\n+     * Reads the record file header, updates the message digest with data from the header, and sets corresponding\n+     * {@link RecordFile} fields. {@code dis} should point at the beginning of the stream. The header should contain\n+     * file version, HAPI version, and the previous file hash.\n+     *\n+     * @param dis the {@link DataInputStream} of the record file\n+     * @param digest the {@link RecordFileDigest} to update the digest with\n+     * @param recordFile the {@link RecordFile} object\n+     * @throws IOException\n+     */\n+    private void readHeader(DataInputStream dis, RecordFileDigest digest, RecordFile recordFile) throws IOException {\n+        // record file version\n+        int version = dis.readInt();\n+        checkField(version, readerVersion, \"record file version\", recordFile.getName());\n+\n+        int hapiVersion = dis.readInt();\n+\n+        // previous record file hash\n+        byte marker = dis.readByte();\n+        checkField(marker, PREV_HASH_MARKER, \"previous hash marker\", recordFile.getName());\n+        byte[] prevHash = dis.readNBytes(HASH_SIZE);\n+        checkField(prevHash.length, HASH_SIZE, \"previous hash size\", recordFile.getName());\n+\n+        digest.updateHeader(Ints.toByteArray(version));\n+        digest.updateHeader(Ints.toByteArray(hapiVersion));\n+        digest.updateHeader(marker);\n+        digest.updateHeader(prevHash);\n+\n+        recordFile.setRecordFormatVersion(version);\n+        recordFile.setPreviousHash(Hex.encodeHexString(prevHash));\n+    }\n+\n+    /**\n+     * Reads the record file body, updates the message digest with data from the body, and sets corresponding\n+     * {@link RecordFile} fields. {@code dis} should point at the beginning of the body. The body should contain\n+     * a variable number of transaction and record pairs ordered by consensus timestamp. The body may also contain\n+     * metadata to mark the boundary of the pairs.\n+     *\n+     * @param dis the {@link DataInputStream} of the record file\n+     * @param digest the {@link RecordFileDigest} to update the digest with\n+     * @param itemConsumer the {@link Consumer} to process individual {@link RecordItem}s\n+     * @param recordFile the {@link RecordFile} object\n+     * @throws IOException\n+     */\n+    private void readBody(DataInputStream dis, RecordFileDigest digest, Consumer<RecordItem> itemConsumer,\n+            RecordFile recordFile) throws IOException {\n+        long count = 0;\n+        long consensusStart = 0;\n+        long consensusEnd = 0;\n+\n+        while (dis.available() != 0) {\n+            byte marker = dis.readByte();\n+            checkField(marker, RECORD_MARKER, \"record marker\", recordFile.getName());\n+\n+            byte[] transactionBytes = readLengthAndBytes(dis);\n+            byte[] recordBytes = readLengthAndBytes(dis);\n+\n+            digest.updateBody(marker);\n+            digest.updateBody(Ints.toByteArray(transactionBytes.length));\n+            digest.updateBody(transactionBytes);\n+            digest.updateBody(Ints.toByteArray(recordBytes.length));\n+            digest.updateBody(recordBytes);\n+\n+            boolean isFirstTransaction = count == 0;\n+            boolean isLastTransaction = dis.available() == 0;\n+\n+            // We need the first and last transaction timestamps for metrics\n+            if (itemConsumer != null || isFirstTransaction || isLastTransaction) {\n+                RecordItem recordItem = new RecordItem(transactionBytes, recordBytes);\n+\n+                if (itemConsumer != null) {\n+                    itemConsumer.accept(recordItem);\n+                }\n+\n+                if (isFirstTransaction) {\n+                    consensusStart = recordItem.getConsensusTimestamp();\n+                }\n+\n+                if (isLastTransaction) {\n+                    consensusEnd = recordItem.getConsensusTimestamp();\n+                }\n+            }\n+\n+            count++;\n+        }\n+\n+        recordFile.setConsensusStart(consensusStart);\n+        recordFile.setConsensusEnd(consensusEnd);\n+        recordFile.setCount(count);\n+    }\n+\n+    private byte[] readLengthAndBytes(DataInputStream dis) throws IOException {\n+        int len = dis.readInt();\n+        byte[] bytes = new byte[len];\n+        dis.readFully(bytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6115e66dad9cd1b40777d8c5a77c22d3387b06f2"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTM5NDYwNw==", "bodyText": "readNBytes is a best effort to read up to N bytes and what needed here is read exactly len bytes and otherwise errors out", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r551394607", "createdAt": "2021-01-04T15:43:41Z", "author": {"login": "xin-hedera"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/AbstractRecordFileReader.java", "diffHunk": "@@ -0,0 +1,178 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import com.google.common.primitives.Ints;\n+import java.io.BufferedInputStream;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.util.function.Consumer;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.commons.io.FilenameUtils;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamFileData;\n+import com.hedera.mirror.importer.exception.ImporterException;\n+import com.hedera.mirror.importer.exception.InvalidStreamFileException;\n+import com.hedera.mirror.importer.exception.StreamFileReaderException;\n+import com.hedera.mirror.importer.parser.domain.RecordItem;\n+\n+@RequiredArgsConstructor\n+public abstract class AbstractRecordFileReader implements RecordFileReader {\n+\n+    protected static final String HASH_ALGORITHM = \"SHA-384\";\n+    protected static final int HASH_SIZE = 48; // 48-byte SHA-384 hash\n+    protected static final byte PREV_HASH_MARKER = 1;\n+    protected static final byte RECORD_MARKER = 2;\n+\n+    protected final Logger log = LogManager.getLogger(getClass());\n+    private final int readerVersion;\n+\n+    @Override\n+    public RecordFile read(@NonNull StreamFileData streamFileData, Consumer<RecordItem> itemConsumer) {\n+        try (DataInputStream dis = new DataInputStream(new BufferedInputStream(streamFileData.getInputStream()))) {\n+            RecordFile recordFile = new RecordFile();\n+            RecordFileDigest digest = getRecordFileDigest();\n+\n+            recordFile.setName(FilenameUtils.getName(streamFileData.getFilename()));\n+            readHeader(dis, digest, recordFile);\n+            readBody(dis, digest, itemConsumer, recordFile);\n+            recordFile.setFileHash(Hex.encodeHexString(digest.digest()));\n+\n+            return recordFile;\n+        } catch (ImporterException e) {\n+            throw e;\n+        }  catch (Exception e) {\n+            throw new StreamFileReaderException(\"Error reading record file \" + streamFileData.getFilename(), e);\n+        }\n+    }\n+\n+    protected abstract RecordFileDigest getRecordFileDigest();\n+\n+    /**\n+     * Reads the record file header, updates the message digest with data from the header, and sets corresponding\n+     * {@link RecordFile} fields. {@code dis} should point at the beginning of the stream. The header should contain\n+     * file version, HAPI version, and the previous file hash.\n+     *\n+     * @param dis the {@link DataInputStream} of the record file\n+     * @param digest the {@link RecordFileDigest} to update the digest with\n+     * @param recordFile the {@link RecordFile} object\n+     * @throws IOException\n+     */\n+    private void readHeader(DataInputStream dis, RecordFileDigest digest, RecordFile recordFile) throws IOException {\n+        // record file version\n+        int version = dis.readInt();\n+        checkField(version, readerVersion, \"record file version\", recordFile.getName());\n+\n+        int hapiVersion = dis.readInt();\n+\n+        // previous record file hash\n+        byte marker = dis.readByte();\n+        checkField(marker, PREV_HASH_MARKER, \"previous hash marker\", recordFile.getName());\n+        byte[] prevHash = dis.readNBytes(HASH_SIZE);\n+        checkField(prevHash.length, HASH_SIZE, \"previous hash size\", recordFile.getName());\n+\n+        digest.updateHeader(Ints.toByteArray(version));\n+        digest.updateHeader(Ints.toByteArray(hapiVersion));\n+        digest.updateHeader(marker);\n+        digest.updateHeader(prevHash);\n+\n+        recordFile.setRecordFormatVersion(version);\n+        recordFile.setPreviousHash(Hex.encodeHexString(prevHash));\n+    }\n+\n+    /**\n+     * Reads the record file body, updates the message digest with data from the body, and sets corresponding\n+     * {@link RecordFile} fields. {@code dis} should point at the beginning of the body. The body should contain\n+     * a variable number of transaction and record pairs ordered by consensus timestamp. The body may also contain\n+     * metadata to mark the boundary of the pairs.\n+     *\n+     * @param dis the {@link DataInputStream} of the record file\n+     * @param digest the {@link RecordFileDigest} to update the digest with\n+     * @param itemConsumer the {@link Consumer} to process individual {@link RecordItem}s\n+     * @param recordFile the {@link RecordFile} object\n+     * @throws IOException\n+     */\n+    private void readBody(DataInputStream dis, RecordFileDigest digest, Consumer<RecordItem> itemConsumer,\n+            RecordFile recordFile) throws IOException {\n+        long count = 0;\n+        long consensusStart = 0;\n+        long consensusEnd = 0;\n+\n+        while (dis.available() != 0) {\n+            byte marker = dis.readByte();\n+            checkField(marker, RECORD_MARKER, \"record marker\", recordFile.getName());\n+\n+            byte[] transactionBytes = readLengthAndBytes(dis);\n+            byte[] recordBytes = readLengthAndBytes(dis);\n+\n+            digest.updateBody(marker);\n+            digest.updateBody(Ints.toByteArray(transactionBytes.length));\n+            digest.updateBody(transactionBytes);\n+            digest.updateBody(Ints.toByteArray(recordBytes.length));\n+            digest.updateBody(recordBytes);\n+\n+            boolean isFirstTransaction = count == 0;\n+            boolean isLastTransaction = dis.available() == 0;\n+\n+            // We need the first and last transaction timestamps for metrics\n+            if (itemConsumer != null || isFirstTransaction || isLastTransaction) {\n+                RecordItem recordItem = new RecordItem(transactionBytes, recordBytes);\n+\n+                if (itemConsumer != null) {\n+                    itemConsumer.accept(recordItem);\n+                }\n+\n+                if (isFirstTransaction) {\n+                    consensusStart = recordItem.getConsensusTimestamp();\n+                }\n+\n+                if (isLastTransaction) {\n+                    consensusEnd = recordItem.getConsensusTimestamp();\n+                }\n+            }\n+\n+            count++;\n+        }\n+\n+        recordFile.setConsensusStart(consensusStart);\n+        recordFile.setConsensusEnd(consensusEnd);\n+        recordFile.setCount(count);\n+    }\n+\n+    private byte[] readLengthAndBytes(DataInputStream dis) throws IOException {\n+        int len = dis.readInt();\n+        byte[] bytes = new byte[len];\n+        dis.readFully(bytes);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ4MzI5Nw=="}, "originalCommit": {"oid": "6115e66dad9cd1b40777d8c5a77c22d3387b06f2"}, "originalPosition": 168}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjMzNjUzOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/RecordFileReaderImplV1.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMToxMjo1MlrOIMCHRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMToxMjo1MlrOIMCHRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ4ODQ1NQ==", "bodyText": "Bit confusing to call updateHeader() for an updateBody(). Would either replace with md.update(input) or create a new method private void update(byte... input) that they call.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r549488455", "createdAt": "2020-12-28T21:12:52Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/reader/record/RecordFileReaderImplV1.java", "diffHunk": "@@ -0,0 +1,78 @@\n+package com.hedera.mirror.importer.reader.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import javax.inject.Named;\n+\n+import com.hedera.mirror.importer.exception.StreamFileReaderException;\n+\n+@Named\n+public class RecordFileReaderImplV1 extends AbstractRecordFileReader {\n+\n+    public RecordFileReaderImplV1() {\n+        super(1);\n+    }\n+\n+    @Override\n+    protected RecordFileDigest getRecordFileDigest() {\n+        try {\n+            return new RecordFileDigestV1();\n+        } catch (NoSuchAlgorithmException e) {\n+            throw new StreamFileReaderException(\"Unable to instantiate RecordFileDigestV1\" , e);\n+        }\n+    }\n+\n+    private static class RecordFileDigestV1 implements RecordFileDigest {\n+\n+        private final MessageDigest md;\n+\n+        RecordFileDigestV1() throws NoSuchAlgorithmException {\n+            md = MessageDigest.getInstance(HASH_ALGORITHM);\n+        }\n+\n+        @Override\n+        public void updateHeader(byte input) {\n+            md.update(input);\n+        }\n+\n+        @Override\n+        public void updateHeader(byte[] input) {\n+            md.update(input);\n+        }\n+\n+        @Override\n+        public void updateBody(byte input) {\n+            updateHeader(input);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6115e66dad9cd1b40777d8c5a77c22d3387b06f2"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NjM0MjEzOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/domain/StreamFileDataTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMToxNTozMFrOIMCKMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQyMToxNTozMFrOIMCKMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTQ4OTIwMA==", "bodyText": "nit: A parameterized test with so many if/else and booleans parameters is harder to follow then separate tests in my opinion. Not saying you have to change it, just want to make the observation.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1382#discussion_r549489200", "createdAt": "2020-12-28T21:15:30Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/domain/StreamFileDataTest.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.hedera.mirror.importer.domain;\n+\n+/*-\n+ *\n+ *  Hedera Mirror Node\n+ *  \u200b\n+ *  Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ *  \u200b\n+ *  Licensed under the Apache License, Version 2.0 (the \"License\");\n+ *  you may not use this file except in compliance with the License.\n+ *  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ *\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.jupiter.api.Assertions.*;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Path;\n+import org.apache.commons.io.FileUtils;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n+\n+class StreamFileDataTest {\n+\n+    @TempDir\n+    Path dataPath;\n+\n+    @ParameterizedTest(name = \"create StreamFileData from {3}\")\n+    @CsvSource({\n+            \"true, false, false, empty file should return valid StreamFileData object\",\n+            \"true, true, false, file with content should return valid StreamFileData object\",\n+            \"false, false, false, non-existent file expect exception\",\n+            \"false, false, true, directory expect exception\",\n+    })\n+    void from(boolean createFile, boolean writeData, boolean createDirectory, String testName) throws IOException {\n+        File file = FileUtils.getFile(dataPath.toFile(), \"testfile\");\n+\n+        if (createFile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6115e66dad9cd1b40777d8c5a77c22d3387b06f2"}, "originalPosition": 50}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1365, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}