{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MzQ3MjU2", "number": 936, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNDo1Nzo1NVrOEWP3Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQyMDo0MjowOVrOEbHqsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzY0MDUwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNDo1Nzo1NVrOG9dLEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo0Nzo0OFrOG-ArfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NDI5MA==", "bodyText": "This sort can be removed since it's already sorted above.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467094290", "createdAt": "2020-08-07T14:57:55Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)\n+                        .collect(Collectors.toList());\n+\n+                if (fullPaths != null && fullPaths.size() != 0) {\n+                    log.trace(\"Processing record files: {}\", fullPaths);\n+                    loadRecordFiles(fullPaths);\n+                } else {\n+                    log.debug(\"No files to parse\");\n+                }\n+            } else {\n+                log.error(\"Input parameter is not a folder: {}\", path);\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Error parsing files\", e);\n+        }\n+    }\n+\n+    /**\n+     * read and parse a list of record files\n+     *\n+     * @throws Exception\n+     */\n+    private void loadRecordFiles(List<String> filePaths) {\n+        Collections.sort(filePaths);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjAyOA==", "bodyText": "Removed", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676028", "createdAt": "2020-08-10T03:47:48Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)\n+                        .collect(Collectors.toList());\n+\n+                if (fullPaths != null && fullPaths.size() != 0) {\n+                    log.trace(\"Processing record files: {}\", fullPaths);\n+                    loadRecordFiles(fullPaths);\n+                } else {\n+                    log.debug(\"No files to parse\");\n+                }\n+            } else {\n+                log.error(\"Input parameter is not a folder: {}\", path);\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Error parsing files\", e);\n+        }\n+    }\n+\n+    /**\n+     * read and parse a list of record files\n+     *\n+     * @throws Exception\n+     */\n+    private void loadRecordFiles(List<String> filePaths) {\n+        Collections.sort(filePaths);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NDI5MA=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzY1MTQwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTowMDo1MFrOG9dRxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo0Nzo1MFrOG-Arig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NjAwNQ==", "bodyText": "This can return null. We should return here if so.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467096005", "createdAt": "2020-08-07T15:00:50Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjA0Mg==", "bodyText": "Added", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676042", "createdAt": "2020-08-10T03:47:50Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NjAwNQ=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzY1NTQ1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTowMjowMFrOG9dUbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo0Nzo1MlrOG-Arjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NjY4NA==", "bodyText": "It would be more efficient to not loop over the files twice: once here and once again in loadRecordFiles(). Better to just add the valid path inside the loop in loadRecordFiles.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467096684", "createdAt": "2020-08-07T15:02:00Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjA0Ng==", "bodyText": "Agreed, removed this additional loop.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676046", "createdAt": "2020-08-10T03:47:52Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NjY4NA=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzY3NDAyOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTowNzoyMlrOG9dgIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo0Nzo1NVrOG-ArlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5OTY4MQ==", "bodyText": "Can we change to use validPath.resolve(s) here so it's Windows compatible?", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467099681", "createdAt": "2020-08-07T15:07:22Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjA1Mw==", "bodyText": "No need here as this list creation was removed. Added it when creating the File in loadRecordFiles()", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676053", "createdAt": "2020-08-10T03:47:55Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5OTY4MQ=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzcyNjIwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFilePollerTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNToyMTozOFrOG9d_2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMDoxNDo1N1rOG9mmGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEwNzgwMQ==", "bodyText": "As order is very important here and this approach does not verify that, we should use InOrder mock to verify files are processed in order.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467107801", "createdAt": "2020-08-07T15:21:38Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFilePollerTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoInteractions;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.List;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Mock;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+\n+import com.hedera.mirror.importer.FileCopier;\n+import com.hedera.mirror.importer.MirrorProperties;\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamType;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+\n+@ExtendWith(MockitoExtension.class)\n+public class RecordFilePollerTest {\n+\n+    @TempDir\n+    Path dataPath;\n+    @Mock\n+    RecordFileParser recordFileParser;\n+    private FileCopier fileCopier;\n+    private RecordFilePoller recordFilePoller;\n+    private RecordParserProperties parserProperties;\n+\n+    private File file1;\n+    private File file2;\n+    private static final long FILE1_CONSENSUS_START = 1567188600419072000L;\n+    private static final int NUM_TXNS_FILE_1 = 19;\n+    private static final int NUM_TXNS_FILE_2 = 15;\n+    private static RecordFile recordFile1;\n+    private static RecordFile recordFile2;\n+\n+    @BeforeEach\n+    void before() {\n+        var mirrorProperties = new MirrorProperties();\n+        mirrorProperties.setDataPath(dataPath);\n+        parserProperties = new RecordParserProperties(mirrorProperties);\n+        parserProperties.setKeepFiles(false);\n+        parserProperties.init();\n+        recordFilePoller = new RecordFilePoller(parserProperties, recordFileParser);\n+        StreamType streamType = StreamType.RECORD;\n+        fileCopier = FileCopier\n+                .create(Path.of(getClass().getClassLoader().getResource(\"data\").getPath()), dataPath)\n+                .from(streamType.getPath(), \"v2\", \"record0.0.3\")\n+                .filterFiles(\"*.rcd\")\n+                .to(streamType.getPath(), streamType.getValid());\n+        file1 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_00.419072Z.rcd\").toFile();\n+        file2 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_05.249678Z.rcd\").toFile();\n+        recordFile1 = new RecordFile(1567188600419072000L, 1567188604906443001L, null, file1.getName(), 0L, 0L,\n+                \"591558e059bd1629ee386c4e35a6875b4c67a096718f5d225772a651042715189414df7db5588495efb2a85dc4a0ffda\",\n+                \"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", 2);\n+\n+        recordFile2 = new RecordFile(1567188605249678000L, 1567188609705382001L, null, file2.getName(), 0L, 0L,\n+                \"5ed51baeff204eb6a2a68b76bbaadcb9b6e7074676c1746b99681d075bef009e8d57699baaa6342feec4e83726582d36\",\n+                recordFile1.getFileHash(), 2);\n+    }\n+\n+    @Test\n+    void poll() throws Exception {\n+        // given\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void pollAndKeepFiles() throws Exception {\n+        // given\n+        parserProperties.setKeepFiles(true);\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void noFiles() throws Exception {\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertParsedFiles();\n+        verifyNoInteractions(recordFileParser);\n+    }\n+\n+    // Asserts that recordFileParser.parse is called wth exactly the given fileNames.\n+    private void assertParse(String... fileNames) {\n+        ArgumentCaptor<StreamFileData> captor = ArgumentCaptor.forClass(StreamFileData.class);\n+        verify(recordFileParser, times(fileNames.length)).parse(captor.capture());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI0ODY2Ng==", "bodyText": "Sure, could achieve the same with below, since InOrder will make method less general as you have to have a verify for each occurrence of the method with the expected parameter.\nassertThat(actualArgs)\n                .extracting(StreamFileData::getFilename)\n                .isEqualTo(Arrays.asList(fileNames));", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467248666", "createdAt": "2020-08-07T20:14:57Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFilePollerTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoInteractions;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.List;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Mock;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+\n+import com.hedera.mirror.importer.FileCopier;\n+import com.hedera.mirror.importer.MirrorProperties;\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamType;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+\n+@ExtendWith(MockitoExtension.class)\n+public class RecordFilePollerTest {\n+\n+    @TempDir\n+    Path dataPath;\n+    @Mock\n+    RecordFileParser recordFileParser;\n+    private FileCopier fileCopier;\n+    private RecordFilePoller recordFilePoller;\n+    private RecordParserProperties parserProperties;\n+\n+    private File file1;\n+    private File file2;\n+    private static final long FILE1_CONSENSUS_START = 1567188600419072000L;\n+    private static final int NUM_TXNS_FILE_1 = 19;\n+    private static final int NUM_TXNS_FILE_2 = 15;\n+    private static RecordFile recordFile1;\n+    private static RecordFile recordFile2;\n+\n+    @BeforeEach\n+    void before() {\n+        var mirrorProperties = new MirrorProperties();\n+        mirrorProperties.setDataPath(dataPath);\n+        parserProperties = new RecordParserProperties(mirrorProperties);\n+        parserProperties.setKeepFiles(false);\n+        parserProperties.init();\n+        recordFilePoller = new RecordFilePoller(parserProperties, recordFileParser);\n+        StreamType streamType = StreamType.RECORD;\n+        fileCopier = FileCopier\n+                .create(Path.of(getClass().getClassLoader().getResource(\"data\").getPath()), dataPath)\n+                .from(streamType.getPath(), \"v2\", \"record0.0.3\")\n+                .filterFiles(\"*.rcd\")\n+                .to(streamType.getPath(), streamType.getValid());\n+        file1 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_00.419072Z.rcd\").toFile();\n+        file2 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_05.249678Z.rcd\").toFile();\n+        recordFile1 = new RecordFile(1567188600419072000L, 1567188604906443001L, null, file1.getName(), 0L, 0L,\n+                \"591558e059bd1629ee386c4e35a6875b4c67a096718f5d225772a651042715189414df7db5588495efb2a85dc4a0ffda\",\n+                \"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", 2);\n+\n+        recordFile2 = new RecordFile(1567188605249678000L, 1567188609705382001L, null, file2.getName(), 0L, 0L,\n+                \"5ed51baeff204eb6a2a68b76bbaadcb9b6e7074676c1746b99681d075bef009e8d57699baaa6342feec4e83726582d36\",\n+                recordFile1.getFileHash(), 2);\n+    }\n+\n+    @Test\n+    void poll() throws Exception {\n+        // given\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void pollAndKeepFiles() throws Exception {\n+        // given\n+        parserProperties.setKeepFiles(true);\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void noFiles() throws Exception {\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertParsedFiles();\n+        verifyNoInteractions(recordFileParser);\n+    }\n+\n+    // Asserts that recordFileParser.parse is called wth exactly the given fileNames.\n+    private void assertParse(String... fileNames) {\n+        ArgumentCaptor<StreamFileData> captor = ArgumentCaptor.forClass(StreamFileData.class);\n+        verify(recordFileParser, times(fileNames.length)).parse(captor.capture());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEwNzgwMQ=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzc1NTI3OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNToyOToyOVrOG9eRig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo0ODoxNFrOG-Arxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExMjMzMA==", "bodyText": "This exception is already logged in poller, so this log can be removed.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467112330", "createdAt": "2020-08-07T15:29:29Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -165,6 +155,10 @@ public void loadRecordFile(StreamFileData streamFileData) {\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (Exception ex) {\n+            log.warn(\"Failed to parse {}\", streamFileData.getFilename(), ex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjEwMw==", "bodyText": "Removed", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676103", "createdAt": "2020-08-10T03:48:14Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -165,6 +155,10 @@ public void loadRecordFile(StreamFileData streamFileData) {\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (Exception ex) {\n+            log.warn(\"Failed to parse {}\", streamFileData.getFilename(), ex);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExMjMzMA=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzgxOTU2OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNTo0NzozMFrOG9e4eA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo0ODo1M1rOG-AsIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMjI5Ng==", "bodyText": "This seems to leak details of the parser. I think it makes more sense to catch DuplicateFileException in the parser and ignore it there.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467122296", "createdAt": "2020-08-07T15:47:30Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)\n+                        .collect(Collectors.toList());\n+\n+                if (fullPaths != null && fullPaths.size() != 0) {\n+                    log.trace(\"Processing record files: {}\", fullPaths);\n+                    loadRecordFiles(fullPaths);\n+                } else {\n+                    log.debug(\"No files to parse\");\n+                }\n+            } else {\n+                log.error(\"Input parameter is not a folder: {}\", path);\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Error parsing files\", e);\n+        }\n+    }\n+\n+    /**\n+     * read and parse a list of record files\n+     *\n+     * @throws Exception\n+     */\n+    private void loadRecordFiles(List<String> filePaths) {\n+        Collections.sort(filePaths);\n+        for (String filePath : filePaths) {\n+            if (ShutdownHelper.isStopping()) {\n+                return;\n+            }\n+\n+            File file = new File(filePath);\n+\n+            try (InputStream fileInputStream = new FileInputStream(file)) {\n+                recordFileParser.parse(new StreamFileData(filePath, fileInputStream));\n+\n+                if (parserProperties.isKeepFiles()) {\n+                    Utility.archiveFile(file, parserProperties.getParsedPath());\n+                } else {\n+                    FileUtils.deleteQuietly(file);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.warn(\"File does not exist {}\", filePath);\n+                return;\n+            } catch (Exception e) {\n+                log.error(\"Error parsing file {}\", filePath, e);\n+                if (!(e instanceof DuplicateFileException)) { // if DuplicateFileException, continue with other", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjE5Mw==", "bodyText": "DuplicateFileException catch log and ignore added to parser", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676193", "createdAt": "2020-08-10T03:48:53Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,121 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.exception.DuplicateFileException;\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                // add directory prefix to get full path\n+                List<String> fullPaths = Arrays.asList(files).stream()\n+                        .map(s -> file + \"/\" + s)\n+                        .collect(Collectors.toList());\n+\n+                if (fullPaths != null && fullPaths.size() != 0) {\n+                    log.trace(\"Processing record files: {}\", fullPaths);\n+                    loadRecordFiles(fullPaths);\n+                } else {\n+                    log.debug(\"No files to parse\");\n+                }\n+            } else {\n+                log.error(\"Input parameter is not a folder: {}\", path);\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Error parsing files\", e);\n+        }\n+    }\n+\n+    /**\n+     * read and parse a list of record files\n+     *\n+     * @throws Exception\n+     */\n+    private void loadRecordFiles(List<String> filePaths) {\n+        Collections.sort(filePaths);\n+        for (String filePath : filePaths) {\n+            if (ShutdownHelper.isStopping()) {\n+                return;\n+            }\n+\n+            File file = new File(filePath);\n+\n+            try (InputStream fileInputStream = new FileInputStream(file)) {\n+                recordFileParser.parse(new StreamFileData(filePath, fileInputStream));\n+\n+                if (parserProperties.isKeepFiles()) {\n+                    Utility.archiveFile(file, parserProperties.getParsedPath());\n+                } else {\n+                    FileUtils.deleteQuietly(file);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.warn(\"File does not exist {}\", filePath);\n+                return;\n+            } catch (Exception e) {\n+                log.error(\"Error parsing file {}\", filePath, e);\n+                if (!(e instanceof DuplicateFileException)) { // if DuplicateFileException, continue with other", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMjI5Ng=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzg4ODcwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowNzowMVrOG9fifQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo1MDo0MlrOG-AtUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzA1Mw==", "bodyText": "There's a problem here. In SqlEntityListener we manually manage the commit and that can cause issues. From connection.setAutoCommit():\nIf this method is called during a transaction and the auto-commit mode is changed, the transaction is committed. If setAutoCommit is called and the auto-commit mode is not changed, the call is a no-op.\n\nAlso, I'm not sure the impact of explicitly calling commit on a connection with an outer transaction in progress. It would be better to remove setAutoCommit and not call commit/rollback manually. This annotation should handle it for us. But please verify with tests.\nI'm not sure about the connection and statements in SqlEntityListener. I'm guessing we still need to manually close those but since they're proxied they won't actually close until the outer transaction completes.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467133053", "createdAt": "2020-08-07T16:07:01Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -135,11 +123,13 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n     }\n \n     /**\n-     * Given a service record name, read and parse and return as a list of service record pair\n+     * Given a stream file data representing an rcd file from the service parse record items and persist changes\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) {\n+    @Override\n+    @Transactional", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjQ5OQ==", "bodyText": "Manual close is good practice, but you're right the outer transaction close will probably handle it.\nI did remove the setAutoCommit", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676499", "createdAt": "2020-08-10T03:50:42Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -135,11 +123,13 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n     }\n \n     /**\n-     * Given a service record name, read and parse and return as a list of service record pair\n+     * Given a stream file data representing an rcd file from the service parse record items and persist changes\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) {\n+    @Override\n+    @Transactional", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzA1Mw=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzg5NDEwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowODozNlrOG9fl1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo1MToxNVrOG-AtmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzkxMQ==", "bodyText": "Can remove rollback changes comment, considering previous change.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467133911", "createdAt": "2020-08-07T16:08:36Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -165,6 +155,10 @@ public void loadRecordFile(StreamFileData streamFileData) {\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (Exception ex) {\n+            log.warn(\"Failed to parse {}\", streamFileData.getFilename(), ex);\n+            recordStreamFileListener.onError(); // rollback changes", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjU2OA==", "bodyText": "Removed comment", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676568", "createdAt": "2020-08-10T03:51:15Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -165,6 +155,10 @@ public void loadRecordFile(StreamFileData streamFileData) {\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (Exception ex) {\n+            log.warn(\"Failed to parse {}\", streamFileData.getFilename(), ex);\n+            recordStreamFileListener.onError(); // rollback changes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzMzkxMQ=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzg5ODA5OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjowOTo0MFrOG9foNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo1MTozMlrOG-Atxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzNDUxNg==", "bodyText": "Unused", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467134516", "createdAt": "2020-08-07T16:09:40Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NjYxNQ==", "bodyText": "Removed leftover", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676615", "createdAt": "2020-08-10T03:51:32Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzNDUxNg=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 238}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzkyMDc1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjoxNjoyOFrOG9f1_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QwMjowMzo1NlrOG_5Yhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODA0Ng==", "bodyText": "I don't think it's possible to truly test database rollback with mocks. Since it's probably tricky to convert this to not use mocks, we can probably create a separate integration test file just for the rollback test.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467138046", "createdAt": "2020-08-07T16:16:28Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();\n+        doThrow(ParserSQLException.class).when(applicationStatusRepository).updateStatusValue(any(), any());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3Njc1MQ==", "bodyText": "Working on integration test.  Parse method makes it a bit tricky to force an error after commit to I'm having to at last mock the applicationStatus Repository call for such a test.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467676751", "createdAt": "2020-08-10T03:52:38Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();\n+        doThrow(ParserSQLException.class).when(applicationStatusRepository).updateStatusValue(any(), any());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODA0Ng=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3MjE5MA==", "bodyText": "Proving difficult to get a test that captures this.\nMaybe it's sufficient to rely on @transactional functioning correctly based on our configuration.\nI did update RecordFilePrser to use the spring version so we could get coverage for the RunTimeExceptions which are not caught and therefore don't force a rollback when the base @transactional is used.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r468172190", "createdAt": "2020-08-10T20:38:37Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();\n+        doThrow(ParserSQLException.class).when(applicationStatusRepository).updateStatusValue(any(), any());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODA0Ng=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY1MzYzOQ==", "bodyText": "Added a rollback test to the RecordFileParserIntegrationTest.\nMore importantly verified against a local running parser that the rollback works for both pgcopy and prepared statement produced inserts", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r469653639", "createdAt": "2020-08-13T02:03:56Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();\n+        doThrow(ParserSQLException.class).when(applicationStatusRepository).updateStatusValue(any(), any());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODA0Ng=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 239}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzkyMDk2OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjoxNjozMFrOG9f2Fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwMzo1Nzo0OFrOG-AxlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODA3MQ==", "bodyText": "There should be nothing processed, right? This test is suspect.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467138071", "createdAt": "2020-08-07T16:16:30Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();\n+        doThrow(ParserSQLException.class).when(applicationStatusRepository).updateStatusValue(any(), any());\n+\n+        // when\n+        Assertions.assertThrows(ParserSQLException.class, () -> {\n+            recordFileParser.parse(streamFileData2);\n+        });\n+\n+        // then\n+        assertProcessedFile(streamFileData2, recordFile2, NUM_TXNS_FILE_2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3NzU4OQ==", "bodyText": "Actually in this test everything would have been processed as expected, expect after persistence but before file complete an error occurred.\nTest is removed though", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467677589", "createdAt": "2020-08-10T03:57:48Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFileParserTest.java", "diffHunk": "@@ -188,47 +166,63 @@ void bypassHashMismatch() throws Exception {\n         // given\n         parserProperties.getMirrorProperties().setVerifyHashAfter(Instant.parse(\"2019-09-01T00:00:00.000000Z\"));\n         when(applicationStatusRepository.findByStatusCode(LAST_PROCESSED_RECORD_HASH)).thenReturn(\"123\");\n-        fileCopier.copy();\n \n         // when\n-        recordFileParser.parse();\n+        recordFileParser.parse(streamFileData1);\n \n         // then\n-        assertAllProcessed();\n+        verify(recordStreamFileListener, never()).onError();\n+        assertProcessedFile(streamFileData1, recordFile1, NUM_TXNS_FILE_1);\n     }\n \n     @Test\n-    void failureProcessingItemShouldRollback() throws Exception {\n+    void failureProcessingItemShouldRollback() {\n         // given\n-        fileCopier.copy();\n         doThrow(ParserSQLException.class).when(recordItemListener).onItem(any());\n \n         // when\n-        recordFileParser.parse();\n+        Assertions.assertThrows(IllegalArgumentException.class, () -> {\n+            recordFileParser.parse(streamFileData1);\n+        });\n \n         // then\n-        assertValidFiles();\n+        verify(recordStreamFileListener).onStart(streamFileData1);\n+        verify(recordStreamFileListener, never()).onEnd(recordFile1);\n         verify(recordStreamFileListener).onError();\n     }\n \n     @Test\n-    void skipFileOnDuplicateFileException() throws Exception {\n+    void skipFileOnDuplicateFileException() {\n         // given\n-        fileCopier.copy();\n-        String fileName = file1.toString();\n-        recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+        recordFileParser.parse(streamFileData1);\n         doThrow(DuplicateFileException.class).when(recordStreamFileListener).onStart(any());\n \n         // when: load same file again\n         // then: throws exception\n         Assertions.assertThrows(DuplicateFileException.class, () -> {\n-            recordFileParser.loadRecordFile(new StreamFileData(fileName, new FileInputStream(file1)));\n+            recordFileParser.parse(streamFileData1);\n         });\n         verify(recordItemListener, times(NUM_TXNS_FILE_1)).onItem(any());\n         verify(recordStreamFileListener, times(2)).onStart(any());\n         verify(recordStreamFileListener, times(1)).onEnd(any());\n     }\n \n+    @Test\n+    void verifyRollbackOnErrorAfterPersistence() throws Exception {\n+        // given\n+        String fileName = file1.toString();\n+        doThrow(ParserSQLException.class).when(applicationStatusRepository).updateStatusValue(any(), any());\n+\n+        // when\n+        Assertions.assertThrows(ParserSQLException.class, () -> {\n+            recordFileParser.parse(streamFileData2);\n+        });\n+\n+        // then\n+        assertProcessedFile(streamFileData2, recordFile2, NUM_TXNS_FILE_2);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODA3MQ=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 247}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxNzkyMzM5OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFilePollerTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjoxNzowN1rOG9f3hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwNDowMzoxN1rOG-A1Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODQzNw==", "bodyText": "Please add a test for not a directory. Would also be good to check code coverage report and see if any other paths not tested.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467138437", "createdAt": "2020-08-07T16:17:07Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFilePollerTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoInteractions;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.List;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Mock;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+\n+import com.hedera.mirror.importer.FileCopier;\n+import com.hedera.mirror.importer.MirrorProperties;\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamType;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+\n+@ExtendWith(MockitoExtension.class)\n+public class RecordFilePollerTest {\n+\n+    @TempDir\n+    Path dataPath;\n+    @Mock\n+    RecordFileParser recordFileParser;\n+    private FileCopier fileCopier;\n+    private RecordFilePoller recordFilePoller;\n+    private RecordParserProperties parserProperties;\n+\n+    private File file1;\n+    private File file2;\n+    private static final long FILE1_CONSENSUS_START = 1567188600419072000L;\n+    private static final int NUM_TXNS_FILE_1 = 19;\n+    private static final int NUM_TXNS_FILE_2 = 15;\n+    private static RecordFile recordFile1;\n+    private static RecordFile recordFile2;\n+\n+    @BeforeEach\n+    void before() {\n+        var mirrorProperties = new MirrorProperties();\n+        mirrorProperties.setDataPath(dataPath);\n+        parserProperties = new RecordParserProperties(mirrorProperties);\n+        parserProperties.setKeepFiles(false);\n+        parserProperties.init();\n+        recordFilePoller = new RecordFilePoller(parserProperties, recordFileParser);\n+        StreamType streamType = StreamType.RECORD;\n+        fileCopier = FileCopier\n+                .create(Path.of(getClass().getClassLoader().getResource(\"data\").getPath()), dataPath)\n+                .from(streamType.getPath(), \"v2\", \"record0.0.3\")\n+                .filterFiles(\"*.rcd\")\n+                .to(streamType.getPath(), streamType.getValid());\n+        file1 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_00.419072Z.rcd\").toFile();\n+        file2 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_05.249678Z.rcd\").toFile();\n+        recordFile1 = new RecordFile(1567188600419072000L, 1567188604906443001L, null, file1.getName(), 0L, 0L,\n+                \"591558e059bd1629ee386c4e35a6875b4c67a096718f5d225772a651042715189414df7db5588495efb2a85dc4a0ffda\",\n+                \"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", 2);\n+\n+        recordFile2 = new RecordFile(1567188605249678000L, 1567188609705382001L, null, file2.getName(), 0L, 0L,\n+                \"5ed51baeff204eb6a2a68b76bbaadcb9b6e7074676c1746b99681d075bef009e8d57699baaa6342feec4e83726582d36\",\n+                recordFile1.getFileHash(), 2);\n+    }\n+\n+    @Test\n+    void poll() throws Exception {\n+        // given\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void pollAndKeepFiles() throws Exception {\n+        // given\n+        parserProperties.setKeepFiles(true);\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void noFiles() throws Exception {\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertParsedFiles();\n+        verifyNoInteractions(recordFileParser);\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzY3ODQ3MQ==", "bodyText": "Added not a directory test. Adding more to increase coverage for Poller.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r467678471", "createdAt": "2020-08-10T04:03:17Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/test/java/com/hedera/mirror/importer/parser/record/RecordFilePollerTest.java", "diffHunk": "@@ -0,0 +1,168 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.verifyNoInteractions;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.List;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Mock;\n+import org.mockito.junit.jupiter.MockitoExtension;\n+\n+import com.hedera.mirror.importer.FileCopier;\n+import com.hedera.mirror.importer.MirrorProperties;\n+import com.hedera.mirror.importer.domain.RecordFile;\n+import com.hedera.mirror.importer.domain.StreamType;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+\n+@ExtendWith(MockitoExtension.class)\n+public class RecordFilePollerTest {\n+\n+    @TempDir\n+    Path dataPath;\n+    @Mock\n+    RecordFileParser recordFileParser;\n+    private FileCopier fileCopier;\n+    private RecordFilePoller recordFilePoller;\n+    private RecordParserProperties parserProperties;\n+\n+    private File file1;\n+    private File file2;\n+    private static final long FILE1_CONSENSUS_START = 1567188600419072000L;\n+    private static final int NUM_TXNS_FILE_1 = 19;\n+    private static final int NUM_TXNS_FILE_2 = 15;\n+    private static RecordFile recordFile1;\n+    private static RecordFile recordFile2;\n+\n+    @BeforeEach\n+    void before() {\n+        var mirrorProperties = new MirrorProperties();\n+        mirrorProperties.setDataPath(dataPath);\n+        parserProperties = new RecordParserProperties(mirrorProperties);\n+        parserProperties.setKeepFiles(false);\n+        parserProperties.init();\n+        recordFilePoller = new RecordFilePoller(parserProperties, recordFileParser);\n+        StreamType streamType = StreamType.RECORD;\n+        fileCopier = FileCopier\n+                .create(Path.of(getClass().getClassLoader().getResource(\"data\").getPath()), dataPath)\n+                .from(streamType.getPath(), \"v2\", \"record0.0.3\")\n+                .filterFiles(\"*.rcd\")\n+                .to(streamType.getPath(), streamType.getValid());\n+        file1 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_00.419072Z.rcd\").toFile();\n+        file2 = parserProperties.getValidPath().resolve(\"2019-08-30T18_10_05.249678Z.rcd\").toFile();\n+        recordFile1 = new RecordFile(1567188600419072000L, 1567188604906443001L, null, file1.getName(), 0L, 0L,\n+                \"591558e059bd1629ee386c4e35a6875b4c67a096718f5d225772a651042715189414df7db5588495efb2a85dc4a0ffda\",\n+                \"000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\", 2);\n+\n+        recordFile2 = new RecordFile(1567188605249678000L, 1567188609705382001L, null, file2.getName(), 0L, 0L,\n+                \"5ed51baeff204eb6a2a68b76bbaadcb9b6e7074676c1746b99681d075bef009e8d57699baaa6342feec4e83726582d36\",\n+                recordFile1.getFileHash(), 2);\n+    }\n+\n+    @Test\n+    void poll() throws Exception {\n+        // given\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void pollAndKeepFiles() throws Exception {\n+        // given\n+        parserProperties.setKeepFiles(true);\n+        fileCopier.copy();\n+\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertAllProcessed();\n+    }\n+\n+    @Test\n+    void noFiles() throws Exception {\n+        // when\n+        recordFilePoller.poll();\n+\n+        // then\n+        assertParsedFiles();\n+        verifyNoInteractions(recordFileParser);\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEzODQzNw=="}, "originalCommit": {"oid": "482f49fce85efd2655cd1826898083f8a8a0ec05"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNTMzMTAzOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMjowOToyNFrOG-hgQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMjowOToyNFrOG-hgQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIxMzgyNw==", "bodyText": "This is missing a return statement. We don't want to continue to the next file if a parsing error on current file.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r468213827", "createdAt": "2020-08-10T22:09:24Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFilePoller.java", "diffHunk": "@@ -0,0 +1,110 @@\n+package com.hedera.mirror.importer.parser.record;\n+\n+/*-\n+ * \u200c\n+ * Hedera Mirror Node\n+ * \u200b\n+ * Copyright (C) 2019 - 2020 Hedera Hashgraph, LLC\n+ * \u200b\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ * \u200d\n+ */\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import javax.inject.Named;\n+import lombok.AllArgsConstructor;\n+import lombok.extern.log4j.Log4j2;\n+import org.apache.commons.io.FileUtils;\n+import org.springframework.scheduling.annotation.Scheduled;\n+\n+import com.hedera.mirror.importer.parser.FilePoller;\n+import com.hedera.mirror.importer.parser.domain.StreamFileData;\n+import com.hedera.mirror.importer.util.ShutdownHelper;\n+import com.hedera.mirror.importer.util.Utility;\n+\n+@Log4j2\n+@Named\n+@ConditionalOnRecordParser\n+@AllArgsConstructor\n+public class RecordFilePoller implements FilePoller {\n+\n+    private final RecordParserProperties parserProperties;\n+    private final RecordFileParser recordFileParser;\n+\n+    @Override\n+    @Scheduled(fixedRateString = \"${hedera.mirror.importer.parser.record.frequency:100}\")\n+    public void poll() {\n+        if (ShutdownHelper.isStopping()) {\n+            return;\n+        }\n+        Path path = parserProperties.getValidPath();\n+        log.debug(\"Parsing record files from {}\", path);\n+        try {\n+            File file = path.toFile();\n+            if (file.isDirectory()) {\n+\n+                String[] files = file.list();\n+                if (files == null || files.length == 0) {\n+                    log.debug(\"No files to parse in directory {}\", file.getPath());\n+                    return;\n+                }\n+\n+                Arrays.sort(files);           // sorted by name (timestamp)\n+\n+                log.trace(\"Processing record files: {}\", files);\n+                loadRecordFiles(files);\n+            } else {\n+                log.error(\"Input parameter is not a folder: {}\", path);\n+            }\n+        } catch (Exception e) {\n+            log.error(\"Error parsing files\", e);\n+        }\n+    }\n+\n+    /**\n+     * read and parse a list of record files\n+     *\n+     * @throws Exception\n+     */\n+    private void loadRecordFiles(String[] filePaths) {\n+        Path validPath = parserProperties.getValidPath();\n+        for (String filePath : filePaths) {\n+            if (ShutdownHelper.isStopping()) {\n+                return;\n+            }\n+\n+            // get file from full path\n+            File file = validPath.resolve(filePath).toFile();\n+\n+            try (InputStream fileInputStream = new FileInputStream(file)) {\n+                recordFileParser.parse(new StreamFileData(file.getAbsolutePath(), fileInputStream));\n+\n+                if (parserProperties.isKeepFiles()) {\n+                    Utility.archiveFile(file, parserProperties.getParsedPath());\n+                } else {\n+                    FileUtils.deleteQuietly(file);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.warn(\"File does not exist {}\", filePath);\n+                return;\n+            } catch (Exception e) {\n+                log.error(String.format(\"Error parsing file %s\", filePath), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d83065ef5dd67f75f00548058af9bd067ab89a0b"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNTQ4MzQ5OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMzoxNzoyMFrOG-i6lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMzoyOToyNFrOG-jJPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIzNjk1MQ==", "bodyText": "Should remove rollbackFor. I don't see a reason to exclude Error.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r468236951", "createdAt": "2020-08-10T23:17:20Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -135,11 +124,13 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n     }\n \n     /**\n-     * Given a service record name, read and parse and return as a list of service record pair\n+     * Given a stream file data representing an rcd file from the service parse record items and persist changes\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) {\n+    @Override\n+    @Transactional(rollbackFor = Exception.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d83065ef5dd67f75f00548058af9bd067ab89a0b"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI0MDcwMA==", "bodyText": "My understanding was without this RuntimeExceptions wouldn't result in a rollback. But yeah excluding Errors was not an intended impact.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r468240700", "createdAt": "2020-08-10T23:29:24Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -135,11 +124,13 @@ public RecordFileParser(ApplicationStatusRepository applicationStatusRepository,\n     }\n \n     /**\n-     * Given a service record name, read and parse and return as a list of service record pair\n+     * Given a stream file data representing an rcd file from the service parse record items and persist changes\n      *\n      * @param streamFileData containing information about file to be processed\n      */\n-    public void loadRecordFile(StreamFileData streamFileData) {\n+    @Override\n+    @Transactional(rollbackFor = Exception.class)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIzNjk1MQ=="}, "originalCommit": {"oid": "d83065ef5dd67f75f00548058af9bd067ab89a0b"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNTQ4ODMwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMzoxOTo1MlrOG-i9bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQyMzoxOTo1MlrOG-i9bQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIzNzY3Nw==", "bodyText": "I think it's fine to just log this instead of rethrow. Also, because we want to ensure cleanup() is always called.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r468237677", "createdAt": "2020-08-10T23:19:52Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -198,9 +184,11 @@ private void executeBatch(PreparedStatement ps, String entity) {\n \n     private void closeConnectionAndStatements() {\n         try {\n-            sqlInsertEntityId.close();\n-            sqlNotifyTopicMessage.close();\n-            connection.close();\n+            if (connection != null) {\n+                sqlInsertEntityId.close();\n+                sqlNotifyTopicMessage.close();\n+                connection.close();\n+            }\n         } catch (SQLException e) {\n             throw new ParserSQLException(\"Error closing connection\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d83065ef5dd67f75f00548058af9bd067ab89a0b"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MjkxOTEyOnYy", "diffSide": "LEFT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxODo1NzoyMFrOHCh0VQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwNzoyOToyM1rOHEfTDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxMzI2OQ==", "bodyText": "I believe moving this into SqlEntityListener breaks PubSub as it doesn't update the last processed hash.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r472413269", "createdAt": "2020-08-18T18:57:20Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -160,16 +149,19 @@ public void loadRecordFile(StreamFileData streamFileData) {\n             recordFile.setLoadStart(startTime.getEpochSecond());\n             recordFile.setLoadEnd(Instant.now().getEpochSecond());\n             recordStreamFileListener.onEnd(recordFile);\n-            applicationStatusRepository.updateStatusValue(\n-                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE2NTQ4NA==", "bodyText": "Good point", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474165484", "createdAt": "2020-08-20T17:47:47Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -160,16 +149,19 @@ public void loadRecordFile(StreamFileData streamFileData) {\n             recordFile.setLoadStart(startTime.getEpochSecond());\n             recordFile.setLoadEnd(Instant.now().getEpochSecond());\n             recordStreamFileListener.onEnd(recordFile);\n-            applicationStatusRepository.updateStatusValue(\n-                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxMzI2OQ=="}, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQ2OTEzNQ==", "bodyText": "I think it's more important for SqlEntityListener to have control of the db writes. So I stuck with this and added the hash update to the pubs flow", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474469135", "createdAt": "2020-08-21T07:29:23Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -160,16 +149,19 @@ public void loadRecordFile(StreamFileData streamFileData) {\n             recordFile.setLoadStart(startTime.getEpochSecond());\n             recordFile.setLoadEnd(Instant.now().getEpochSecond());\n             recordStreamFileListener.onEnd(recordFile);\n-            applicationStatusRepository.updateStatusValue(\n-                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxMzI2OQ=="}, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1Mjk0MjQwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxOTowNDoyNlrOHCiCvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxOTowNDoyNlrOHCiCvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxNjk1Ng==", "bodyText": "nit: Would prefer parameters be named the same as the fields as most of the other fields are.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r472416956", "createdAt": "2020-08-18T19:04:26Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -95,13 +105,20 @@\n     private PreparedStatement sqlNotifyTopicMessage;\n     private Connection connection;\n     private long batchCount;\n+    private RecordFile partialRecordFile;\n \n     public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n                              RecordFileRepository recordFileRepository, MeterRegistry meterRegistry,\n-                             @Qualifier(CacheConfiguration.NEVER_EXPIRE_LARGE) CacheManager cacheManager) {\n+                             @Qualifier(CacheConfiguration.NEVER_EXPIRE_LARGE) CacheManager cacheManager,\n+                             ApplicationStatusRepository applicationStatusRepository,\n+                             TransactionRepository transactionRepository,\n+                             RecordParserProperties parserProps) {\n         this.dataSource = dataSource;\n+        this.applicationStatusRepository = applicationStatusRepository;\n         this.recordFileRepository = recordFileRepository;\n+        this.transactionRepository = transactionRepository;\n         sqlProperties = properties;\n+        parserProperties = parserProps;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1Mjk5ODQ3OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxOToyMDo0MlrOHCil3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQxNzo1NzoxMVrOHENFnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQyNTk0OQ==", "bodyText": "Turns out record_file.name has a unique constraint so this can't happen. Remove", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r472425949", "createdAt": "2020-08-18T19:20:42Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -160,16 +149,19 @@ public void loadRecordFile(StreamFileData streamFileData) {\n             recordFile.setLoadStart(startTime.getEpochSecond());\n             recordFile.setLoadEnd(Instant.now().getEpochSecond());\n             recordStreamFileListener.onEnd(recordFile);\n-            applicationStatusRepository.updateStatusValue(\n-                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (DuplicateFileException ex) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE3MDc4Mw==", "bodyText": "Removed", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474170783", "createdAt": "2020-08-20T17:57:11Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/RecordFileParser.java", "diffHunk": "@@ -160,16 +149,19 @@ public void loadRecordFile(StreamFileData streamFileData) {\n             recordFile.setLoadStart(startTime.getEpochSecond());\n             recordFile.setLoadEnd(Instant.now().getEpochSecond());\n             recordStreamFileListener.onEnd(recordFile);\n-            applicationStatusRepository.updateStatusValue(\n-                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n \n             recordParserLatencyMetric(recordFile);\n             success = true;\n+        } catch (DuplicateFileException ex) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQyNTk0OQ=="}, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1MzUyOTEzOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQyMTo0Mzo1OVrOHCnwUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQxNzo1MDoyN1rOHEM2lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjUxMDU0Ng==", "bodyText": "name is unique so this repository method can only ever return one entry. Should be adjusted to return single optional and handle method be adjusted.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r472510546", "createdAt": "2020-08-18T21:43:59Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -120,14 +137,18 @@ public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n         liveHashes = new ArrayList<>();\n         entityIds = new HashSet<>();\n         topicMessages = new ArrayList<>();\n+        partialRecordFile = null;\n     }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = FilenameUtils.getName(streamFileData.getFilename());\n \n-        if (recordFileRepository.findByName(fileName).size() > 0) {\n-            throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n+        List<RecordFile> matchingRecords = recordFileRepository.findByName(fileName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE2NjkzNQ==", "bodyText": "Handle method removed :)", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474166935", "createdAt": "2020-08-20T17:50:27Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -120,14 +137,18 @@ public SqlEntityListener(SqlProperties properties, DataSource dataSource,\n         liveHashes = new ArrayList<>();\n         entityIds = new HashSet<>();\n         topicMessages = new ArrayList<>();\n+        partialRecordFile = null;\n     }\n \n     @Override\n     public void onStart(StreamFileData streamFileData) {\n         String fileName = FilenameUtils.getName(streamFileData.getFilename());\n \n-        if (recordFileRepository.findByName(fileName).size() > 0) {\n-            throw new DuplicateFileException(\"File already exists in the database: \" + fileName);\n+        List<RecordFile> matchingRecords = recordFileRepository.findByName(fileName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjUxMDU0Ng=="}, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NzE5ODAyOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNTowNDo0MVrOHDLu9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQxNzo1MToxNVrOHEM4Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEwMDAyMA==", "bodyText": "Filesystem is case sensitive so comparison should be as well.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r473100020", "createdAt": "2020-08-19T15:04:41Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -314,6 +339,65 @@ private void notifyTopicMessages() {\n         executeBatch(sqlNotifyTopicMessage, \"topic notifications\");\n     }\n \n+    /**\n+     * Handle purely duplicate file in addition to faulty transaction management between spring repositories and manual\n+     * java sql operations. Check whether the given file was partially processed and matches the last record file\n+     * processed where the record_file table was updated but application_status table LAST_PROCESSED_RECORD_HASH was not\n+     * updated. If so update application_status table LAST_PROCESSED_RECORD_HASH appropriately with correct hash. Also\n+     * handle case where transaction data was rolled bach but not record_file entry\n+     *\n+     * @param recordFile record file object that was partially processed file\n+     */\n+    private void handleDuplicateOrPartiallyProcessedRecordFile(RecordFile recordFile) {\n+        // handle faulty transaction management between spring repositories and manual java sql operations\n+        DuplicateFileException duplicateFileException = new DuplicateFileException(\"File already exists in the \" +\n+                \"database: \" + recordFile.getName());\n+        RecordFile latestRecordFile = recordFileRepository.findTopByOrderByConsensusEndDesc().get();\n+        String lastRecordFileHash =\n+                applicationStatusRepository.findByStatusCode(ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH);\n+\n+        // check if latest record file is a match processed file by comparing name and hash\n+        if (latestRecordFile.getName().equalsIgnoreCase(recordFile.getName()) &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDE2NzM5MQ==", "bodyText": "No longer applicable.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474167391", "createdAt": "2020-08-20T17:51:15Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -314,6 +339,65 @@ private void notifyTopicMessages() {\n         executeBatch(sqlNotifyTopicMessage, \"topic notifications\");\n     }\n \n+    /**\n+     * Handle purely duplicate file in addition to faulty transaction management between spring repositories and manual\n+     * java sql operations. Check whether the given file was partially processed and matches the last record file\n+     * processed where the record_file table was updated but application_status table LAST_PROCESSED_RECORD_HASH was not\n+     * updated. If so update application_status table LAST_PROCESSED_RECORD_HASH appropriately with correct hash. Also\n+     * handle case where transaction data was rolled bach but not record_file entry\n+     *\n+     * @param recordFile record file object that was partially processed file\n+     */\n+    private void handleDuplicateOrPartiallyProcessedRecordFile(RecordFile recordFile) {\n+        // handle faulty transaction management between spring repositories and manual java sql operations\n+        DuplicateFileException duplicateFileException = new DuplicateFileException(\"File already exists in the \" +\n+                \"database: \" + recordFile.getName());\n+        RecordFile latestRecordFile = recordFileRepository.findTopByOrderByConsensusEndDesc().get();\n+        String lastRecordFileHash =\n+                applicationStatusRepository.findByStatusCode(ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH);\n+\n+        // check if latest record file is a match processed file by comparing name and hash\n+        if (latestRecordFile.getName().equalsIgnoreCase(recordFile.getName()) &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEwMDAyMA=="}, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NzIxODQwOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNTowOToxOFrOHDL75A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxNTowOToxOFrOHDL75A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEwMzMzMg==", "bodyText": "I don't see any point to this method. It only recovers from the scenario when record file saves but application status does not. This scenario can be easily handled by putting a @Transactional on onEnd() to ensure they're committed together. There shouldn't be any problem with that approach since they're both repositories.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r473103332", "createdAt": "2020-08-19T15:09:18Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -314,6 +339,65 @@ private void notifyTopicMessages() {\n         executeBatch(sqlNotifyTopicMessage, \"topic notifications\");\n     }\n \n+    /**\n+     * Handle purely duplicate file in addition to faulty transaction management between spring repositories and manual\n+     * java sql operations. Check whether the given file was partially processed and matches the last record file\n+     * processed where the record_file table was updated but application_status table LAST_PROCESSED_RECORD_HASH was not\n+     * updated. If so update application_status table LAST_PROCESSED_RECORD_HASH appropriately with correct hash. Also\n+     * handle case where transaction data was rolled bach but not record_file entry\n+     *\n+     * @param recordFile record file object that was partially processed file\n+     */\n+    private void handleDuplicateOrPartiallyProcessedRecordFile(RecordFile recordFile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1784e6532764d38f2188cdacb62970cf3f4feba0"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2Nzk2Nzg1OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNzoyNDo0N1rOHE1UCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQxNjowMTo0MVrOHFrzNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgyOTgzMg==", "bodyText": "This is not quite correct. If we inserted 0.0.2 during a previous file then get a rollback error next file it would be unnecessarily removed even though it exists in db. Please wrap entityCache with transactionAwareCacheDecorator instead.\nOr remove entityCache and entityIds and put @Cacheable(cacheManager = CacheConfiguration.NEVER_EXPIRE_LARGE) on entityRepository.insertEntityId().", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474829832", "createdAt": "2020-08-21T17:24:47Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk5OTQ1MQ==", "bodyText": "True, Applying the cache on successful cases of entityRepository.insertEntityId encapsulates the desired outcome", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474999451", "createdAt": "2020-08-21T22:42:44Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgyOTgzMg=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTE0NDg2Nw==", "bodyText": "Ended up going with transactionAwareCacheDecorator as @Cacheable on repository on wasn't rolling back.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r475144867", "createdAt": "2020-08-22T22:31:05Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgyOTgzMg=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTE1NzY0NQ==", "bodyText": "You missed my comment above that you need to Please also wrap all CacheManagers in a TransactionAwareCacheManagerProxy so they are transaction aware via annotations. This needs to be done regardless of approach here. I'm not sure if a TransactionAwareCacheManagerProxy also needs a transactionAwareCacheDecorator when used without annotations. Probably not.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r475157645", "createdAt": "2020-08-23T01:44:12Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgyOTgzMg=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTcyMjU0OQ==", "bodyText": "Good call, missed this.\nAdopted the TransactionAwareCacheManagerProxy and yeah with that the transactionAwareCacheDecorator is not needed", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r475722549", "createdAt": "2020-08-24T16:01:41Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgyOTgzMg=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2ODAwMjEzOnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxNzozNjowMlrOHE1pJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQyMjozNDoyMlrOHE_iTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgzNTIzNw==", "bodyText": "I think we should always clear entityIds and release connection. The connection doesn't really get released, it just gets a reference decremented since it's wrapped in an outer connection.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474835237", "createdAt": "2020-08-21T17:36:02Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));\n+        }\n+\n+        closeQuietly(sqlNotifyTopicMessage);\n+        sqlNotifyTopicMessage = null;\n         batchCount = 0;\n         contractResults.clear();\n         cryptoTransfers.clear();\n-        entityIds.clear();\n         fileData.clear();\n         liveHashes.clear();\n         nonFeeTransfers.clear();\n         topicMessages.clear();\n         transactions.clear();\n+\n+        if (releaseConnection) {\n+            entityIds.clear();\n+            DataSourceUtils.releaseConnection(connection, dataSource);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk5NjcxOA==", "bodyText": "Holding onto entities till end of file allowed for the right entityIds to be evicted from cache.\nWith the other approach you suggested I can revert this.\nThe release connection to decrement the reference seems to be necessary otherwise the connections stay open and the pool gets filled up with active connections.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474996718", "createdAt": "2020-08-21T22:31:44Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));\n+        }\n+\n+        closeQuietly(sqlNotifyTopicMessage);\n+        sqlNotifyTopicMessage = null;\n         batchCount = 0;\n         contractResults.clear();\n         cryptoTransfers.clear();\n-        entityIds.clear();\n         fileData.clear();\n         liveHashes.clear();\n         nonFeeTransfers.clear();\n         topicMessages.clear();\n         transactions.clear();\n+\n+        if (releaseConnection) {\n+            entityIds.clear();\n+            DataSourceUtils.releaseConnection(connection, dataSource);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgzNTIzNw=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk5NzMyNg==", "bodyText": "Right, I didn't suggest to remove releaseConnection but to always releaseConnection.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474997326", "createdAt": "2020-08-21T22:34:22Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -151,39 +146,43 @@ public void onStart(StreamFileData streamFileData) {\n     public void onEnd(RecordFile recordFile) {\n         executeBatches();\n \n+        boolean releaseConnection = false;\n         try {\n-            connection.commit();\n             recordFileRepository.save(recordFile);\n-            closeConnectionAndStatements();\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n+            applicationStatusRepository.updateStatusValue(\n+                    ApplicationStatusCode.LAST_PROCESSED_RECORD_HASH, recordFile.getFileHash());\n+            releaseConnection = true;\n+        } finally {\n+            cleanup(false, releaseConnection);\n         }\n     }\n \n     @Override\n     public void onError() {\n-        try {\n-            if (connection != null) {\n-                connection.rollback();\n-                closeConnectionAndStatements();\n-            }\n-        } catch (SQLException e) {\n-            log.error(\"Exception while rolling transaction back\", e);\n-        } finally {\n-            cleanup();\n-        }\n+        cleanup(true, true);\n     }\n \n-    private void cleanup() {\n+    private void cleanup(boolean isRollBackCleanup, boolean releaseConnection) {\n+        if (isRollBackCleanup) {\n+            // evict entities from cache from this round of batches as they were rolled back\n+            entityIds.forEach(entityId -> entityCache.evict(entityId.getId()));\n+        }\n+\n+        closeQuietly(sqlNotifyTopicMessage);\n+        sqlNotifyTopicMessage = null;\n         batchCount = 0;\n         contractResults.clear();\n         cryptoTransfers.clear();\n-        entityIds.clear();\n         fileData.clear();\n         liveHashes.clear();\n         nonFeeTransfers.clear();\n         topicMessages.clear();\n         transactions.clear();\n+\n+        if (releaseConnection) {\n+            entityIds.clear();\n+            DataSourceUtils.releaseConnection(connection, dataSource);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgzNTIzNw=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2ODcyNjI2OnYy", "diffSide": "RIGHT", "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQyMDo0MjowOVrOHE8x1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQxNToyNzowN1rOHFqYEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk1MjE1MA==", "bodyText": "This breaks encapsulation of SqlEntityListener since now it relies on EntityRecordItemListener to populate the same cache for this to work. So if SqlEntityListener is used outside of this scope, say from a test, it would not be functionally correct.\nBetter option would be to add a @CachePut to entityRepository.save() that updates the entity id cache. You can use @Caching to group.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474952150", "createdAt": "2020-08-21T20:42:09Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -241,22 +240,7 @@ public void onTransaction(Transaction transaction) throws ImporterException {\n \n     @Override\n     public void onEntityId(EntityId entityId) throws ImporterException {\n-        // add entities not found in cache to list of entities to be persisted\n-        if (entityCache.get(entityId.getId()) != null) {\n-            return;\n-        }\n-\n-        try {\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ID.ordinal(), entityId.getId());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_SHARD.ordinal(), entityId.getShardNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_REALM.ordinal(), entityId.getRealmNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_NUM.ordinal(), entityId.getEntityNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.TYPE.ordinal(), entityId.getType());\n-            sqlInsertEntityId.addBatch();\n-            entityIds.add(entityId);\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+        entityIds.add(entityId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk5NzM0MQ==", "bodyText": "Fair point.", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r474997341", "createdAt": "2020-08-21T22:34:26Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -241,22 +240,7 @@ public void onTransaction(Transaction transaction) throws ImporterException {\n \n     @Override\n     public void onEntityId(EntityId entityId) throws ImporterException {\n-        // add entities not found in cache to list of entities to be persisted\n-        if (entityCache.get(entityId.getId()) != null) {\n-            return;\n-        }\n-\n-        try {\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ID.ordinal(), entityId.getId());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_SHARD.ordinal(), entityId.getShardNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_REALM.ordinal(), entityId.getRealmNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_NUM.ordinal(), entityId.getEntityNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.TYPE.ordinal(), entityId.getType());\n-            sqlInsertEntityId.addBatch();\n-            entityIds.add(entityId);\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+        entityIds.add(entityId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk1MjE1MA=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTE0NDkwMw==", "bodyText": "Put cache management in SqlEntityListener using the recommended transactionAwareCacheDecorator .", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r475144903", "createdAt": "2020-08-22T22:31:59Z", "author": {"login": "Nana-EC"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -241,22 +240,7 @@ public void onTransaction(Transaction transaction) throws ImporterException {\n \n     @Override\n     public void onEntityId(EntityId entityId) throws ImporterException {\n-        // add entities not found in cache to list of entities to be persisted\n-        if (entityCache.get(entityId.getId()) != null) {\n-            return;\n-        }\n-\n-        try {\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ID.ordinal(), entityId.getId());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_SHARD.ordinal(), entityId.getShardNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_REALM.ordinal(), entityId.getRealmNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_NUM.ordinal(), entityId.getEntityNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.TYPE.ordinal(), entityId.getType());\n-            sqlInsertEntityId.addBatch();\n-            entityIds.add(entityId);\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+        entityIds.add(entityId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk1MjE1MA=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTY5OTIxOQ==", "bodyText": "Do we want to cache entityRepository.save() as mentioned above?", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/936#discussion_r475699219", "createdAt": "2020-08-24T15:27:07Z", "author": {"login": "steven-sheehy"}, "path": "hedera-mirror-importer/src/main/java/com/hedera/mirror/importer/parser/record/entity/sql/SqlEntityListener.java", "diffHunk": "@@ -241,22 +240,7 @@ public void onTransaction(Transaction transaction) throws ImporterException {\n \n     @Override\n     public void onEntityId(EntityId entityId) throws ImporterException {\n-        // add entities not found in cache to list of entities to be persisted\n-        if (entityCache.get(entityId.getId()) != null) {\n-            return;\n-        }\n-\n-        try {\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ID.ordinal(), entityId.getId());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_SHARD.ordinal(), entityId.getShardNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_REALM.ordinal(), entityId.getRealmNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.ENTITY_NUM.ordinal(), entityId.getEntityNum());\n-            sqlInsertEntityId.setLong(F_ENTITY_ID.TYPE.ordinal(), entityId.getType());\n-            sqlInsertEntityId.addBatch();\n-            entityIds.add(entityId);\n-        } catch (SQLException e) {\n-            throw new ParserSQLException(e);\n-        }\n+        entityIds.add(entityId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk1MjE1MA=="}, "originalCommit": {"oid": "93ad6221c56a7010f711163484f0bd61c42f4054"}, "originalPosition": 223}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 937, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}