{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg4NzI2NTU0", "number": 53583, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwNzozNDozOVrODom_EQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMDoxMDozNlrODpqT1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzOTA4MzY5OnYy", "diffSide": "RIGHT", "path": "docs/plugins/analysis-nori.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xN1QwNzozNDozOVrOF3QnbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNzoyNTo1NFrOF445Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ4ODIzNg==", "bodyText": "We also need to expose discard_punctuation in the tokenizer in order to detect floating point numbers and to split numbers correctly if they are separated with whitespaces ? We should then advise in the documentation to set discard_punctuation to false when using the nori_number filter and to remove punctuations in a subsequent filter ?", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r393488236", "createdAt": "2020-03-17T07:34:39Z", "author": {"login": "jimczi"}, "path": "docs/plugins/analysis-nori.asciidoc", "diffHunk": "@@ -434,3 +434,52 @@ Which responds with:\n --------------------------------------------------\n \n <1> The Hanja form is replaced by the Hangul translation.\n+\n+\n+[[analysis-nori-number]]\n+==== `nori_number` token filter", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5fa6abaa71e0b0406c69eb90edb98a17b5f74443"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE5NjcwNg==", "bodyText": "I think so.\nI added more description for that.\nPlease check the additional commits.", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395196706", "createdAt": "2020-03-19T17:25:54Z", "author": {"login": "danmuzi"}, "path": "docs/plugins/analysis-nori.asciidoc", "diffHunk": "@@ -434,3 +434,52 @@ Which responds with:\n --------------------------------------------------\n \n <1> The Hanja form is replaced by the Hangul translation.\n+\n+\n+[[analysis-nori-number]]\n+==== `nori_number` token filter", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQ4ODIzNg=="}, "originalCommit": {"oid": "5fa6abaa71e0b0406c69eb90edb98a17b5f74443"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MDExMzc1OnYy", "diffSide": "RIGHT", "path": "plugins/analysis-nori/src/test/java/org/elasticsearch/index/analysis/NoriAnalysisTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMDoxMDozMVrOF4-rHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxODozOTo0OFrOF5fPww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQyMg==", "bodyText": "Can you add a small test for the new discard_punctuation option of the tokenizer ?", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395291422", "createdAt": "2020-03-19T20:10:31Z", "author": {"login": "jimczi"}, "path": "plugins/analysis-nori/src/test/java/org/elasticsearch/index/analysis/NoriAnalysisTests.java", "diffHunk": "@@ -159,6 +162,20 @@ public void testNoriReadingForm() throws IOException {\n         assertTokenStreamContents(stream, new String[] {\"\ud5a5\uac00\"});\n     }\n \n+    public void testNoriNumber() throws IOException {\n+        Settings settings = Settings.builder()\n+            .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)\n+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())\n+            .put(\"index.analysis.filter.my_filter.type\", \"nori_number\")\n+            .build();\n+        TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(settings, new AnalysisNoriPlugin());\n+        TokenFilterFactory factory = analysis.tokenFilter.get(\"my_filter\");\n+        Tokenizer tokenizer = new KoreanTokenizer();\n+        tokenizer.setReader(new StringReader(\"\uc624\ub298 \uc2ed\ub9cc\uc774\ucc9c\uc624\ubc31\uc6d0\uc9dc\ub9ac \uc640\uc778 \uad6c\uc785\"));\n+        TokenStream stream = factory.create(tokenizer);\n+        assertTokenStreamContents(stream, new String[] {\"\uc624\ub298\", \"102500\", \"\uc6d0\", \"\uc9dc\ub9ac\", \"\uc640\uc778\", \"\uad6c\uc785\"});\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82191a3032f19729807dd2377af70a5c8272b0a4"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgyNTA5MQ==", "bodyText": "Done!", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395825091", "createdAt": "2020-03-20T18:39:48Z", "author": {"login": "danmuzi"}, "path": "plugins/analysis-nori/src/test/java/org/elasticsearch/index/analysis/NoriAnalysisTests.java", "diffHunk": "@@ -159,6 +162,20 @@ public void testNoriReadingForm() throws IOException {\n         assertTokenStreamContents(stream, new String[] {\"\ud5a5\uac00\"});\n     }\n \n+    public void testNoriNumber() throws IOException {\n+        Settings settings = Settings.builder()\n+            .put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)\n+            .put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString())\n+            .put(\"index.analysis.filter.my_filter.type\", \"nori_number\")\n+            .build();\n+        TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(settings, new AnalysisNoriPlugin());\n+        TokenFilterFactory factory = analysis.tokenFilter.get(\"my_filter\");\n+        Tokenizer tokenizer = new KoreanTokenizer();\n+        tokenizer.setReader(new StringReader(\"\uc624\ub298 \uc2ed\ub9cc\uc774\ucc9c\uc624\ubc31\uc6d0\uc9dc\ub9ac \uc640\uc778 \uad6c\uc785\"));\n+        TokenStream stream = factory.create(tokenizer);\n+        assertTokenStreamContents(stream, new String[] {\"\uc624\ub298\", \"102500\", \"\uc6d0\", \"\uc9dc\ub9ac\", \"\uc640\uc778\", \"\uad6c\uc785\"});\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQyMg=="}, "originalCommit": {"oid": "82191a3032f19729807dd2377af70a5c8272b0a4"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MDExNDEzOnYy", "diffSide": "RIGHT", "path": "docs/plugins/analysis-nori.asciidoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMDoxMDozNlrOF4-rUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxODozOTo0M1rOF5fPog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQ3NA==", "bodyText": "Maybe add add a NOTE: to emphasize this part ?", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395291474", "createdAt": "2020-03-19T20:10:36Z", "author": {"login": "jimczi"}, "path": "docs/plugins/analysis-nori.asciidoc", "diffHunk": "@@ -434,3 +439,105 @@ Which responds with:\n --------------------------------------------------\n \n <1> The Hanja form is replaced by the Hangul translation.\n+\n+\n+[[analysis-nori-number]]\n+==== `nori_number` token filter\n+\n+The `nori_number` token filter normalizes Korean numbers\n+to regular Arabic decimal numbers in half-width characters.\n+\n+Korean numbers are often written using a combination of Hangul and Arabic numbers with various kinds punctuation.\n+For example, \uff13\uff0e\uff12\ucc9c means 3200.\n+This filter does this kind of normalization and allows a search for 3200 to match \uff13\uff0e\uff12\ucc9c in text,\n+but can also be used to make range facets based on the normalized numbers and so on.\n+\n+Notice that this analyzer uses a token composition scheme and relies on punctuation tokens", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "82191a3032f19729807dd2377af70a5c8272b0a4"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgyNTA1OA==", "bodyText": "Done!", "url": "https://github.com/elastic/elasticsearch/pull/53583#discussion_r395825058", "createdAt": "2020-03-20T18:39:43Z", "author": {"login": "danmuzi"}, "path": "docs/plugins/analysis-nori.asciidoc", "diffHunk": "@@ -434,3 +439,105 @@ Which responds with:\n --------------------------------------------------\n \n <1> The Hanja form is replaced by the Hangul translation.\n+\n+\n+[[analysis-nori-number]]\n+==== `nori_number` token filter\n+\n+The `nori_number` token filter normalizes Korean numbers\n+to regular Arabic decimal numbers in half-width characters.\n+\n+Korean numbers are often written using a combination of Hangul and Arabic numbers with various kinds punctuation.\n+For example, \uff13\uff0e\uff12\ucc9c means 3200.\n+This filter does this kind of normalization and allows a search for 3200 to match \uff13\uff0e\uff12\ucc9c in text,\n+but can also be used to make range facets based on the normalized numbers and so on.\n+\n+Notice that this analyzer uses a token composition scheme and relies on punctuation tokens", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTI5MTQ3NA=="}, "originalCommit": {"oid": "82191a3032f19729807dd2377af70a5c8272b0a4"}, "originalPosition": 36}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3258, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}