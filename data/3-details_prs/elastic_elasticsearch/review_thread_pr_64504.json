{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE0MjkyMTgw", "number": 64504, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMjozNToyNVrOE0yX5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToxMToyM1rOE29Fig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNzg2NzI0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMjozNToyNVrOHss1fA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzo1MToyOFrOHs6H1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYzMzk4MA==", "bodyText": "Can we assert that leafReader.luceneReaderHolder.get() == null?\nOr perhaps even\nassert leafReader.luceneReaderHolder.trySet(null)\n\nto ensure it throws if we later trigger the creation of the lucene reader?\nI wonder if we could delegate from the single-doc-leaf-reader to two underlying leaf-readers and instantiate it at this time? I.e., we explicitly set leafReader.delegate to one of two implementations (translog or lucene memory backed) once we have found out which version we need? That would make the code slightly more explicit about this.", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r516633980", "createdAt": "2020-11-03T12:35:25Z", "author": {"login": "henningandersen"}, "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "diffHunk": "@@ -618,14 +619,30 @@ private ExternalReaderManager createReaderManager(RefreshWarmerListener external\n         }\n     }\n \n+    private GetResult getFromTranslog(Get get, Translog.Index index, DocumentMapper mapper,\n+                                      Function<Searcher, Searcher> searcherWrapper) throws IOException {\n+        assert get.isReadFromTranslog();\n+        final SingleDocLeafReader leafReader =\n+            new SingleDocLeafReader(shardId, index, mapper, config().getAnalyzer());\n+        final DirectoryReader reader = ElasticsearchDirectoryReader.wrap(new SingleDocDirectoryReader(leafReader), shardId);\n+        final Engine.Searcher searcher = new Engine.Searcher(\n+            \"realtime_get\", reader, config().getSimilarity(), config().getQueryCache(), config().getQueryCachingPolicy(), reader);\n+        final Searcher wrappedSearcher = searcherWrapper.apply(searcher);\n+        if (wrappedSearcher == searcher) {\n+            return new GetResult(searcher,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d34f893553f381b6c6275b7349f6ef2944e1d18"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg1MTY3MQ==", "bodyText": "++, I pushed 1aa952b to remove TranslogLeafReader from SingleDocLeafReader.", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r516851671", "createdAt": "2020-11-03T17:51:28Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "diffHunk": "@@ -618,14 +619,30 @@ private ExternalReaderManager createReaderManager(RefreshWarmerListener external\n         }\n     }\n \n+    private GetResult getFromTranslog(Get get, Translog.Index index, DocumentMapper mapper,\n+                                      Function<Searcher, Searcher> searcherWrapper) throws IOException {\n+        assert get.isReadFromTranslog();\n+        final SingleDocLeafReader leafReader =\n+            new SingleDocLeafReader(shardId, index, mapper, config().getAnalyzer());\n+        final DirectoryReader reader = ElasticsearchDirectoryReader.wrap(new SingleDocDirectoryReader(leafReader), shardId);\n+        final Engine.Searcher searcher = new Engine.Searcher(\n+            \"realtime_get\", reader, config().getSimilarity(), config().getQueryCache(), config().getQueryCachingPolicy(), reader);\n+        final Searcher wrappedSearcher = searcherWrapper.apply(searcher);\n+        if (wrappedSearcher == searcher) {\n+            return new GetResult(searcher,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYzMzk4MA=="}, "originalCommit": {"oid": "5d34f893553f381b6c6275b7349f6ef2944e1d18"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTEyMTkwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/engine/Engine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoyMjoxNlrOHs5CNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoyMjoxNlrOHs5CNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzMzg0Nw==", "bodyText": "The message talks about core cache keys while the if statement looks at the reader cache key.", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r516833847", "createdAt": "2020-11-03T17:22:16Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/engine/Engine.java", "diffHunk": "@@ -676,6 +678,80 @@ public Searcher acquireSearcher(String source, SearcherScope scope, Function<Sea\n         }\n     }\n \n+    public static Engine.Searcher wrapSearcher(Engine.Searcher searcher,\n+                                               CheckedFunction<DirectoryReader, DirectoryReader, IOException> readerWrapper) {\n+        assert readerWrapper != null;\n+        final ElasticsearchDirectoryReader esReader =\n+            ElasticsearchDirectoryReader.getElasticsearchDirectoryReader(searcher.getDirectoryReader());\n+        if (esReader == null) {\n+            throw new IllegalStateException(\"Can't wrap non elasticsearch directory reader\");\n+        }\n+        boolean success = false;\n+        DirectoryReader toClose = null;\n+        try {\n+            NonClosingReaderWrapper nonClosingReaderWrapper = new NonClosingReaderWrapper(searcher.getDirectoryReader());\n+            DirectoryReader wrappedReader = readerWrapper.apply(nonClosingReaderWrapper);\n+            toClose = wrappedReader;\n+            if (wrappedReader == nonClosingReaderWrapper) {\n+                success = true;\n+                return searcher;\n+            }\n+            if (wrappedReader.getReaderCacheHelper() != esReader.getReaderCacheHelper()) {\n+                throw new IllegalStateException(\"wrapped directory reader doesn't delegate IndexReader#getCoreCacheKey,\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1aa952bee4c604c51aee496b0686e4e401908a58"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzOTEzNDgwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/engine/Engine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoyNTozNlrOHs5KmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzoyNTozNlrOHs5KmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgzNTk5Mw==", "bodyText": "This line isn't needen, right? I also worry it's not future-proof as if someone added a line below that threw an exception, we'd still want to close toClose since we wouldn't have returned the wrappedSearcher yet?", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r516835993", "createdAt": "2020-11-03T17:25:36Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/engine/Engine.java", "diffHunk": "@@ -676,6 +678,80 @@ public Searcher acquireSearcher(String source, SearcherScope scope, Function<Sea\n         }\n     }\n \n+    public static Engine.Searcher wrapSearcher(Engine.Searcher searcher,\n+                                               CheckedFunction<DirectoryReader, DirectoryReader, IOException> readerWrapper) {\n+        assert readerWrapper != null;\n+        final ElasticsearchDirectoryReader esReader =\n+            ElasticsearchDirectoryReader.getElasticsearchDirectoryReader(searcher.getDirectoryReader());\n+        if (esReader == null) {\n+            throw new IllegalStateException(\"Can't wrap non elasticsearch directory reader\");\n+        }\n+        boolean success = false;\n+        DirectoryReader toClose = null;\n+        try {\n+            NonClosingReaderWrapper nonClosingReaderWrapper = new NonClosingReaderWrapper(searcher.getDirectoryReader());\n+            DirectoryReader wrappedReader = readerWrapper.apply(nonClosingReaderWrapper);\n+            toClose = wrappedReader;\n+            if (wrappedReader == nonClosingReaderWrapper) {\n+                success = true;\n+                return searcher;\n+            }\n+            if (wrappedReader.getReaderCacheHelper() != esReader.getReaderCacheHelper()) {\n+                throw new IllegalStateException(\"wrapped directory reader doesn't delegate IndexReader#getCoreCacheKey,\" +\n+                    \" wrappers must override this method and delegate to the original readers core cache key. Wrapped readers can't be \"\n+                    + \"used as cache keys since their are used only per request which would lead to subtle bugs\");\n+            }\n+            if (ElasticsearchDirectoryReader.getElasticsearchDirectoryReader(wrappedReader) != esReader) {\n+                // prevent that somebody wraps with a non-filter reader\n+                throw new IllegalStateException(\"wrapped directory reader hides actual ElasticsearchDirectoryReader but shouldn't\");\n+            }\n+            // we close the reader to make sure wrappers can release resources if needed....\n+            // our NonClosingReaderWrapper makes sure that our reader is not closed\n+            final Searcher wrappedSearcher = new Searcher(searcher.source(), wrappedReader,\n+                searcher.getSimilarity(), searcher.getQueryCache(), searcher.getQueryCachingPolicy(),\n+                () -> IOUtils.close(\n+                    wrappedReader, // this will close the wrappers excluding the NonClosingReaderWrapper\n+                    searcher));\n+            toClose = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1aa952bee4c604c51aee496b0686e4e401908a58"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MTI1NTA1OnYy", "diffSide": "LEFT", "path": "server/src/main/java/org/elasticsearch/index/shard/IndexShard.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODowNDoxNlrOHtMx1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNzowMDoyMlrOHthTtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1NzMzMg==", "bodyText": "I think we could leave this method where it is, since it is only used by IndexShard (except for tests)?\nIt might require a hack in the test code though to not have it public (similar to EngineAccess), but my preference would be to do that. But is kind of a small thing, can also accept this as is.", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r517157332", "createdAt": "2020-11-04T08:04:16Z", "author": {"login": "henningandersen"}, "path": "server/src/main/java/org/elasticsearch/index/shard/IndexShard.java", "diffHunk": "@@ -1235,87 +1235,7 @@ private void markSearcherAccessed() {\n         readAllowed();\n         markSearcherAccessed();\n         final Engine engine = getEngine();\n-        return engine.acquireSearcher(source, scope, this::wrapSearcher);\n-    }\n-\n-    private Engine.Searcher wrapSearcher(Engine.Searcher searcher) {\n-        assert ElasticsearchDirectoryReader.unwrap(searcher.getDirectoryReader())\n-            != null : \"DirectoryReader must be an instance or ElasticsearchDirectoryReader\";\n-        boolean success = false;\n-        try {\n-            final Engine.Searcher newSearcher = readerWrapper == null ? searcher : wrapSearcher(searcher, readerWrapper);\n-            assert newSearcher != null;\n-            success = true;\n-            return newSearcher;\n-        } catch (IOException ex) {\n-            throw new ElasticsearchException(\"failed to wrap searcher\", ex);\n-        } finally {\n-            if (success == false) {\n-                Releasables.close(success, searcher);\n-            }\n-        }\n-    }\n-\n-    static Engine.Searcher wrapSearcher(Engine.Searcher engineSearcher,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a48123ddc4eeb9698f8c7052a3df29acaffda14"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ5MzY4Nw==", "bodyText": "Yeah, I reverted it in a1dcfed.", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r517493687", "createdAt": "2020-11-04T17:00:22Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/shard/IndexShard.java", "diffHunk": "@@ -1235,87 +1235,7 @@ private void markSearcherAccessed() {\n         readAllowed();\n         markSearcherAccessed();\n         final Engine engine = getEngine();\n-        return engine.acquireSearcher(source, scope, this::wrapSearcher);\n-    }\n-\n-    private Engine.Searcher wrapSearcher(Engine.Searcher searcher) {\n-        assert ElasticsearchDirectoryReader.unwrap(searcher.getDirectoryReader())\n-            != null : \"DirectoryReader must be an instance or ElasticsearchDirectoryReader\";\n-        boolean success = false;\n-        try {\n-            final Engine.Searcher newSearcher = readerWrapper == null ? searcher : wrapSearcher(searcher, readerWrapper);\n-            assert newSearcher != null;\n-            success = true;\n-            return newSearcher;\n-        } catch (IOException ex) {\n-            throw new ElasticsearchException(\"failed to wrap searcher\", ex);\n-        } finally {\n-            if (success == false) {\n-                Releasables.close(success, searcher);\n-            }\n-        }\n-    }\n-\n-    static Engine.Searcher wrapSearcher(Engine.Searcher engineSearcher,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1NzMzMg=="}, "originalCommit": {"oid": "5a48123ddc4eeb9698f8c7052a3df29acaffda14"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MTI5MDQ4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/engine/SingleDocDirectoryReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODoxNToxOFrOHtNHKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQxNjo1ODo0MVrOHthPQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE2Mjc5Mg==", "bodyText": "Should we also close the delegate (if set)?", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r517162792", "createdAt": "2020-11-04T08:15:18Z", "author": {"login": "henningandersen"}, "path": "server/src/main/java/org/elasticsearch/index/engine/SingleDocDirectoryReader.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.index.engine;\n+\n+import org.apache.lucene.analysis.Analyzer;\n+import org.apache.lucene.index.BinaryDocValues;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.FieldInfos;\n+import org.apache.lucene.index.Fields;\n+import org.apache.lucene.index.IndexCommit;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.LeafMetaData;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.SortedDocValues;\n+import org.apache.lucene.index.SortedNumericDocValues;\n+import org.apache.lucene.index.SortedSetDocValues;\n+import org.apache.lucene.index.StoredFieldVisitor;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.Bits;\n+import org.elasticsearch.common.xcontent.XContentHelper;\n+import org.elasticsearch.index.mapper.DocumentMapper;\n+import org.elasticsearch.index.mapper.ParsedDocument;\n+import org.elasticsearch.index.mapper.SourceToParse;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.translog.Translog;\n+\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * A {@link DirectoryReader} contains a single leaf reader delegating to an in-memory Lucene segment that is lazily created from\n+ * a single document.\n+ */\n+final class SingleDocDirectoryReader extends DirectoryReader {\n+    private final SingleDocLeafReader leafReader;\n+\n+    SingleDocDirectoryReader(ShardId shardId, Translog.Index operation, DocumentMapper mapper, Analyzer analyzer) throws IOException {\n+        this(new SingleDocLeafReader(shardId, operation, mapper, analyzer));\n+    }\n+\n+    private SingleDocDirectoryReader(SingleDocLeafReader leafReader) throws IOException {\n+        super(leafReader.directory, new LeafReader[]{leafReader});\n+        this.leafReader = leafReader;\n+    }\n+\n+    boolean assertMemorySegmentStatus(boolean loaded) {\n+        return leafReader.assertMemorySegmentStatus(loaded);\n+    }\n+\n+    private static UnsupportedOperationException unsupported() {\n+        assert false : \"unsupported operation\";\n+        return new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged(IndexCommit commit) {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged(IndexWriter writer, boolean applyAllDeletes) {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public long getVersion() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public boolean isCurrent() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public IndexCommit getIndexCommit() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected void doClose() throws IOException {\n+        leafReader.close();\n+    }\n+\n+    @Override\n+    public CacheHelper getReaderCacheHelper() {\n+        return leafReader.getReaderCacheHelper();\n+    }\n+\n+    private static class SingleDocLeafReader extends LeafReader {\n+\n+        private final ShardId shardId;\n+        private final Translog.Index operation;\n+        private final DocumentMapper mapper;\n+        private final Analyzer analyzer;\n+        private final Directory directory;\n+        private final AtomicReference<LeafReader> delegate = new AtomicReference<>();\n+\n+        SingleDocLeafReader(ShardId shardId, Translog.Index operation, DocumentMapper mapper, Analyzer analyzer) {\n+            this.shardId = shardId;\n+            this.operation = operation;\n+            this.mapper = mapper;\n+            this.analyzer = analyzer;\n+            this.directory = new ByteBuffersDirectory();\n+        }\n+\n+        private LeafReader getDelegate() {\n+            LeafReader reader = delegate.get();\n+            if (reader == null) {\n+                synchronized (this) {\n+                    reader = delegate.get();\n+                    if (reader == null) {\n+                        reader = createInMemoryLeafReader();\n+                        final LeafReader existing = delegate.getAndSet(reader);\n+                        assert existing == null;\n+                    }\n+                }\n+            }\n+            return reader;\n+        }\n+\n+        private LeafReader createInMemoryLeafReader() {\n+            assert Thread.holdsLock(this);\n+            final ParsedDocument parsedDocs = mapper.parse(new SourceToParse(shardId.getIndexName(), operation.id(),\n+                operation.source(), XContentHelper.xContentType(operation.source()), operation.routing()));\n+            parsedDocs.updateSeqID(operation.seqNo(), operation.primaryTerm());\n+            parsedDocs.version().setLongValue(operation.version());\n+            final IndexWriterConfig writeConfig = new IndexWriterConfig(analyzer).setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n+            try (IndexWriter writer = new IndexWriter(directory, writeConfig)) {\n+                writer.addDocument(parsedDocs.rootDoc());\n+                final DirectoryReader reader = open(writer);\n+                if (reader.leaves().size() != 1 || reader.leaves().get(0).reader().numDocs() != 1) {\n+                    reader.close();\n+                    throw new IllegalStateException(\"Expected a single document segment; \" +\n+                        \"but [\" + reader.leaves().size() + \" segments with \" + reader.leaves().get(0).reader().numDocs() + \" documents\");\n+                }\n+                return reader.leaves().get(0).reader();\n+            } catch (IOException e) {\n+                throw new EngineException(shardId, \"failed to create an in-memory segment for get [\" + operation.id() + \"]\", e);\n+            }\n+        }\n+\n+        @Override\n+        public CacheHelper getCoreCacheHelper() {\n+            return getDelegate().getCoreCacheHelper();\n+        }\n+\n+        @Override\n+        public CacheHelper getReaderCacheHelper() {\n+            return getDelegate().getReaderCacheHelper();\n+        }\n+\n+        @Override\n+        public Terms terms(String field) throws IOException {\n+            return getDelegate().terms(field);\n+        }\n+\n+        @Override\n+        public NumericDocValues getNumericDocValues(String field) throws IOException {\n+            return getDelegate().getNumericDocValues(field);\n+        }\n+\n+        @Override\n+        public BinaryDocValues getBinaryDocValues(String field) throws IOException {\n+            return getDelegate().getBinaryDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedDocValues getSortedDocValues(String field) throws IOException {\n+            return getDelegate().getSortedDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {\n+            return getDelegate().getSortedNumericDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {\n+            return getDelegate().getSortedSetDocValues(field);\n+        }\n+\n+        @Override\n+        public NumericDocValues getNormValues(String field) throws IOException {\n+            return getDelegate().getNormValues(field);\n+        }\n+\n+        @Override\n+        public FieldInfos getFieldInfos() {\n+            return getDelegate().getFieldInfos();\n+        }\n+\n+        @Override\n+        public Bits getLiveDocs() {\n+            return getDelegate().getLiveDocs();\n+        }\n+\n+        @Override\n+        public PointValues getPointValues(String field) throws IOException {\n+            return getDelegate().getPointValues(field);\n+        }\n+\n+        @Override\n+        public void checkIntegrity() throws IOException {\n+        }\n+\n+        @Override\n+        public LeafMetaData getMetaData() {\n+            return getDelegate().getMetaData();\n+        }\n+\n+        @Override\n+        public Fields getTermVectors(int docID) throws IOException {\n+            return getDelegate().getTermVectors(docID);\n+        }\n+\n+        @Override\n+        public int numDocs() {\n+            return 1;\n+        }\n+\n+        @Override\n+        public int maxDoc() {\n+            return 1;\n+        }\n+\n+        synchronized boolean assertMemorySegmentStatus(boolean loaded) {\n+            if (loaded && delegate.get() == null) {\n+                assert false : \"Expected an in memory segment was loaded; but it wasn't. Please check the reader wrapper implementation\";\n+            }\n+            if (loaded == false && delegate.get() != null) {\n+                assert false : \"Expected an in memory segment wasn't loaded; but it was. Please check the reader wrapper implementation\";\n+            }\n+            return true;\n+        }\n+\n+        @Override\n+        public void document(int docID, StoredFieldVisitor visitor) throws IOException {\n+            assert assertMemorySegmentStatus(true);\n+            getDelegate().document(docID, visitor);\n+        }\n+\n+        @Override\n+        protected void doClose() throws IOException {\n+            directory.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a48123ddc4eeb9698f8c7052a3df29acaffda14"}, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3ODgyMg==", "bodyText": "+1 Could you add a unit test that if you register a close listener on this reader, then it gets called when the reader is called?\nAnd could you also make getDelegate() fail once close() has been called?", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r517178822", "createdAt": "2020-11-04T08:44:31Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/engine/SingleDocDirectoryReader.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.index.engine;\n+\n+import org.apache.lucene.analysis.Analyzer;\n+import org.apache.lucene.index.BinaryDocValues;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.FieldInfos;\n+import org.apache.lucene.index.Fields;\n+import org.apache.lucene.index.IndexCommit;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.LeafMetaData;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.SortedDocValues;\n+import org.apache.lucene.index.SortedNumericDocValues;\n+import org.apache.lucene.index.SortedSetDocValues;\n+import org.apache.lucene.index.StoredFieldVisitor;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.Bits;\n+import org.elasticsearch.common.xcontent.XContentHelper;\n+import org.elasticsearch.index.mapper.DocumentMapper;\n+import org.elasticsearch.index.mapper.ParsedDocument;\n+import org.elasticsearch.index.mapper.SourceToParse;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.translog.Translog;\n+\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * A {@link DirectoryReader} contains a single leaf reader delegating to an in-memory Lucene segment that is lazily created from\n+ * a single document.\n+ */\n+final class SingleDocDirectoryReader extends DirectoryReader {\n+    private final SingleDocLeafReader leafReader;\n+\n+    SingleDocDirectoryReader(ShardId shardId, Translog.Index operation, DocumentMapper mapper, Analyzer analyzer) throws IOException {\n+        this(new SingleDocLeafReader(shardId, operation, mapper, analyzer));\n+    }\n+\n+    private SingleDocDirectoryReader(SingleDocLeafReader leafReader) throws IOException {\n+        super(leafReader.directory, new LeafReader[]{leafReader});\n+        this.leafReader = leafReader;\n+    }\n+\n+    boolean assertMemorySegmentStatus(boolean loaded) {\n+        return leafReader.assertMemorySegmentStatus(loaded);\n+    }\n+\n+    private static UnsupportedOperationException unsupported() {\n+        assert false : \"unsupported operation\";\n+        return new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged(IndexCommit commit) {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged(IndexWriter writer, boolean applyAllDeletes) {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public long getVersion() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public boolean isCurrent() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public IndexCommit getIndexCommit() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected void doClose() throws IOException {\n+        leafReader.close();\n+    }\n+\n+    @Override\n+    public CacheHelper getReaderCacheHelper() {\n+        return leafReader.getReaderCacheHelper();\n+    }\n+\n+    private static class SingleDocLeafReader extends LeafReader {\n+\n+        private final ShardId shardId;\n+        private final Translog.Index operation;\n+        private final DocumentMapper mapper;\n+        private final Analyzer analyzer;\n+        private final Directory directory;\n+        private final AtomicReference<LeafReader> delegate = new AtomicReference<>();\n+\n+        SingleDocLeafReader(ShardId shardId, Translog.Index operation, DocumentMapper mapper, Analyzer analyzer) {\n+            this.shardId = shardId;\n+            this.operation = operation;\n+            this.mapper = mapper;\n+            this.analyzer = analyzer;\n+            this.directory = new ByteBuffersDirectory();\n+        }\n+\n+        private LeafReader getDelegate() {\n+            LeafReader reader = delegate.get();\n+            if (reader == null) {\n+                synchronized (this) {\n+                    reader = delegate.get();\n+                    if (reader == null) {\n+                        reader = createInMemoryLeafReader();\n+                        final LeafReader existing = delegate.getAndSet(reader);\n+                        assert existing == null;\n+                    }\n+                }\n+            }\n+            return reader;\n+        }\n+\n+        private LeafReader createInMemoryLeafReader() {\n+            assert Thread.holdsLock(this);\n+            final ParsedDocument parsedDocs = mapper.parse(new SourceToParse(shardId.getIndexName(), operation.id(),\n+                operation.source(), XContentHelper.xContentType(operation.source()), operation.routing()));\n+            parsedDocs.updateSeqID(operation.seqNo(), operation.primaryTerm());\n+            parsedDocs.version().setLongValue(operation.version());\n+            final IndexWriterConfig writeConfig = new IndexWriterConfig(analyzer).setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n+            try (IndexWriter writer = new IndexWriter(directory, writeConfig)) {\n+                writer.addDocument(parsedDocs.rootDoc());\n+                final DirectoryReader reader = open(writer);\n+                if (reader.leaves().size() != 1 || reader.leaves().get(0).reader().numDocs() != 1) {\n+                    reader.close();\n+                    throw new IllegalStateException(\"Expected a single document segment; \" +\n+                        \"but [\" + reader.leaves().size() + \" segments with \" + reader.leaves().get(0).reader().numDocs() + \" documents\");\n+                }\n+                return reader.leaves().get(0).reader();\n+            } catch (IOException e) {\n+                throw new EngineException(shardId, \"failed to create an in-memory segment for get [\" + operation.id() + \"]\", e);\n+            }\n+        }\n+\n+        @Override\n+        public CacheHelper getCoreCacheHelper() {\n+            return getDelegate().getCoreCacheHelper();\n+        }\n+\n+        @Override\n+        public CacheHelper getReaderCacheHelper() {\n+            return getDelegate().getReaderCacheHelper();\n+        }\n+\n+        @Override\n+        public Terms terms(String field) throws IOException {\n+            return getDelegate().terms(field);\n+        }\n+\n+        @Override\n+        public NumericDocValues getNumericDocValues(String field) throws IOException {\n+            return getDelegate().getNumericDocValues(field);\n+        }\n+\n+        @Override\n+        public BinaryDocValues getBinaryDocValues(String field) throws IOException {\n+            return getDelegate().getBinaryDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedDocValues getSortedDocValues(String field) throws IOException {\n+            return getDelegate().getSortedDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {\n+            return getDelegate().getSortedNumericDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {\n+            return getDelegate().getSortedSetDocValues(field);\n+        }\n+\n+        @Override\n+        public NumericDocValues getNormValues(String field) throws IOException {\n+            return getDelegate().getNormValues(field);\n+        }\n+\n+        @Override\n+        public FieldInfos getFieldInfos() {\n+            return getDelegate().getFieldInfos();\n+        }\n+\n+        @Override\n+        public Bits getLiveDocs() {\n+            return getDelegate().getLiveDocs();\n+        }\n+\n+        @Override\n+        public PointValues getPointValues(String field) throws IOException {\n+            return getDelegate().getPointValues(field);\n+        }\n+\n+        @Override\n+        public void checkIntegrity() throws IOException {\n+        }\n+\n+        @Override\n+        public LeafMetaData getMetaData() {\n+            return getDelegate().getMetaData();\n+        }\n+\n+        @Override\n+        public Fields getTermVectors(int docID) throws IOException {\n+            return getDelegate().getTermVectors(docID);\n+        }\n+\n+        @Override\n+        public int numDocs() {\n+            return 1;\n+        }\n+\n+        @Override\n+        public int maxDoc() {\n+            return 1;\n+        }\n+\n+        synchronized boolean assertMemorySegmentStatus(boolean loaded) {\n+            if (loaded && delegate.get() == null) {\n+                assert false : \"Expected an in memory segment was loaded; but it wasn't. Please check the reader wrapper implementation\";\n+            }\n+            if (loaded == false && delegate.get() != null) {\n+                assert false : \"Expected an in memory segment wasn't loaded; but it was. Please check the reader wrapper implementation\";\n+            }\n+            return true;\n+        }\n+\n+        @Override\n+        public void document(int docID, StoredFieldVisitor visitor) throws IOException {\n+            assert assertMemorySegmentStatus(true);\n+            getDelegate().document(docID, visitor);\n+        }\n+\n+        @Override\n+        protected void doClose() throws IOException {\n+            directory.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE2Mjc5Mg=="}, "originalCommit": {"oid": "5a48123ddc4eeb9698f8c7052a3df29acaffda14"}, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ5MjU0NA==", "bodyText": "++, fixed in 0dc1a54", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r517492544", "createdAt": "2020-11-04T16:58:41Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/SingleDocDirectoryReader.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.index.engine;\n+\n+import org.apache.lucene.analysis.Analyzer;\n+import org.apache.lucene.index.BinaryDocValues;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.FieldInfos;\n+import org.apache.lucene.index.Fields;\n+import org.apache.lucene.index.IndexCommit;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.LeafMetaData;\n+import org.apache.lucene.index.LeafReader;\n+import org.apache.lucene.index.NumericDocValues;\n+import org.apache.lucene.index.PointValues;\n+import org.apache.lucene.index.SortedDocValues;\n+import org.apache.lucene.index.SortedNumericDocValues;\n+import org.apache.lucene.index.SortedSetDocValues;\n+import org.apache.lucene.index.StoredFieldVisitor;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.Bits;\n+import org.elasticsearch.common.xcontent.XContentHelper;\n+import org.elasticsearch.index.mapper.DocumentMapper;\n+import org.elasticsearch.index.mapper.ParsedDocument;\n+import org.elasticsearch.index.mapper.SourceToParse;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.index.translog.Translog;\n+\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+/**\n+ * A {@link DirectoryReader} contains a single leaf reader delegating to an in-memory Lucene segment that is lazily created from\n+ * a single document.\n+ */\n+final class SingleDocDirectoryReader extends DirectoryReader {\n+    private final SingleDocLeafReader leafReader;\n+\n+    SingleDocDirectoryReader(ShardId shardId, Translog.Index operation, DocumentMapper mapper, Analyzer analyzer) throws IOException {\n+        this(new SingleDocLeafReader(shardId, operation, mapper, analyzer));\n+    }\n+\n+    private SingleDocDirectoryReader(SingleDocLeafReader leafReader) throws IOException {\n+        super(leafReader.directory, new LeafReader[]{leafReader});\n+        this.leafReader = leafReader;\n+    }\n+\n+    boolean assertMemorySegmentStatus(boolean loaded) {\n+        return leafReader.assertMemorySegmentStatus(loaded);\n+    }\n+\n+    private static UnsupportedOperationException unsupported() {\n+        assert false : \"unsupported operation\";\n+        return new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged(IndexCommit commit) {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected DirectoryReader doOpenIfChanged(IndexWriter writer, boolean applyAllDeletes) {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public long getVersion() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public boolean isCurrent() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    public IndexCommit getIndexCommit() {\n+        throw unsupported();\n+    }\n+\n+    @Override\n+    protected void doClose() throws IOException {\n+        leafReader.close();\n+    }\n+\n+    @Override\n+    public CacheHelper getReaderCacheHelper() {\n+        return leafReader.getReaderCacheHelper();\n+    }\n+\n+    private static class SingleDocLeafReader extends LeafReader {\n+\n+        private final ShardId shardId;\n+        private final Translog.Index operation;\n+        private final DocumentMapper mapper;\n+        private final Analyzer analyzer;\n+        private final Directory directory;\n+        private final AtomicReference<LeafReader> delegate = new AtomicReference<>();\n+\n+        SingleDocLeafReader(ShardId shardId, Translog.Index operation, DocumentMapper mapper, Analyzer analyzer) {\n+            this.shardId = shardId;\n+            this.operation = operation;\n+            this.mapper = mapper;\n+            this.analyzer = analyzer;\n+            this.directory = new ByteBuffersDirectory();\n+        }\n+\n+        private LeafReader getDelegate() {\n+            LeafReader reader = delegate.get();\n+            if (reader == null) {\n+                synchronized (this) {\n+                    reader = delegate.get();\n+                    if (reader == null) {\n+                        reader = createInMemoryLeafReader();\n+                        final LeafReader existing = delegate.getAndSet(reader);\n+                        assert existing == null;\n+                    }\n+                }\n+            }\n+            return reader;\n+        }\n+\n+        private LeafReader createInMemoryLeafReader() {\n+            assert Thread.holdsLock(this);\n+            final ParsedDocument parsedDocs = mapper.parse(new SourceToParse(shardId.getIndexName(), operation.id(),\n+                operation.source(), XContentHelper.xContentType(operation.source()), operation.routing()));\n+            parsedDocs.updateSeqID(operation.seqNo(), operation.primaryTerm());\n+            parsedDocs.version().setLongValue(operation.version());\n+            final IndexWriterConfig writeConfig = new IndexWriterConfig(analyzer).setOpenMode(IndexWriterConfig.OpenMode.CREATE);\n+            try (IndexWriter writer = new IndexWriter(directory, writeConfig)) {\n+                writer.addDocument(parsedDocs.rootDoc());\n+                final DirectoryReader reader = open(writer);\n+                if (reader.leaves().size() != 1 || reader.leaves().get(0).reader().numDocs() != 1) {\n+                    reader.close();\n+                    throw new IllegalStateException(\"Expected a single document segment; \" +\n+                        \"but [\" + reader.leaves().size() + \" segments with \" + reader.leaves().get(0).reader().numDocs() + \" documents\");\n+                }\n+                return reader.leaves().get(0).reader();\n+            } catch (IOException e) {\n+                throw new EngineException(shardId, \"failed to create an in-memory segment for get [\" + operation.id() + \"]\", e);\n+            }\n+        }\n+\n+        @Override\n+        public CacheHelper getCoreCacheHelper() {\n+            return getDelegate().getCoreCacheHelper();\n+        }\n+\n+        @Override\n+        public CacheHelper getReaderCacheHelper() {\n+            return getDelegate().getReaderCacheHelper();\n+        }\n+\n+        @Override\n+        public Terms terms(String field) throws IOException {\n+            return getDelegate().terms(field);\n+        }\n+\n+        @Override\n+        public NumericDocValues getNumericDocValues(String field) throws IOException {\n+            return getDelegate().getNumericDocValues(field);\n+        }\n+\n+        @Override\n+        public BinaryDocValues getBinaryDocValues(String field) throws IOException {\n+            return getDelegate().getBinaryDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedDocValues getSortedDocValues(String field) throws IOException {\n+            return getDelegate().getSortedDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedNumericDocValues getSortedNumericDocValues(String field) throws IOException {\n+            return getDelegate().getSortedNumericDocValues(field);\n+        }\n+\n+        @Override\n+        public SortedSetDocValues getSortedSetDocValues(String field) throws IOException {\n+            return getDelegate().getSortedSetDocValues(field);\n+        }\n+\n+        @Override\n+        public NumericDocValues getNormValues(String field) throws IOException {\n+            return getDelegate().getNormValues(field);\n+        }\n+\n+        @Override\n+        public FieldInfos getFieldInfos() {\n+            return getDelegate().getFieldInfos();\n+        }\n+\n+        @Override\n+        public Bits getLiveDocs() {\n+            return getDelegate().getLiveDocs();\n+        }\n+\n+        @Override\n+        public PointValues getPointValues(String field) throws IOException {\n+            return getDelegate().getPointValues(field);\n+        }\n+\n+        @Override\n+        public void checkIntegrity() throws IOException {\n+        }\n+\n+        @Override\n+        public LeafMetaData getMetaData() {\n+            return getDelegate().getMetaData();\n+        }\n+\n+        @Override\n+        public Fields getTermVectors(int docID) throws IOException {\n+            return getDelegate().getTermVectors(docID);\n+        }\n+\n+        @Override\n+        public int numDocs() {\n+            return 1;\n+        }\n+\n+        @Override\n+        public int maxDoc() {\n+            return 1;\n+        }\n+\n+        synchronized boolean assertMemorySegmentStatus(boolean loaded) {\n+            if (loaded && delegate.get() == null) {\n+                assert false : \"Expected an in memory segment was loaded; but it wasn't. Please check the reader wrapper implementation\";\n+            }\n+            if (loaded == false && delegate.get() != null) {\n+                assert false : \"Expected an in memory segment wasn't loaded; but it was. Please check the reader wrapper implementation\";\n+            }\n+            return true;\n+        }\n+\n+        @Override\n+        public void document(int docID, StoredFieldVisitor visitor) throws IOException {\n+            assert assertMemorySegmentStatus(true);\n+            getDelegate().document(docID, visitor);\n+        }\n+\n+        @Override\n+        protected void doClose() throws IOException {\n+            directory.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE2Mjc5Mg=="}, "originalCommit": {"oid": "5a48123ddc4eeb9698f8c7052a3df29acaffda14"}, "originalPosition": 272}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0MTMxODM2OnYy", "diffSide": "RIGHT", "path": "test/framework/src/main/java/org/elasticsearch/index/engine/EngineTestCase.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODoyNDowMVrOHtNYtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQyMDowNToxNVrOHtnz7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE2NzI4Nw==", "bodyText": "Might mostly be for my understanding, but since the wrapped directory reader could return different results from the inner reader, should we not return null here?", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r517167287", "createdAt": "2020-11-04T08:24:01Z", "author": {"login": "henningandersen"}, "path": "test/framework/src/main/java/org/elasticsearch/index/engine/EngineTestCase.java", "diffHunk": "@@ -1259,4 +1276,76 @@ public static long getInFlightDocCount(Engine engine) {\n     public static void assertNoInFlightDocuments(Engine engine) throws Exception {\n         assertBusy(() -> assertThat(getInFlightDocCount(engine), equalTo(0L)));\n     }\n+\n+    public static final class MatchingDirectoryReader extends FilterDirectoryReader {\n+        private final Query query;\n+\n+        public MatchingDirectoryReader(DirectoryReader in, Query query) throws IOException {\n+            super(in, new SubReaderWrapper() {\n+                @Override\n+                public LeafReader wrap(LeafReader leaf) {\n+                    try {\n+                        final IndexSearcher searcher = new IndexSearcher(leaf);\n+                        final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1.0f);\n+                        final Scorer scorer = weight.scorer(leaf.getContext());\n+                        final DocIdSetIterator iterator = scorer != null ? scorer.iterator() : null;\n+                        final FixedBitSet liveDocs = new FixedBitSet(leaf.maxDoc());\n+                        if (iterator != null) {\n+                            for (int docId = iterator.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = iterator.nextDoc()) {\n+                                if (leaf.getLiveDocs() == null || leaf.getLiveDocs().get(docId)) {\n+                                    liveDocs.set(docId);\n+                                }\n+                            }\n+                        }\n+                        return new FilterLeafReader(leaf) {\n+                            @Override\n+                            public Bits getLiveDocs() {\n+                                return liveDocs;\n+                            }\n+\n+                            @Override\n+                            public CacheHelper getCoreCacheHelper() {\n+                                return leaf.getCoreCacheHelper();\n+                            }\n+\n+                            @Override\n+                            public CacheHelper getReaderCacheHelper() {\n+                                return null; // modify liveDocs\n+                            }\n+                        };\n+                    } catch (IOException e) {\n+                        throw new UncheckedIOException(e);\n+                    }\n+                }\n+            });\n+            this.query = query;\n+        }\n+\n+        @Override\n+        protected DirectoryReader doWrapDirectoryReader(DirectoryReader in) throws IOException {\n+            return new MatchingDirectoryReader(in, query);\n+        }\n+\n+        @Override\n+        public CacheHelper getReaderCacheHelper() {\n+            return in.getReaderCacheHelper();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a48123ddc4eeb9698f8c7052a3df29acaffda14"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzQ5MzA5Mg==", "bodyText": "Good catch. We should return null here as we modify the liveDocs. I addressed it in eb968e4.", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r517493092", "createdAt": "2020-11-04T16:59:29Z", "author": {"login": "dnhatn"}, "path": "test/framework/src/main/java/org/elasticsearch/index/engine/EngineTestCase.java", "diffHunk": "@@ -1259,4 +1276,76 @@ public static long getInFlightDocCount(Engine engine) {\n     public static void assertNoInFlightDocuments(Engine engine) throws Exception {\n         assertBusy(() -> assertThat(getInFlightDocCount(engine), equalTo(0L)));\n     }\n+\n+    public static final class MatchingDirectoryReader extends FilterDirectoryReader {\n+        private final Query query;\n+\n+        public MatchingDirectoryReader(DirectoryReader in, Query query) throws IOException {\n+            super(in, new SubReaderWrapper() {\n+                @Override\n+                public LeafReader wrap(LeafReader leaf) {\n+                    try {\n+                        final IndexSearcher searcher = new IndexSearcher(leaf);\n+                        final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1.0f);\n+                        final Scorer scorer = weight.scorer(leaf.getContext());\n+                        final DocIdSetIterator iterator = scorer != null ? scorer.iterator() : null;\n+                        final FixedBitSet liveDocs = new FixedBitSet(leaf.maxDoc());\n+                        if (iterator != null) {\n+                            for (int docId = iterator.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = iterator.nextDoc()) {\n+                                if (leaf.getLiveDocs() == null || leaf.getLiveDocs().get(docId)) {\n+                                    liveDocs.set(docId);\n+                                }\n+                            }\n+                        }\n+                        return new FilterLeafReader(leaf) {\n+                            @Override\n+                            public Bits getLiveDocs() {\n+                                return liveDocs;\n+                            }\n+\n+                            @Override\n+                            public CacheHelper getCoreCacheHelper() {\n+                                return leaf.getCoreCacheHelper();\n+                            }\n+\n+                            @Override\n+                            public CacheHelper getReaderCacheHelper() {\n+                                return null; // modify liveDocs\n+                            }\n+                        };\n+                    } catch (IOException e) {\n+                        throw new UncheckedIOException(e);\n+                    }\n+                }\n+            });\n+            this.query = query;\n+        }\n+\n+        @Override\n+        protected DirectoryReader doWrapDirectoryReader(DirectoryReader in) throws IOException {\n+            return new MatchingDirectoryReader(in, query);\n+        }\n+\n+        @Override\n+        public CacheHelper getReaderCacheHelper() {\n+            return in.getReaderCacheHelper();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE2NzI4Nw=="}, "originalCommit": {"oid": "5a48123ddc4eeb9698f8c7052a3df29acaffda14"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzYwMDIzNg==", "bodyText": "I had to reverted this and added a comment instead. Please see: e176bf2", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r517600236", "createdAt": "2020-11-04T20:05:15Z", "author": {"login": "dnhatn"}, "path": "test/framework/src/main/java/org/elasticsearch/index/engine/EngineTestCase.java", "diffHunk": "@@ -1259,4 +1276,76 @@ public static long getInFlightDocCount(Engine engine) {\n     public static void assertNoInFlightDocuments(Engine engine) throws Exception {\n         assertBusy(() -> assertThat(getInFlightDocCount(engine), equalTo(0L)));\n     }\n+\n+    public static final class MatchingDirectoryReader extends FilterDirectoryReader {\n+        private final Query query;\n+\n+        public MatchingDirectoryReader(DirectoryReader in, Query query) throws IOException {\n+            super(in, new SubReaderWrapper() {\n+                @Override\n+                public LeafReader wrap(LeafReader leaf) {\n+                    try {\n+                        final IndexSearcher searcher = new IndexSearcher(leaf);\n+                        final Weight weight = searcher.createWeight(query, ScoreMode.COMPLETE_NO_SCORES, 1.0f);\n+                        final Scorer scorer = weight.scorer(leaf.getContext());\n+                        final DocIdSetIterator iterator = scorer != null ? scorer.iterator() : null;\n+                        final FixedBitSet liveDocs = new FixedBitSet(leaf.maxDoc());\n+                        if (iterator != null) {\n+                            for (int docId = iterator.nextDoc(); docId != DocIdSetIterator.NO_MORE_DOCS; docId = iterator.nextDoc()) {\n+                                if (leaf.getLiveDocs() == null || leaf.getLiveDocs().get(docId)) {\n+                                    liveDocs.set(docId);\n+                                }\n+                            }\n+                        }\n+                        return new FilterLeafReader(leaf) {\n+                            @Override\n+                            public Bits getLiveDocs() {\n+                                return liveDocs;\n+                            }\n+\n+                            @Override\n+                            public CacheHelper getCoreCacheHelper() {\n+                                return leaf.getCoreCacheHelper();\n+                            }\n+\n+                            @Override\n+                            public CacheHelper getReaderCacheHelper() {\n+                                return null; // modify liveDocs\n+                            }\n+                        };\n+                    } catch (IOException e) {\n+                        throw new UncheckedIOException(e);\n+                    }\n+                }\n+            });\n+            this.query = query;\n+        }\n+\n+        @Override\n+        protected DirectoryReader doWrapDirectoryReader(DirectoryReader in) throws IOException {\n+            return new MatchingDirectoryReader(in, query);\n+        }\n+\n+        @Override\n+        public CacheHelper getReaderCacheHelper() {\n+            return in.getReaderCacheHelper();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE2NzI4Nw=="}, "originalCommit": {"oid": "5a48123ddc4eeb9698f8c7052a3df29acaffda14"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDU5NDAyOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToxMToyM1rOHwBwXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQwMDozODoyNVrOHwHJ3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEyMjQ2MQ==", "bodyText": "GetResult.isFromTranslog is slightly confusing now, in that in this case, the value is really taken from translog, but still we return false here. I think we should add a comment to isFromTranslog to clarify this in detail to avoid someone relying on the method, expecting a different meaning.", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r520122461", "createdAt": "2020-11-09T21:11:23Z", "author": {"login": "henningandersen"}, "path": "server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java", "diffHunk": "@@ -903,6 +906,53 @@ public void testSimpleOperations() throws Exception {\n         searchResult.close();\n     }\n \n+    public void testGetWithSearcherWrapper() throws Exception {\n+        engine.refresh(\"warm_up\");\n+        engine.index(indexForDoc(createParsedDoc(\"1\", null)));\n+        assertThat(engine.lastRefreshedCheckpoint(), equalTo(NO_OPS_PERFORMED));\n+        try (Engine.GetResult get = engine.get(new Engine.Get(true, true, \"1\"), docMapper(), randomSearcherWrapper())) {\n+            // we do not track the translog location yet\n+            assertTrue(get.exists());\n+            assertFalse(get.isFromTranslog());\n+        }\n+        // refresh triggered, as we did not track translog location until the first realtime get.\n+        assertThat(engine.lastRefreshedCheckpoint(), equalTo(0L));\n+\n+        engine.index(indexForDoc(createParsedDoc(\"1\", null)));\n+        try (Engine.GetResult get = engine.get(new Engine.Get(true, true, \"1\"), docMapper(), searcher -> searcher)) {\n+            assertTrue(get.exists());\n+            assertTrue(get.isFromTranslog());\n+        }\n+        assertThat(engine.lastRefreshedCheckpoint(), equalTo(0L)); // no refresh; just read from translog\n+        if (randomBoolean()) {\n+            engine.index(indexForDoc(createParsedDoc(\"1\", null)));\n+        }\n+        try (Engine.GetResult get = engine.get(new Engine.Get(true, true, \"1\"), docMapper(),\n+            searcher -> SearcherHelper.wrapSearcher(searcher, reader -> new MatchingDirectoryReader(reader, new MatchAllDocsQuery())))) {\n+            assertTrue(get.exists());\n+            assertFalse(get.isFromTranslog());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e176bf2a86bfebac45b18a7e4dff1b53dd09f426"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDIxMDkxMA==", "bodyText": "++, added in 792ab09", "url": "https://github.com/elastic/elasticsearch/pull/64504#discussion_r520210910", "createdAt": "2020-11-10T00:38:25Z", "author": {"login": "dnhatn"}, "path": "server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java", "diffHunk": "@@ -903,6 +906,53 @@ public void testSimpleOperations() throws Exception {\n         searchResult.close();\n     }\n \n+    public void testGetWithSearcherWrapper() throws Exception {\n+        engine.refresh(\"warm_up\");\n+        engine.index(indexForDoc(createParsedDoc(\"1\", null)));\n+        assertThat(engine.lastRefreshedCheckpoint(), equalTo(NO_OPS_PERFORMED));\n+        try (Engine.GetResult get = engine.get(new Engine.Get(true, true, \"1\"), docMapper(), randomSearcherWrapper())) {\n+            // we do not track the translog location yet\n+            assertTrue(get.exists());\n+            assertFalse(get.isFromTranslog());\n+        }\n+        // refresh triggered, as we did not track translog location until the first realtime get.\n+        assertThat(engine.lastRefreshedCheckpoint(), equalTo(0L));\n+\n+        engine.index(indexForDoc(createParsedDoc(\"1\", null)));\n+        try (Engine.GetResult get = engine.get(new Engine.Get(true, true, \"1\"), docMapper(), searcher -> searcher)) {\n+            assertTrue(get.exists());\n+            assertTrue(get.isFromTranslog());\n+        }\n+        assertThat(engine.lastRefreshedCheckpoint(), equalTo(0L)); // no refresh; just read from translog\n+        if (randomBoolean()) {\n+            engine.index(indexForDoc(createParsedDoc(\"1\", null)));\n+        }\n+        try (Engine.GetResult get = engine.get(new Engine.Get(true, true, \"1\"), docMapper(),\n+            searcher -> SearcherHelper.wrapSearcher(searcher, reader -> new MatchingDirectoryReader(reader, new MatchAllDocsQuery())))) {\n+            assertTrue(get.exists());\n+            assertFalse(get.isFromTranslog());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEyMjQ2MQ=="}, "originalCommit": {"oid": "e176bf2a86bfebac45b18a7e4dff1b53dd09f426"}, "originalPosition": 148}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4029, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}