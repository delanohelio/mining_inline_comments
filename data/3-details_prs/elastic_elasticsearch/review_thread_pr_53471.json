{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg3MTkyMjk2", "number": 53471, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMTo1ODozNFrODnb9YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMTo1ODozNFrODnb9YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyNjc5MTM2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxMTo1ODozNFrOF1brEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxNDoyOToxOVrOF1hBpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTU3MjI0Mg==", "bodyText": "This won't work in the long run.\nWhat if the model is removed from a pipeline while loading?\n\nModel A is referenced by a pipeline on this node\nThat model happens to fall out of cache\nData node says calls loadModelAndCache for model A\nPipeline is then deleted in the middle of loading it up\nData node gets Cancelling load of model [A] as it is no longer referenced by a pipeline failure\n\nThis type of complexity is one of the many reasons I think we need to move away from a cache based system and towards a deployment based system.", "url": "https://github.com/elastic/elasticsearch/pull/53471#discussion_r391572242", "createdAt": "2020-03-12T11:58:34Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -182,6 +205,24 @@ private boolean loadModelIfNecessary(String modelId, ActionListener<Model> model\n         } // synchronized (loadingListeners)\n     }\n \n+    private void loadModelAndCache(String modelId, ActionListener<Model> modelActionListener) {\n+        synchronized (loadingListeners) {\n+            Model cachedModel = localModelCache.get(modelId);\n+            if (cachedModel != null) {\n+                modelActionListener.onResponse(cachedModel);\n+                return;\n+            }\n+\n+            if (loadingListeners.computeIfPresent(\n+                    modelId,\n+                    (storedModelKey, listenerQueue) -> addFluently(listenerQueue, modelActionListener)) == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "89239cb1b5c12d506b6943516789e630d5864612"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTY1OTk0Mw==", "bodyText": "Good point. I didn't want to add any more complexity to ModelLoadingService but adding this second use certainly does that. I thought I could split my usage out to a separate class but that then defeats the sharing the cache.\nRegarding the error case you describe at least it will error and the listener will be notified rather than silently timing out.\n\nThis is probably ok as long as it never is intended for production.\n\nLet's agree this feature branch will never be merged into master.  I agree the whole caching strategy is liable to change and there are many unanswered questions in general, which is kinda the point of doing this PoC.", "url": "https://github.com/elastic/elasticsearch/pull/53471#discussion_r391659943", "createdAt": "2020-03-12T14:29:19Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/loadingservice/ModelLoadingService.java", "diffHunk": "@@ -182,6 +205,24 @@ private boolean loadModelIfNecessary(String modelId, ActionListener<Model> model\n         } // synchronized (loadingListeners)\n     }\n \n+    private void loadModelAndCache(String modelId, ActionListener<Model> modelActionListener) {\n+        synchronized (loadingListeners) {\n+            Model cachedModel = localModelCache.get(modelId);\n+            if (cachedModel != null) {\n+                modelActionListener.onResponse(cachedModel);\n+                return;\n+            }\n+\n+            if (loadingListeners.computeIfPresent(\n+                    modelId,\n+                    (storedModelKey, listenerQueue) -> addFluently(listenerQueue, modelActionListener)) == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTU3MjI0Mg=="}, "originalCommit": {"oid": "89239cb1b5c12d506b6943516789e630d5864612"}, "originalPosition": 94}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3326, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}