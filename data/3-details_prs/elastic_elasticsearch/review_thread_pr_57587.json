{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI3MDI3NzMx", "number": 57587, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNTo0Mjo1NVrOECNi0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNTo0Mjo1NVrOECNi0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzU0NTEyOnYy", "diffSide": "RIGHT", "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/ClusterHealthIT.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNTo0Mjo1NVrOGeh5lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjo1NTo1NlrOGelAgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NTg3Ng==", "bodyText": "We could maybe use a PlainActionFuture<Void> here and resolve it if keepSubmittingTasks.get() is false or on failure. Then we can just .get() on it at the end of the test and not leak the update task beyond the test time? (also I think we could get some strange test failures without that, where we set false for keep submitting, then leak the task, and get an onFailure when we shut down the cluster after the test?)", "url": "https://github.com/elastic/elasticsearch/pull/57587#discussion_r434665876", "createdAt": "2020-06-03T15:42:55Z", "author": {"login": "original-brownbear"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/ClusterHealthIT.java", "diffHunk": "@@ -311,4 +312,34 @@ public void testHealthOnMasterFailover() throws Exception {\n             assertSame(responseFuture.get().getStatus(), ClusterHealthStatus.GREEN);\n         }\n     }\n+\n+    public void testWaitForEventsTimesOutIfMasterBusy() {\n+        final AtomicBoolean keepSubmittingTasks = new AtomicBoolean(true);\n+        final ClusterService clusterService = internalCluster().getInstance(ClusterService.class, internalCluster().getMasterName());\n+        clusterService.submitStateUpdateTask(\"looping task\", new ClusterStateUpdateTask(Priority.LOW) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3485341558b6f1574b31e41d8b2ecd15fbb6303"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcxNjgwMg==", "bodyText": "Good point, yes. Also in the other test that submits a looping cluster state task - we're waiting for it to complete assuming everything else goes smoothly but otherwise we don't clean it up, so I adjusted that too. See 40945b7.", "url": "https://github.com/elastic/elasticsearch/pull/57587#discussion_r434716802", "createdAt": "2020-06-03T16:55:56Z", "author": {"login": "DaveCTurner"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/ClusterHealthIT.java", "diffHunk": "@@ -311,4 +312,34 @@ public void testHealthOnMasterFailover() throws Exception {\n             assertSame(responseFuture.get().getStatus(), ClusterHealthStatus.GREEN);\n         }\n     }\n+\n+    public void testWaitForEventsTimesOutIfMasterBusy() {\n+        final AtomicBoolean keepSubmittingTasks = new AtomicBoolean(true);\n+        final ClusterService clusterService = internalCluster().getInstance(ClusterService.class, internalCluster().getMasterName());\n+        clusterService.submitStateUpdateTask(\"looping task\", new ClusterStateUpdateTask(Priority.LOW) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NTg3Ng=="}, "originalCommit": {"oid": "a3485341558b6f1574b31e41d8b2ecd15fbb6303"}, "originalPosition": 16}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3753, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}