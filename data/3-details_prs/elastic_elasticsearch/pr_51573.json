{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY4MjQwNzU4", "number": 51573, "title": "[ML][Inference] Fix model pagination with models as resources", "bodyText": "This adds logic to handle paging problems when the ID pattern + tags reference models stored as resources.\nMost of the complexity comes from the issue where a model stored as a resource could be at the start, or the end of a page or when we are on the last page.\ncloses #51543", "createdAt": "2020-01-28T21:33:52Z", "url": "https://github.com/elastic/elasticsearch/pull/51573", "merged": true, "mergeCommit": {"oid": "108ebc1baa479e75663f082591e05148deb036ff"}, "closed": true, "closedAt": "2020-01-31T11:52:26Z", "author": {"login": "benwtrent"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb-4QcsgH2gAyMzY4MjQwNzU4OmI4YzBkMmU5ZDE5NjIxYzQ4ZGFhZGJiYTFmNWY4MDE1YjY4NmRjYTg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb_rn_CgFqTM1MTM3NzU5OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/b8c0d2e9d19621c48daadbba1f5f8015b686dca8", "committedDate": "2020-01-28T21:30:53Z", "message": "[ML][Inference] Fix model pagination with models as resources"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ5ODkwOTYy", "url": "https://github.com/elastic/elasticsearch/pull/51573#pullrequestreview-349890962", "createdAt": "2020-01-29T06:51:05Z", "commit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNjo1MTowNlrOFi-G5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwODozMjoxMFrOFjAHwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIxMzQ3OA==", "bodyText": "Should we also have   - length: { trained_model_configs: 2 } assertion here?", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372213478", "createdAt": "2020-01-29T06:51:06Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/ml/inference_crud.yml", "diffHunk": "@@ -119,7 +172,7 @@ setup:\n         model_id: \"*\"\n         from: 0\n         size: 2\n-  - match: { count: 4 }\n+  - match: { count: 6 }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIxMzQ5Ng==", "bodyText": "Should we also have   - length: { trained_model_configs: 6 } assertion here?", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372213496", "createdAt": "2020-01-29T06:51:11Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/ml/inference_crud.yml", "diffHunk": "@@ -102,10 +152,13 @@ setup:\n   - do:\n       ml.get_trained_models:\n         model_id: \"*\"\n-  - match: { count: 4 }\n+  - match: { count: 6 }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIxNzkwOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    //NOTE: these test assume that the query pagination results are \"buffered\"\n          \n          \n            \n                    // NOTE: these tests assume that the query pagination results are \"buffered\"", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372217908", "createdAt": "2020-01-29T07:08:50Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProviderTests.java", "diffHunk": "@@ -86,6 +90,63 @@ public void testExpandIdsQuery() {\n         });\n     }\n \n+    public void testExpandIdsPagination() {\n+        //NOTE: these test assume that the query pagination results are \"buffered\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0MjU5MQ==", "bodyText": "So, are pageParams guaranteed to be non-null?", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372242591", "createdAt": "2020-01-29T08:22:32Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0NDQ1MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        }\n          \n          \n            \n                        } else {", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372244451", "createdAt": "2020-01-29T08:27:22Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,73 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0NTM3Mw==", "bodyText": "Not sure if it makes sense to optimize this, but I could imagine having an iterator going through allFoundIds and calling .remove() on the iterator rather than removing the smallest item from the collection directly.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372245373", "createdAt": "2020-01-29T08:29:37Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,73 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0NjQ2Ng==", "bodyText": "I found this method somewhat hard to follow.\nShould the logic be similar to merge sort? I.e. we have two sorted collections and take size elements (in total) from whichever collection has the lowest id at the front. from would be per-collection (so we'd have two froms).\nLMK if you think it's possible to rewrite it this way.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372246466", "createdAt": "2020-01-29T08:32:10Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,73 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 109}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/e7e8249ac9c6787ed8625501519fd627d5f0a619", "committedDate": "2020-01-29T12:27:54Z", "message": "addressing pr comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUwMjQ1NDY2", "url": "https://github.com/elastic/elasticsearch/pull/51573#pullrequestreview-350245466", "createdAt": "2020-01-29T16:28:09Z", "commit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUwMTkwMDQz", "url": "https://github.com/elastic/elasticsearch/pull/51573#pullrequestreview-350190043", "createdAt": "2020-01-29T15:21:55Z", "commit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNToyMTo1NVrOFjMY5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNjo0MToxM1rOFjPfKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ0NzQ2MQ==", "bodyText": "Unrelated nit: Can MODELS_STORED_AS_RESOURCE be a Collections.unmodifiableSet then line 520\n return new HashSet<>(MODELS_STORED_AS_RESOURCE);\nin matchedResourceIds wouldn't have to wrap a set in a set", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372447461", "createdAt": "2020-01-29T15:21:55Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,\n                           Set<String> tags,\n                           ActionListener<Tuple<Long, Set<String>>> idsListener) {\n         String[] tokens = Strings.tokenizeToStringArray(idExpression, \",\");\n+        Set<String> foundResourceIds = new HashSet<>();\n+        if (tags.isEmpty()) {\n+            foundResourceIds.addAll(matchedResourceIds(tokens));\n+        } else {\n+            for(String resourceId : matchedResourceIds(tokens)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ1NDc1OA==", "bodyText": "Could this error in a valid situation?\nfrom = 0, size = 10, idExpression = 'bar*,foo'\nI have the models bar-1, bar-2 ... bar-10  and foo.\nThe first 10 bar-n are returned by the search but foo is unmatched.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372454758", "createdAt": "2020-01-29T15:32:52Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ1NTY5Nw==", "bodyText": "So the strategy is to get extra then figure out where the resource model ids would fit in that sorted list \ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372455697", "createdAt": "2020-01-29T15:34:21Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,\n                           Set<String> tags,\n                           ActionListener<Tuple<Long, Set<String>>> idsListener) {\n         String[] tokens = Strings.tokenizeToStringArray(idExpression, \",\");\n+        Set<String> foundResourceIds = new HashSet<>();\n+        if (tags.isEmpty()) {\n+            foundResourceIds.addAll(matchedResourceIds(tokens));\n+        } else {\n+            for(String resourceId : matchedResourceIds(tokens)) {\n+                // Does the model as a resource have all the tags?\n+                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n+                    foundResourceIds.add(resourceId);\n+                }\n+            }\n+        }\n         SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()\n             .sort(SortBuilders.fieldSort(TrainedModelConfig.MODEL_ID.getPreferredName())\n                 // If there are no resources, there might be no mapping for the id field.\n                 // This makes sure we don't get an error if that happens.\n                 .unmappedType(\"long\"))\n-            .query(buildExpandIdsQuery(tokens, tags));\n-        if (pageParams != null) {\n-            sourceBuilder.from(pageParams.getFrom()).size(pageParams.getSize());\n-        }\n+            .query(buildExpandIdsQuery(tokens, tags))\n+            // We \"buffer\" the from and size to take into account models stored as resources.\n+            // This is so we handle the edge cases when the model that is stored as a resource is at the start/end of\n+            // a page.\n+            .from(Math.max(0, pageParams.getFrom() - foundResourceIds.size()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ2MzY2OA==", "bodyText": "If the number of search hits and and resource model ids are < pageParams.getSize() then that is the size limit. from isn't a factor here", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372463668", "createdAt": "2020-01-29T15:46:41Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ3MzUwMQ==", "bodyText": "Maybe shortcut:\nif (foundFromResources.size() == 0) { return foundFromDocs }\nfoundFromDocs will be the right size as you won't have changed the from and size parameters to the query by adding 0 to them. (lines 407 & 408)", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372473501", "createdAt": "2020-01-29T16:01:39Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ5ODIxNg==", "bodyText": "My way of looking at it is this: we have a request for 10 ids and 2 models stored as resource. We query for 14 ids so we can figure out where the resource model ids would fit. We have a result like\nAAOOOOOOOOOOBB\nwhere the As and Bs are the padding.\nIf the model resource Ids come before the As then they are outside the ordering and all the Os are returned. Same if the model resource ids come after the Bs.\nIf they are inserted middle (ids M & N) we have an ordering like\nAAOOOOOMOOONOOBB\nwe we take the first 10 after the As.\nSame for\nAAMOOOOOONOOOOBB\nMNAAOOOOOOOOOOBB\nAMAOOOOOONOOOOBB\nAAOOOOOOOOOOBBMN\nI think we have to track the insertion position of the model resource Ids. Collections.binarySearch() would give the insert position.\nThere is a further complication when the search does not return size hits and you have\nAAOOOOOOOOOO\nor\nAAOOOO", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372498216", "createdAt": "2020-01-29T16:41:13Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            } else {\n+                // If we have removed all items belonging on the previous page, but are still over sized, this means we should\n+                // remove items that belong on the next page.\n+                allFoundIds.remove(allFoundIds.last());\n+            }\n+        }\n+        return allFoundIds;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 139}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a7052f66fe30947a49d67e5e4e043f5e9cf8247", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/5a7052f66fe30947a49d67e5e4e043f5e9cf8247", "committedDate": "2020-01-29T19:37:58Z", "message": "addressing PR comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87f28c9053b4d0e0d1f84bb67a125880f488983d", "author": {"user": {"login": "benwtrent", "name": "Benjamin Trent"}}, "url": "https://github.com/elastic/elasticsearch/commit/87f28c9053b4d0e0d1f84bb67a125880f488983d", "committedDate": "2020-01-30T16:12:22Z", "message": "Simplifying paging logic"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUxMzc3NTk5", "url": "https://github.com/elastic/elasticsearch/pull/51573#pullrequestreview-351377599", "createdAt": "2020-01-31T09:21:45Z", "commit": {"oid": "87f28c9053b4d0e0d1f84bb67a125880f488983d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3206, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}