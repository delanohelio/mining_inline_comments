{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzczNDg5NDgz", "number": 52182, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxMjo0MFrODfCQpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxMDoxNzo0NFrODqUD8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODY5NDc3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxMjo0MFrOFoghmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxMjo0MFrOFoghmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMDI1MA==", "bodyText": "nit: sequenceNum -> maxSeqNo", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378020250", "createdAt": "2020-02-12T03:12:40Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1640,6 +1656,72 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    /**\n+     * Extracts an instance of {@link SegmentInfos} for each snapshot tracked in the given {@link BlobStoreIndexShardSnapshots} and\n+     * compares it against the given {@link IndexCommit}.\n+     * If the sequence number, local checkpoint, primary term and history UUID of the given commit and the found shard snapshot are equal\n+     * then we will not snapshot the files in the index commit but rather assign the files in the existing matching snapshot to the new\n+     * shard snapshot.\n+     * Compared to merely comparing files in the index commit and repository, the logic here will be able to identify shards with equal\n+     * content but different segment structure (as may be the case after replica promotion) and avoid redundant snapshot creation in this\n+     * situation.\n+     * Note: This method does not load any blobs from the repository. It instead exploits the fact that segment info files are stored\n+     * alongside their content as hash in the {@link BlobStoreIndexShardSnapshots} metadata to quickly load all {@link SegmentInfos} for\n+     * each shard snapshot.\n+     * Also see {@link StoreFileMetaData#hashEqualsContents()}.\n+     *\n+     * @param snapshotIndexCommit Index commit to snapshot\n+     * @param snapshots shard snapshots already in the repository\n+     * @return List of files already in the repository to use as the given shard's snapshot or {@code null} if no such set of files could\n+     *         be found in the current shard snapshots\n+     */\n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(\n+            IndexCommit snapshotIndexCommit, BlobStoreIndexShardSnapshots snapshots) {\n+        final Map<String, String> userCommitData;\n+        try {\n+            userCommitData = snapshotIndexCommit.getUserData();\n+        } catch (IOException e) {\n+            assert false : new AssertionError(\"Did not expect #getUserData to throw but saw exception\", e);\n+            return null;\n+        }\n+        final String sequenceNum = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 196}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODY5NjAxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxMzozOFrOFogiVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxMzozOFrOFogiVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMDQzNg==", "bodyText": "nit: maxSeqNo, localCheckpoint, and historyUUID are not null.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378020436", "createdAt": "2020-02-12T03:13:38Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1640,6 +1656,72 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    /**\n+     * Extracts an instance of {@link SegmentInfos} for each snapshot tracked in the given {@link BlobStoreIndexShardSnapshots} and\n+     * compares it against the given {@link IndexCommit}.\n+     * If the sequence number, local checkpoint, primary term and history UUID of the given commit and the found shard snapshot are equal\n+     * then we will not snapshot the files in the index commit but rather assign the files in the existing matching snapshot to the new\n+     * shard snapshot.\n+     * Compared to merely comparing files in the index commit and repository, the logic here will be able to identify shards with equal\n+     * content but different segment structure (as may be the case after replica promotion) and avoid redundant snapshot creation in this\n+     * situation.\n+     * Note: This method does not load any blobs from the repository. It instead exploits the fact that segment info files are stored\n+     * alongside their content as hash in the {@link BlobStoreIndexShardSnapshots} metadata to quickly load all {@link SegmentInfos} for\n+     * each shard snapshot.\n+     * Also see {@link StoreFileMetaData#hashEqualsContents()}.\n+     *\n+     * @param snapshotIndexCommit Index commit to snapshot\n+     * @param snapshots shard snapshots already in the repository\n+     * @return List of files already in the repository to use as the given shard's snapshot or {@code null} if no such set of files could\n+     *         be found in the current shard snapshots\n+     */\n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(\n+            IndexCommit snapshotIndexCommit, BlobStoreIndexShardSnapshots snapshots) {\n+        final Map<String, String> userCommitData;\n+        try {\n+            userCommitData = snapshotIndexCommit.getUserData();\n+        } catch (IOException e) {\n+            assert false : new AssertionError(\"Did not expect #getUserData to throw but saw exception\", e);\n+            return null;\n+        }\n+        final String sequenceNum = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);\n+        final String localCheckpoint = userCommitData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY);\n+        final String primaryTerm = userCommitData.get(Engine.MAX_PRIMARY_TERM);\n+        final String historyUUID = userCommitData.get(Engine.HISTORY_UUID_KEY);\n+        if (sequenceNum != null && localCheckpoint != null && primaryTerm != null && historyUUID != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODcwMDU4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxNzowMVrOFoglFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxNzowMVrOFoglFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTE0Mg==", "bodyText": "Should we add maxPrimaryTerm to SequenceNumbers#CommitInfo then use its equals here?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021142", "createdAt": "2020-02-12T03:17:01Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1640,6 +1656,72 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    /**\n+     * Extracts an instance of {@link SegmentInfos} for each snapshot tracked in the given {@link BlobStoreIndexShardSnapshots} and\n+     * compares it against the given {@link IndexCommit}.\n+     * If the sequence number, local checkpoint, primary term and history UUID of the given commit and the found shard snapshot are equal\n+     * then we will not snapshot the files in the index commit but rather assign the files in the existing matching snapshot to the new\n+     * shard snapshot.\n+     * Compared to merely comparing files in the index commit and repository, the logic here will be able to identify shards with equal\n+     * content but different segment structure (as may be the case after replica promotion) and avoid redundant snapshot creation in this\n+     * situation.\n+     * Note: This method does not load any blobs from the repository. It instead exploits the fact that segment info files are stored\n+     * alongside their content as hash in the {@link BlobStoreIndexShardSnapshots} metadata to quickly load all {@link SegmentInfos} for\n+     * each shard snapshot.\n+     * Also see {@link StoreFileMetaData#hashEqualsContents()}.\n+     *\n+     * @param snapshotIndexCommit Index commit to snapshot\n+     * @param snapshots shard snapshots already in the repository\n+     * @return List of files already in the repository to use as the given shard's snapshot or {@code null} if no such set of files could\n+     *         be found in the current shard snapshots\n+     */\n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(\n+            IndexCommit snapshotIndexCommit, BlobStoreIndexShardSnapshots snapshots) {\n+        final Map<String, String> userCommitData;\n+        try {\n+            userCommitData = snapshotIndexCommit.getUserData();\n+        } catch (IOException e) {\n+            assert false : new AssertionError(\"Did not expect #getUserData to throw but saw exception\", e);\n+            return null;\n+        }\n+        final String sequenceNum = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);\n+        final String localCheckpoint = userCommitData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY);\n+        final String primaryTerm = userCommitData.get(Engine.MAX_PRIMARY_TERM);\n+        final String historyUUID = userCommitData.get(Engine.HISTORY_UUID_KEY);\n+        if (sequenceNum != null && localCheckpoint != null && primaryTerm != null && historyUUID != null) {\n+            for (SnapshotFiles snapshotFileSet : snapshots.snapshots()) {\n+                final List<BlobStoreIndexShardSnapshot.FileInfo> files = snapshotFileSet.indexFiles();\n+                final SegmentInfos segmentInfos;\n+                try {\n+                    final Directory dir = new ByteBuffersDirectory();\n+                    for (BlobStoreIndexShardSnapshot.FileInfo f : files) {\n+                        final StoreFileMetaData m = f.metadata();\n+                        if (m.hashEqualsContents() == false) {\n+                            continue;\n+                        }\n+                        try (IndexOutput indexOutput = dir.createOutput(m.name(), IOContext.DEFAULT)) {\n+                            final BytesRef fileContent = m.hash();\n+                            indexOutput.writeBytes(fileContent.bytes, fileContent.offset, fileContent.length);\n+                        }\n+                    }\n+                    segmentInfos = SegmentInfos.readLatestCommit(dir);\n+                } catch (IOException e) {\n+                    logger.debug(\"Failed to read SegmentInfos from files {}\", files);\n+                    continue;\n+                }\n+                final Map<String, String> snapshotUserCommitData = segmentInfos.getUserData();\n+                if (sequenceNum.equals(snapshotUserCommitData.get(SequenceNumbers.MAX_SEQ_NO)) &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 222}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODcwMTQzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/engine/Engine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxNzo0MlrOFoglmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxNzo0MlrOFoglmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTI3NQ==", "bodyText": "Should we move this tag to SequenceNumbers.java?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021275", "createdAt": "2020-02-12T03:17:42Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/Engine.java", "diffHunk": "@@ -111,6 +111,7 @@\n \n     public static final String SYNC_COMMIT_ID = \"sync_id\"; // TODO: Remove sync_id in 9.0\n     public static final String HISTORY_UUID_KEY = \"history_uuid\";\n+    public static final String MAX_PRIMARY_TERM = \"max_primary_term\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODcwMjA2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxODowOFrOFogl9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxODowOFrOFogl9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTM2Ng==", "bodyText": "perhaps call this maxPrimaryTerm?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021366", "createdAt": "2020-02-12T03:18:08Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "diffHunk": "@@ -180,6 +180,8 @@\n     @Nullable\n     private final String historyUUID;\n \n+    private volatile long lastOpPrimaryTerm;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODcwMzcyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxOToxMFrOFogm3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxOToxMFrOFogm3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTU5OQ==", "bodyText": "I think we should advance the maxPrimaryTerm using the term of the operation instead of the current term of the replication group.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021599", "createdAt": "2020-02-12T03:19:10Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "diffHunk": "@@ -2366,6 +2375,7 @@ public long getPersistedLocalCheckpoint() {\n      */\n     protected final void markSeqNoAsSeen(long seqNo) {\n         localCheckpointTracker.advanceMaxSeqNo(seqNo);\n+        lastOpPrimaryTerm = engineConfig.getPrimaryTermSupplier().getAsLong();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODcwNTI3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoyMDozOVrOFogn2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoyMDozOVrOFogn2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTg1MA==", "bodyText": "We don't need to do it here.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021850", "createdAt": "2020-02-12T03:20:39Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "diffHunk": "@@ -2602,6 +2612,7 @@ public void advanceMaxSeqNoOfUpdatesOrDeletes(long maxSeqNoOfUpdatesOnPrimary) {\n             throw new IllegalArgumentException(\"max_seq_no_of_updates on primary is unassigned\");\n         }\n         this.maxSeqNoOfUpdatesOrDeletes.updateAndGet(curr -> Math.max(curr, maxSeqNoOfUpdatesOnPrimary));\n+        lastOpPrimaryTerm = engineConfig.getPrimaryTermSupplier().getAsLong();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODcwNzM1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoyMjoxM1rOFogpAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoyMjoxM1rOFogpAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMjE0Nw==", "bodyText": "nice javadocs :)", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378022147", "createdAt": "2020-02-12T03:22:13Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1640,6 +1656,72 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    /**\n+     * Extracts an instance of {@link SegmentInfos} for each snapshot tracked in the given {@link BlobStoreIndexShardSnapshots} and\n+     * compares it against the given {@link IndexCommit}.\n+     * If the sequence number, local checkpoint, primary term and history UUID of the given commit and the found shard snapshot are equal\n+     * then we will not snapshot the files in the index commit but rather assign the files in the existing matching snapshot to the new\n+     * shard snapshot.\n+     * Compared to merely comparing files in the index commit and repository, the logic here will be able to identify shards with equal", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODczMDYwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzozODozOVrOFog20A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNFQwOToyMjowNVrOFtZqQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyNTY4MA==", "bodyText": "Should we still upload new files after force merge?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378025680", "createdAt": "2020-02-12T03:38:39Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1482,75 +1487,86 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n \n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n-            }\n+            // First inspect all known SegmentInfos instances to see if we already have an equivalent commit in the repository\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> filesFromSegmentInfos =\n+                findMatchingShardSnapshot(snapshotIndexCommit, snapshots);\n+\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles;\n             int indexIncrementalFileCount = 0;\n             int indexTotalNumberOfFiles = 0;\n             long indexIncrementalSize = 0;\n-            long indexTotalFileCount = 0;\n-            for (String fileName : fileNames) {\n-                if (snapshotStatus.isAborted()) {\n-                    logger.debug(\"[{}] [{}] Aborted on the file [{}], exiting\", shardId, snapshotId, fileName);\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Aborted\");\n+            long indexTotalFileSize = 0;\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n+            // If we did not find a set of files that is equal to the current commit we determine the files to upload by comparing files\n+            // in the commit with files already in the repository", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MTQ4Ng==", "bodyText": "This one is still open, @ywelsch do you have any new thoughts on this.\nPersonally, I'd say we should address force merges explicitly somehow by keeping track of what shards we force-merged in some form and then not doing the sequence num based snapshot if a shard was force merged.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r381951486", "createdAt": "2020-02-20T11:50:20Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1482,75 +1487,86 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n \n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n-            }\n+            // First inspect all known SegmentInfos instances to see if we already have an equivalent commit in the repository\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> filesFromSegmentInfos =\n+                findMatchingShardSnapshot(snapshotIndexCommit, snapshots);\n+\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles;\n             int indexIncrementalFileCount = 0;\n             int indexTotalNumberOfFiles = 0;\n             long indexIncrementalSize = 0;\n-            long indexTotalFileCount = 0;\n-            for (String fileName : fileNames) {\n-                if (snapshotStatus.isAborted()) {\n-                    logger.debug(\"[{}] [{}] Aborted on the file [{}], exiting\", shardId, snapshotId, fileName);\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Aborted\");\n+            long indexTotalFileSize = 0;\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n+            // If we did not find a set of files that is equal to the current commit we determine the files to upload by comparing files\n+            // in the commit with files already in the repository", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyNTY4MA=="}, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU5MTM3NQ==", "bodyText": "yeah, I think we should do it that way, preferably as a predecessor PR to this. The only way I think we can record this is through the liveCommitData. We could for example set a fresh UUID whenever we force-merge.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382591375", "createdAt": "2020-02-21T13:52:38Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1482,75 +1487,86 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n \n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n-            }\n+            // First inspect all known SegmentInfos instances to see if we already have an equivalent commit in the repository\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> filesFromSegmentInfos =\n+                findMatchingShardSnapshot(snapshotIndexCommit, snapshots);\n+\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles;\n             int indexIncrementalFileCount = 0;\n             int indexTotalNumberOfFiles = 0;\n             long indexIncrementalSize = 0;\n-            long indexTotalFileCount = 0;\n-            for (String fileName : fileNames) {\n-                if (snapshotStatus.isAborted()) {\n-                    logger.debug(\"[{}] [{}] Aborted on the file [{}], exiting\", shardId, snapshotId, fileName);\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Aborted\");\n+            long indexTotalFileSize = 0;\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n+            // If we did not find a set of files that is equal to the current commit we determine the files to upload by comparing files\n+            // in the commit with files already in the repository", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyNTY4MA=="}, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzE1MDY1Ng==", "bodyText": "I opened #52694 for this", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r383150656", "createdAt": "2020-02-24T09:22:05Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1482,75 +1487,86 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n \n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n-            }\n+            // First inspect all known SegmentInfos instances to see if we already have an equivalent commit in the repository\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> filesFromSegmentInfos =\n+                findMatchingShardSnapshot(snapshotIndexCommit, snapshots);\n+\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles;\n             int indexIncrementalFileCount = 0;\n             int indexTotalNumberOfFiles = 0;\n             long indexIncrementalSize = 0;\n-            long indexTotalFileCount = 0;\n-            for (String fileName : fileNames) {\n-                if (snapshotStatus.isAborted()) {\n-                    logger.debug(\"[{}] [{}] Aborted on the file [{}], exiting\", shardId, snapshotId, fileName);\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Aborted\");\n+            long indexTotalFileSize = 0;\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n+            // If we did not find a set of files that is equal to the current commit we determine the files to upload by comparing files\n+            // in the commit with files already in the repository", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyNTY4MA=="}, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NDQ4NjM3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxMTo0ODozNlrOFsQa3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQxMzo1Mzo1OFrOF5vzjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MDY4Nw==", "bodyText": "Funny enough with the strange way the serialization worked here, we actually didn't need any BwC logic to keep it readable by older ES versions, that's why there's no version specific serialization logic here.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r381950687", "createdAt": "2020-02-20T11:48:36Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -250,15 +259,19 @@ public static BlobStoreIndexShardSnapshots fromXContent(XContentParser parser) t\n                         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n                             if (token == XContentParser.Token.FIELD_NAME) {\n                                 currentFieldName = parser.currentName();\n-                                if (parser.nextToken() == XContentParser.Token.START_ARRAY) {\n-                                    if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) == false) {\n-                                        throw new ElasticsearchParseException(\"unknown array [{}]\", currentFieldName);\n-                                    }\n+                                if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3NjQ1Mg==", "bodyText": "Should we make the parsing non-lenient now so that we are not silently dropping fields in the future?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382576452", "createdAt": "2020-02-21T13:19:13Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -250,15 +259,19 @@ public static BlobStoreIndexShardSnapshots fromXContent(XContentParser parser) t\n                         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n                             if (token == XContentParser.Token.FIELD_NAME) {\n                                 currentFieldName = parser.currentName();\n-                                if (parser.nextToken() == XContentParser.Token.START_ARRAY) {\n-                                    if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) == false) {\n-                                        throw new ElasticsearchParseException(\"unknown array [{}]\", currentFieldName);\n-                                    }\n+                                if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MDY4Nw=="}, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA5NjM5Nw==", "bodyText": "Yea let's do that but maybe keep that in a separate PR? The parsing here is pretty weird and overly complicated anyway IMO, I'd clean it up and make it non-lenient in one clean go after adding this change?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r396096397", "createdAt": "2020-03-22T13:53:58Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -250,15 +259,19 @@ public static BlobStoreIndexShardSnapshots fromXContent(XContentParser parser) t\n                         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n                             if (token == XContentParser.Token.FIELD_NAME) {\n                                 currentFieldName = parser.currentName();\n-                                if (parser.nextToken() == XContentParser.Token.START_ARRAY) {\n-                                    if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) == false) {\n-                                        throw new ElasticsearchParseException(\"unknown array [{}]\", currentFieldName);\n-                                    }\n+                                if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MDY4Nw=="}, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NDUwMzc0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxMTo1NDozNVrOFsQlyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxMTo1NDozNVrOFsQlyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MzQ4Mg==", "bodyText": "I'm not sure 100% I like computing this here and passing it as yet another parameter to repository.snapshotShard.\nIn hindsight, the API here would be a lot easier/flexible to deal with had we not removed the IndexShard in #42213 ... maybe in a follow up we could introduce some object that exposes the shard related things like Store etc. to simplify this API.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r381953482", "createdAt": "2020-02-20T11:54:35Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "diffHunk": "@@ -340,8 +342,9 @@ private void snapshot(final ShardId shardId, final Snapshot snapshot, final Inde\n             try {\n                 // we flush first to make sure we get the latest writes snapshotted\n                 snapshotRef = indexShard.acquireLastIndexCommit(true);\n+                final IndexCommit indexCommit = snapshotRef.getIndexCommit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NTY2MzkwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxNzowNzo0OFrOFsbxJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMTo0ODo1NVrOFs0aeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzNjYxNQ==", "bodyText": "Can we pass the global checkpoint here instead of isSafeCommit? Also, can we always store the global checkpoint and the history UUID even when the commit is not safe? If so, I think we can move all the logic to a single place in \"findMatchingShardSnapshot\"?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382136615", "createdAt": "2020-02-20T17:07:48Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1457,8 +1459,8 @@ private void writeAtomic(final String blobName, final BytesReference bytesRef, b\n \n     @Override\n     public void snapshotShard(Store store, MapperService mapperService, SnapshotId snapshotId, IndexId indexId,\n-                              IndexCommit snapshotIndexCommit, IndexShardSnapshotStatus snapshotStatus, Version repositoryMetaVersion,\n-                              Map<String, Object> userMetadata, ActionListener<String> listener) {\n+                              IndexCommit snapshotIndexCommit, boolean isSafeCommit, IndexShardSnapshotStatus snapshotStatus,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU0MDQxMA==", "bodyText": "Yea that's much nicer :) Adjusted the PR accordingly.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382540410", "createdAt": "2020-02-21T11:48:55Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1457,8 +1459,8 @@ private void writeAtomic(final String blobName, final BytesReference bytesRef, b\n \n     @Override\n     public void snapshotShard(Store store, MapperService mapperService, SnapshotId snapshotId, IndexId indexId,\n-                              IndexCommit snapshotIndexCommit, IndexShardSnapshotStatus snapshotStatus, Version repositoryMetaVersion,\n-                              Map<String, Object> userMetadata, ActionListener<String> listener) {\n+                              IndexCommit snapshotIndexCommit, boolean isSafeCommit, IndexShardSnapshotStatus snapshotStatus,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzNjYxNQ=="}, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NTY3MDM4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxNzowOTo0NVrOFsb1OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxNzowOTo0NVrOFsb1OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzNzY1Ng==", "bodyText": "We should use the local checkpoint from the commit (not the current local checkpoint) and the persisted global checkpoint.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382137656", "createdAt": "2020-02-20T17:09:45Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "diffHunk": "@@ -352,6 +355,19 @@ private void snapshot(final ShardId shardId, final Snapshot snapshot, final Inde\n         }\n     }\n \n+    /**\n+     * Checks whether the index commit is from a sequence number that is equal to the shards global and local checkpoint.\n+     *\n+     * @param indexShard  shard\n+     * @param indexCommit index commit\n+     * @return true if max sequence number in the index commit is equal to the shard's global and local checkpoint\n+     */\n+    private static boolean isSafeIndexCommit(IndexShard indexShard, IndexCommit indexCommit) throws IOException {\n+        final long localCheckPoint = indexShard.getLocalCheckpoint();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2NTY4MTcwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxNzoxMzowN1rOFsb8NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMTo0OTozNFrOFs0bWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzOTQ0NQ==", "bodyText": "I think we should avoid using sequence_num. We should use either global_checkpoint, or local_checkpoint, or max_seq_no.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382139445", "createdAt": "2020-02-20T17:13:07Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -135,6 +136,8 @@ public FileInfo findNameFile(String name) {\n \n     static final class ParseFields {\n         static final ParseField FILES = new ParseField(\"files\");\n+        static final ParseField SEQUENCE_NUM = new ParseField(\"sequence_num\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU0MDYzMg==", "bodyText": "Went with global_checkpoint now :)", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382540632", "createdAt": "2020-02-21T11:49:34Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -135,6 +136,8 @@ public FileInfo findNameFile(String name) {\n \n     static final class ParseFields {\n         static final ParseField FILES = new ParseField(\"files\");\n+        static final ParseField SEQUENCE_NUM = new ParseField(\"sequence_num\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzOTQ0NQ=="}, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2ODQ2ODUzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzoxNjo1OVrOFs2jfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQxNDowMjozMlrOF5v2iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3NTQ4Nw==", "bodyText": "should this be globalCheckpoints?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382575487", "createdAt": "2020-02-21T13:16:59Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -219,6 +226,8 @@ public static BlobStoreIndexShardSnapshots fromXContent(XContentParser parser) t\n             token = parser.nextToken();\n         }\n         Map<String, List<String>> snapshotsMap = new HashMap<>();\n+        Map<String, String> historyUUIDs = new HashMap<>();\n+        Map<String, Long> sequenceNumbers = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA5NzE2MA==", "bodyText": "Irrelevant now, only passing a single identifier with latest changes.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r396097160", "createdAt": "2020-03-22T14:02:32Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -219,6 +226,8 @@ public static BlobStoreIndexShardSnapshots fromXContent(XContentParser parser) t\n             token = parser.nextToken();\n         }\n         Map<String, List<String>> snapshotsMap = new HashMap<>();\n+        Map<String, String> historyUUIDs = new HashMap<>();\n+        Map<String, Long> sequenceNumbers = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3NTQ4Nw=="}, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2ODQ4ODU5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzoyNDoyNlrOFs2v8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQxNDowMzoyMlrOF5v28w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3ODY3NQ==", "bodyText": "I'm not sure whether we should call this the global checkpoint. We're only supposed to record this if global checkpoint == max sequence number. I would rather introduce a new term here.\n(same for other occurrences of \"globalCheckpoint\")", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382578675", "createdAt": "2020-02-21T13:24:26Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java", "diffHunk": "@@ -48,9 +52,25 @@ public String snapshot() {\n      * @param snapshot   snapshot name\n      * @param indexFiles index files\n      */\n-    public SnapshotFiles(String snapshot, List<FileInfo> indexFiles ) {\n+    public SnapshotFiles(String snapshot, List<FileInfo> indexFiles, long globalCheckpoint, String historyUUID) {\n         this.snapshot = snapshot;\n         this.indexFiles = indexFiles;\n+        this.globalCheckpoint = globalCheckpoint;\n+        this.historyUUID = historyUUID;\n+    }\n+\n+    /**\n+     * Returns the shard's global checkpoint at the time the snapshot was taken\n+     */\n+    public long globalCheckpoint() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjY2OTM4OA==", "bodyText": "How about \"safeCheckpoint\" with appropriate Javadoc in Repository and then compute that number in SnapshotsShardsService and padd down + store a -2 if the three sequence numbers don't match up?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382669388", "createdAt": "2020-02-21T16:11:30Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java", "diffHunk": "@@ -48,9 +52,25 @@ public String snapshot() {\n      * @param snapshot   snapshot name\n      * @param indexFiles index files\n      */\n-    public SnapshotFiles(String snapshot, List<FileInfo> indexFiles ) {\n+    public SnapshotFiles(String snapshot, List<FileInfo> indexFiles, long globalCheckpoint, String historyUUID) {\n         this.snapshot = snapshot;\n         this.indexFiles = indexFiles;\n+        this.globalCheckpoint = globalCheckpoint;\n+        this.historyUUID = historyUUID;\n+    }\n+\n+    /**\n+     * Returns the shard's global checkpoint at the time the snapshot was taken\n+     */\n+    public long globalCheckpoint() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3ODY3NQ=="}, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA5NzI2Nw==", "bodyText": "I simplified this a lot now. The checkpoint isn't explicitly used in the repository code anymore as will be explained below.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r396097267", "createdAt": "2020-03-22T14:03:22Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java", "diffHunk": "@@ -48,9 +52,25 @@ public String snapshot() {\n      * @param snapshot   snapshot name\n      * @param indexFiles index files\n      */\n-    public SnapshotFiles(String snapshot, List<FileInfo> indexFiles ) {\n+    public SnapshotFiles(String snapshot, List<FileInfo> indexFiles, long globalCheckpoint, String historyUUID) {\n         this.snapshot = snapshot;\n         this.indexFiles = indexFiles;\n+        this.globalCheckpoint = globalCheckpoint;\n+        this.historyUUID = historyUUID;\n+    }\n+\n+    /**\n+     * Returns the shard's global checkpoint at the time the snapshot was taken\n+     */\n+    public long globalCheckpoint() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3ODY3NQ=="}, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2ODQ5MDMwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzoyNToxMVrOFs2xFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzoyNToxMVrOFs2xFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3ODk2NA==", "bodyText": "We typically name this maxSeqNo", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382578964", "createdAt": "2020-02-21T13:25:11Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1569,76 +1571,99 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                 throw new IndexShardSnapshotFailedException(shardId,\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n-\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n+            final Map<String, String> userCommitData = snapshotIndexCommit.getUserData();\n+            // We only check the sequence number to see if the shard has changed if we know that the commit is safe,\n+            // otherwise we short-circuit things here by not reading the sequence number from the commit\n+            final String maxSequenceNumString = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);\n+            final long maxSequenceNum;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2ODQ5NTcwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzoyNzowNVrOFs20Ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzoyNzowNVrOFs20Ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3OTgwMw==", "bodyText": "load this using SequenceNumbers.loadSeqNoInfoFromLuceneCommit instead", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382579803", "createdAt": "2020-02-21T13:27:05Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1569,76 +1571,99 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                 throw new IndexShardSnapshotFailedException(shardId,\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n-\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n+            final Map<String, String> userCommitData = snapshotIndexCommit.getUserData();\n+            // We only check the sequence number to see if the shard has changed if we know that the commit is safe,\n+            // otherwise we short-circuit things here by not reading the sequence number from the commit\n+            final String maxSequenceNumString = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM2ODUxMDIwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzozMjoyMVrOFs29Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQxNDowNDoxOFrOF5v3Sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU4MjA0Ng==", "bodyText": "I think this logic should live in the caller where we determine whether we want to do any kind of lookup by this special number (need a name for it) that is equal to global checkpoint, local checkpoint, and max sequence number.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382582046", "createdAt": "2020-02-21T13:32:21Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1729,6 +1754,21 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(long globalCheckpoint, long maxSequenceNum,\n+                                                                                        String historyUUID,\n+                                                                                        BlobStoreIndexShardSnapshots snapshots) {\n+        if (maxSequenceNum == SequenceNumbers.UNASSIGNED_SEQ_NO || globalCheckpoint != maxSequenceNum) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjA5NzM1NQ==", "bodyText": "Right, this makes everything so much simpler => moved this to SnapshotShardsService now instead of messing with the explicit global checkpoint in the repository.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r396097355", "createdAt": "2020-03-22T14:04:18Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1729,6 +1754,21 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(long globalCheckpoint, long maxSequenceNum,\n+                                                                                        String historyUUID,\n+                                                                                        BlobStoreIndexShardSnapshots snapshots) {\n+        if (maxSequenceNum == SequenceNumbers.UNASSIGNED_SEQ_NO || globalCheckpoint != maxSequenceNum) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU4MjA0Ng=="}, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 194}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1Njk1NDczOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/snapshots/BlobStoreIncrementalityIT.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxMDoxNzo0NFrOF5-0Jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxMjoxOTo1MlrOF6C5Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM0MjMxMA==", "bodyText": "Can you also add a test that checks that force-merging is leading to another full snapshot?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r396342310", "createdAt": "2020-03-23T10:17:44Z", "author": {"login": "ywelsch"}, "path": "server/src/test/java/org/elasticsearch/snapshots/BlobStoreIncrementalityIT.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.DocWriteResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStats;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.bulk.BulkItemResponse;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.search.SearchRequest;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.routing.UnassignedInfo;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.test.ESIntegTestCase;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.concurrent.ExecutionException;\n+\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class BlobStoreIncrementalityIT extends AbstractSnapshotIntegTestCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "983eb58aa5b2ba59c2dcaa4548587473e02b5f23"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQwOTE1MA==", "bodyText": "Sure thing, how about this one b508fb3", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r396409150", "createdAt": "2020-03-23T12:19:52Z", "author": {"login": "original-brownbear"}, "path": "server/src/test/java/org/elasticsearch/snapshots/BlobStoreIncrementalityIT.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.DocWriteResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStats;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.bulk.BulkItemResponse;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.search.SearchRequest;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.routing.UnassignedInfo;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.test.ESIntegTestCase;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.concurrent.ExecutionException;\n+\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class BlobStoreIncrementalityIT extends AbstractSnapshotIntegTestCase {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM0MjMxMA=="}, "originalCommit": {"oid": "983eb58aa5b2ba59c2dcaa4548587473e02b5f23"}, "originalPosition": 45}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4739, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}