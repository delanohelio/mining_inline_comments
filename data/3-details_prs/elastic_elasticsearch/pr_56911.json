{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE5NTEyMDgx", "number": 56911, "title": "Enable Fully Concurrent Snapshot Operations", "bodyText": "Enables fully concurrent snapshot operations:\n\nSnapshot create- and delete operations can be started in any order\nDelete operations wait for snapshot finalization to finish, are batched as much as possible to improve efficiency and once enqueued in the cluster state prevent new snapshots from starting on data nodes until executed\n\nWe could be even more concurrent here in a follow-up by interleaving deletes and snapshots on a per-shard level. I decided not to do this for now since it seemed not worth the added complexity yet. Due to batching+deduplicating of deletes the pain of having a delete stuck behind a long -running snapshot seemed manageable (dropped client connections + resulting retries don't cause issues due to deduplication of delete jobs, batching of deletes allows enqueuing more and more deletes even if a snapshot blocks for a long time that will all be executed in essentially constant time (due to bulk snapshot deletion, deleting multiple snapshots is mostly about as fast as deleting a single one))\n\n\nSnapshot creation is completely concurrent across shards, but per shard snapshots are linearized for each repository as are snapshot finalizations\n\nSee updated JavaDoc and added test cases for more details and illustration on the functionality.\nSome notes:\nThe queuing of snapshot finalizations and deletes and the related locking/synchronization is a little awkward in this version but can be much simplified with some refactoring.  The problem is that snapshot finalizations resolve their listeners on the SNAPSHOT pool while deletes resolve the listener on the master update thread. With some refactoring both of these could be moved to the master update thread, effectively removing the need for any synchronization around the SnapshotService state. I didn't do this refactoring here because it's a fairly large change and not necessary for the functionality but plan to do so in a follow-up.\nThis change allows for completely removing any trickery around synchronizing deletes and snapshots from SLM and 100% does away with SLM errors from collisions between deletes and snapshots.\nSnapshotting a single index in parallel to a long running full backup will execute without having to wait for the long running backup as required by the ILM/SLM use case of moving indices to \"snapshot tier\". Finalizations are linearized but ordered according to which snapshot saw all of its shards complete first", "createdAt": "2020-05-18T13:43:58Z", "url": "https://github.com/elastic/elasticsearch/pull/56911", "merged": true, "mergeCommit": {"oid": "d333dacb4abb14a58b36f3108521547ee374a06a"}, "closed": true, "closedAt": "2020-07-10T13:19:09Z", "author": {"login": "original-brownbear"}, "timelineItems": {"totalCount": 168, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcucxnYAH2gAyNDE5NTEyMDgxOjc2MjdhMzA3NmQyODY3NTE0ZmJiMjI5NzRhZTFlYTRkYjZlYTlhNDI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABczifuvgH2gAyNDE5NTEyMDgxOjllMWEwZGI2ZTEwZTU4ODJkZGJjMGI2Y2E4MDU2MGUyYmZmMzhkZjU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "7627a3076d2867514fbb22974ae1ea4db6ea9a42", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/7627a3076d2867514fbb22974ae1ea4db6ea9a42", "committedDate": "2020-06-24T16:38:08Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c951e3bc35effe08f4ee374289853e98d28c3b2d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/c951e3bc35effe08f4ee374289853e98d28c3b2d", "committedDate": "2020-06-25T18:57:11Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cedcd8838e4a8f6940a0168f444929d0faebb412", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cedcd8838e4a8f6940a0168f444929d0faebb412", "committedDate": "2020-06-25T20:43:03Z", "message": "resolve some conflicts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa02752aef9e51f22d58c947777139febfdf939b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/aa02752aef9e51f22d58c947777139febfdf939b", "committedDate": "2020-06-26T10:35:59Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5bf38b7f754703014ff257015c3835e8d2d1a0e3", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/5bf38b7f754703014ff257015c3835e8d2d1a0e3", "committedDate": "2020-06-26T12:59:32Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3eef5af7b5e02fff40b78faab050a21b7cd57236", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/3eef5af7b5e02fff40b78faab050a21b7cd57236", "committedDate": "2020-06-26T13:31:11Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MzExODE5", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-438311819", "createdAt": "2020-06-26T13:50:11Z", "commit": {"oid": "cedcd8838e4a8f6940a0168f444929d0faebb412"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMzo1MDoxMVrOGphptg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNTozMjozN1rOGplT5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5NjE1MA==", "bodyText": "Given the complexity of the SnapshotsService class and friends, I wonder if we should try to get branch coverage on that class, to see if our tests (when run repeatedly) at least somehow cover all implementation aspects. Play a bit with a coverage tool (I used jacoco with Intellij for this when testing Zen2)", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446196150", "createdAt": "2020-06-26T13:50:11Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,889 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateObserver;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.node.NodeClosedException;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3NTQ0NQ=="}, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIwNzc3OA==", "bodyText": "or maybe STARTED. If you want to refine the states further here, you can still rename it from STARTED to META_DATA or whatever afterwards.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446207778", "createdAt": "2020-06-26T14:09:48Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -257,4 +361,26 @@ public long repositoryStateId() {\n             return repositoryStateId;\n         }\n     }\n+\n+    public enum State {\n+        WAITING((byte) 0),\n+        META_DATA((byte) 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyOTgxNg=="}, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMDIxOQ==", "bodyText": "can we assert that this is always greater than the current gen? Maybe that it's +1?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446210219", "createdAt": "2020-06-26T14:14:03Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -175,6 +175,16 @@ public Entry(Entry entry, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards)\n             this(entry, entry.state, shards, entry.failure);\n         }\n \n+        public Entry withRepoGen(long newRepoGen) {\n+            return new Entry(snapshot, includeGlobalState, partial, state, indices, dataStreams, startTime, newRepoGen, shards, failure,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cedcd8838e4a8f6940a0168f444929d0faebb412"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMzQ4OQ==", "bodyText": "Can we assert that INIT state has always a non-null nodeId?\nIsn't it sufficient to look at shard states here that have an non-null node id?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446213489", "createdAt": "2020-06-26T14:19:52Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -404,6 +425,16 @@ public String reason() {\n             return reason;\n         }\n \n+        /**\n+         * Checks if this shard snapshot is actively executing.\n+         * A shard is defined as actively executing if it either is in a state that may write to the repository\n+         * ({@link ShardState#INIT} or {@link ShardState#ABORTED}) or is in state {@link ShardState#WAITING} with a concrete non-null\n+         * node id assignment (i.e. waiting for a shard relocation/initialization to finish).\n+         */\n+        public boolean isAssigned() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkyNzQ3MA=="}, "originalCommit": {"oid": "5d3d447246ffa5c86522994a87db91385a878a81"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIyNTQ5Ng==", "bodyText": "should we make this method static?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446225496", "createdAt": "2020-06-26T14:40:12Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1614,6 +1613,45 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n         }, listener::onFailure);\n     }\n \n+    /**\n+     * Updates the repository generation that running deletes and snapshot finalizations will be based on for this repository if any such\n+     * operations are found in the cluster state while setting the safe repository generation.\n+     *\n+     * @param state  cluster state to update\n+     * @param oldGen previous safe repository generation\n+     * @param newGen new safe repository generation\n+     * @return updated cluster state\n+     */\n+    private ClusterState updateRepositoryGenerationsIfNecessary(ClusterState state, long oldGen, long newGen) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cedcd8838e4a8f6940a0168f444929d0faebb412"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI1NjEwMg==", "bodyText": "Should we also test for the situation where a repository is mounted multiple times under different name, but same path, and check that the repo does not become corrupted when concurrently accessed?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r446256102", "createdAt": "2020-06-26T15:32:37Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,971 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateObserver;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.node.NodeClosedException;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cedcd8838e4a8f6940a0168f444929d0faebb412"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc88a1c6b50b83be2277385ce618473005378c9e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cc88a1c6b50b83be2277385ce618473005378c9e", "committedDate": "2020-06-26T17:57:30Z", "message": "changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72c4019fb07e939b32561583e4ea7a45b354c063", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/72c4019fb07e939b32561583e4ea7a45b354c063", "committedDate": "2020-06-26T18:04:28Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "095023c23a65ff4583bd7fa031937b088c4a7ebc", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/095023c23a65ff4583bd7fa031937b088c4a7ebc", "committedDate": "2020-06-26T18:07:59Z", "message": "fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10092979aacea40b6d173705d575ffeb62df0f2e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/10092979aacea40b6d173705d575ffeb62df0f2e", "committedDate": "2020-06-27T08:37:04Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "291e5f55c4e3af55136e3af82363e81be1cef447", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/291e5f55c4e3af55136e3af82363e81be1cef447", "committedDate": "2020-06-27T15:10:02Z", "message": "fix corner case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "737cd485253a7fc19bae20200a0c303ae6670d43", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/737cd485253a7fc19bae20200a0c303ae6670d43", "committedDate": "2020-06-29T05:11:50Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86d00719e3fcf6fbde9039c00d5e0a86de994b0f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/86d00719e3fcf6fbde9039c00d5e0a86de994b0f", "committedDate": "2020-06-29T05:17:09Z", "message": "rename"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d09f68e7fae97ce830b6033d3afbe24b576e102a", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d09f68e7fae97ce830b6033d3afbe24b576e102a", "committedDate": "2020-06-29T07:39:24Z", "message": "add assertion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e7662f15abf9cac9b123f5bfd82d1bc56f2ef85", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/3e7662f15abf9cac9b123f5bfd82d1bc56f2ef85", "committedDate": "2020-06-29T07:57:55Z", "message": "add assertion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "974bd62ec687e5cde0b08adc6b847762440c7107", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/974bd62ec687e5cde0b08adc6b847762440c7107", "committedDate": "2020-06-29T09:02:32Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e388a6e1bbd26487e3384c68e2f360de7649ad7c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e388a6e1bbd26487e3384c68e2f360de7649ad7c", "committedDate": "2020-06-29T10:56:28Z", "message": "add queued state for shard snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "647e2ed525bb8ebfdc652523b3a3ad5aa879b6c8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/647e2ed525bb8ebfdc652523b3a3ad5aa879b6c8", "committedDate": "2020-06-29T11:36:48Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b63b2f7fef1586c538a26576a497539f05f29fa", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/4b63b2f7fef1586c538a26576a497539f05f29fa", "committedDate": "2020-06-29T12:41:51Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f84c9b717cbcc386223ec75e25546014dab26ece", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/f84c9b717cbcc386223ec75e25546014dab26ece", "committedDate": "2020-06-29T15:34:29Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fba7536e206203f4988c014ee09d9836737fcba7", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fba7536e206203f4988c014ee09d9836737fcba7", "committedDate": "2020-06-29T20:43:34Z", "message": "50% fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "878fa07d6a108562e68dcc1e2eee48464fb336ac", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/878fa07d6a108562e68dcc1e2eee48464fb336ac", "committedDate": "2020-06-30T08:08:31Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b4041628d1c4debb5bee92ce9e2314999f33c97", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/5b4041628d1c4debb5bee92ce9e2314999f33c97", "committedDate": "2020-06-30T08:37:29Z", "message": "fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cefd32328b71daae7bd313f64e188d3f82200d3e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cefd32328b71daae7bd313f64e188d3f82200d3e", "committedDate": "2020-06-30T09:54:31Z", "message": "fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "94d4608ed49554c337b60df40c1831cef8b8a0b4", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/94d4608ed49554c337b60df40c1831cef8b8a0b4", "committedDate": "2020-06-30T10:15:47Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7674a69154e34c4992b51e9566d860c333e5514", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d7674a69154e34c4992b51e9566d860c333e5514", "committedDate": "2020-06-30T10:47:03Z", "message": "moar coverage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dcfc8fd85354fb37409d1331381df651bfa962ab", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/dcfc8fd85354fb37409d1331381df651bfa962ab", "committedDate": "2020-06-30T11:22:58Z", "message": "moar coverage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d8a6b25c11edabc47171f1ffd8bcbb39815b57e3", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d8a6b25c11edabc47171f1ffd8bcbb39815b57e3", "committedDate": "2020-06-30T11:24:39Z", "message": "docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d97b421de62a1147144dcc24468442dd475c842", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6d97b421de62a1147144dcc24468442dd475c842", "committedDate": "2020-06-30T12:38:24Z", "message": "even moar coverage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f74e4d1af9f9e476e37f30b76cc93f6b55acbb3d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/f74e4d1af9f9e476e37f30b76cc93f6b55acbb3d", "committedDate": "2020-06-30T12:38:36Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "217137d88deb906dd740ca1250d87de48b27533e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/217137d88deb906dd740ca1250d87de48b27533e", "committedDate": "2020-06-30T12:47:57Z", "message": "fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "68aaa8ecea064a715fe0965def2260cd3e40371b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/68aaa8ecea064a715fe0965def2260cd3e40371b", "committedDate": "2020-06-30T13:26:15Z", "message": "cover another tricky corner case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc36ffde9cd83d9ffc45431ea3942bafe825cfd4", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cc36ffde9cd83d9ffc45431ea3942bafe825cfd4", "committedDate": "2020-06-30T13:37:31Z", "message": "and another corner case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2bfadd6b7ba493850ea0159876ccedc9cecde10", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e2bfadd6b7ba493850ea0159876ccedc9cecde10", "committedDate": "2020-06-30T14:03:25Z", "message": "almost full coverage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0afdbb52ca19ffba71de23b7a6eceb7cc87fc687", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/0afdbb52ca19ffba71de23b7a6eceb7cc87fc687", "committedDate": "2020-06-30T14:03:45Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8c06c9db8c8b41d723c8a7fb1287a9abed6636c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/b8c06c9db8c8b41d723c8a7fb1287a9abed6636c", "committedDate": "2020-06-30T14:28:33Z", "message": "another failure handler tested"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f44a8f7f35ef0693521b75e0cd637dd2455dcd8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2f44a8f7f35ef0693521b75e0cd637dd2455dcd8", "committedDate": "2020-06-30T15:29:09Z", "message": "fix bugs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6cdba2e96e73b422214c384ffc5387ae3ce47de", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/b6cdba2e96e73b422214c384ffc5387ae3ce47de", "committedDate": "2020-07-01T00:24:47Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c65059140535141282c739293088c4eda30fbae", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/9c65059140535141282c739293088c4eda30fbae", "committedDate": "2020-07-01T01:36:58Z", "message": "docs and simpler"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "355432b063ee01f78113c23bfca565a4d490d9d5", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/355432b063ee01f78113c23bfca565a4d490d9d5", "committedDate": "2020-07-01T02:57:13Z", "message": "cover and fix very edgy edge case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cf751acbe431dcba5d31a978a577a2c6f1a1447", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/8cf751acbe431dcba5d31a978a577a2c6f1a1447", "committedDate": "2020-07-02T12:51:18Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "773f90529ad292aa08a12a1312cf9af09eb0b631", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/773f90529ad292aa08a12a1312cf9af09eb0b631", "committedDate": "2020-07-02T13:00:32Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de169f0c3f2735235a2ee7684602d00c356a63c9", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/de169f0c3f2735235a2ee7684602d00c356a63c9", "committedDate": "2020-07-02T13:57:07Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "945ef0cbf491f1fc3bd5870bfd61d70b421cc407", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/945ef0cbf491f1fc3bd5870bfd61d70b421cc407", "committedDate": "2020-07-02T14:05:23Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/81fdf77e28c32ce4748cc315758ca517995329e4", "committedDate": "2020-07-02T14:14:42Z", "message": "fix conflict"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyMTg1MDkz", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-442185093", "createdAt": "2020-07-03T06:54:04Z", "commit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwNjo1NDowNVrOGslsCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwOToyMTo0MVrOGsp9ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQwODAwOQ==", "bodyText": "s/then/than", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449408009", "createdAt": "2020-07-03T06:54:05Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -151,6 +152,18 @@ public Entry(Entry entry, ImmutableOpenMap<ShardId, ShardSnapshotStatus> shards)\n             this(entry, entry.state, shards, entry.failure);\n         }\n \n+        public Entry withRepoGen(long newRepoGen) {\n+            assert newRepoGen > repositoryStateId : \"Updated repository generation [\" + newRepoGen\n+                    + \"] must be higher then current generation [\" + repositoryStateId + \"]\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQxMzg1NQ==", "bodyText": "maybe call this isActive now", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449413855", "createdAt": "2020-07-03T07:09:10Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -404,6 +425,16 @@ public String reason() {\n             return reason;\n         }\n \n+        /**\n+         * Checks if this shard snapshot is actively executing.\n+         * A shard is defined as actively executing if it either is in a state that may write to the repository\n+         * ({@link ShardState#INIT} or {@link ShardState#ABORTED}) or is in state {@link ShardState#WAITING} with a concrete non-null\n+         * node id assignment (i.e. waiting for a shard relocation/initialization to finish).\n+         */\n+        public boolean isAssigned() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkyNzQ3MA=="}, "originalCommit": {"oid": "5d3d447246ffa5c86522994a87db91385a878a81"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQxNzgwMg==", "bodyText": "Should we check that at the end of integration tests this is always properly cleaned up (same for the other state in this class)?\nContext for my question is that I'm wondering if all of this is always properly cleaned up on master failover", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449417802", "createdAt": "2020-07-03T07:18:31Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -134,6 +141,14 @@\n     private final Map<Snapshot, List<ActionListener<Tuple<RepositoryData, SnapshotInfo>>>> snapshotCompletionListeners =\n         new ConcurrentHashMap<>();\n \n+    /**\n+     * Listeners for snapshot deletion keyed by delete uuid as returned from {@link SnapshotDeletionsInProgress.Entry#uuid()}\n+     */\n+    private final Map<String, List<ActionListener<Void>>> snapshotDeletionListeners = new HashMap<>();\n+\n+    //Set of repositories currently running either a snapshot finalization or a snapshot delete.\n+    private final Set<String> currentlyFinalizing = Collections.synchronizedSet(new HashSet<>());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyMDEyMA==", "bodyText": "Let's also add a node/cluster setting that limits the number of concurrent operations, just so that we have a safeguard (and something to set to 1 in case there's a concurrency bug).", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449420120", "createdAt": "2020-07-03T07:23:53Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -204,24 +221,34 @@ public ClusterState execute(ClusterState currentState) {\n                     throw new InvalidSnapshotNameException(\n                             repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n                 }\n+                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries();\n+                if (runningSnapshots.stream().anyMatch(s -> {\n+                    final Snapshot running = s.snapshot();\n+                    return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+                })) {\n+                    throw new InvalidSnapshotNameException(\n+                            repository.getMetadata().name(), snapshotName, \"snapshot with the same name is already in-progress\");\n+                }\n                 validate(repositoryName, snapshotName, currentState);\n-                final SnapshotDeletionsInProgress deletionsInProgress =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletionsInProgress.hasDeletionsInProgress()) {\n+                final Version minNodeVersion = currentState.nodes().getMinNodeVersion();\n+                SnapshotDeletionsInProgress deletionsInProgress =\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                if (deletionsInProgress.hasDeletionsInProgress() && minNodeVersion.before(FULL_CONCURRENCY_VERSION)) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");\n                 }\n                 final RepositoryCleanupInProgress repositoryCleanupInProgress =\n-                    currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n+                        currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n                 if (repositoryCleanupInProgress.hasCleanupInProgress()) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n                 }\n-                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n                 // Fail if there are any concurrently running snapshots. The only exception to this being a snapshot in INIT state from a\n                 // previous master that we can simply ignore and remove from the cluster state because we would clean it up from the\n                 // cluster state anyway in #applyClusterState.\n-                if (snapshots.entries().stream().anyMatch(entry -> entry.state() != State.INIT)) {\n+                if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)\n+                        && runningSnapshots.stream().anyMatch(entry -> entry.state() != State.INIT)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyNjQ3MA==", "bodyText": "I'm confused why we recheck this condition here and don't put it under the same if block as above", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449426470", "createdAt": "2020-07-03T07:38:38Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1047,45 +1291,101 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n             };\n         }\n         return new ClusterStateUpdateTask(priority) {\n+\n+            private SnapshotDeletionsInProgress.Entry newDelete;\n+\n+            private boolean reusedExistingDelete = false;\n+\n+            private final Collection<SnapshotsInProgress.Entry> completedSnapshots = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n-                final SnapshotDeletionsInProgress deletionsInProgress =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletionsInProgress.hasDeletionsInProgress()) {\n-                    throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)),\n-                        \"cannot delete - another snapshot is currently being deleted in [\" + deletionsInProgress + \"]\");\n+                SnapshotDeletionsInProgress deletionsInProgress =\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                final Version minNodeVersion = currentState.nodes().getMinNodeVersion();\n+                if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)) {\n+                    if (deletionsInProgress.hasDeletionsInProgress()) {\n+                        throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)),\n+                                \"cannot delete - another snapshot is currently being deleted in [\" + deletionsInProgress + \"]\");\n+                    }\n                 }\n                 final RepositoryCleanupInProgress repositoryCleanupInProgress =\n-                    currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n+                        currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n                 if (repositoryCleanupInProgress.hasCleanupInProgress()) {\n                     throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)),\n-                        \"cannot delete snapshots while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n+                            \"cannot delete snapshots while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n                 }\n                 final RestoreInProgress restoreInProgress = currentState.custom(RestoreInProgress.TYPE, RestoreInProgress.EMPTY);\n                 // don't allow snapshot deletions while a restore is taking place,\n                 // otherwise we could end up deleting a snapshot that is being restored\n                 // and the files the restore depends on would all be gone\n+\n                 for (RestoreInProgress.Entry entry : restoreInProgress) {\n                     if (repoName.equals(entry.snapshot().getRepository()) && snapshotIds.contains(entry.snapshot().getSnapshotId())) {\n                         throw new ConcurrentSnapshotExecutionException(new Snapshot(repoName, snapshotIds.get(0)),\n-                            \"cannot delete snapshot during a restore in progress in [\" + restoreInProgress + \"]\");\n+                                \"cannot delete snapshot during a restore in progress in [\" + restoreInProgress + \"]\");\n                     }\n                 }\n-                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n-                if (snapshots.entries().isEmpty() == false) {\n-                    // However other snapshots are running - cannot continue\n-                    throw new ConcurrentSnapshotExecutionException(\n-                            repoName, snapshotIds.toString(), \"another snapshot is currently running cannot delete\");\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final SnapshotsInProgress updatedSnapshots;\n+                if (minNodeVersion.onOrAfter(FULL_CONCURRENCY_VERSION)) {\n+                    updatedSnapshots = SnapshotsInProgress.of(snapshots.entries().stream()\n+                            .map(existing -> {\n+                                // snapshot is started - mark every non completed shard as aborted\n+                                if (existing.state() == State.STARTED && snapshotIds.contains(existing.snapshot().getSnapshotId())) {\n+                                    final ImmutableOpenMap<ShardId, ShardSnapshotStatus> abortedShards = abortEntry(existing);\n+                                    final boolean isCompleted = completed(abortedShards.values());\n+                                    final SnapshotsInProgress.Entry abortedEntry = new SnapshotsInProgress.Entry(\n+                                            existing, isCompleted ? State.SUCCESS : State.ABORTED, abortedShards,\n+                                            \"Snapshot was aborted by deletion\");\n+                                    if (isCompleted) {\n+                                        completedSnapshots.add(abortedEntry);\n+                                    }\n+                                    return abortedEntry;\n+                                }\n+                                return existing;\n+                            }).collect(Collectors.toUnmodifiableList()));\n+                } else {\n+                    if (snapshots.entries().isEmpty() == false) {\n+                        // However other snapshots are running - cannot continue\n+                        throw new ConcurrentSnapshotExecutionException(\n+                                repoName, snapshotIds.toString(), \"another snapshot is currently running cannot delete\");\n+                    }\n+                    updatedSnapshots = snapshots;\n                 }\n                 // add the snapshot deletion to the cluster state\n-                SnapshotDeletionsInProgress.Entry entry = new SnapshotDeletionsInProgress.Entry(\n+                SnapshotDeletionsInProgress.Entry replacedEntry = deletionsInProgress.getEntries().stream().filter(entry ->\n+                        entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.WAITING)\n+                        .findFirst().orElse(null);\n+                if (replacedEntry == null) {\n+                    final Optional<SnapshotDeletionsInProgress.Entry> foundDuplicate =\n+                            deletionsInProgress.getEntries().stream().filter(entry ->\n+                                    entry.repository().equals(repoName) && entry.state() == SnapshotDeletionsInProgress.State.STARTED\n+                                            && entry.getSnapshots().containsAll(snapshotIds)).findFirst();\n+                    if (foundDuplicate.isPresent()) {\n+                        newDelete = foundDuplicate.get();\n+                        reusedExistingDelete = true;\n+                        return currentState;\n+                    }\n+                }\n+                if (replacedEntry == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "originalPosition": 978}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQyODI5NA==", "bodyText": "debug level? Are we also logging the actual deletion at info level?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449428294", "createdAt": "2020-07-03T07:42:39Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1095,11 +1395,64 @@ public void onFailure(String source, Exception e) {\n \n             @Override\n             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n-                deleteSnapshotsFromRepository(repoName, snapshotIds, listener, repositoryStateId, newState.nodes().getMinNodeVersion());\n+                addDeleteListener(newDelete.uuid(), listener);\n+                if (reusedExistingDelete) {\n+                    return;\n+                }\n+                if (newDelete.state() == SnapshotDeletionsInProgress.State.STARTED) {\n+                    if (tryEnterRepoLoop(repoName)) {\n+                        deleteSnapshotsFromRepository(newDelete, repositoryData, newState.nodes().getMinNodeVersion());\n+                    } else {\n+                        logger.trace(\"Delete [{}] could not execute directly and was queued\", newDelete);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "originalPosition": 1016}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQzMTI0OQ==", "bodyText": "let's add debug logging here", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449431249", "createdAt": "2020-07-03T07:48:38Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1152,82 +1505,359 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n-     * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n+     * @param deleteEntry       delete entry in cluster state\n+     * @param minNodeVersion    minimum node version in the cluster\n+     */\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry, Version minNodeVersion) {\n+        final long expectedRepoGen = deleteEntry.repositoryStateId();\n+        repositoriesService.getRepositoryData(deleteEntry.repository(), new ActionListener<>() {\n+            @Override\n+            public void onResponse(RepositoryData repositoryData) {\n+                assert repositoryData.getGenId() == expectedRepoGen :\n+                        \"Repository generation should not change as long as a ready delete is found in the cluster state but found [\"\n+                                + expectedRepoGen + \"] in cluster state and [\" + repositoryData.getGenId() + \"] in the repository\";\n+                deleteSnapshotsFromRepository(deleteEntry, repositoryData, minNodeVersion);\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                clusterService.submitStateUpdateTask(\"fail repo tasks for [\" + deleteEntry.repository() + \"]\",\n+                        new FailPendingRepoTasksTask(deleteEntry.repository(), e));\n+            }\n+        });\n+    }\n+\n+    /** Deletes snapshot from repository\n+     *\n+     * @param deleteEntry       delete entry in cluster state\n+     * @param repositoryData    the {@link RepositoryData} of the repository to delete from\n      * @param minNodeVersion    minimum node version in the cluster\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n-                                               long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                               RepositoryData repositoryData, Version minNodeVersion) {\n+        if (repositoryOperations.startDeletion(deleteEntry.uuid())) {\n+            assert currentlyFinalizing.contains(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.STARTED :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repositoriesService.repository(deleteEntry.repository()).deleteSnapshots(\n                 snapshotIds,\n-                repositoryStateId,\n+                repositoryData.getGenId(),\n                 minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+                ActionListener.wrap(updatedRepoData -> {\n+                        logger.info(\"snapshots {} deleted\", snapshotIds);\n+                        removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                    }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                ));\n+        }\n     }\n \n     /**\n-     * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n+     * Removes a {@link SnapshotDeletionsInProgress.Entry} from {@link SnapshotDeletionsInProgress} in the cluster state after it executed\n+     * on the repository.\n+     *\n+     * @param deleteEntry delete entry to remove from the cluster state\n+     * @param failure     failure encountered while executing the delete on the repository or {@code null} if the delete executed\n+     *                    successfully\n+     * @param repositoryData current {@link RepositoryData} for the repository we just ran the delete on.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n-        clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n-            @Override\n-            public ClusterState execute(ClusterState currentState) {\n-                final SnapshotDeletionsInProgress deletions =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletions.hasDeletionsInProgress()) {\n-                    assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                    SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                    return ClusterState.builder(currentState).putCustom(\n-                        SnapshotDeletionsInProgress.TYPE, deletions.withRemovedEntry(entry)).build();\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, final RepositoryData repositoryData) {\n+        final ClusterStateUpdateTask clusterStateUpdateTask;\n+        if (failure == null) {\n+            // If we didn't have a failure during the snapshot delete we will remove all snapshot ids that the delete successfully removed\n+            // from the repository from enqueued snapshot delete entries during the cluster state update. After the cluster state update we\n+            // resolve the delete listeners with the latest repository data from after the delete.\n+            clusterStateUpdateTask = new RemoveSnapshotDeletionAndContinueTask(deleteEntry, repositoryData) {\n+                @Override\n+                protected SnapshotDeletionsInProgress filterDeletions(SnapshotDeletionsInProgress deletions) {\n+                    boolean changed = false;\n+                    List<SnapshotDeletionsInProgress.Entry> updatedEntries = new ArrayList<>(deletions.getEntries().size());\n+                    for (SnapshotDeletionsInProgress.Entry entry : deletions.getEntries()) {\n+                        if (entry.repository().equals(deleteEntry.repository())) {\n+                            final List<SnapshotId> updatedSnapshotIds = new ArrayList<>(entry.getSnapshots());\n+                            if (updatedSnapshotIds.removeAll(deleteEntry.getSnapshots())) {\n+                                changed = true;\n+                                updatedEntries.add(entry.withSnapshots(updatedSnapshotIds));\n+                            } else {\n+                                updatedEntries.add(entry);\n+                            }\n+                        } else {\n+                            updatedEntries.add(entry);\n+                        }\n+                    }\n+                    return changed ? SnapshotDeletionsInProgress.of(updatedEntries) : deletions;\n+                }\n+\n+                @Override\n+                protected void handleListeners(List<ActionListener<Void>> deleteListeners) {\n+                    assert repositoryData.getSnapshotIds().stream().noneMatch(deleteEntry.getSnapshots()::contains)\n+                            : \"Repository data contained snapshot ids \" + repositoryData.getSnapshotIds()\n+                            + \" that should should been deleted by [\" + deleteEntry + \"]\";\n+                    completeListenersIgnoringException(deleteListeners, null);\n+                }\n+            };\n+        } else {\n+            // The delete failed to execute on the repository. We remove it from the cluster state and then fail all listeners associated\n+            // with it.\n+            clusterStateUpdateTask = new RemoveSnapshotDeletionAndContinueTask(deleteEntry, repositoryData) {\n+                @Override\n+                protected void handleListeners(List<ActionListener<Void>> deleteListeners) {\n+                    failListenersIgnoringException(deleteListeners, failure);\n+                }\n+            };\n+        }\n+        clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", clusterStateUpdateTask);\n+    }\n+\n+    /**\n+     * Handle snapshot or delete failure due to not being master any more so we don't try to do run additional cluster state updates.\n+     * The next master will try handling the missing operations. All we can do is fail all the listeners on this master node so that\n+     * transport requests return and we don't leak listeners.\n+     *\n+     * @param e exception that caused us to realize we are not master any longer\n+     */\n+    private void failAllListenersOnMasterFailOver(Exception e) {\n+        synchronized (currentlyFinalizing) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "originalPosition": 1213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ3MzY4Mg==", "bodyText": "should we log this at info level? This is an important event?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449473682", "createdAt": "2020-07-03T09:13:06Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1463,4 +2157,171 @@ protected ClusterBlockException checkBlock(UpdateIndexShardSnapshotStatusRequest\n             return null;\n         }\n     }\n+\n+    /**\n+     * Cluster state update task that removes all {@link SnapshotsInProgress.Entry} and {@link SnapshotDeletionsInProgress.Entry} for a\n+     * given repository from the cluster state and afterwards fails all relevant listeners in {@link #snapshotCompletionListeners} and\n+     * {@link #snapshotDeletionListeners}.\n+     */\n+    private final class FailPendingRepoTasksTask extends ClusterStateUpdateTask {\n+\n+        // Snapshots to fail after the state update\n+        private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+        // Delete uuids to fail because after the state update\n+        private final List<String> deletionsToFail = new ArrayList<>();\n+\n+        // Failure that caused the decision to fail all snapshots and deletes for a repo\n+        private final Exception failure;\n+\n+        private final String repository;\n+\n+        FailPendingRepoTasksTask(String repository, Exception failure) {\n+            this.repository = repository;\n+            this.failure = failure;\n+        }\n+\n+        @Override\n+        public ClusterState execute(ClusterState currentState) {\n+            final SnapshotDeletionsInProgress deletionsInProgress =\n+                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+            boolean changed = false;\n+            final List<SnapshotDeletionsInProgress.Entry> remainingEntries = deletionsInProgress.getEntries();\n+            List<SnapshotDeletionsInProgress.Entry> updatedEntries = new ArrayList<>(remainingEntries.size());\n+            for (SnapshotDeletionsInProgress.Entry entry : remainingEntries) {\n+                if (entry.repository().equals(repository)) {\n+                    changed = true;\n+                    deletionsToFail.add(entry.uuid());\n+                } else {\n+                    updatedEntries.add(entry);\n+                }\n+            }\n+            final SnapshotDeletionsInProgress updatedDeletions = changed ? SnapshotDeletionsInProgress.of(updatedEntries) : null;\n+            final SnapshotsInProgress snapshotsInProgress =\n+                currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+            boolean changedSnapshots = false;\n+            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                if (entry.repository().equals(repository)) {\n+                    // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                    // retry these kinds of issues so we fail all the pending snapshots\n+                    snapshotsToFail.add(entry.snapshot());\n+                    changedSnapshots = true;\n+                } else {\n+                    // Entry is for another repository we just keep it as is\n+                    snapshotEntries.add(entry);\n+                }\n+            }\n+            final SnapshotsInProgress updatedSnapshotsInProgress = changedSnapshots ? SnapshotsInProgress.of(snapshotEntries) : null;\n+            return updateWithSnapshots(currentState, updatedSnapshotsInProgress, updatedDeletions);\n+        }\n+\n+        @Override\n+        public void onFailure(String source, Exception e) {\n+            logger.debug(\n+                    () -> new ParameterizedMessage(\"Failed to remove all snapshot tasks for repo [{}] from cluster state\", repository), e);\n+            failAllListenersOnMasterFailOver(e);\n+        }\n+\n+        @Override\n+        public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n+            logger.trace(\"Removed all snapshot tasks for repository [{}] from cluster state, now failing listeners\", repository);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "originalPosition": 1742}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ3Mzg4Mg==", "bodyText": "should we log at info level? Important event, no?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449473882", "createdAt": "2020-07-03T09:13:33Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1463,4 +2157,171 @@ protected ClusterBlockException checkBlock(UpdateIndexShardSnapshotStatusRequest\n             return null;\n         }\n     }\n+\n+    /**\n+     * Cluster state update task that removes all {@link SnapshotsInProgress.Entry} and {@link SnapshotDeletionsInProgress.Entry} for a\n+     * given repository from the cluster state and afterwards fails all relevant listeners in {@link #snapshotCompletionListeners} and\n+     * {@link #snapshotDeletionListeners}.\n+     */\n+    private final class FailPendingRepoTasksTask extends ClusterStateUpdateTask {\n+\n+        // Snapshots to fail after the state update\n+        private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+        // Delete uuids to fail because after the state update\n+        private final List<String> deletionsToFail = new ArrayList<>();\n+\n+        // Failure that caused the decision to fail all snapshots and deletes for a repo\n+        private final Exception failure;\n+\n+        private final String repository;\n+\n+        FailPendingRepoTasksTask(String repository, Exception failure) {\n+            this.repository = repository;\n+            this.failure = failure;\n+        }\n+\n+        @Override\n+        public ClusterState execute(ClusterState currentState) {\n+            final SnapshotDeletionsInProgress deletionsInProgress =\n+                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+            boolean changed = false;\n+            final List<SnapshotDeletionsInProgress.Entry> remainingEntries = deletionsInProgress.getEntries();\n+            List<SnapshotDeletionsInProgress.Entry> updatedEntries = new ArrayList<>(remainingEntries.size());\n+            for (SnapshotDeletionsInProgress.Entry entry : remainingEntries) {\n+                if (entry.repository().equals(repository)) {\n+                    changed = true;\n+                    deletionsToFail.add(entry.uuid());\n+                } else {\n+                    updatedEntries.add(entry);\n+                }\n+            }\n+            final SnapshotDeletionsInProgress updatedDeletions = changed ? SnapshotDeletionsInProgress.of(updatedEntries) : null;\n+            final SnapshotsInProgress snapshotsInProgress =\n+                currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+            boolean changedSnapshots = false;\n+            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                if (entry.repository().equals(repository)) {\n+                    // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                    // retry these kinds of issues so we fail all the pending snapshots\n+                    snapshotsToFail.add(entry.snapshot());\n+                    changedSnapshots = true;\n+                } else {\n+                    // Entry is for another repository we just keep it as is\n+                    snapshotEntries.add(entry);\n+                }\n+            }\n+            final SnapshotsInProgress updatedSnapshotsInProgress = changedSnapshots ? SnapshotsInProgress.of(snapshotEntries) : null;\n+            return updateWithSnapshots(currentState, updatedSnapshotsInProgress, updatedDeletions);\n+        }\n+\n+        @Override\n+        public void onFailure(String source, Exception e) {\n+            logger.debug(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "81fdf77e28c32ce4748cc315758ca517995329e4"}, "originalPosition": 1735}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTQ3Nzk5NA==", "bodyText": "I wonder if instead of forbidding, different repo definitions pointing to the same path should be treated the same way as if they were the same repo (i.e. concurrency limitations). Perhaps a first overapproximation would be to treat every repo as if it was the same (i.e. remove the \"isSameRepo\" checks in this PR).\nMost users are making use of one repository, not many. They get concurrency benefits with this PR even if it treats every repo as being the same. We can open the system to more concurrency possibly later then once we have a stronger notion of \"same repo\"?\nWDYT?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r449477994", "createdAt": "2020-07-03T09:21:41Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,971 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateObserver;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.node.NodeClosedException;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI1NjEwMg=="}, "originalCommit": {"oid": "cedcd8838e4a8f6940a0168f444929d0faebb412"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95a1dc53812d7254acf479ba805155b922d82a6e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/95a1dc53812d7254acf479ba805155b922d82a6e", "committedDate": "2020-07-03T11:38:39Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e79b5cfbbd7edc8b5c0b8456c4742258858ee1b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/5e79b5cfbbd7edc8b5c0b8456c4742258858ee1b", "committedDate": "2020-07-03T12:34:10Z", "message": "CR: small fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2e97de304df5aca337205d572ca95bf10a96d07", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d2e97de304df5aca337205d572ca95bf10a96d07", "committedDate": "2020-07-03T12:36:45Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c0fcafb38491de387ab2db7720dc56f28426cf6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/4c0fcafb38491de387ab2db7720dc56f28426cf6", "committedDate": "2020-07-03T14:42:49Z", "message": "CR: add setting to limit concurrency"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1f75a52688a63d47319987f65c2b87cdc3abe6e5", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/1f75a52688a63d47319987f65c2b87cdc3abe6e5", "committedDate": "2020-07-06T12:22:01Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bfff04ede64a14237e955b8ee74b934b1362a875", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/bfff04ede64a14237e955b8ee74b934b1362a875", "committedDate": "2020-07-06T12:43:17Z", "message": "merge in master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "528cc81f5feb672f854810f8d91aecd8cdadea47", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/528cc81f5feb672f854810f8d91aecd8cdadea47", "committedDate": "2020-07-07T09:00:49Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08177e1a9c97ea1562a52e1852ee9dc42b8800db", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/08177e1a9c97ea1562a52e1852ee9dc42b8800db", "committedDate": "2020-07-07T09:09:25Z", "message": "merge in master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb9cd4343e0efdf6330dc169d4de40adcb3363f1", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/bb9cd4343e0efdf6330dc169d4de40adcb3363f1", "committedDate": "2020-07-07T13:15:08Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "883d2203f3000eb9d2784deaeed02468b789db95", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/883d2203f3000eb9d2784deaeed02468b789db95", "committedDate": "2020-07-07T13:16:06Z", "message": "Merge branch 'master' of github.com:elastic/elasticsearch into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "906f51b294e6c6ab6020bd5804361ebd101a1be0", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/906f51b294e6c6ab6020bd5804361ebd101a1be0", "committedDate": "2020-07-07T14:36:11Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7bb2d25eda5b3b101fb04ed1b80c887883eb6be", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a7bb2d25eda5b3b101fb04ed1b80c887883eb6be", "committedDate": "2020-07-08T11:45:07Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb9af07eca04727e3a9f6df148d880ceaa8ab021", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cb9af07eca04727e3a9f6df148d880ceaa8ab021", "committedDate": "2020-07-08T12:25:44Z", "message": "add test for concurrent ops and parallel ops"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca50bf4760a231b5a03cbbcad290651bdd6749dc", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/ca50bf4760a231b5a03cbbcad290651bdd6749dc", "committedDate": "2020-07-09T04:38:59Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f5a44d4bfa3d8bd5940cc09f694ec3bc99b15b1d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/f5a44d4bfa3d8bd5940cc09f694ec3bc99b15b1d", "committedDate": "2020-07-09T05:58:50Z", "message": "be smarter with shard generations to always allow for concurrent stuff"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ1NTY4MTIz", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-445568123", "createdAt": "2020-07-09T12:29:38Z", "commit": {"oid": "f5a44d4bfa3d8bd5940cc09f694ec3bc99b15b1d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9b03116ca4716488201b0775a87ed3ea222791e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e9b03116ca4716488201b0775a87ed3ea222791e", "committedDate": "2020-07-09T12:29:55Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/897e6d1160b52aa70db2365907f0e8d9c19347f7", "committedDate": "2020-07-09T12:36:58Z", "message": "CR: limit 1k"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2MTc1MjQ3", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-446175247", "createdAt": "2020-07-10T07:19:01Z", "commit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "state": "APPROVED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQwNzoxOTowMVrOGvsl2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQwODoxODoxNlrOGvuUOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY2Njg0MQ==", "bodyText": "nit: add extra line", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452666841", "createdAt": "2020-07-10T07:19:01Z", "author": {"login": "tlrx"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,1313 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return Arrays.asList(MockTransportService.TestPlugin.class, MockRepository.Plugin.class);\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder().put(super.nodeSettings(nodeOrdinal))\n+                .put(AbstractDisruptionTestCase.DEFAULT_SETTINGS)\n+                .build();\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"slow-snapshot\", repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String indexFast = \"index-fast\";\n+        createIndexWithContent(indexFast, dataNode2, dataNode);\n+\n+        assertSuccessful(client().admin().cluster().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testDeletesAreBatched() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        createIndex(\"foo\");\n+        ensureGreen();\n+\n+        final int numSnapshots = randomIntBetween(1, 4);\n+        final PlainActionFuture<Collection<CreateSnapshotResponse>> allSnapshotsDone = PlainActionFuture.newFuture();\n+        final ActionListener<CreateSnapshotResponse> snapshotsListener = new GroupedActionListener<>(allSnapshotsDone, numSnapshots);\n+        final Collection<String> snapshotNames = new HashSet<>();\n+        for (int i = 0; i < numSnapshots; i++) {\n+            final String snapshot = \"snap-\" + i;\n+            snapshotNames.add(snapshot);\n+            client().admin().cluster().prepareCreateSnapshot(repoName, snapshot).setWaitForCompletion(true)\n+                    .execute(snapshotsListener);\n+        }\n+        final Collection<CreateSnapshotResponse> snapshotResponses = allSnapshotsDone.get();\n+        for (CreateSnapshotResponse snapshotResponse : snapshotResponses) {\n+            assertThat(snapshotResponse.getSnapshotInfo().state(), is(SnapshotState.SUCCESS));\n+        }\n+\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", repoName, dataNode);\n+\n+        final Collection<StepListener<AcknowledgedResponse>> deleteFutures = new ArrayList<>();\n+        while (snapshotNames.isEmpty() == false) {\n+            final Collection<String> toDelete = randomSubsetOf(snapshotNames);\n+            if (toDelete.isEmpty()) {\n+                continue;\n+            }\n+            snapshotNames.removeAll(toDelete);\n+            final StepListener<AcknowledgedResponse> future = new StepListener<>();\n+            client().admin().cluster().prepareDeleteSnapshot(repoName, toDelete.toArray(Strings.EMPTY_ARRAY)).execute(future);\n+            deleteFutures.add(future);\n+        }\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+\n+        final long repoGenAfterInitialSnapshots = getRepositoryData(repoName).getGenId();\n+        assertThat(repoGenAfterInitialSnapshots, is(numSnapshots - 1L));\n+        unblockNode(repoName, dataNode);\n+\n+        final SnapshotInfo slowSnapshotInfo = assertSuccessful(createSlowFuture);\n+\n+        logger.info(\"--> waiting for batched deletes to finish\");\n+        final PlainActionFuture<Collection<AcknowledgedResponse>> allDeletesDone = new PlainActionFuture<>();\n+        final ActionListener<AcknowledgedResponse> deletesListener = new GroupedActionListener<>(allDeletesDone, deleteFutures.size());\n+        for (StepListener<AcknowledgedResponse> deleteFuture : deleteFutures) {\n+            deleteFuture.whenComplete(deletesListener::onResponse, deletesListener::onFailure);\n+        }\n+        allDeletesDone.get();\n+\n+        logger.info(\"--> verifying repository state\");\n+        final RepositoryData repositoryDataAfterDeletes = getRepositoryData(repoName);\n+        // One increment for snapshot, one for all the deletes\n+        assertThat(repositoryDataAfterDeletes.getGenId(), is(repoGenAfterInitialSnapshots + 2));\n+        assertThat(repositoryDataAfterDeletes.getSnapshotIds(), contains(slowSnapshotInfo.snapshotId()));\n+    }\n+\n+    public void testBlockedRepoDoesNotBlockOtherRepos() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndex(\"foo\");\n+        ensureGreen();\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startAndBlockFailingFullSnapshot(blockedRepoName, \"blocked-snapshot\");\n+\n+        client().admin().cluster().prepareCreateSnapshot(otherRepoName, \"snapshot\")\n+                .setIndices(\"does-not-exist-*\")\n+                .setWaitForCompletion(false).get();\n+\n+        unblockNode(blockedRepoName, internalCluster().getMasterName());\n+        expectThrows(SnapshotException.class, createSlowFuture::actionGet);\n+\n+        assertBusy(() -> assertThat(currentSnapshots(otherRepoName), empty()), 30L, TimeUnit.SECONDS);\n+    }\n+\n+    public void testMultipleReposAreIndependent() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second concurrent snapshot.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent2() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second repository's concurrent operations.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent3() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        createFullSnapshot( blockedRepoName, \"blocked-snapshot\");\n+        blockNodeOnAnyFiles(blockedRepoName, masterNode);\n+        final ActionFuture<AcknowledgedResponse> slowDeleteFuture = startDelete(blockedRepoName, \"*\");\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, masterNode);\n+        assertAcked(slowDeleteFuture.actionGet());\n+    }\n+\n+    private static String startDataNodeWithLargeSnapshotPool() {\n+        return internalCluster().startDataOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+    }\n+    public void testSnapshotRunsAfterInProgressDelete() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY2OTg3OQ==", "bodyText": "Can we check that the 1st snapshot failed because it was aborted?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452669879", "createdAt": "2020-07-10T07:25:37Z", "author": {"login": "tlrx"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,1313 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return Arrays.asList(MockTransportService.TestPlugin.class, MockRepository.Plugin.class);\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder().put(super.nodeSettings(nodeOrdinal))\n+                .put(AbstractDisruptionTestCase.DEFAULT_SETTINGS)\n+                .build();\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"slow-snapshot\", repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String indexFast = \"index-fast\";\n+        createIndexWithContent(indexFast, dataNode2, dataNode);\n+\n+        assertSuccessful(client().admin().cluster().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testDeletesAreBatched() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        createIndex(\"foo\");\n+        ensureGreen();\n+\n+        final int numSnapshots = randomIntBetween(1, 4);\n+        final PlainActionFuture<Collection<CreateSnapshotResponse>> allSnapshotsDone = PlainActionFuture.newFuture();\n+        final ActionListener<CreateSnapshotResponse> snapshotsListener = new GroupedActionListener<>(allSnapshotsDone, numSnapshots);\n+        final Collection<String> snapshotNames = new HashSet<>();\n+        for (int i = 0; i < numSnapshots; i++) {\n+            final String snapshot = \"snap-\" + i;\n+            snapshotNames.add(snapshot);\n+            client().admin().cluster().prepareCreateSnapshot(repoName, snapshot).setWaitForCompletion(true)\n+                    .execute(snapshotsListener);\n+        }\n+        final Collection<CreateSnapshotResponse> snapshotResponses = allSnapshotsDone.get();\n+        for (CreateSnapshotResponse snapshotResponse : snapshotResponses) {\n+            assertThat(snapshotResponse.getSnapshotInfo().state(), is(SnapshotState.SUCCESS));\n+        }\n+\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", repoName, dataNode);\n+\n+        final Collection<StepListener<AcknowledgedResponse>> deleteFutures = new ArrayList<>();\n+        while (snapshotNames.isEmpty() == false) {\n+            final Collection<String> toDelete = randomSubsetOf(snapshotNames);\n+            if (toDelete.isEmpty()) {\n+                continue;\n+            }\n+            snapshotNames.removeAll(toDelete);\n+            final StepListener<AcknowledgedResponse> future = new StepListener<>();\n+            client().admin().cluster().prepareDeleteSnapshot(repoName, toDelete.toArray(Strings.EMPTY_ARRAY)).execute(future);\n+            deleteFutures.add(future);\n+        }\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+\n+        final long repoGenAfterInitialSnapshots = getRepositoryData(repoName).getGenId();\n+        assertThat(repoGenAfterInitialSnapshots, is(numSnapshots - 1L));\n+        unblockNode(repoName, dataNode);\n+\n+        final SnapshotInfo slowSnapshotInfo = assertSuccessful(createSlowFuture);\n+\n+        logger.info(\"--> waiting for batched deletes to finish\");\n+        final PlainActionFuture<Collection<AcknowledgedResponse>> allDeletesDone = new PlainActionFuture<>();\n+        final ActionListener<AcknowledgedResponse> deletesListener = new GroupedActionListener<>(allDeletesDone, deleteFutures.size());\n+        for (StepListener<AcknowledgedResponse> deleteFuture : deleteFutures) {\n+            deleteFuture.whenComplete(deletesListener::onResponse, deletesListener::onFailure);\n+        }\n+        allDeletesDone.get();\n+\n+        logger.info(\"--> verifying repository state\");\n+        final RepositoryData repositoryDataAfterDeletes = getRepositoryData(repoName);\n+        // One increment for snapshot, one for all the deletes\n+        assertThat(repositoryDataAfterDeletes.getGenId(), is(repoGenAfterInitialSnapshots + 2));\n+        assertThat(repositoryDataAfterDeletes.getSnapshotIds(), contains(slowSnapshotInfo.snapshotId()));\n+    }\n+\n+    public void testBlockedRepoDoesNotBlockOtherRepos() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndex(\"foo\");\n+        ensureGreen();\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startAndBlockFailingFullSnapshot(blockedRepoName, \"blocked-snapshot\");\n+\n+        client().admin().cluster().prepareCreateSnapshot(otherRepoName, \"snapshot\")\n+                .setIndices(\"does-not-exist-*\")\n+                .setWaitForCompletion(false).get();\n+\n+        unblockNode(blockedRepoName, internalCluster().getMasterName());\n+        expectThrows(SnapshotException.class, createSlowFuture::actionGet);\n+\n+        assertBusy(() -> assertThat(currentSnapshots(otherRepoName), empty()), 30L, TimeUnit.SECONDS);\n+    }\n+\n+    public void testMultipleReposAreIndependent() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second concurrent snapshot.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent2() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second repository's concurrent operations.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent3() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        createFullSnapshot( blockedRepoName, \"blocked-snapshot\");\n+        blockNodeOnAnyFiles(blockedRepoName, masterNode);\n+        final ActionFuture<AcknowledgedResponse> slowDeleteFuture = startDelete(blockedRepoName, \"*\");\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, masterNode);\n+        assertAcked(slowDeleteFuture.actionGet());\n+    }\n+\n+    private static String startDataNodeWithLargeSnapshotPool() {\n+        return internalCluster().startDataOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+    }\n+    public void testSnapshotRunsAfterInProgressDelete() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        ensureGreen();\n+        createIndexWithContent(\"index-test\");\n+\n+        final String firstSnapshot = \"first-snapshot\";\n+        createFullSnapshot(repoName, firstSnapshot);\n+\n+        blockMasterFromFinalizingSnapshotOnIndexFile(repoName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDelete(repoName, firstSnapshot);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = startFullSnapshot(repoName, \"second-snapshot\");\n+\n+        unblockNode(repoName, masterNode);\n+        final UncategorizedExecutionException ex = expectThrows(UncategorizedExecutionException.class, deleteFuture::actionGet);\n+        assertThat(ex.getRootCause(), instanceOf(IOException.class));\n+\n+        assertSuccessful(snapshotFuture);\n+    }\n+\n+    public void testAbortOneOfMultipleSnapshots() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String firstIndex = \"index-one\";\n+        createIndexWithContent(firstIndex);\n+\n+        final String firstSnapshot = \"snapshot-one\";\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse =\n+                startFullSnapshotBlockedOnDataNode(firstSnapshot, repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String secondIndex = \"index-two\";\n+        createIndexWithContent(secondIndex, dataNode2, dataNode);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot);\n+\n+        logger.info(\"--> wait for snapshot on second data node to finish\");\n+        awaitClusterState(state -> {\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress);\n+        });\n+\n+        final ActionFuture<AcknowledgedResponse> deleteSnapshotsResponse = startDelete(repoName, firstSnapshot);\n+        awaitNDeletionsInProgress(1);\n+\n+        logger.info(\"--> start third snapshot\");\n+        final ActionFuture<CreateSnapshotResponse> thirdSnapshotResponse = client().admin().cluster()\n+                .prepareCreateSnapshot(repoName, \"snapshot-three\").setIndices(secondIndex).setWaitForCompletion(true).execute();\n+\n+        assertThat(firstSnapshotResponse.isDone(), is(false));\n+        assertThat(secondSnapshotResponse.isDone(), is(false));\n+\n+        unblockNode(repoName, dataNode);\n+        assertThat(firstSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "originalPosition": 337}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY3NzIxMA==", "bodyText": "Is there a meaningful error message we could check here?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452677210", "createdAt": "2020-07-10T07:41:41Z", "author": {"login": "tlrx"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,1313 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.ShardGenerations;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.containsString;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return Arrays.asList(MockTransportService.TestPlugin.class, MockRepository.Plugin.class);\n+    }\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder().put(super.nodeSettings(nodeOrdinal))\n+                .put(AbstractDisruptionTestCase.DEFAULT_SETTINGS)\n+                .build();\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"slow-snapshot\", repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String indexFast = \"index-fast\";\n+        createIndexWithContent(indexFast, dataNode2, dataNode);\n+\n+        assertSuccessful(client().admin().cluster().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testDeletesAreBatched() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        createIndex(\"foo\");\n+        ensureGreen();\n+\n+        final int numSnapshots = randomIntBetween(1, 4);\n+        final PlainActionFuture<Collection<CreateSnapshotResponse>> allSnapshotsDone = PlainActionFuture.newFuture();\n+        final ActionListener<CreateSnapshotResponse> snapshotsListener = new GroupedActionListener<>(allSnapshotsDone, numSnapshots);\n+        final Collection<String> snapshotNames = new HashSet<>();\n+        for (int i = 0; i < numSnapshots; i++) {\n+            final String snapshot = \"snap-\" + i;\n+            snapshotNames.add(snapshot);\n+            client().admin().cluster().prepareCreateSnapshot(repoName, snapshot).setWaitForCompletion(true)\n+                    .execute(snapshotsListener);\n+        }\n+        final Collection<CreateSnapshotResponse> snapshotResponses = allSnapshotsDone.get();\n+        for (CreateSnapshotResponse snapshotResponse : snapshotResponses) {\n+            assertThat(snapshotResponse.getSnapshotInfo().state(), is(SnapshotState.SUCCESS));\n+        }\n+\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", repoName, dataNode);\n+\n+        final Collection<StepListener<AcknowledgedResponse>> deleteFutures = new ArrayList<>();\n+        while (snapshotNames.isEmpty() == false) {\n+            final Collection<String> toDelete = randomSubsetOf(snapshotNames);\n+            if (toDelete.isEmpty()) {\n+                continue;\n+            }\n+            snapshotNames.removeAll(toDelete);\n+            final StepListener<AcknowledgedResponse> future = new StepListener<>();\n+            client().admin().cluster().prepareDeleteSnapshot(repoName, toDelete.toArray(Strings.EMPTY_ARRAY)).execute(future);\n+            deleteFutures.add(future);\n+        }\n+\n+        assertThat(createSlowFuture.isDone(), is(false));\n+\n+        final long repoGenAfterInitialSnapshots = getRepositoryData(repoName).getGenId();\n+        assertThat(repoGenAfterInitialSnapshots, is(numSnapshots - 1L));\n+        unblockNode(repoName, dataNode);\n+\n+        final SnapshotInfo slowSnapshotInfo = assertSuccessful(createSlowFuture);\n+\n+        logger.info(\"--> waiting for batched deletes to finish\");\n+        final PlainActionFuture<Collection<AcknowledgedResponse>> allDeletesDone = new PlainActionFuture<>();\n+        final ActionListener<AcknowledgedResponse> deletesListener = new GroupedActionListener<>(allDeletesDone, deleteFutures.size());\n+        for (StepListener<AcknowledgedResponse> deleteFuture : deleteFutures) {\n+            deleteFuture.whenComplete(deletesListener::onResponse, deletesListener::onFailure);\n+        }\n+        allDeletesDone.get();\n+\n+        logger.info(\"--> verifying repository state\");\n+        final RepositoryData repositoryDataAfterDeletes = getRepositoryData(repoName);\n+        // One increment for snapshot, one for all the deletes\n+        assertThat(repositoryDataAfterDeletes.getGenId(), is(repoGenAfterInitialSnapshots + 2));\n+        assertThat(repositoryDataAfterDeletes.getSnapshotIds(), contains(slowSnapshotInfo.snapshotId()));\n+    }\n+\n+    public void testBlockedRepoDoesNotBlockOtherRepos() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndex(\"foo\");\n+        ensureGreen();\n+        createIndexWithContent(\"index-slow\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startAndBlockFailingFullSnapshot(blockedRepoName, \"blocked-snapshot\");\n+\n+        client().admin().cluster().prepareCreateSnapshot(otherRepoName, \"snapshot\")\n+                .setIndices(\"does-not-exist-*\")\n+                .setWaitForCompletion(false).get();\n+\n+        unblockNode(blockedRepoName, internalCluster().getMasterName());\n+        expectThrows(SnapshotException.class, createSlowFuture::actionGet);\n+\n+        assertBusy(() -> assertThat(currentSnapshots(otherRepoName), empty()), 30L, TimeUnit.SECONDS);\n+    }\n+\n+    public void testMultipleReposAreIndependent() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second concurrent snapshot.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent2() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        // We're blocking a some of the snapshot threads when we block the first repo below so we have to make sure we have enough threads\n+        // left for the second repository's concurrent operations.\n+        final String dataNode = startDataNodeWithLargeSnapshotPool();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        final ActionFuture<CreateSnapshotResponse> createSlowFuture =\n+                startFullSnapshotBlockedOnDataNode(\"blocked-snapshot\", blockedRepoName, dataNode);\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, dataNode);\n+        assertSuccessful(createSlowFuture);\n+    }\n+\n+    public void testMultipleReposAreIndependent3() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String blockedRepoName = \"test-repo-blocked\";\n+        final String otherRepoName = \"test-repo\";\n+        createRepository(blockedRepoName, \"mock\");\n+        createRepository(otherRepoName, \"fs\");\n+        createIndexWithContent(\"test-index\");\n+\n+        createFullSnapshot( blockedRepoName, \"blocked-snapshot\");\n+        blockNodeOnAnyFiles(blockedRepoName, masterNode);\n+        final ActionFuture<AcknowledgedResponse> slowDeleteFuture = startDelete(blockedRepoName, \"*\");\n+\n+        logger.info(\"--> waiting for concurrent snapshot(s) to finish\");\n+        createNSnapshots(otherRepoName, randomIntBetween(1, 5));\n+        assertAcked(startDelete(otherRepoName, \"*\").get());\n+\n+        unblockNode(blockedRepoName, masterNode);\n+        assertAcked(slowDeleteFuture.actionGet());\n+    }\n+\n+    private static String startDataNodeWithLargeSnapshotPool() {\n+        return internalCluster().startDataOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+    }\n+    public void testSnapshotRunsAfterInProgressDelete() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        ensureGreen();\n+        createIndexWithContent(\"index-test\");\n+\n+        final String firstSnapshot = \"first-snapshot\";\n+        createFullSnapshot(repoName, firstSnapshot);\n+\n+        blockMasterFromFinalizingSnapshotOnIndexFile(repoName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDelete(repoName, firstSnapshot);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = startFullSnapshot(repoName, \"second-snapshot\");\n+\n+        unblockNode(repoName, masterNode);\n+        final UncategorizedExecutionException ex = expectThrows(UncategorizedExecutionException.class, deleteFuture::actionGet);\n+        assertThat(ex.getRootCause(), instanceOf(IOException.class));\n+\n+        assertSuccessful(snapshotFuture);\n+    }\n+\n+    public void testAbortOneOfMultipleSnapshots() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String firstIndex = \"index-one\";\n+        createIndexWithContent(firstIndex);\n+\n+        final String firstSnapshot = \"snapshot-one\";\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse =\n+                startFullSnapshotBlockedOnDataNode(firstSnapshot, repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        final String secondIndex = \"index-two\";\n+        createIndexWithContent(secondIndex, dataNode2, dataNode);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot);\n+\n+        logger.info(\"--> wait for snapshot on second data node to finish\");\n+        awaitClusterState(state -> {\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress);\n+        });\n+\n+        final ActionFuture<AcknowledgedResponse> deleteSnapshotsResponse = startDelete(repoName, firstSnapshot);\n+        awaitNDeletionsInProgress(1);\n+\n+        logger.info(\"--> start third snapshot\");\n+        final ActionFuture<CreateSnapshotResponse> thirdSnapshotResponse = client().admin().cluster()\n+                .prepareCreateSnapshot(repoName, \"snapshot-three\").setIndices(secondIndex).setWaitForCompletion(true).execute();\n+\n+        assertThat(firstSnapshotResponse.isDone(), is(false));\n+        assertThat(secondSnapshotResponse.isDone(), is(false));\n+\n+        unblockNode(repoName, dataNode);\n+        assertThat(firstSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+\n+        final SnapshotInfo secondSnapshotInfo = assertSuccessful(secondSnapshotResponse);\n+        final SnapshotInfo thirdSnapshotInfo = assertSuccessful(thirdSnapshotResponse);\n+\n+        assertThat(deleteSnapshotsResponse.get().isAcknowledged(), is(true));\n+\n+        logger.info(\"--> verify that the first snapshot is gone\");\n+        assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName),\n+                containsInAnyOrder(secondSnapshotInfo, thirdSnapshotInfo));\n+    }\n+\n+    public void testCascadedAborts() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+\n+        final String firstSnapshot = \"snapshot-one\";\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse =\n+                startFullSnapshotBlockedOnDataNode(firstSnapshot, repoName, dataNode);\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        createIndexWithContent(\"index-two\", dataNode2, dataNode);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot);\n+\n+        logger.info(\"--> wait for snapshot on second data node to finish\");\n+        awaitClusterState(state -> {\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress);\n+        });\n+\n+        final ActionFuture<AcknowledgedResponse> deleteSnapshotsResponse = startDelete(repoName, firstSnapshot);\n+        awaitNDeletionsInProgress(1);\n+\n+        final ActionFuture<CreateSnapshotResponse> thirdSnapshotResponse = startFullSnapshot(repoName, \"snapshot-three\");\n+\n+        assertThat(firstSnapshotResponse.isDone(), is(false));\n+        assertThat(secondSnapshotResponse.isDone(), is(false));\n+\n+        logger.info(\"--> waiting for all three snapshots to show up as in-progress\");\n+        assertBusy(() -> assertThat(currentSnapshots(repoName), hasSize(3)), 30L, TimeUnit.SECONDS);\n+\n+        final ActionFuture<AcknowledgedResponse> allDeletedResponse = startDelete(repoName, \"*\");\n+\n+        logger.info(\"--> waiting for second and third snapshot to finish\");\n+        assertBusy(() -> {\n+            assertThat(currentSnapshots(repoName), hasSize(1));\n+            final SnapshotsInProgress snapshotsInProgress = clusterService().state().custom(SnapshotsInProgress.TYPE);\n+            assertThat(snapshotsInProgress.entries().get(0).state(), is(SnapshotsInProgress.State.ABORTED));\n+        }, 30L, TimeUnit.SECONDS);\n+\n+        unblockNode(repoName, dataNode);\n+\n+        logger.info(\"--> verify all snapshots were aborted\");\n+        assertThat(firstSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+        assertThat(secondSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+        assertThat(thirdSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+\n+        logger.info(\"--> verify both deletes have completed\");\n+        assertAcked(deleteSnapshotsResponse.get());\n+        assertAcked(allDeletedResponse.get());\n+\n+        logger.info(\"--> verify that all snapshots are gone\");\n+        assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty());\n+    }\n+\n+    public void testMasterFailOverWithQueuedDeletes() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String firstIndex = \"index-one\";\n+        createIndexWithContent(firstIndex);\n+\n+        final String firstSnapshot = \"snapshot-one\";\n+        blockDataNode(repoName, dataNode);\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse = startFullSnapshotFromNonMasterClient(repoName, firstSnapshot);\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final String dataNode2 = internalCluster().startDataOnlyNode();\n+        ensureStableCluster(5);\n+        final String secondIndex = \"index-two\";\n+        createIndexWithContent(secondIndex, dataNode2, dataNode);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshot(repoName, secondSnapshot);\n+\n+        logger.info(\"--> wait for snapshot on second data node to finish\");\n+        awaitClusterState(state -> {\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+            return snapshotsInProgress.entries().size() == 2 && snapshotHasCompletedShard(secondSnapshot, snapshotsInProgress);\n+        });\n+\n+        final ActionFuture<AcknowledgedResponse> firstDeleteFuture = startDeleteFromNonMasterClient(repoName, firstSnapshot);\n+        awaitNDeletionsInProgress(1);\n+\n+        blockDataNode(repoName, dataNode2);\n+        final ActionFuture<CreateSnapshotResponse> snapshotThreeFuture = startFullSnapshotFromNonMasterClient(repoName, \"snapshot-three\");\n+        waitForBlock(dataNode2, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        assertThat(firstSnapshotResponse.isDone(), is(false));\n+        assertThat(secondSnapshotResponse.isDone(), is(false));\n+\n+        logger.info(\"--> waiting for all three snapshots to show up as in-progress\");\n+        assertBusy(() -> assertThat(currentSnapshots(repoName), hasSize(3)), 30L, TimeUnit.SECONDS);\n+\n+        final ActionFuture<AcknowledgedResponse> deleteAllSnapshots = startDeleteFromNonMasterClient(repoName, \"*\");\n+        logger.info(\"--> wait for delete to be enqueued in cluster state\");\n+        awaitClusterState(state -> {\n+            final SnapshotDeletionsInProgress deletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE);\n+            return deletionsInProgress.getEntries().size() == 1 && deletionsInProgress.getEntries().get(0).getSnapshots().size() == 3;\n+        });\n+\n+        logger.info(\"--> waiting for second snapshot to finish and the other two snapshots to become aborted\");\n+        assertBusy(() -> {\n+            assertThat(currentSnapshots(repoName), hasSize(2));\n+            for (SnapshotsInProgress.Entry entry\n+                    : clusterService().state().custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n+                assertThat(entry.state(), is(SnapshotsInProgress.State.ABORTED));\n+                assertThat(entry.snapshot().getSnapshotId().getName(), not(secondSnapshot));\n+            }\n+        }, 30L, TimeUnit.SECONDS);\n+\n+        logger.info(\"--> stopping current master node\");\n+        internalCluster().stopCurrentMasterNode();\n+\n+        unblockNode(repoName, dataNode);\n+        unblockNode(repoName, dataNode2);\n+\n+        assertAcked(firstDeleteFuture.get());\n+        assertAcked(deleteAllSnapshots.get());\n+        expectThrows(SnapshotException.class, snapshotThreeFuture::actionGet);\n+\n+        logger.info(\"--> verify that all snapshots are gone and no more work is left in the cluster state\");\n+        assertBusy(() -> {\n+            assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty());\n+            final ClusterState state = clusterService().state();\n+            final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE);\n+            assertThat(snapshotsInProgress.entries(), empty());\n+            final SnapshotDeletionsInProgress snapshotDeletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE);\n+            assertThat(snapshotDeletionsInProgress.getEntries(), empty());\n+        }, 30L, TimeUnit.SECONDS);\n+    }\n+\n+    public void testAssertMultipleSnapshotsAndPrimaryFailOver() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String testIndex = \"index-one\";\n+        createIndex(testIndex, Settings.builder().put(SETTING_NUMBER_OF_SHARDS, 1).put(SETTING_NUMBER_OF_REPLICAS, 1).build());\n+        ensureYellow(testIndex);\n+        indexDoc(testIndex, \"some_id\", \"foo\", \"bar\");\n+\n+        blockDataNode(repoName, dataNode);\n+        final ActionFuture<CreateSnapshotResponse> firstSnapshotResponse = startFullSnapshotFromMasterClient(repoName, \"snapshot-one\");\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        internalCluster().startDataOnlyNode();\n+        ensureStableCluster(3);\n+        ensureGreen(testIndex);\n+\n+        final String secondSnapshot = \"snapshot-two\";\n+        final ActionFuture<CreateSnapshotResponse> secondSnapshotResponse = startFullSnapshotFromMasterClient(repoName, secondSnapshot);\n+\n+        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n+\n+        assertThat(firstSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.PARTIAL));\n+        assertThat(secondSnapshotResponse.get().getSnapshotInfo().state(), is(SnapshotState.PARTIAL));\n+    }\n+\n+    public void testQueuedDeletesWithFailures() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        blockMasterFromFinalizingSnapshotOnIndexFile(repoName);\n+        final ActionFuture<AcknowledgedResponse> firstDeleteFuture = startDelete(repoName, \"*\");\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = startFullSnapshot(repoName, \"snapshot-queued\");\n+        awaitNSnapshotsInProgress(1);\n+\n+        final ActionFuture<AcknowledgedResponse> secondDeleteFuture = startDelete(repoName, \"*\");\n+        awaitNDeletionsInProgress(2);\n+\n+        unblockNode(repoName, masterNode);\n+        expectThrows(UncategorizedExecutionException.class, firstDeleteFuture::actionGet);\n+\n+        // Second delete works out cleanly since the repo is unblocked now\n+        assertThat(secondDeleteFuture.get().isAcknowledged(), is(true));\n+        // Snapshot should have been aborted\n+        assertThat(snapshotFuture.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+\n+        assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty());\n+    }\n+\n+    public void testQueuedDeletesWithOverlap() throws Exception {\n+        final String masterNode = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        final ActionFuture<AcknowledgedResponse> firstDeleteFuture = startAndBlockOnDeleteSnapshot(repoName, \"*\");\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = startFullSnapshot(repoName, \"snapshot-queued\");\n+        awaitNSnapshotsInProgress(1);\n+\n+        final ActionFuture<AcknowledgedResponse> secondDeleteFuture = startDelete(repoName, \"*\");\n+        awaitNDeletionsInProgress(2);\n+\n+        unblockNode(repoName, masterNode);\n+        assertThat(firstDeleteFuture.get().isAcknowledged(), is(true));\n+\n+        // Second delete works out cleanly since the repo is unblocked now\n+        assertThat(secondDeleteFuture.get().isAcknowledged(), is(true));\n+        // Snapshot should have been aborted\n+        assertThat(snapshotFuture.get().getSnapshotInfo().state(), is(SnapshotState.FAILED));\n+\n+        assertThat(client().admin().cluster().prepareGetSnapshots(repoName).get().getSnapshots(repoName), empty());\n+    }\n+\n+    public void testQueuedOperationsOnMasterRestart() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        startAndBlockOnDeleteSnapshot(repoName, \"*\");\n+\n+        client().admin().cluster().prepareCreateSnapshot(repoName, \"snapshot-three\").setWaitForCompletion(false).get();\n+\n+        startDelete(repoName, \"*\");\n+        awaitNDeletionsInProgress(2);\n+\n+        internalCluster().stopCurrentMasterNode();\n+        ensureStableCluster(3);\n+\n+        awaitNoMoreRunningOperations();\n+    }\n+\n+    public void testQueuedOperationsOnMasterDisconnect() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        final String masterNode = internalCluster().getMasterName();\n+        final NetworkDisruption networkDisruption = isolateMasterDisruption(NetworkDisruption.DISCONNECT);\n+        internalCluster().setDisruptionScheme(networkDisruption);\n+\n+        blockNodeOnAnyFiles(repoName, masterNode);\n+        ActionFuture<AcknowledgedResponse> firstDeleteFuture = client(masterNode).admin().cluster()\n+                .prepareDeleteSnapshot(repoName, \"*\").execute();\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final ActionFuture<CreateSnapshotResponse> createThirdSnapshot = client(masterNode).admin().cluster()\n+                .prepareCreateSnapshot(repoName, \"snapshot-three\").setWaitForCompletion(true).execute();\n+        awaitNSnapshotsInProgress(1);\n+\n+        final ActionFuture<AcknowledgedResponse> secondDeleteFuture =\n+                client(masterNode).admin().cluster().prepareDeleteSnapshot(repoName, \"*\").execute();\n+        awaitNDeletionsInProgress(2);\n+\n+        networkDisruption.startDisrupting();\n+        ensureStableCluster(3, dataNode);\n+        unblockNode(repoName, masterNode);\n+        networkDisruption.stopDisrupting();\n+\n+        logger.info(\"--> make sure all failing requests get a response\");\n+        expectThrows(RepositoryException.class, firstDeleteFuture::actionGet);\n+        expectThrows(RepositoryException.class, secondDeleteFuture::actionGet);\n+        expectThrows(SnapshotException.class, createThirdSnapshot::actionGet);\n+\n+        awaitNoMoreRunningOperations();\n+    }\n+\n+    public void testQueuedOperationsOnMasterDisconnectAndRepoFailure() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        final String masterNode = internalCluster().getMasterName();\n+        final NetworkDisruption networkDisruption = isolateMasterDisruption(NetworkDisruption.DISCONNECT);\n+        internalCluster().setDisruptionScheme(networkDisruption);\n+\n+        blockMasterFromFinalizingSnapshotOnIndexFile(repoName);\n+        final ActionFuture<CreateSnapshotResponse> firstFailedSnapshotFuture =\n+                startFullSnapshotFromMasterClient(repoName, \"failing-snapshot-1\");\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+        final ActionFuture<CreateSnapshotResponse> secondFailedSnapshotFuture =\n+                startFullSnapshotFromMasterClient(repoName, \"failing-snapshot-2\");\n+        awaitNSnapshotsInProgress(2);\n+\n+        final ActionFuture<AcknowledgedResponse> failedDeleteFuture =\n+                client(masterNode).admin().cluster().prepareDeleteSnapshot(repoName, \"*\").execute();\n+        awaitNDeletionsInProgress(1);\n+\n+        networkDisruption.startDisrupting();\n+        ensureStableCluster(3, dataNode);\n+        unblockNode(repoName, masterNode);\n+        networkDisruption.stopDisrupting();\n+\n+        logger.info(\"--> make sure all failing requests get a response\");\n+        expectThrows(SnapshotException.class, firstFailedSnapshotFuture::actionGet);\n+        expectThrows(SnapshotException.class, secondFailedSnapshotFuture::actionGet);\n+        expectThrows(RepositoryException.class, failedDeleteFuture::actionGet);\n+\n+        awaitNoMoreRunningOperations();\n+    }\n+\n+    public void testQueuedOperationsAndBrokenRepoOnMasterFailOver() throws Exception {\n+        disableRepoConsistencyCheck(\"This test corrupts the repository on purpose\");\n+\n+        internalCluster().startMasterOnlyNodes(3);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        final Path repoPath = randomRepoPath();\n+        createRepository(repoName, \"mock\", repoPath);\n+        createIndexWithContent(\"index-one\");\n+        createNSnapshots(repoName, randomIntBetween(2, 5));\n+\n+        final long generation = getRepositoryData(repoName).getGenId();\n+\n+        startAndBlockOnDeleteSnapshot(repoName, \"*\");\n+\n+        corruptIndexN(repoPath, generation);\n+\n+        client().admin().cluster().prepareCreateSnapshot(repoName, \"snapshot-three\").setWaitForCompletion(false).get();\n+\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDeleteFromNonMasterClient(repoName, \"*\");\n+        awaitNDeletionsInProgress(2);\n+\n+        internalCluster().stopCurrentMasterNode();\n+        ensureStableCluster(3);\n+\n+        awaitNoMoreRunningOperations();\n+        expectThrows(RepositoryException.class, deleteFuture::actionGet);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "originalPosition": 692}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY4MjI2OQ==", "bodyText": "nit: assertConsistency -> assertNoConcurrentDeletionsForSameRepository() ?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452682269", "createdAt": "2020-07-10T07:52:35Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -61,7 +67,18 @@ public static SnapshotDeletionsInProgress of(List<SnapshotDeletionsInProgress.En\n     }\n \n     public SnapshotDeletionsInProgress(StreamInput in) throws IOException {\n-        this.entries = Collections.unmodifiableList(in.readList(Entry::new));\n+        this(in.readList(Entry::new));\n+    }\n+\n+    private static boolean assertConsistency(List<Entry> entries) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY4NjQ1Ng==", "bodyText": "I'm not sure to understand why we catch and rethrow here", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452686456", "createdAt": "2020-07-10T08:00:31Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -438,16 +477,34 @@ public static State fromValue(byte value) {\n \n     private final List<Entry> entries;\n \n+    private static boolean assertConsistentEntries(List<Entry> entries) {\n+        final Map<String, Set<ShardId>> assignedShardsByRepo = new HashMap<>();\n+        for (Entry entry : entries) {\n+            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shard : entry.shards()) {\n+                if (shard.value.isActive()) {\n+                    assert assignedShardsByRepo.computeIfAbsent(entry.repository(), k -> new HashSet<>()).add(shard.key) :\n+                            \"Found duplicate shard assignments in \" + entries;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n     public static SnapshotsInProgress of(List<Entry> entries) {\n         if (entries.isEmpty()) {\n             return EMPTY;\n         }\n         return new SnapshotsInProgress(Collections.unmodifiableList(entries));\n     }\n \n-    private SnapshotsInProgress(List<Entry> entries) {\n-        this.entries = entries;\n-    }\n+    private SnapshotsInProgress(List <Entry> entries) {\n+            this.entries = entries;\n+            try {\n+                assert assertConsistentEntries(entries);\n+            } catch (AssertionError e) {\n+                throw e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5MTIwNA==", "bodyText": "allow -> allowed", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452691204", "createdAt": "2020-07-10T08:10:18Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -142,6 +158,18 @@\n \n     private final TransportService transportService;\n \n+    private final OngoingRepositoryOperations repositoryOperations = new OngoingRepositoryOperations();\n+\n+    /**\n+     * Setting that specifies the maximum number of allow concurrent snapshot create and delete operations in the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5MjYzMg==", "bodyText": "When backporting, we could maybe indicate in the error message that concurrent snapshot/deletions are possible in version 7.9?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452692632", "createdAt": "2020-07-10T08:13:17Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -204,26 +235,36 @@ public ClusterState execute(ClusterState currentState) {\n                     throw new InvalidSnapshotNameException(\n                             repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n                 }\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries();\n+                if (runningSnapshots.stream().anyMatch(s -> {\n+                    final Snapshot running = s.snapshot();\n+                    return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+                })) {\n+                    throw new InvalidSnapshotNameException(\n+                            repository.getMetadata().name(), snapshotName, \"snapshot with the same name is already in-progress\");\n+                }\n                 validate(repositoryName, snapshotName, currentState);\n+                final boolean concurrentOperationsAllowed = currentState.nodes().getMinNodeVersion().onOrAfter(FULL_CONCURRENCY_VERSION);\n                 final SnapshotDeletionsInProgress deletionsInProgress =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletionsInProgress.hasDeletionsInProgress()) {\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                if (deletionsInProgress.hasDeletionsInProgress() && concurrentOperationsAllowed == false) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjY5NTA5Nw==", "bodyText": "What happen if multiple snapshot operations are started but the maxConcurrentOperations settings is updated to a value lower than the current number of concurrent ops? Would it still be possible to enque more ops?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r452695097", "createdAt": "2020-07-10T08:18:16Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -204,26 +235,36 @@ public ClusterState execute(ClusterState currentState) {\n                     throw new InvalidSnapshotNameException(\n                             repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n                 }\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries();\n+                if (runningSnapshots.stream().anyMatch(s -> {\n+                    final Snapshot running = s.snapshot();\n+                    return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+                })) {\n+                    throw new InvalidSnapshotNameException(\n+                            repository.getMetadata().name(), snapshotName, \"snapshot with the same name is already in-progress\");\n+                }\n                 validate(repositoryName, snapshotName, currentState);\n+                final boolean concurrentOperationsAllowed = currentState.nodes().getMinNodeVersion().onOrAfter(FULL_CONCURRENCY_VERSION);\n                 final SnapshotDeletionsInProgress deletionsInProgress =\n-                    currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n-                if (deletionsInProgress.hasDeletionsInProgress()) {\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                if (deletionsInProgress.hasDeletionsInProgress() && concurrentOperationsAllowed == false) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");\n                 }\n                 final RepositoryCleanupInProgress repositoryCleanupInProgress =\n-                    currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n+                        currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n                 if (repositoryCleanupInProgress.hasCleanupInProgress()) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n                 }\n-                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n                 // Fail if there are any concurrently running snapshots. The only exception to this being a snapshot in INIT state from a\n                 // previous master that we can simply ignore and remove from the cluster state because we would clean it up from the\n                 // cluster state anyway in #applyClusterState.\n-                if (snapshots.entries().stream().anyMatch(entry -> entry.state() != State.INIT)) {\n+                if (concurrentOperationsAllowed == false && runningSnapshots.stream().anyMatch(entry -> entry.state() != State.INIT)) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, \" a snapshot is already running\");\n                 }\n+                ensureBelowConcurrencyLimit(repositoryName, snapshotName, snapshots, deletionsInProgress);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897e6d1160b52aa70db2365907f0e8d9c19347f7"}, "originalPosition": 124}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "382962b4026bf4c373e9f0e557c8588922fa0b87", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/382962b4026bf4c373e9f0e557c8588922fa0b87", "committedDate": "2020-07-10T11:53:28Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e1a0db6e10e5882ddbc0b6ca80560e2bff38df5", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/9e1a0db6e10e5882ddbc0b6ca80560e2bff38df5", "committedDate": "2020-07-10T12:07:39Z", "message": "review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "346f306de9f8644f0a0b995377a6364c78f8abf5", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/346f306de9f8644f0a0b995377a6364c78f8abf5", "committedDate": "2020-06-15T09:10:38Z", "message": "Fully Concurrent Snapshots\n\nImplements fully concurrent snapshot operations. See documentation changes\nto snapshot package level JavaDoc for details."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "923aebd3be2a2efb28f67871367f7db3f92fe4b7", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/923aebd3be2a2efb28f67871367f7db3f92fe4b7", "committedDate": "2020-06-15T09:08:57Z", "message": "docs"}, "afterCommit": {"oid": "346f306de9f8644f0a0b995377a6364c78f8abf5", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/346f306de9f8644f0a0b995377a6364c78f8abf5", "committedDate": "2020-06-15T09:10:38Z", "message": "Fully Concurrent Snapshots\n\nImplements fully concurrent snapshot operations. See documentation changes\nto snapshot package level JavaDoc for details."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwNDcyMDEz", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-430472013", "createdAt": "2020-06-15T09:28:01Z", "commit": {"oid": "346f306de9f8644f0a0b995377a6364c78f8abf5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwOToyODowMVrOGjqUGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwOToyODowMVrOGjqUGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA0NjYxNw==", "bodyText": "All scenarios covered by this test become obsolete. The actual premise of this test (checking that we don't dead-lock from blocked threads) is covered by the fact that SnapshotResiliencyTests work for the most part anyway.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r440046617", "createdAt": "2020-06-15T09:28:01Z", "author": {"login": "original-brownbear"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/MinThreadsSnapshotRestoreIT.java", "diffHunk": "@@ -1,155 +0,0 @@\n-/*\n- * Licensed to Elasticsearch under one or more contributor\n- * license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright\n- * ownership. Elasticsearch licenses this file to you under\n- * the Apache License, Version 2.0 (the \"License\"); you may\n- * not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-\n-package org.elasticsearch.snapshots;\n-\n-import org.elasticsearch.action.ActionFuture;\n-import org.elasticsearch.action.support.master.AcknowledgedResponse;\n-import org.elasticsearch.common.settings.Settings;\n-import org.elasticsearch.common.unit.TimeValue;\n-import org.elasticsearch.plugins.Plugin;\n-import org.elasticsearch.repositories.RepositoriesService;\n-import org.elasticsearch.snapshots.mockstore.MockRepository;\n-\n-import java.util.Collection;\n-import java.util.Collections;\n-\n-import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n-import static org.hamcrest.Matchers.containsString;\n-\n-/**\n- * Tests for snapshot/restore that require at least 2 threads available\n- * in the thread pool (for example, tests that use the mock repository that\n- * block on master).\n- */\n-public class MinThreadsSnapshotRestoreIT extends AbstractSnapshotIntegTestCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "346f306de9f8644f0a0b995377a6364c78f8abf5"}, "originalPosition": 41}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fffc109c03ae853f5a6f870b9a49d5f0b92fa1f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/0fffc109c03ae853f5a6f870b9a49d5f0b92fa1f", "committedDate": "2020-06-15T09:37:46Z", "message": "cleanup todo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "26e4bced273146494dbb9cf5720944d26ddf5265", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/26e4bced273146494dbb9cf5720944d26ddf5265", "committedDate": "2020-06-15T09:42:56Z", "message": "docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7076a0f88008de717ff836d0ee00aa6d68806ba5", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/7076a0f88008de717ff836d0ee00aa6d68806ba5", "committedDate": "2020-06-15T09:45:27Z", "message": "cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7cd1c4e174648f4f00997d4d13fd84f650af89b8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/7cd1c4e174648f4f00997d4d13fd84f650af89b8", "committedDate": "2020-06-15T09:48:18Z", "message": "doc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9b894c8be2321730e3e2ee77cd2528133a13da2", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a9b894c8be2321730e3e2ee77cd2528133a13da2", "committedDate": "2020-06-15T09:51:14Z", "message": "reduce noise"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwNDkzMzY5", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-430493369", "createdAt": "2020-06-15T09:56:06Z", "commit": {"oid": "a9b894c8be2321730e3e2ee77cd2528133a13da2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwOTo1NjowNlrOGjrT5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwOTo1NjowNlrOGjrT5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA2Mjk1MA==", "bodyText": "This fixes a tricky bug that might have been trouble for aborts with pre-7.6 repository metadata on S3 and generally wastes a lot of shard status updates. I'll fix this in a separate PR right away, just adding a node here to not forget to handle that first.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r440062950", "createdAt": "2020-06-15T09:56:06Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "diffHunk": "@@ -232,7 +232,7 @@ private void startNewSnapshots(SnapshotsInProgress snapshotsInProgress) {\n                     if (snapshotStatus == null) {\n                         // due to CS batching we might have missed the INIT state and straight went into ABORTED\n                         // notify master that abort has completed by moving to FAILED\n-                        if (shard.value.state() == ShardState.ABORTED) {\n+                        if (shard.value.state() == ShardState.ABORTED && localNodeId.equals(shard.value.nodeId())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a9b894c8be2321730e3e2ee77cd2528133a13da2"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "98516c6a2ba50a6237d94ab95c01f767c21d41cb", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/98516c6a2ba50a6237d94ab95c01f767c21d41cb", "committedDate": "2020-06-15T09:59:13Z", "message": "less noise"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "37d439f3254ad08e6515f4e36c97507dff715499", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/37d439f3254ad08e6515f4e36c97507dff715499", "committedDate": "2020-06-15T10:50:34Z", "message": "fix test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d10c4c505ee209f514b29c7a3a7e412883be077", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/8d10c4c505ee209f514b29c7a3a7e412883be077", "committedDate": "2020-06-15T11:04:20Z", "message": "cover primary fail-over"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca110eceade017a0bd3e530b2604d101f3a8df19", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/ca110eceade017a0bd3e530b2604d101f3a8df19", "committedDate": "2020-06-15T11:38:18Z", "message": "drier tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75a4aaddf1350e075fd1bfd60e2e34006fd02a8c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/75a4aaddf1350e075fd1bfd60e2e34006fd02a8c", "committedDate": "2020-06-15T11:47:15Z", "message": "fix test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "37e799430b19d86e96d582749249438d19dbbfcc", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/37e799430b19d86e96d582749249438d19dbbfcc", "committedDate": "2020-06-15T11:54:19Z", "message": "update comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a5eed5af63247266515abc432bd7b8e35f6f11d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/8a5eed5af63247266515abc432bd7b8e35f6f11d", "committedDate": "2020-06-15T12:04:58Z", "message": "drier"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "457c4438783b3ac466874c169805d71c99fc783b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/457c4438783b3ac466874c169805d71c99fc783b", "committedDate": "2020-06-15T12:25:28Z", "message": "make things nicer looking"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66c642857c5ea4bdf69e61d775503d79c91539fe", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/66c642857c5ea4bdf69e61d775503d79c91539fe", "committedDate": "2020-06-15T12:35:46Z", "message": "drop pointless short-circuit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72dbbcfd8f6c350d30374903ac9b7e1256d62993", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/72dbbcfd8f6c350d30374903ac9b7e1256d62993", "committedDate": "2020-06-15T12:40:19Z", "message": "doc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc267dcd23b13958eeb01308d6219d683ba16e8d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fc267dcd23b13958eeb01308d6219d683ba16e8d", "committedDate": "2020-06-15T12:59:51Z", "message": "moar test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa5176f102fe07b5e3488f32baca30ada7cb80fb", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fa5176f102fe07b5e3488f32baca30ada7cb80fb", "committedDate": "2020-06-15T16:24:55Z", "message": "another corner case fixed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c976fce0eafb1eda86ba79fd25bf6911f466af38", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/c976fce0eafb1eda86ba79fd25bf6911f466af38", "committedDate": "2020-06-15T16:25:01Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6cb598911b2d6e116d3b57f28bdbf74e59d0c1a6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6cb598911b2d6e116d3b57f28bdbf74e59d0c1a6", "committedDate": "2020-06-15T20:42:11Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "075a0b74a2ed638574e88d998680bfa652734f17", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/075a0b74a2ed638574e88d998680bfa652734f17", "committedDate": "2020-06-15T21:12:27Z", "message": "more docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e27ec41367b339ce7c155233090b7b16247d35ae", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e27ec41367b339ce7c155233090b7b16247d35ae", "committedDate": "2020-06-15T21:13:36Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "711770757a75a1822f1a4e59f71875c50c9f95f9", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/711770757a75a1822f1a4e59f71875c50c9f95f9", "committedDate": "2020-06-16T07:48:29Z", "message": "save some more repo data loading"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd142a9602e21fc608975f0bb028a5bad8a1631c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cd142a9602e21fc608975f0bb028a5bad8a1631c", "committedDate": "2020-06-16T07:52:31Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34f934551f1ec665538cebc342c0204cfcb05732", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/34f934551f1ec665538cebc342c0204cfcb05732", "committedDate": "2020-06-16T10:07:08Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1cdab7555676144c07d3ba03c4ee6dc26eaf3f84", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/1cdab7555676144c07d3ba03c4ee6dc26eaf3f84", "committedDate": "2020-06-16T10:10:37Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de9a3f77dcd0009a8ebdc90fb25036ac956c0dea", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/de9a3f77dcd0009a8ebdc90fb25036ac956c0dea", "committedDate": "2020-06-16T12:30:19Z", "message": "moar corner cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "166e639b9013eaaa30eb17918cb4edf3649dd279", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/166e639b9013eaaa30eb17918cb4edf3649dd279", "committedDate": "2020-06-16T13:32:10Z", "message": "cleanup tests more"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cadad70d5bf03d0d88ecace425225ce506646b11", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cadad70d5bf03d0d88ecace425225ce506646b11", "committedDate": "2020-06-16T14:51:30Z", "message": "cleanups"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/0e726a009a0b6a4fa50871f346a7c3b912fd6e24", "committedDate": "2020-06-16T14:51:41Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNjE4Nzg3", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-431618787", "createdAt": "2020-06-16T15:17:14Z", "commit": {"oid": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNToxNzoxNFrOGkgY_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNToxNzoxNFrOGkgY_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkzMjYwNg==", "bodyText": "This is a scenario that was complicated before but became very complicated now, both fail-over on delete and create. We could put more effort into this by doing things like #54350 but I think it's not a priority relative to getting concurrent operations work functionally so I just went with the brute-force fail everything on master fail-over approach here as that's the current behavior for single snapshots.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r440932606", "createdAt": "2020-06-16T15:17:14Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1441,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            synchronized (currentlyFinalizing) {\n+                boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+                assert added;\n+                Repository repository = repositoriesService.repository(deleteEntry.repository());\n+                final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+                assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                        \"incorrect state for entry [\" + deleteEntry + \"]\";\n+                repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                        snapshotIds,\n+                        repositoryStateId,\n+                        minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                        ActionListener.wrap(updatedRepoData -> {\n+                                    logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                    removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                                }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                        )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+            }\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {\n+                                                // TODO: this could be made more efficient by not recomputing assignments for shards that\n+                                                //       are already in reAssignedShardIds\n+                                                final ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardAssignments =\n+                                                        shards(currentState, entry.indices(),\n+                                                                entry.version().onOrAfter(SHARD_GEN_IN_REPO_DATA_VERSION),\n+                                                                repositoryData, entry.repository(), true);\n+                                                final ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus>\n+                                                        updatedAssignmentsBuilder = ImmutableOpenMap.builder();\n+                                                for (ObjectCursor<ShardId> key : entry.shards().keys()) {\n+                                                    final ShardSnapshotStatus existing = entry.shards().get(key.value);\n+                                                    if (existing.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)\n+                                                            && reAssignedShardIds.add(key.value)) {\n+                                                        updatedAssignmentsBuilder.put(key.value, shardAssignments.get(key.value));\n+                                                    } else {\n+                                                        updatedAssignmentsBuilder.put(key.value, existing);\n+                                                    }\n+                                                }\n+                                                snapshotEntries.add(entry.withShards(updatedAssignmentsBuilder.build()));\n+                                                updatedQueuedSnapshot = true;\n+                                                break;\n+                                            }\n+                                        }\n+                                        if (updatedQueuedSnapshot == false) {\n+                                            // Nothing to update in this snapshot so we just add it as is\n+                                            snapshotEntries.add(entry);\n+                                        }\n+                                    } else {\n+                                        // Entry is already completed so we will finalize it now that the delete doesn't block us after\n+                                        // this CS update finishes\n+                                        newFinalizations.add(entry);\n+                                        snapshotEntries.add(entry);\n+                                    }\n+                                } else {\n+                                    // Entry is for another repository we just keep it as is\n+                                    snapshotEntries.add(entry);\n+                                }\n+                            }\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress(snapshotEntries);\n+                        }\n+                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions)\n+                                .putCustom(SnapshotsInProgress.TYPE, updatedSnapshotsInProgress).build();\n                     }\n                 }\n                 return currentState;\n             }\n \n             @Override\n             public void onFailure(String source, Exception e) {\n-                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", snapshotIds), e);\n-                if (listener != null) {\n-                    listener.onFailure(e);\n+                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", deleteEntry), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24"}, "originalPosition": 914}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMxNjIzMjg3", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-431623287", "createdAt": "2020-06-16T15:21:53Z", "commit": {"oid": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNToyMTo1M1rOGkgr4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNToyMTo1M1rOGkgr4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkzNzQ0MA==", "bodyText": "Here it was easy to expand coverage but unfortunately SnapshotResiliencyTests don't really cover repository exception handling so that's where ConcurrentSnapshotITs come from.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r440937440", "createdAt": "2020-06-16T15:21:53Z", "author": {"login": "original-brownbear"}, "path": "server/src/test/java/org/elasticsearch/snapshots/SnapshotResiliencyTests.java", "diffHunk": "@@ -594,11 +582,16 @@ public void testBulkSnapshotDeleteWithAbort() {\n                 createIndexResponse -> client().admin().cluster().prepareCreateSnapshot(repoName, snapshotName)\n                         .setWaitForCompletion(true).execute(createSnapshotResponseStepListener));\n \n-        final StepListener<CreateSnapshotResponse> createOtherSnapshotResponseStepListener = new StepListener<>();\n+        final int inProgressSnapshots = randomIntBetween(1, 5);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "05d87183ba412dea5ddd36511ed9efeb5ff55cfc", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/05d87183ba412dea5ddd36511ed9efeb5ff55cfc", "committedDate": "2020-06-16T16:44:03Z", "message": "be smarter about failover handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf39da642d4a2f14a85dc8fb25b8e14e81cdb181", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cf39da642d4a2f14a85dc8fb25b8e14e81cdb181", "committedDate": "2020-06-16T17:19:10Z", "message": "fewer CS updates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7762343df83fe8f630d7a593823dbc12ccf2c72b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/7762343df83fe8f630d7a593823dbc12ccf2c72b", "committedDate": "2020-06-16T18:38:59Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e90de9dba7c9ae2dc51e164fc18be2b9e61f44fb", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e90de9dba7c9ae2dc51e164fc18be2b9e61f44fb", "committedDate": "2020-06-16T19:54:05Z", "message": "nicer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e69d0b72ce99a5f45c25cade22d442a6a8cdc454", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e69d0b72ce99a5f45c25cade22d442a6a8cdc454", "committedDate": "2020-06-16T20:15:14Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2abeda287ac168b509633c4763d1c6e9d78c52e4", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2abeda287ac168b509633c4763d1c6e9d78c52e4", "committedDate": "2020-06-16T20:34:59Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/c44bdb3e9ec001fff2f83efad144a48e5c990fd2", "committedDate": "2020-06-17T03:26:19Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDUyNDc5", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432052479", "createdAt": "2020-06-17T04:37:20Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDozNzoyMFrOGk1UNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDozNzoyMFrOGk1UNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3NTQ0NQ==", "bodyText": "I know this class is massive, but I did my best to get full coverage of all possible permutations of master fail-over and repository IO issues here (we definitely had some gaps in our testing without it even for single snapshots as this one caught #58214).", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441275445", "createdAt": "2020-06-17T04:37:20Z", "author": {"login": "original-brownbear"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/ConcurrentSnapshotsIT.java", "diffHunk": "@@ -0,0 +1,889 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import com.carrotsearch.hppc.cursors.ObjectCursor;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.StepListener;\n+import org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest;\n+import org.elasticsearch.action.support.GroupedActionListener;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ClusterStateObserver;\n+import org.elasticsearch.cluster.SnapshotDeletionsInProgress;\n+import org.elasticsearch.cluster.SnapshotsInProgress;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.util.concurrent.UncategorizedExecutionException;\n+import org.elasticsearch.discovery.AbstractDisruptionTestCase;\n+import org.elasticsearch.node.NodeClosedException;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.repositories.RepositoryData;\n+import org.elasticsearch.repositories.RepositoryException;\n+import org.elasticsearch.repositories.blobstore.BlobStoreRepository;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.disruption.NetworkDisruption;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.threadpool.ThreadPool;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_REPLICAS;\n+import static org.elasticsearch.cluster.metadata.IndexMetadata.SETTING_NUMBER_OF_SHARDS;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertFileExists;\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ConcurrentSnapshotsIT extends AbstractSnapshotIntegTestCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDUyOTcy", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432052972", "createdAt": "2020-06-17T04:39:08Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDozOTowOVrOGk1Vwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDozOTowOVrOGk1Vwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3NTg0Mg==", "bodyText": "Needed the new id here to get some fixed coordinate/key for a delete operation to connect completion listeners to when batching deletes up.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441275842", "createdAt": "2020-06-17T04:39:09Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -176,17 +231,23 @@ public String toString() {\n     public static final class Entry implements Writeable, RepositoryOperation {\n         private final List<SnapshotId> snapshots;\n         private final String repoName;\n+        private final State state;\n         private final long startTime;\n         private final long repositoryStateId;\n+        private final String uuid;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 119}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDU1NDYz", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432055463", "createdAt": "2020-06-17T04:47:35Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDo0NzozNVrOGk1eMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDo0NzozNVrOGk1eMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODAwMQ==", "bodyText": "Needed to know whether or not a delete can be executed yet or is still open to have snapshot ids added to it. Somewhat analogous to snapshot in progress state.\nTechnically, a boolean would be enough here. But I'd like to keep that state enum here to maybe further enhance the state machine with a cleanup step. As an improvement for concurrent operations, we could already move on to the next snapshot operation after master is done writing the new metadata during a delete but before it deleted all the unreferrenced blobs. It would be nice for the cleanup to still be recorded in the cluster state though as otherwise back-to-back deletes might list out and delete the same blobs concurrently+redundantly. It would just be easier to create that follow-up if we didn't go with a flag here now.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441278001", "createdAt": "2020-06-17T04:47:35Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -176,17 +231,23 @@ public String toString() {\n     public static final class Entry implements Writeable, RepositoryOperation {\n         private final List<SnapshotId> snapshots;\n         private final String repoName;\n+        private final State state;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDU2MjAy", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432056202", "createdAt": "2020-06-17T04:50:12Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDo1MDoxMlrOGk1gtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDo1MDoxMlrOGk1gtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODY0NA==", "bodyText": "If two or more deletes share overlapping snapshots but couldn't be batched into one we need to \"clean up\" deleted snapshots from existing deletes so that the remaining snapshots don't fail trying to delete already deleted snapshots.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441278644", "createdAt": "2020-06-17T04:50:12Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -76,13 +92,52 @@ public SnapshotDeletionsInProgress withAddedEntry(Entry entry) {\n     }\n \n     /**\n-     * Returns a new instance of {@link SnapshotDeletionsInProgress} which removes\n-     * the given entry from the invoking instance.\n+     * Returns a new instance of {@link SnapshotDeletionsInProgress} that has the entry with the given {@code deleteUUID} removed from its\n+     * entries.\n      */\n-    public SnapshotDeletionsInProgress withRemovedEntry(Entry entry) {\n-        List<Entry> entries = new ArrayList<>(getEntries());\n-        entries.remove(entry);\n-        return new SnapshotDeletionsInProgress(entries);\n+    public SnapshotDeletionsInProgress withRemovedEntry(String deleteUUID) {\n+        List<Entry> updatedEntries = new ArrayList<>(entries.size() - 1);\n+        boolean removed = false;\n+        for (Entry entry : entries) {\n+            if (entry.uuid().equals(deleteUUID)) {\n+                removed = true;\n+            } else {\n+                updatedEntries.add(entry);\n+            }\n+        }\n+        return removed ? new SnapshotDeletionsInProgress(updatedEntries) : this;\n+    }\n+\n+    public SnapshotDeletionsInProgress withRemovedSnapshotIds(String repository, Collection<SnapshotId> snapshotIds) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDU2NDc4", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432056478", "createdAt": "2020-06-17T04:51:08Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDo1MTowOFrOGk1hwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDo1MTowOFrOGk1hwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODkxMw==", "bodyText": "If we fail loading repository data for a repository but have queued up deletes we just drop all the deletes for the repo on failure.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441278913", "createdAt": "2020-06-17T04:51:08Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -76,13 +92,52 @@ public SnapshotDeletionsInProgress withAddedEntry(Entry entry) {\n     }\n \n     /**\n-     * Returns a new instance of {@link SnapshotDeletionsInProgress} which removes\n-     * the given entry from the invoking instance.\n+     * Returns a new instance of {@link SnapshotDeletionsInProgress} that has the entry with the given {@code deleteUUID} removed from its\n+     * entries.\n      */\n-    public SnapshotDeletionsInProgress withRemovedEntry(Entry entry) {\n-        List<Entry> entries = new ArrayList<>(getEntries());\n-        entries.remove(entry);\n-        return new SnapshotDeletionsInProgress(entries);\n+    public SnapshotDeletionsInProgress withRemovedEntry(String deleteUUID) {\n+        List<Entry> updatedEntries = new ArrayList<>(entries.size() - 1);\n+        boolean removed = false;\n+        for (Entry entry : entries) {\n+            if (entry.uuid().equals(deleteUUID)) {\n+                removed = true;\n+            } else {\n+                updatedEntries.add(entry);\n+            }\n+        }\n+        return removed ? new SnapshotDeletionsInProgress(updatedEntries) : this;\n+    }\n+\n+    public SnapshotDeletionsInProgress withRemovedSnapshotIds(String repository, Collection<SnapshotId> snapshotIds) {\n+        boolean changed = false;\n+        List<Entry> updatedEntries = new ArrayList<>(entries.size());\n+        for (Entry entry : entries) {\n+            if (entry.repository().equals(repository)) {\n+                final List<SnapshotId> updatedSnapshotIds = new ArrayList<>(entry.getSnapshots());\n+                if (updatedSnapshotIds.removeAll(snapshotIds)) {\n+                    changed = true;\n+                    updatedEntries.add(entry.withSnapshots(updatedSnapshotIds));\n+                } else {\n+                    updatedEntries.add(entry);\n+                }\n+            } else {\n+                updatedEntries.add(entry);\n+            }\n+        }\n+        return changed ? new SnapshotDeletionsInProgress(updatedEntries) : this;\n+    }\n+\n+    public SnapshotDeletionsInProgress withRemovedRepository(String repository) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 98}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDU3OTc2", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432057976", "createdAt": "2020-06-17T04:56:24Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDo1NjoyNFrOGk1msw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNDo1NjoyNFrOGk1msw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MDE3OQ==", "bodyText": "It's somewhat questionable to even continue tracking the generation of an operation at this point since it's always equal to the safe repository generation (except for the corner case of having just mounted the repository maybe ...). I added the generation updating here for now to keep the change-set small but I'd look into removing the generation from deletes and snapshots in a follow-up (technically easy ... but BwC of that change is tricky).", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441280179", "createdAt": "2020-06-17T04:56:24Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1616,6 +1615,50 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n         }, listener::onFailure);\n     }\n \n+    /**\n+     * Updates the repository generation that running deletes and snapshot finalizations will be based on for this repository if any such\n+     * operations are found in the cluster state while setting the safe repository generation.\n+     *\n+     * @param state  cluster state to update\n+     * @param oldGen previous safe repository generation\n+     * @param newGen new safe repository generation\n+     * @return updated cluster state\n+     */\n+    private ClusterState updateRepositoryGenerations(ClusterState state, long oldGen, long newGen) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 85}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDU5MjQ4", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432059248", "createdAt": "2020-06-17T05:00:38Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNTowMDozOFrOGk1qwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNTowMDozOFrOGk1qwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MTIxNw==", "bodyText": "This is somewhat stupid, but I made this work the same way shard state updates are processed right now (setting success if all the shards completed). A follow-up to simplify and speed things up would be to simply remove snapshots that haven't done any work yet here right away instead of going through the redundant cycle of finalizing them and then deleting them again right away.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441281217", "createdAt": "2020-06-17T05:00:38Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1046,25 +1247,76 @@ public ClusterState execute(ClusterState currentState) {\n                         }\n                     }\n                 }\n-                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n-                if (snapshots != null && snapshots.entries().isEmpty() == false) {\n-                    // However other snapshots are running - cannot continue\n-                    throw new ConcurrentSnapshotExecutionException(\n-                            repoName, snapshotIds.toString(), \"another snapshot is currently running cannot delete\");\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n+                final SnapshotsInProgress updatedSnapshots;\n+\n+                if (snapshots == null) {\n+                    updatedSnapshots = new SnapshotsInProgress();\n+                } else if (minNodeVersion.onOrAfter(FULL_CONCURRENCY_VERSION)) {\n+                    updatedSnapshots = new SnapshotsInProgress(snapshots.entries().stream()\n+                            .map(existing -> {\n+                                // snapshot is started - mark every non completed shard as aborted\n+                                if (existing.state() == State.STARTED && snapshotIds.contains(existing.snapshot().getSnapshotId())) {\n+                                    final ImmutableOpenMap<ShardId, ShardSnapshotStatus> abortedShards = abortEntry(existing);\n+                                    final boolean isCompleted = completed(abortedShards.values());\n+                                    final SnapshotsInProgress.Entry abortedEntry = new SnapshotsInProgress.Entry(\n+                                            existing, isCompleted ? State.SUCCESS : State.ABORTED, abortedShards,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 693}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDYwNjY5", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432060669", "createdAt": "2020-06-17T05:05:06Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNTowNTowNlrOGk1vVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNTowNTowNlrOGk1vVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MjM4OA==", "bodyText": "This is fairly complex code now unfortunately since we have to deal with assigning new work where possible across queued up snapshots, truncate queued up deletes and handle the complex failure situation of having failed to read repository data before executing the delete.\nI did my best to comment reasonably and tried to reuse the shard assignment logic used during snapshot create to keep it simple instead of optimal (see added TODO about optimizing this logic).", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441282388", "createdAt": "2020-06-17T05:05:06Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 910}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMDYxNzAz", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432061703", "createdAt": "2020-06-17T05:08:37Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNTowODozN1rOGk1ylg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNTowODozN1rOGk1ylg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MzIyMg==", "bodyText": "Same as with snapshot finalizations, much easier to keep things consistent (and faster) by passing the repository data through the listeners instead of having to reload it over and over.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441283222", "createdAt": "2020-06-17T05:08:37Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/Repository.java", "diffHunk": "@@ -152,7 +152,7 @@ void finalizeSnapshot(SnapshotId snapshotId, ShardGenerations shardGenerations,\n      * @param listener              completion listener\n      */\n     void deleteSnapshots(Collection<SnapshotId> snapshotIds, long repositoryStateId, Version repositoryMetaVersion,\n-                         ActionListener<Void> listener);\n+                         ActionListener<RepositoryData> listener);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5cc68d4e6717c57eb4f29a284f74e9d0411b845d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/5cc68d4e6717c57eb4f29a284f74e9d0411b845d", "committedDate": "2020-06-17T09:55:12Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72d12f7a3bffabd455fa85b40dc55b16e44ab09e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/72d12f7a3bffabd455fa85b40dc55b16e44ab09e", "committedDate": "2020-06-17T10:37:29Z", "message": "reproducer"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyMTE0MzEy", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-432114312", "createdAt": "2020-06-17T07:06:21Z", "commit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwNzowNjoyMVrOGk4YqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMTo1Mjo0NVrOGlCQXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyNTczNw==", "bodyText": "for for", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441325737", "createdAt": "2020-06-17T07:06:21Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -51,10 +54,23 @@\n \n     public SnapshotDeletionsInProgress(List<Entry> entries) {\n         this.entries = Collections.unmodifiableList(entries);\n+        assert entries.size() == entries.stream().map(Entry::uuid).distinct().count() : \"Found duplicate UUIDs in entries \" + entries;\n+        assert assertConsistency(entries);\n     }\n \n     public SnapshotDeletionsInProgress(StreamInput in) throws IOException {\n-        this.entries = Collections.unmodifiableList(in.readList(Entry::new));\n+        this(Collections.unmodifiableList(in.readList(Entry::new)));\n+    }\n+\n+    private static boolean assertConsistency(List<Entry> entries) {\n+        final Set<String> activeRepositories = new HashSet<>();\n+        for (Entry entry : entries) {\n+            if (entry.state() == State.META_DATA) {\n+                final boolean added = activeRepositories.add(entry.repository());\n+                assert added : \"Found multiple running deletes for for a single repository in \" + entries;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyOTgxNg==", "bodyText": "can you add some docs here as to what these States mean?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441329816", "createdAt": "2020-06-17T07:14:36Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -257,4 +361,26 @@ public long repositoryStateId() {\n             return repositoryStateId;\n         }\n     }\n+\n+    public enum State {\n+        WAITING((byte) 0),\n+        META_DATA((byte) 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzMjMyMw==", "bodyText": "Instead of doing the lookup twice, perhaps just get the value and check if null", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441332323", "createdAt": "2020-06-17T07:19:29Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/RepositoryData.java", "diffHunk": "@@ -410,15 +410,22 @@ public IndexId resolveIndexId(final String indexName) {\n     /**\n      * Resolve the given index names to index ids, creating new index ids for\n      * new indices in the repository.\n+     *\n+     * @param indicesToResolve names of indices to resolve\n+     * @param inFlightIds      name to index mapping for currently in-flight snapshots not yet in the repository data to fall back to\n      */\n-    public List<IndexId> resolveNewIndices(final List<String> indicesToResolve) {\n+    public List<IndexId> resolveNewIndices(List<String> indicesToResolve, Map<String, IndexId> inFlightIds) {\n         List<IndexId> snapshotIndices = new ArrayList<>();\n         for (String index : indicesToResolve) {\n             final IndexId indexId;\n             if (indices.containsKey(index)) {\n                 indexId = indices.get(index);\n             } else {\n-                indexId = new IndexId(index, UUIDs.randomBase64UUID());\n+                if (inFlightIds.containsKey(index)) {\n+                    indexId = inFlightIds.get(index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzMzE0NQ==", "bodyText": "how do we expect the repositoryStateId to compare across the operations? Will it always be the same?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441333145", "createdAt": "2020-06-17T07:21:04Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -425,7 +425,6 @@ public void updateState(ClusterState state) {\n \n     private long bestGeneration(Collection<? extends RepositoryOperation> operations) {\n         final String repoName = metadata.name();\n-        assert operations.size() <= 1 : \"Assumed one or no operations but received \" + operations;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMzOTU4Mg==", "bodyText": "What do you mean by \"cover repository exception handling\"?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441339582", "createdAt": "2020-06-17T07:32:25Z", "author": {"login": "ywelsch"}, "path": "server/src/test/java/org/elasticsearch/snapshots/SnapshotResiliencyTests.java", "diffHunk": "@@ -594,11 +582,16 @@ public void testBulkSnapshotDeleteWithAbort() {\n                 createIndexResponse -> client().admin().cluster().prepareCreateSnapshot(repoName, snapshotName)\n                         .setWaitForCompletion(true).execute(createSnapshotResponseStepListener));\n \n-        final StepListener<CreateSnapshotResponse> createOtherSnapshotResponseStepListener = new StepListener<>();\n+        final int inProgressSnapshots = randomIntBetween(1, 5);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkzNzQ0MA=="}, "originalCommit": {"oid": "0e726a009a0b6a4fa50871f346a7c3b912fd6e24"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM0NzQ0Mg==", "bodyText": "should we disallow concurrent operations in this case where there are still entries with UNKNOWN_REPO_GEN? Just eliminates one more odd configuration to handle?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441347442", "createdAt": "2020-06-17T07:45:46Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -666,8 +753,36 @@ private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata) {\n             return;\n         }\n         final Snapshot snapshot = entry.snapshot();\n+        if (entry.repositoryStateId() == RepositoryData.UNKNOWN_REPO_GEN) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 302}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1MTUxOQ==", "bodyText": "This pattern is repeated quite often. I wonder if we should provide an overloaded method ClusterState.custom(String name, Supplier<Custom> defaultSupplier)", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441351519", "createdAt": "2020-06-17T07:52:50Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1046,25 +1247,76 @@ public ClusterState execute(ClusterState currentState) {\n                         }\n                     }\n                 }\n-                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n-                if (snapshots != null && snapshots.entries().isEmpty() == false) {\n-                    // However other snapshots are running - cannot continue\n-                    throw new ConcurrentSnapshotExecutionException(\n-                            repoName, snapshotIds.toString(), \"another snapshot is currently running cannot delete\");\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n+                final SnapshotsInProgress updatedSnapshots;\n+\n+                if (snapshots == null) {\n+                    updatedSnapshots = new SnapshotsInProgress();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 684}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NDI1Mw==", "bodyText": "can we assert something after doing all these transformations?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441354253", "createdAt": "2020-06-17T07:57:25Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {\n+                                                // TODO: this could be made more efficient by not recomputing assignments for shards that\n+                                                //       are already in reAssignedShardIds\n+                                                final ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardAssignments =\n+                                                        shards(currentState, entry.indices(),\n+                                                                entry.version().onOrAfter(SHARD_GEN_IN_REPO_DATA_VERSION),\n+                                                                repositoryData, entry.repository(), true);\n+                                                final ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus>\n+                                                        updatedAssignmentsBuilder = ImmutableOpenMap.builder();\n+                                                for (ObjectCursor<ShardId> key : entry.shards().keys()) {\n+                                                    final ShardSnapshotStatus existing = entry.shards().get(key.value);\n+                                                    if (existing.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)\n+                                                            && reAssignedShardIds.add(key.value)) {\n+                                                        updatedAssignmentsBuilder.put(key.value, shardAssignments.get(key.value));\n+                                                    } else {\n+                                                        updatedAssignmentsBuilder.put(key.value, existing);\n+                                                    }\n+                                                }\n+                                                snapshotEntries.add(entry.withShards(updatedAssignmentsBuilder.build()));\n+                                                updatedQueuedSnapshot = true;\n+                                                break;\n+                                            }\n+                                        }\n+                                        if (updatedQueuedSnapshot == false) {\n+                                            // Nothing to update in this snapshot so we just add it as is\n+                                            snapshotEntries.add(entry);\n+                                        }\n+                                    } else {\n+                                        // Entry is already completed so we will finalize it now that the delete doesn't block us after\n+                                        // this CS update finishes\n+                                        newFinalizations.add(entry);\n+                                        snapshotEntries.add(entry);\n+                                    }\n+                                } else {\n+                                    // Entry is for another repository we just keep it as is\n+                                    snapshotEntries.add(entry);\n+                                }\n+                            }\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress(snapshotEntries);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 981}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM1NjU0Nw==", "bodyText": "These are two big objects to hold onto :(\nThere's a risk here for a node to go OOM when holding on to CS for too long. Is there a way to avoid this? We would have to write the metadata prior to enqueuing here, right?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441356547", "createdAt": "2020-06-17T08:01:11Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1433,4 +1943,16 @@ protected ClusterBlockException checkBlock(UpdateIndexShardSnapshotStatusRequest\n             return null;\n         }\n     }\n+\n+    private static final class SnapshotFinalization {\n+\n+        private final SnapshotsInProgress.Entry entry;\n+\n+        private final Metadata metadata;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 1196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM5OTExNg==", "bodyText": "this is wrapping the list twice using unmodifiableList", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441399116", "createdAt": "2020-06-17T09:09:58Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -51,10 +54,23 @@\n \n     public SnapshotDeletionsInProgress(List<Entry> entries) {\n         this.entries = Collections.unmodifiableList(entries);\n+        assert entries.size() == entries.stream().map(Entry::uuid).distinct().count() : \"Found duplicate UUIDs in entries \" + entries;\n+        assert assertConsistency(entries);\n     }\n \n     public SnapshotDeletionsInProgress(StreamInput in) throws IOException {\n-        this.entries = Collections.unmodifiableList(in.readList(Entry::new));\n+        this(Collections.unmodifiableList(in.readList(Entry::new)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQwMjE5MQ==", "bodyText": "Do we just silently drop the deletes? How are listeners of those notified?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441402191", "createdAt": "2020-06-17T09:14:58Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -76,13 +92,52 @@ public SnapshotDeletionsInProgress withAddedEntry(Entry entry) {\n     }\n \n     /**\n-     * Returns a new instance of {@link SnapshotDeletionsInProgress} which removes\n-     * the given entry from the invoking instance.\n+     * Returns a new instance of {@link SnapshotDeletionsInProgress} that has the entry with the given {@code deleteUUID} removed from its\n+     * entries.\n      */\n-    public SnapshotDeletionsInProgress withRemovedEntry(Entry entry) {\n-        List<Entry> entries = new ArrayList<>(getEntries());\n-        entries.remove(entry);\n-        return new SnapshotDeletionsInProgress(entries);\n+    public SnapshotDeletionsInProgress withRemovedEntry(String deleteUUID) {\n+        List<Entry> updatedEntries = new ArrayList<>(entries.size() - 1);\n+        boolean removed = false;\n+        for (Entry entry : entries) {\n+            if (entry.uuid().equals(deleteUUID)) {\n+                removed = true;\n+            } else {\n+                updatedEntries.add(entry);\n+            }\n+        }\n+        return removed ? new SnapshotDeletionsInProgress(updatedEntries) : this;\n+    }\n+\n+    public SnapshotDeletionsInProgress withRemovedSnapshotIds(String repository, Collection<SnapshotId> snapshotIds) {\n+        boolean changed = false;\n+        List<Entry> updatedEntries = new ArrayList<>(entries.size());\n+        for (Entry entry : entries) {\n+            if (entry.repository().equals(repository)) {\n+                final List<SnapshotId> updatedSnapshotIds = new ArrayList<>(entry.getSnapshots());\n+                if (updatedSnapshotIds.removeAll(snapshotIds)) {\n+                    changed = true;\n+                    updatedEntries.add(entry.withSnapshots(updatedSnapshotIds));\n+                } else {\n+                    updatedEntries.add(entry);\n+                }\n+            } else {\n+                updatedEntries.add(entry);\n+            }\n+        }\n+        return changed ? new SnapshotDeletionsInProgress(updatedEntries) : this;\n+    }\n+\n+    public SnapshotDeletionsInProgress withRemovedRepository(String repository) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI3ODkxMw=="}, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQwNTY2Nw==", "bodyText": "I would prefer a static readFrom method and implements Writeable on this class, so that we have the serialization covered here directly", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441405667", "createdAt": "2020-06-17T09:20:36Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -257,4 +361,26 @@ public long repositoryStateId() {\n             return repositoryStateId;\n         }\n     }\n+\n+    public enum State {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQxMDI1Mw==", "bodyText": "This got me confused a bit, because we do not have a STARTED state on ShardState and because this talks about startedShardsByRepo.\nI wonder if we should add a new method isPossiblyActivelyRunning to ShardState, which only returns true for INIT and ABORTED, and use that here (with similar naming for the hashmap)", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441410253", "createdAt": "2020-06-17T09:27:40Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -466,10 +487,25 @@ public static State fromValue(byte value) {\n \n     public SnapshotsInProgress(List<Entry> entries) {\n         this.entries = entries;\n+        assert assertConsistentEntries(entries);\n     }\n \n     public SnapshotsInProgress(Entry... entries) {\n-        this.entries = Arrays.asList(entries);\n+        this(Arrays.asList(entries));\n+    }\n+\n+    private static boolean assertConsistentEntries(List<Entry> entries) {\n+        final Map<String, Set<ShardId>> startedShardsByRepo = new HashMap<>();\n+        for (Entry entry : entries) {\n+            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shard : entry.shards()) {\n+                final ShardState shardState = shard.value.state();\n+                if (shardState == ShardState.INIT || shardState == ShardState.ABORTED) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQxODU4OA==", "bodyText": "Do we have a test somewhere that ensures that we're not resolving to new indices for concurrent snaps? (which would always work, but give us less incrementality)", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441418588", "createdAt": "2020-06-17T09:40:59Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -202,37 +228,54 @@ public void createSnapshot(final CreateSnapshotRequest request, final ActionList\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 // check if the snapshot name already exists in the repository\n-                if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName))) {\n+                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots == null ? List.of() : snapshots.entries();\n+                if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName)) ||\n+                        runningSnapshots.stream().anyMatch(s -> {\n+                            final Snapshot running = s.snapshot();\n+                            return running.getRepository().equals(repositoryName)\n+                                    && running.getSnapshotId().getName().equals(snapshotName);\n+                        })) {\n                     throw new InvalidSnapshotNameException(\n                             repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n                 }\n                 validate(repositoryName, snapshotName, currentState);\n+                final Version minNodeVersion = currentState.nodes().getMinNodeVersion();\n                 SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n+                boolean readyToExecute = true;\n                 if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) {\n-                    throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n-                        \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");\n+                    if (minNodeVersion.before(FULL_CONCURRENCY_VERSION)) {\n+                        throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n+                                \"cannot snapshot while a snapshot deletion is in-progress in [\" + deletionsInProgress + \"]\");\n+                    } else {\n+                        readyToExecute = deletionsInProgress.getEntries().stream().noneMatch(entry ->\n+                                entry.repository().equals(repositoryName) && entry.state() == SnapshotDeletionsInProgress.State.META_DATA);\n+                    }\n                 }\n                 final RepositoryCleanupInProgress repositoryCleanupInProgress = currentState.custom(RepositoryCleanupInProgress.TYPE);\n                 if (repositoryCleanupInProgress != null && repositoryCleanupInProgress.hasCleanupInProgress()) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n                         \"cannot snapshot while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n                 }\n-                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n                 // Fail if there are any concurrently running snapshots. The only exception to this being a snapshot in INIT state from a\n                 // previous master that we can simply ignore and remove from the cluster state because we would clean it up from the\n                 // cluster state anyway in #applyClusterState.\n-                if (snapshots != null && snapshots.entries().stream().anyMatch(entry -> entry.state() != State.INIT)) {\n+                if (minNodeVersion.before(FULL_CONCURRENCY_VERSION) && snapshots != null\n+                        && runningSnapshots.stream().anyMatch(entry -> entry.state() != State.INIT)) {\n                     throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, \" a snapshot is already running\");\n                 }\n                 // Store newSnapshot here to be processed in clusterStateProcessed\n                 List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndexNames(currentState,\n                     request.indicesOptions(), request.indices()));\n                 logger.trace(\"[{}][{}] creating snapshot for indices [{}]\", repositoryName, snapshotName, indices);\n \n-                final List<IndexId> indexIds = repositoryData.resolveNewIndices(indices);\n+                final List<IndexId> indexIds = repositoryData.resolveNewIndices(\n+                        indices, runningSnapshots.stream().filter(entry -> entry.repository().equals(repositoryName))\n+                                .flatMap(entry -> entry.indices().stream()).distinct()\n+                                .collect(Collectors.toMap(IndexId::getName, Function.identity())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMDUyNA==", "bodyText": "Should this be called \"updateRepositoryGenerationsIfNecessary\"?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441430524", "createdAt": "2020-06-17T10:00:38Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1616,6 +1615,50 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n         }, listener::onFailure);\n     }\n \n+    /**\n+     * Updates the repository generation that running deletes and snapshot finalizations will be based on for this repository if any such\n+     * operations are found in the cluster state while setting the safe repository generation.\n+     *\n+     * @param state  cluster state to update\n+     * @param oldGen previous safe repository generation\n+     * @param newGen new safe repository generation\n+     * @return updated cluster state\n+     */\n+    private ClusterState updateRepositoryGenerations(ClusterState state, long oldGen, long newGen) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQzMDkyMw==", "bodyText": "what are the situations where we expect entry.repositoryStateId() != oldGen? Is there something we can assert here?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441430923", "createdAt": "2020-06-17T10:01:17Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1616,6 +1615,50 @@ public void clusterStateProcessed(String source, ClusterState oldState, ClusterS\n         }, listener::onFailure);\n     }\n \n+    /**\n+     * Updates the repository generation that running deletes and snapshot finalizations will be based on for this repository if any such\n+     * operations are found in the cluster state while setting the safe repository generation.\n+     *\n+     * @param state  cluster state to update\n+     * @param oldGen previous safe repository generation\n+     * @param newGen new safe repository generation\n+     * @return updated cluster state\n+     */\n+    private ClusterState updateRepositoryGenerations(ClusterState state, long oldGen, long newGen) {\n+        final SnapshotsInProgress snapshotsInProgress = state.custom(SnapshotsInProgress.TYPE);\n+        final String repoName = metadata.name();\n+        final List<SnapshotsInProgress.Entry> snapshotEntries;\n+        if (snapshotsInProgress == null) {\n+            snapshotEntries = List.of();\n+        } else {\n+            snapshotEntries = new ArrayList<>();\n+            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                if (entry.repository().equals(repoName) && entry.repositoryStateId() == oldGen) {\n+                    snapshotEntries.add(entry.withRepoGen(newGen));\n+                } else {\n+                    snapshotEntries.add(entry);\n+                }\n+            }\n+        }\n+        final SnapshotDeletionsInProgress snapshotDeletionsInProgress = state.custom(SnapshotDeletionsInProgress.TYPE);\n+        final List<SnapshotDeletionsInProgress.Entry> deletionEntries;\n+        if (snapshotDeletionsInProgress == null) {\n+            deletionEntries = List.of();\n+        } else {\n+            deletionEntries = new ArrayList<>();\n+            for (SnapshotDeletionsInProgress.Entry entry : snapshotDeletionsInProgress.getEntries()) {\n+                if (entry.repositoryStateId() == oldGen) {\n+                    deletionEntries.add(entry.withRepoGen(newGen));\n+                } else {\n+                    deletionEntries.add(entry);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ1NTMxOA==", "bodyText": "does this need adjustment? Not backported yet?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441455318", "createdAt": "2020-06-17T10:46:43Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -113,6 +117,8 @@\n  */\n public class SnapshotsService extends AbstractLifecycleComponent implements ClusterStateApplier {\n \n+    public static final Version FULL_CONCURRENCY_VERSION = Version.V_8_0_0;\n+\n     public static final Version SHARD_GEN_IN_REPO_DATA_VERSION = Version.V_7_6_0;\n \n     public static final Version INDEX_GEN_IN_REPO_DATA_VERSION = Version.V_8_0_0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ1ODExNQ==", "bodyText": "I think I would prefer a different error message here for running snapshots, making it clear that this is conflicting with an already running snapshot.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441458115", "createdAt": "2020-06-17T10:52:17Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -202,37 +228,54 @@ public void createSnapshot(final CreateSnapshotRequest request, final ActionList\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 // check if the snapshot name already exists in the repository\n-                if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName))) {\n+                SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots == null ? List.of() : snapshots.entries();\n+                if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName)) ||\n+                        runningSnapshots.stream().anyMatch(s -> {\n+                            final Snapshot running = s.snapshot();\n+                            return running.getRepository().equals(repositoryName)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3MjkzNw==", "bodyText": "add javadocs here to explain what this method does?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441472937", "createdAt": "2020-06-17T11:23:23Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -725,12 +836,107 @@ private void handleFinalizationFailure(Exception e, SnapshotsInProgress.Entry en\n                 new SnapshotException(snapshot, \"Failed to update cluster state during snapshot finalization\", e));\n         } else {\n             logger.warn(() -> new ParameterizedMessage(\"[{}] failed to finalize snapshot\", snapshot), e);\n-            removeSnapshotFromClusterState(snapshot, e);\n+            removeSnapshotFromClusterState(snapshot, e,\n+                    ActionListener.wrap(() -> runNextQueuedOperation(entry.repositoryStateId(), entry.repository())));\n+        }\n+    }\n+\n+    /**\n+     * Run the next queued up repository operation for the given repository name.\n+     *\n+     * @param newGeneration current repository generation for given repository\n+     * @param repository    repository name\n+     */\n+    private void runNextQueuedOperation(long newGeneration, String repository) {\n+        synchronized (currentlyFinalizing) {\n+            assert currentlyFinalizing.contains(repository);\n+            final Deque<SnapshotFinalization> outstandingForRepo = snapshotsToFinalize.get(repository);\n+            final SnapshotFinalization nextFinalization;\n+            if (outstandingForRepo == null) {\n+                nextFinalization = null;\n+            } else {\n+                nextFinalization = outstandingForRepo.pollFirst();\n+                if (outstandingForRepo.isEmpty()) {\n+                    snapshotsToFinalize.remove(repository);\n+                }\n+            }\n+            if (nextFinalization == null) {\n+                final boolean removed = currentlyFinalizing.remove(repository);\n+                assert removed;\n+                runReadyDeletions();\n+            } else {\n+                logger.trace(\"Moving on to finalizing next snapshot [{}]\", nextFinalization);\n+                finalizeSnapshotEntry(nextFinalization.entry, nextFinalization.metadata, newGeneration);\n+            }\n         }\n     }\n \n+    /**\n+     * Runs a cluster state update that checks whether we have outstanding snapshot deletions that can be executed and executes them.\n+     *\n+     * TODO: optimize this to execute in a single CS update together with finalizing the latest snapshot\n+     */\n+    private void runReadyDeletions() {\n+        clusterService.submitStateUpdateTask(\"Run ready deletions\", new ClusterStateUpdateTask() {\n+\n+            private List<SnapshotDeletionsInProgress.Entry> deletionsToRun;\n+\n+            @Override\n+            public ClusterState execute(ClusterState currentState) {\n+                final Tuple<ClusterState, List<SnapshotDeletionsInProgress.Entry>> res = readyDeletions(currentState);\n+                assert res.v1() == currentState : \"Deletes should have been set to ready by finished snapshot deletes and finalizations\";\n+                deletionsToRun = res.v2();\n+                return res.v1();\n+            }\n+\n+            @Override\n+            public void onFailure(String source, Exception e) {\n+                logger.warn(\"Failed to run ready delete operations\", e);\n+            }\n+\n+            @Override\n+            public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n+                for (SnapshotDeletionsInProgress.Entry entry : deletionsToRun) {\n+                    deleteSnapshotsFromRepository(entry, entry.repositoryStateId(), newState.nodes().getMinNodeVersion());\n+                }\n+            }\n+        });\n+    }\n+\n+    private static Tuple<ClusterState, List<SnapshotDeletionsInProgress.Entry>> readyDeletions(ClusterState currentState) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 442}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ3NTYzMg==", "bodyText": "why is there only at most one?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441475632", "createdAt": "2020-06-17T11:29:03Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -821,22 +1023,23 @@ public void deleteSnapshots(final DeleteSnapshotRequest request, final ActionLis\n \n             @Override\n             public ClusterState execute(ClusterState currentState) throws Exception {\n-                if (snapshotNames.length > 1 && currentState.nodes().getMinNodeVersion().before(MULTI_DELETE_VERSION)) {\n+                final Version minNodeVersion = currentState.nodes().getMinNodeVersion();\n+                if (snapshotNames.length > 1 && minNodeVersion.before(MULTI_DELETE_VERSION)) {\n                     throw new IllegalArgumentException(\"Deleting multiple snapshots in a single request is only supported in version [ \"\n-                            + MULTI_DELETE_VERSION + \"] but cluster contained node of version [\" + currentState.nodes().getMinNodeVersion()\n-                            + \"]\");\n+                            + MULTI_DELETE_VERSION + \"] but cluster contained node of version [\" + minNodeVersion + \"]\");\n                 }\n                 final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\n-                final SnapshotsInProgress.Entry snapshotEntry = findInProgressSnapshot(snapshots, snapshotNames, repositoryName);\n+                final List<SnapshotsInProgress.Entry> snapshotEntries = findInProgressSnapshots(snapshots, snapshotNames, repositoryName);\n                 final List<SnapshotId> snapshotIds = matchingSnapshotIds(\n-                        snapshotEntry == null ? null : snapshotEntry.snapshot().getSnapshotId(),\n-                        repositoryData, snapshotNames, repositoryName);\n-                if (snapshotEntry == null) {\n+                        snapshotEntries.stream().map(e -> e.snapshot().getSnapshotId()).collect(Collectors.toList()), repositoryData,\n+                        snapshotNames, repositoryName);\n+                if (snapshotEntries.isEmpty() || minNodeVersion.onOrAfter(SnapshotsService.FULL_CONCURRENCY_VERSION)) {\n                     deleteFromRepoTask =\n                             createDeleteStateUpdate(snapshotIds, repositoryName, repositoryData.getGenId(), Priority.NORMAL, listener);\n                     return deleteFromRepoTask.execute(currentState);\n                 }\n-\n+                assert snapshotEntries.size() == 1 : \"Expected just a single running snapshot but saw \" + snapshotEntries;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 570}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MDU1Mw==", "bodyText": "do we need to set changed to true here?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441480553", "createdAt": "2020-06-17T11:39:06Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 905}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MTE5OA==", "bodyText": "this should only be a problem if the repository data isn't cached, right?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441481198", "createdAt": "2020-06-17T11:40:20Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 920}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjE1NQ==", "bodyText": "Should there be a return after this line?\nI see usage later on of repositoryData where it's possibly null", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441482155", "createdAt": "2020-06-17T11:42:15Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 922}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjI1Ng==", "bodyText": "Also sounds like we have a testing gap", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441482256", "createdAt": "2020-06-17T11:42:27Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MjE1NQ=="}, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 922}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4MzQ4OQ==", "bodyText": "As mentioned in Slack, this could possibly be result in shards to be done out-of-snapshot-order in case where there is a relocating primary.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441483489", "createdAt": "2020-06-17T11:45:01Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 943}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NDc2OQ==", "bodyText": "should we still call the listeners here?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441484769", "createdAt": "2020-06-17T11:47:36Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {\n+                                                // TODO: this could be made more efficient by not recomputing assignments for shards that\n+                                                //       are already in reAssignedShardIds\n+                                                final ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardAssignments =\n+                                                        shards(currentState, entry.indices(),\n+                                                                entry.version().onOrAfter(SHARD_GEN_IN_REPO_DATA_VERSION),\n+                                                                repositoryData, entry.repository(), true);\n+                                                final ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus>\n+                                                        updatedAssignmentsBuilder = ImmutableOpenMap.builder();\n+                                                for (ObjectCursor<ShardId> key : entry.shards().keys()) {\n+                                                    final ShardSnapshotStatus existing = entry.shards().get(key.value);\n+                                                    if (existing.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)\n+                                                            && reAssignedShardIds.add(key.value)) {\n+                                                        updatedAssignmentsBuilder.put(key.value, shardAssignments.get(key.value));\n+                                                    } else {\n+                                                        updatedAssignmentsBuilder.put(key.value, existing);\n+                                                    }\n+                                                }\n+                                                snapshotEntries.add(entry.withShards(updatedAssignmentsBuilder.build()));\n+                                                updatedQueuedSnapshot = true;\n+                                                break;\n+                                            }\n+                                        }\n+                                        if (updatedQueuedSnapshot == false) {\n+                                            // Nothing to update in this snapshot so we just add it as is\n+                                            snapshotEntries.add(entry);\n+                                        }\n+                                    } else {\n+                                        // Entry is already completed so we will finalize it now that the delete doesn't block us after\n+                                        // this CS update finishes\n+                                        newFinalizations.add(entry);\n+                                        snapshotEntries.add(entry);\n+                                    }\n+                                } else {\n+                                    // Entry is for another repository we just keep it as is\n+                                    snapshotEntries.add(entry);\n+                                }\n+                            }\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress(snapshotEntries);\n+                        }\n+                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions)\n+                                .putCustom(SnapshotsInProgress.TYPE, updatedSnapshotsInProgress).build();\n                     }\n                 }\n                 return currentState;\n             }\n \n             @Override\n             public void onFailure(String source, Exception e) {\n-                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", snapshotIds), e);\n-                if (listener != null) {\n-                    listener.onFailure(e);\n+                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", deleteEntry), e);\n+                runningDeletions.remove(deleteEntry.uuid());\n+                final String repoName = deleteEntry.repository();\n+                synchronized (currentlyFinalizing) {\n+                    if (ExceptionsHelper.unwrap(e, NotMasterException.class, FailedToCommitClusterStateException.class) != null) {\n+                        // Failure due to not being master any more so we don't try to do run more cluster state updates. The next master\n+                        // will try handling the missing operations. All we can do is fail all the listeners on this master node so that\n+                        // transport requests return and we don't leak listeners\n+                        final Exception wrapped =\n+                                new RepositoryException(repoName, \"Failed to update cluster state during snapshot delete\", e);\n+                        final Deque<SnapshotFinalization> outstandingSnapshotsForRepo = snapshotsToFinalize.remove(repoName);\n+                        if (outstandingSnapshotsForRepo != null) {\n+                            SnapshotFinalization finalization;\n+                            while ((finalization = outstandingSnapshotsForRepo.poll()) != null) {\n+                                failSnapshotCompletionListeners(finalization.entry.snapshot(), wrapped);\n+                            }\n+                        }\n+                        for (Iterator<List<ActionListener<RepositoryData>>> iterator = snapshotDeletionListeners.values().iterator();\n+                             iterator.hasNext(); ) {\n+                            List<ActionListener<RepositoryData>> listeners = iterator.next();\n+                            iterator.remove();\n+                            failListenersIgnoringException(listeners, wrapped);\n+                        }\n+                        assert snapshotDeletionListeners.isEmpty() :\n+                                \"No new listeners should have been added but saw \" + snapshotDeletionListeners;\n+                    } else {\n+                        assert false : \"Removing snapshot entry should only ever fail because we failed to publish new state\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 1021}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NTM2MA==", "bodyText": "how is this tested?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441485360", "createdAt": "2020-06-17T11:48:47Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1131,72 +1430,232 @@ public static boolean useIndexGenerations(Version repositoryMetaVersion) {\n \n     /** Deletes snapshot from repository\n      *\n-     * @param repoName          repository name\n-     * @param snapshotIds       snapshot ids\n-     * @param listener          listener\n+     * @param deleteEntry       delete entry in cluster state\n      * @param repositoryStateId the unique id representing the state of the repository at the time the deletion began\n      * @param minNodeVersion    minimum node version in the cluster\n+     *\n+     * TODO: This method should take a RepositoryData instead of repository generation as its argument since all but one caller has that\n+     *       available already and we can save loading the repository data in this method again that way\n      */\n-    private void deleteSnapshotsFromRepository(String repoName, Collection<SnapshotId> snapshotIds, @Nullable ActionListener<Void> listener,\n+    private void deleteSnapshotsFromRepository(SnapshotDeletionsInProgress.Entry deleteEntry,\n                                                long repositoryStateId, Version minNodeVersion) {\n-        Repository repository = repositoriesService.repository(repoName);\n-        repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n-                snapshotIds,\n-                repositoryStateId,\n-                minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n-                ActionListener.wrap(v -> {\n-                            logger.info(\"snapshots {} deleted\", snapshotIds);\n-                            removeSnapshotDeletionFromClusterState(snapshotIds, null, listener);\n-                        }, ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)\n-                )), ex -> removeSnapshotDeletionFromClusterState(snapshotIds, ex, listener)));\n+        if (runningDeletions.add(deleteEntry.uuid())) {\n+            boolean added = currentlyFinalizing.add(deleteEntry.repository());\n+            assert added : \"Tried to start snapshot delete while already running operation on repository [\" + deleteEntry + \"]\";\n+            Repository repository = repositoriesService.repository(deleteEntry.repository());\n+            final List<SnapshotId> snapshotIds = deleteEntry.getSnapshots();\n+            assert deleteEntry.state() == SnapshotDeletionsInProgress.State.META_DATA :\n+                    \"incorrect state for entry [\" + deleteEntry + \"]\";\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> repository.deleteSnapshots(\n+                    snapshotIds,\n+                    repositoryStateId,\n+                    minCompatibleVersion(minNodeVersion, repositoryData, snapshotIds),\n+                    ActionListener.wrap(updatedRepoData -> {\n+                                logger.info(\"snapshots {} deleted\", snapshotIds);\n+                                removeSnapshotDeletionFromClusterState(deleteEntry, null, updatedRepoData);\n+                            }, ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, repositoryData)\n+                    )), ex -> removeSnapshotDeletionFromClusterState(deleteEntry, ex, null)));\n+        }\n     }\n \n     /**\n      * Removes the snapshot deletion from {@link SnapshotDeletionsInProgress} in the cluster state.\n      */\n-    private void removeSnapshotDeletionFromClusterState(final Collection<SnapshotId> snapshotIds, @Nullable final Exception failure,\n-                                                        @Nullable final ActionListener<Void> listener) {\n+    private void removeSnapshotDeletionFromClusterState(final SnapshotDeletionsInProgress.Entry deleteEntry,\n+                                                        @Nullable final Exception failure, @Nullable final RepositoryData repositoryData) {\n+\n+        assert failure != null || repositoryData != null : \"Either a failure must have occurred or we should have loaded repository data\";\n+\n         clusterService.submitStateUpdateTask(\"remove snapshot deletion metadata\", new ClusterStateUpdateTask() {\n+\n+            // Snapshots to fail after the state update\n+            private final List<Snapshot> snapshotsToFail = new ArrayList<>();\n+\n+            // Delete uuids to fail because after the state update\n+            private final List<String> deletionsToFail = new ArrayList<>();\n+\n+            // Snapshots that can be finalized after the delete operation has been removed from the cluster state\n+            private final List<SnapshotsInProgress.Entry> newFinalizations = new ArrayList<>();\n+\n             @Override\n             public ClusterState execute(ClusterState currentState) {\n                 SnapshotDeletionsInProgress deletions = currentState.custom(SnapshotDeletionsInProgress.TYPE);\n                 if (deletions != null) {\n                     boolean changed = false;\n                     if (deletions.hasDeletionsInProgress()) {\n-                        assert deletions.getEntries().size() == 1 : \"should have exactly one deletion in progress\";\n-                        SnapshotDeletionsInProgress.Entry entry = deletions.getEntries().get(0);\n-                        deletions = deletions.withRemovedEntry(entry);\n-                        changed = true;\n+                        final SnapshotDeletionsInProgress updatedDeletions = deletions.withRemovedEntry(deleteEntry.uuid());\n+                        changed = updatedDeletions != deletions;\n+                        deletions = updatedDeletions;\n+                        if (failure == null) {\n+                            // The delete worked out so we remove the snapshot ids that it removed from the repository from queued up\n+                            // delete jobs\n+                            deletions = deletions.withRemovedSnapshotIds(deleteEntry.repository(), deleteEntry.getSnapshots());\n+                        }\n                     }\n                     if (changed) {\n-                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions).build();\n+                        // We removed a delete from the cluster state. If that delete caused any snapshots to not have their shard\n+                        // snapshots assigned and we have to assign them now based on the new RepositoryData that resulted from the delete.\n+                        final SnapshotsInProgress snapshotsInProgress = currentState.custom(SnapshotsInProgress.TYPE);\n+                        // There is one special case to deal with here: If the RepositoryData passed to this method is null, then that\n+                        // means that the delete operation failed to load the current RepositoryData, indicating an issue with the\n+                        // repository so we fail all queued up operations.\n+                        final boolean failAllQueuedOperations = repositoryData == null;\n+                        if (failAllQueuedOperations) {\n+                            deletions.getEntries().stream().filter(entry -> entry.repository().equals(deleteEntry.repository()))\n+                                    .map(SnapshotDeletionsInProgress.Entry::uuid).forEach(deletionsToFail::add);\n+                            // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                            // retry this kind of issue so we fail all the pending deletes\n+                            deletions = deletions.withRemovedRepository(deleteEntry.repository());\n+                        }\n+                        final SnapshotsInProgress updatedSnapshotsInProgress;\n+                        if (snapshotsInProgress == null) {\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress();\n+                        } else {\n+                            final List<SnapshotsInProgress.Entry> snapshotEntries = new ArrayList<>();\n+\n+                            // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n+                            // them to multiple snapshots by accident\n+                            final Set<ShardId> reAssignedShardIds = new HashSet<>();\n+\n+                            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n+                                if (entry.repository().equals(deleteEntry.repository())) {\n+                                    if (failAllQueuedOperations) {\n+                                        // We failed to read repository data for this delete, it is not the job of SnapshotsService to\n+                                        // retry these kinds of issues so we fail all the pending snapshots\n+                                      snapshotsToFail.add(entry.snapshot());\n+                                    } else if (entry.state().completed() == false) {\n+                                        boolean updatedQueuedSnapshot = false;\n+                                        for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)) {\n+                                                // TODO: this could be made more efficient by not recomputing assignments for shards that\n+                                                //       are already in reAssignedShardIds\n+                                                final ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shardAssignments =\n+                                                        shards(currentState, entry.indices(),\n+                                                                entry.version().onOrAfter(SHARD_GEN_IN_REPO_DATA_VERSION),\n+                                                                repositoryData, entry.repository(), true);\n+                                                final ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus>\n+                                                        updatedAssignmentsBuilder = ImmutableOpenMap.builder();\n+                                                for (ObjectCursor<ShardId> key : entry.shards().keys()) {\n+                                                    final ShardSnapshotStatus existing = entry.shards().get(key.value);\n+                                                    if (existing.equals(ShardSnapshotStatus.UNASSIGNED_WAITING)\n+                                                            && reAssignedShardIds.add(key.value)) {\n+                                                        updatedAssignmentsBuilder.put(key.value, shardAssignments.get(key.value));\n+                                                    } else {\n+                                                        updatedAssignmentsBuilder.put(key.value, existing);\n+                                                    }\n+                                                }\n+                                                snapshotEntries.add(entry.withShards(updatedAssignmentsBuilder.build()));\n+                                                updatedQueuedSnapshot = true;\n+                                                break;\n+                                            }\n+                                        }\n+                                        if (updatedQueuedSnapshot == false) {\n+                                            // Nothing to update in this snapshot so we just add it as is\n+                                            snapshotEntries.add(entry);\n+                                        }\n+                                    } else {\n+                                        // Entry is already completed so we will finalize it now that the delete doesn't block us after\n+                                        // this CS update finishes\n+                                        newFinalizations.add(entry);\n+                                        snapshotEntries.add(entry);\n+                                    }\n+                                } else {\n+                                    // Entry is for another repository we just keep it as is\n+                                    snapshotEntries.add(entry);\n+                                }\n+                            }\n+                            updatedSnapshotsInProgress = new SnapshotsInProgress(snapshotEntries);\n+                        }\n+                        return ClusterState.builder(currentState).putCustom(SnapshotDeletionsInProgress.TYPE, deletions)\n+                                .putCustom(SnapshotsInProgress.TYPE, updatedSnapshotsInProgress).build();\n                     }\n                 }\n                 return currentState;\n             }\n \n             @Override\n             public void onFailure(String source, Exception e) {\n-                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", snapshotIds), e);\n-                if (listener != null) {\n-                    listener.onFailure(e);\n+                logger.warn(() -> new ParameterizedMessage(\"{} failed to remove snapshot deletion metadata\", deleteEntry), e);\n+                runningDeletions.remove(deleteEntry.uuid());\n+                final String repoName = deleteEntry.repository();\n+                synchronized (currentlyFinalizing) {\n+                    if (ExceptionsHelper.unwrap(e, NotMasterException.class, FailedToCommitClusterStateException.class) != null) {\n+                        // Failure due to not being master any more so we don't try to do run more cluster state updates. The next master\n+                        // will try handling the missing operations. All we can do is fail all the listeners on this master node so that\n+                        // transport requests return and we don't leak listeners\n+                        final Exception wrapped =\n+                                new RepositoryException(repoName, \"Failed to update cluster state during snapshot delete\", e);\n+                        final Deque<SnapshotFinalization> outstandingSnapshotsForRepo = snapshotsToFinalize.remove(repoName);\n+                        if (outstandingSnapshotsForRepo != null) {\n+                            SnapshotFinalization finalization;\n+                            while ((finalization = outstandingSnapshotsForRepo.poll()) != null) {\n+                                failSnapshotCompletionListeners(finalization.entry.snapshot(), wrapped);\n+                            }\n+                        }\n+                        for (Iterator<List<ActionListener<RepositoryData>>> iterator = snapshotDeletionListeners.values().iterator();\n+                             iterator.hasNext(); ) {\n+                            List<ActionListener<RepositoryData>> listeners = iterator.next();\n+                            iterator.remove();\n+                            failListenersIgnoringException(listeners, wrapped);\n+                        }\n+                        assert snapshotDeletionListeners.isEmpty() :\n+                                \"No new listeners should have been added but saw \" + snapshotDeletionListeners;\n+                    } else {\n+                        assert false : \"Removing snapshot entry should only ever fail because we failed to publish new state\";\n+                    }\n+                    currentlyFinalizing.remove(repoName);\n                 }\n             }\n \n             @Override\n             public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n-                if (listener != null) {\n-                    if (failure != null) {\n-                        listener.onFailure(failure);\n-                    } else {\n-                        logger.info(\"Successfully deleted snapshots {}\", snapshotIds);\n-                        listener.onResponse(null);\n+                final List<ActionListener<RepositoryData>> deleteListeners = snapshotDeletionListeners.remove(deleteEntry.uuid());\n+                if (failure == null) {\n+                    completeListenersIgnoringException(deleteListeners, repositoryData);\n+                } else {\n+                    failListenersIgnoringException(deleteListeners, failure);\n+                }\n+                runningDeletions.remove(deleteEntry.uuid());\n+                if (repositoryData == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 1042}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTQ4NzQ1NQ==", "bodyText": "Does this logic still make sure that a snapshot that's taken contains a set of docs that existed sometime between start and end of snapshot command? Or will this batch up snapshots that had been running prior to the command?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r441487455", "createdAt": "2020-06-17T11:52:45Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1322,20 +1811,41 @@ protected void doClose() {\n             if (snapshots != null) {\n                 int changedCount = 0;\n                 final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();\n+                final Map<String, Set<ShardId>> reusedShardIdsByRepo = new HashMap<>();\n                 for (SnapshotsInProgress.Entry entry : snapshots.entries()) {\n                     ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();\n                     boolean updated = false;\n \n                     for (UpdateIndexShardSnapshotStatusRequest updateSnapshotState : tasks) {\n+                        final ShardId finishedShardId = updateSnapshotState.shardId();\n                         if (entry.snapshot().equals(updateSnapshotState.snapshot())) {\n                             logger.trace(\"[{}] Updating shard [{}] with status [{}]\", updateSnapshotState.snapshot(),\n-                                updateSnapshotState.shardId(), updateSnapshotState.status().state());\n+                                    finishedShardId, updateSnapshotState.status().state());\n                             if (updated == false) {\n                                 shards.putAll(entry.shards());\n                                 updated = true;\n                             }\n-                            shards.put(updateSnapshotState.shardId(), updateSnapshotState.status());\n+                            shards.put(finishedShardId, updateSnapshotState.status());\n                             changedCount++;\n+                        } else {\n+                            final Set<ShardId> reusedShardIds =\n+                                    reusedShardIdsByRepo.computeIfAbsent(entry.repository(), k -> new HashSet<>());\n+                            if (entry.state().completed() == false && reusedShardIds.contains(finishedShardId) == false", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 1168}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee8827588c76bba0ab2181691a99027aa6df90be", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/ee8827588c76bba0ab2181691a99027aa6df90be", "committedDate": "2020-06-17T12:03:11Z", "message": "fix waiting+relocation failure propagation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "62559a560df20fdc5fd5043aa90c9b9340be12b4", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/62559a560df20fdc5fd5043aa90c9b9340be12b4", "committedDate": "2020-06-17T12:03:24Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "318822558eb08fc729083d1737c829c568095ada", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/318822558eb08fc729083d1737c829c568095ada", "committedDate": "2020-06-17T12:12:45Z", "message": "docs fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d28bddb1db6d3f5fddf2fdbedac6935db7aea52c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d28bddb1db6d3f5fddf2fdbedac6935db7aea52c", "committedDate": "2020-06-17T12:13:12Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f961f57db67b9ee3ca96327a5f3ecdae2bfac7b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/7f961f57db67b9ee3ca96327a5f3ecdae2bfac7b", "committedDate": "2020-06-17T12:23:03Z", "message": "drop dead code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7aa0bf7a628788daea0bc672f1311d6971c08d1", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a7aa0bf7a628788daea0bc672f1311d6971c08d1", "committedDate": "2020-06-17T14:38:39Z", "message": "stop double wrapping list"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ba1f6154ebc3b00ddcb6c0e9326ec49debbef67", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/4ba1f6154ebc3b00ddcb6c0e9326ec49debbef67", "committedDate": "2020-06-17T14:43:37Z", "message": "rename"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0ff7ddca17f9208ce16f0dad45b41244a9ffd4f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/c0ff7ddca17f9208ce16f0dad45b41244a9ffd4f", "committedDate": "2020-06-17T18:15:12Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a9d7a20468025f6b9a37538e308e711970288f01", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a9d7a20468025f6b9a37538e308e711970288f01", "committedDate": "2020-06-18T05:58:14Z", "message": "further cleanups"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f98b636e0d3d8cbd49f1cf1a65be855ef4b9492", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2f98b636e0d3d8cbd49f1cf1a65be855ef4b9492", "committedDate": "2020-06-18T18:54:02Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38090b7d2f0ebec110700826049b5a65972a1bf0", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/38090b7d2f0ebec110700826049b5a65972a1bf0", "committedDate": "2020-06-19T06:29:09Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e766028e39e1d42dbd20751373d2306978b9e623", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e766028e39e1d42dbd20751373d2306978b9e623", "committedDate": "2020-06-19T08:17:08Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "962a76151cd4d692f9b07859c7f11f1357e5132e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/962a76151cd4d692f9b07859c7f11f1357e5132e", "committedDate": "2020-06-19T10:40:05Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8200eba197ae4137821c7199a3282696e8b22391", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/8200eba197ae4137821c7199a3282696e8b22391", "committedDate": "2020-06-19T17:28:45Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3c5196e7bcc04a469eece7da0dee7324031ea0f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a3c5196e7bcc04a469eece7da0dee7324031ea0f", "committedDate": "2020-06-20T17:15:02Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10b504b331ecef41fd0f854555b85b19c961ecd4", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/10b504b331ecef41fd0f854555b85b19c961ecd4", "committedDate": "2020-06-20T22:33:10Z", "message": "much cuter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea4f9eda94c82aeff1a695d0f0fa419c5d324a50", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/ea4f9eda94c82aeff1a695d0f0fa419c5d324a50", "committedDate": "2020-06-21T07:22:17Z", "message": "fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ab07d9681d1d32dca6e88cbcc327db8d695d646", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6ab07d9681d1d32dca6e88cbcc327db8d695d646", "committedDate": "2020-06-21T11:20:46Z", "message": "moar docs, cleaner logic"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "01a3de8a72951e59b02c7d34cc066d16abcc16b6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/01a3de8a72951e59b02c7d34cc066d16abcc16b6", "committedDate": "2020-06-21T20:54:13Z", "message": "tweaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "685d5453ab0799b67c5c4a41e114f526898cac24", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/685d5453ab0799b67c5c4a41e114f526898cac24", "committedDate": "2020-06-21T21:42:20Z", "message": "fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a72c285117410d0b61a6e1506954380eb07f5ad", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/5a72c285117410d0b61a6e1506954380eb07f5ad", "committedDate": "2020-06-22T07:42:44Z", "message": "test cleanups"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "337643a89a5467c413d956677241f5c919482b0d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/337643a89a5467c413d956677241f5c919482b0d", "committedDate": "2020-06-22T07:52:32Z", "message": "shorter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40abfdd12e02973b76172fc3276f6823c501f22d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/40abfdd12e02973b76172fc3276f6823c501f22d", "committedDate": "2020-06-22T09:45:16Z", "message": "much more readable tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ccfa36e687e6a436a8bd8feb6cb8efa54cc640c9", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/ccfa36e687e6a436a8bd8feb6cb8efa54cc640c9", "committedDate": "2020-06-22T19:29:37Z", "message": "much much stricter tests and fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc9efc20135d9cb70ea07c76ad33457649963380", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/dc9efc20135d9cb70ea07c76ad33457649963380", "committedDate": "2020-06-23T06:28:32Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8bf6f04d769f686945189da70046671a62c3a8c5", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/8bf6f04d769f686945189da70046671a62c3a8c5", "committedDate": "2020-06-23T07:25:49Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b566d482fa067101b14ed3e1460523ef339b92f8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/b566d482fa067101b14ed3e1460523ef339b92f8", "committedDate": "2020-06-23T08:05:18Z", "message": "cleaner"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "58981c208cc342877d06b0c54faea3139891dcc1", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/58981c208cc342877d06b0c54faea3139891dcc1", "committedDate": "2020-06-23T08:21:21Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d31169421e33148a6e66b6314af97ed4267ed50", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6d31169421e33148a6e66b6314af97ed4267ed50", "committedDate": "2020-06-23T09:23:20Z", "message": "even moar docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f207d1549592c0ee94fcc385ec10de94b2286dbb", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/f207d1549592c0ee94fcc385ec10de94b2286dbb", "committedDate": "2020-06-23T09:40:49Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd2199423091914e1f662faccad490eaf7e93d55", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cd2199423091914e1f662faccad490eaf7e93d55", "committedDate": "2020-06-23T10:39:58Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3acf79e00f2808a57360f0df4c26f5f54c013ea", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e3acf79e00f2808a57360f0df4c26f5f54c013ea", "committedDate": "2020-06-23T11:25:06Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1581aa8f0c8288c9fbcb3c1031ff68ce2333b0d0", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/1581aa8f0c8288c9fbcb3c1031ff68ce2333b0d0", "committedDate": "2020-06-23T13:56:50Z", "message": "test for and fix more listener leaks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "808d8279993665172ec381fb5544a4b889a5a71b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/808d8279993665172ec381fb5544a4b889a5a71b", "committedDate": "2020-06-23T14:07:12Z", "message": "better doc + naming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e19d0c6c5f1765837d2e003d2c0d965711d5f5c6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e19d0c6c5f1765837d2e003d2c0d965711d5f5c6", "committedDate": "2020-06-23T14:28:43Z", "message": "optimize"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa822de031d4b0713871c29d8b2ee2445a55189f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fa822de031d4b0713871c29d8b2ee2445a55189f", "committedDate": "2020-06-23T15:40:38Z", "message": "writable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa1006bf5d5d7e416b30754d2ef738b3a5043557", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fa1006bf5d5d7e416b30754d2ef738b3a5043557", "committedDate": "2020-06-23T15:45:56Z", "message": "faster"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a118af0319eb9acd01207e21d8f54af122405a9c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a118af0319eb9acd01207e21d8f54af122405a9c", "committedDate": "2020-06-23T16:10:28Z", "message": "different exceptions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d221d19835a10f872c3575acf26b773d2598344", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6d221d19835a10f872c3575acf26b773d2598344", "committedDate": "2020-06-23T16:14:11Z", "message": "renaming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13b3d4d313461790d684eefe389b1c9db29d1eef", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/13b3d4d313461790d684eefe389b1c9db29d1eef", "committedDate": "2020-06-23T16:53:13Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc11ce13fab85ee07d00250b45a53ec3a22cdaec", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fc11ce13fab85ee07d00250b45a53ec3a22cdaec", "committedDate": "2020-06-24T03:40:52Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d3d447246ffa5c86522994a87db91385a878a81", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/5d3d447246ffa5c86522994a87db91385a878a81", "committedDate": "2020-06-24T03:58:12Z", "message": "simpler listener"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MzA5Njk3", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-436309697", "createdAt": "2020-06-24T04:06:02Z", "commit": {"oid": "5d3d447246ffa5c86522994a87db91385a878a81"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNDowNjowM1rOGoCOOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNDowNjowM1rOGoCOOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDYzMjYzNQ==", "bodyText": "This synchronization on currentlyFinalizing in a bunch of spots is not great admittedly but for step 1 I traded efficiency for safety in many spots like this one.\nI have some planned simplifications that will move all the logic for triggering deletes and finalizations exclusively to the master update thread (most of them already are) which will remove the need for most synchronization on the fields in SnapshotsService.", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444632635", "createdAt": "2020-06-24T04:06:03Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -677,19 +795,44 @@ private static boolean removedNodesCleanupNeeded(SnapshotsInProgress snapshotsIn\n     }\n \n     /**\n-     * Finalizes the shard in repository and then removes it from cluster state\n-     * <p>\n-     * This is non-blocking method that runs on a thread from SNAPSHOT thread pool\n+     * Finalizes the snapshot in the repository.\n      *\n      * @param entry snapshot\n      */\n-    private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata) {\n-        if (endingSnapshots.add(entry.snapshot()) == false) {\n-            return;\n+    private void endSnapshot(SnapshotsInProgress.Entry entry, Metadata metadata, @Nullable RepositoryData repositoryData) {\n+        final boolean newFinalization = endingSnapshots.add(entry.snapshot());\n+        final String repoName = entry.repository();\n+        synchronized (currentlyFinalizing) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d3d447246ffa5c86522994a87db91385a878a81"}, "originalPosition": 433}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2NjkxNzU5", "url": "https://github.com/elastic/elasticsearch/pull/56911#pullrequestreview-436691759", "createdAt": "2020-06-24T14:17:46Z", "commit": {"oid": "5d3d447246ffa5c86522994a87db91385a878a81"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNDoxNzo0NlrOGoUN7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNDo0MjoyMFrOGoVXXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkyNzQ3MA==", "bodyText": "nit: isExecuting() sounds good to me", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444927470", "createdAt": "2020-06-24T14:17:46Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -404,6 +425,16 @@ public String reason() {\n             return reason;\n         }\n \n+        /**\n+         * Checks if this shard snapshot is actively executing.\n+         * A shard is defined as actively executing if it either is in a state that may write to the repository\n+         * ({@link ShardState#INIT} or {@link ShardState#ABORTED}) or is in state {@link ShardState#WAITING} with a concrete non-null\n+         * node id assignment (i.e. waiting for a shard relocation/initialization to finish).\n+         */\n+        public boolean isAssigned() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d3d447246ffa5c86522994a87db91385a878a81"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk0NjI3MQ==", "bodyText": "nit: what about renaming META_DATA to RUNNING ?", "url": "https://github.com/elastic/elasticsearch/pull/56911#discussion_r444946271", "createdAt": "2020-06-24T14:42:20Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotDeletionsInProgress.java", "diffHunk": "@@ -257,4 +361,26 @@ public long repositoryStateId() {\n             return repositoryStateId;\n         }\n     }\n+\n+    public enum State {\n+        WAITING((byte) 0),\n+        META_DATA((byte) 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMyOTgxNg=="}, "originalCommit": {"oid": "c44bdb3e9ec001fff2f83efad144a48e5c990fd2"}, "originalPosition": 218}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d89ead6e6e5ab12b3da463a50f25132538c7226b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d89ead6e6e5ab12b3da463a50f25132538c7226b", "committedDate": "2020-06-24T16:26:44Z", "message": "Merge remote-tracking branch 'elastic/master' into allow-multiple-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6d47f233a51efe69346adc730c1748ac98c89a4", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a6d47f233a51efe69346adc730c1748ac98c89a4", "committedDate": "2020-06-24T16:37:55Z", "message": "shorter"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4858, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}