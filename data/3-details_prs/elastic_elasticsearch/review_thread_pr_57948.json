{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMyNjgyNzMz", "number": 57948, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTowNTo0MVrOEEuVbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo1NDowM1rOEEv1ZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMzg4OTA5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/common/util/SetBackedScalingCuckooFilter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTowNTo0MVrOGihjfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTowNTo0MVrOGihjfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg1NDUyNQ==", "bodyText": "I hate javadocs so much :(  optimizing rendered readability while sacrificing IDE readability :(\nThanks for fixing this :)", "url": "https://github.com/elastic/elasticsearch/pull/57948#discussion_r438854525", "createdAt": "2020-06-11T15:05:41Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/common/util/SetBackedScalingCuckooFilter.java", "diffHunk": "@@ -39,13 +39,13 @@\n  * An approximate set membership datastructure that scales as more unique values are inserted.\n  * Can definitively say if a member does not exist (no false negatives), but may say an item exists\n  * when it does not (has false positives).  Similar in usage to a Bloom Filter.\n- *\n+ * <p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f403c0cf68f9a94db4aaeb609f7855815fcdbb51"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDA2Njk2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongRareTermsAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo0MToyNVrOGijUlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo0MToyNVrOGijUlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4MzQ3OQ==", "bodyText": "Hmm, this is a bit confusingly named I think?  Maybe currentOffset or something?  Not sure, but size feels a bit confusing.", "url": "https://github.com/elastic/elasticsearch/pull/57948#discussion_r438883479", "createdAt": "2020-06-11T15:41:25Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongRareTermsAggregator.java", "diffHunk": "@@ -42,111 +43,144 @@\n /**\n  * An aggregator that finds \"rare\" string values (e.g. terms agg that orders ascending)\n  */\n-public class LongRareTermsAggregator extends AbstractRareTermsAggregator<ValuesSource.Numeric, IncludeExclude.LongFilter, Long> {\n-\n-    protected LongHash bucketOrds;\n-\n-    LongRareTermsAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource, DocValueFormat format,\n-                                   SearchContext aggregationContext, Aggregator parent, IncludeExclude.LongFilter longFilter,\n-                                   int maxDocCount, double precision, Map<String, Object> metadata) throws IOException {\n-        super(name, factories, aggregationContext, parent, metadata, maxDocCount, precision, format, valuesSource, longFilter);\n-        this.bucketOrds = new LongHash(1, aggregationContext.bigArrays());\n+public class LongRareTermsAggregator extends AbstractRareTermsAggregator {\n+    private final ValuesSource.Numeric valuesSource;\n+    private final IncludeExclude.LongFilter filter;\n+    private final LongKeyedBucketOrds bucketOrds;\n+\n+    LongRareTermsAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSource.Numeric valuesSource,\n+        DocValueFormat format,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        IncludeExclude.LongFilter filter,\n+        int maxDocCount,\n+        double precision,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        super(\n+            name,\n+            factories,\n+            aggregationContext,\n+            parent,\n+            metadata,\n+            maxDocCount,\n+            precision,\n+            format,\n+            collectsFromSingleBucket\n+        );\n+        this.valuesSource = valuesSource;\n+        this.filter = filter;\n+        this.bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n     }\n \n     protected SortedNumericDocValues getValues(ValuesSource.Numeric valuesSource, LeafReaderContext ctx) throws IOException {\n         return valuesSource.longValues(ctx);\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-                                                final LeafBucketCollector sub) throws IOException {\n-        final SortedNumericDocValues values = getValues(valuesSource, ctx);\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+        SortedNumericDocValues values = getValues(valuesSource, ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n-\n             @Override\n-            public void collect(int docId, long owningBucketOrdinal) throws IOException {\n-                if (values.advanceExact(docId)) {\n-                    final int valuesCount = values.docValueCount();\n-                    long previous = Long.MAX_VALUE;\n-                    for (int i = 0; i < valuesCount; ++i) {\n-                        final long val = values.nextValue();\n-                        if (previous != val || i == 0) {\n-                            if ((includeExclude == null) || (includeExclude.accept(val))) {\n-                                doCollect(sub, val, docId);\n-                            }\n-                            previous = val;\n-                        }\n+            public void collect(int docId, long owningBucketOrd) throws IOException {\n+                if (false == values.advanceExact(docId)) {\n+                    return;\n+                }\n+                int valuesCount = values.docValueCount();\n+                long previous = Long.MAX_VALUE;\n+                for (int i = 0; i < valuesCount; ++i) {\n+                    long val = values.nextValue();\n+                    if (i == 0 && previous == val) {\n+                        continue;\n+                    }\n+                    previous = val;\n+                    if (filter != null && false == filter.accept(val)) {\n+                        continue;\n+                    }\n+                    long bucketOrdinal = bucketOrds.add(owningBucketOrd, val);\n+                    if (bucketOrdinal < 0) { // already seen\n+                        bucketOrdinal = -1 - bucketOrdinal;\n+                        collectExistingBucket(sub, docId, bucketOrdinal);\n+                    } else {\n+                        collectBucket(sub, docId, bucketOrdinal);\n                     }\n                 }\n             }\n         };\n     }\n \n     @Override\n-    long addValueToOrds(Long value) {\n-        return bucketOrds.add(value);\n-    }\n-\n-    /**\n-     * Merges the ordinals to a minimal set, populates the CuckooFilter and\n-     * generates a final set of buckets.\n-     *\n-     * If a term is below the maxDocCount, it is turned into a Bucket.  Otherwise,\n-     * the term is added to the filter, and pruned from the ordinal map.  If\n-     * necessary the ordinal map is merged down to a minimal set to remove deletions\n-     */\n-    private List<LongRareTerms.Bucket> buildSketch() {\n-        long deletionCount = 0;\n-        LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-        List<LongRareTerms.Bucket> buckets = new ArrayList<>();\n-        try (LongHash oldBucketOrds = bucketOrds) {\n-\n-            long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-            for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                long oldKey = oldBucketOrds.get(i);\n-                long newBucketOrd = -1;\n-\n-                long docCount = bucketDocCount(i);\n-                // if the key is below threshold, reinsert into the new ords\n-                if (docCount <= maxDocCount) {\n-                    newBucketOrd = newBucketOrds.add(oldKey);\n-                    LongRareTerms.Bucket bucket = new LongRareTerms.Bucket(oldKey, docCount, null, format);\n-                    bucket.bucketOrd = newBucketOrd;\n-                    buckets.add(bucket);\n-                } else {\n-                    // Make a note when one of the ords has been deleted\n-                    deletionCount += 1;\n-                    filter.add(oldKey);\n+    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        /*\n+         * Collect the list of buckets, populate the filter with terms\n+         * that are too frequent, and figure out how to merge sub-buckets.\n+         */\n+        LongRareTerms.Bucket[][] rarestPerOrd = new LongRareTerms.Bucket[owningBucketOrds.length][];\n+        SetBackedScalingCuckooFilter[] filters = new SetBackedScalingCuckooFilter[owningBucketOrds.length];\n+        long keepCount = 0;\n+        long[] mergeMap = new long[(int) bucketOrds.size()];\n+        Arrays.fill(mergeMap, -1);\n+        long size = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f403c0cf68f9a94db4aaeb609f7855815fcdbb51"}, "originalPosition": 163}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDA3NzEwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongRareTermsAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo0Mzo1NFrOGijbPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMzo0ODowMFrOGjEmyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4NTE4Mw==", "bodyText": "Should we just change this to a boolean flag?  hasDeletions or whatever?", "url": "https://github.com/elastic/elasticsearch/pull/57948#discussion_r438885183", "createdAt": "2020-06-11T15:43:54Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongRareTermsAggregator.java", "diffHunk": "@@ -42,111 +43,144 @@\n /**\n  * An aggregator that finds \"rare\" string values (e.g. terms agg that orders ascending)\n  */\n-public class LongRareTermsAggregator extends AbstractRareTermsAggregator<ValuesSource.Numeric, IncludeExclude.LongFilter, Long> {\n-\n-    protected LongHash bucketOrds;\n-\n-    LongRareTermsAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource, DocValueFormat format,\n-                                   SearchContext aggregationContext, Aggregator parent, IncludeExclude.LongFilter longFilter,\n-                                   int maxDocCount, double precision, Map<String, Object> metadata) throws IOException {\n-        super(name, factories, aggregationContext, parent, metadata, maxDocCount, precision, format, valuesSource, longFilter);\n-        this.bucketOrds = new LongHash(1, aggregationContext.bigArrays());\n+public class LongRareTermsAggregator extends AbstractRareTermsAggregator {\n+    private final ValuesSource.Numeric valuesSource;\n+    private final IncludeExclude.LongFilter filter;\n+    private final LongKeyedBucketOrds bucketOrds;\n+\n+    LongRareTermsAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSource.Numeric valuesSource,\n+        DocValueFormat format,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        IncludeExclude.LongFilter filter,\n+        int maxDocCount,\n+        double precision,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        super(\n+            name,\n+            factories,\n+            aggregationContext,\n+            parent,\n+            metadata,\n+            maxDocCount,\n+            precision,\n+            format,\n+            collectsFromSingleBucket\n+        );\n+        this.valuesSource = valuesSource;\n+        this.filter = filter;\n+        this.bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n     }\n \n     protected SortedNumericDocValues getValues(ValuesSource.Numeric valuesSource, LeafReaderContext ctx) throws IOException {\n         return valuesSource.longValues(ctx);\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-                                                final LeafBucketCollector sub) throws IOException {\n-        final SortedNumericDocValues values = getValues(valuesSource, ctx);\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+        SortedNumericDocValues values = getValues(valuesSource, ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n-\n             @Override\n-            public void collect(int docId, long owningBucketOrdinal) throws IOException {\n-                if (values.advanceExact(docId)) {\n-                    final int valuesCount = values.docValueCount();\n-                    long previous = Long.MAX_VALUE;\n-                    for (int i = 0; i < valuesCount; ++i) {\n-                        final long val = values.nextValue();\n-                        if (previous != val || i == 0) {\n-                            if ((includeExclude == null) || (includeExclude.accept(val))) {\n-                                doCollect(sub, val, docId);\n-                            }\n-                            previous = val;\n-                        }\n+            public void collect(int docId, long owningBucketOrd) throws IOException {\n+                if (false == values.advanceExact(docId)) {\n+                    return;\n+                }\n+                int valuesCount = values.docValueCount();\n+                long previous = Long.MAX_VALUE;\n+                for (int i = 0; i < valuesCount; ++i) {\n+                    long val = values.nextValue();\n+                    if (i == 0 && previous == val) {\n+                        continue;\n+                    }\n+                    previous = val;\n+                    if (filter != null && false == filter.accept(val)) {\n+                        continue;\n+                    }\n+                    long bucketOrdinal = bucketOrds.add(owningBucketOrd, val);\n+                    if (bucketOrdinal < 0) { // already seen\n+                        bucketOrdinal = -1 - bucketOrdinal;\n+                        collectExistingBucket(sub, docId, bucketOrdinal);\n+                    } else {\n+                        collectBucket(sub, docId, bucketOrdinal);\n                     }\n                 }\n             }\n         };\n     }\n \n     @Override\n-    long addValueToOrds(Long value) {\n-        return bucketOrds.add(value);\n-    }\n-\n-    /**\n-     * Merges the ordinals to a minimal set, populates the CuckooFilter and\n-     * generates a final set of buckets.\n-     *\n-     * If a term is below the maxDocCount, it is turned into a Bucket.  Otherwise,\n-     * the term is added to the filter, and pruned from the ordinal map.  If\n-     * necessary the ordinal map is merged down to a minimal set to remove deletions\n-     */\n-    private List<LongRareTerms.Bucket> buildSketch() {\n-        long deletionCount = 0;\n-        LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-        List<LongRareTerms.Bucket> buckets = new ArrayList<>();\n-        try (LongHash oldBucketOrds = bucketOrds) {\n-\n-            long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-            for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                long oldKey = oldBucketOrds.get(i);\n-                long newBucketOrd = -1;\n-\n-                long docCount = bucketDocCount(i);\n-                // if the key is below threshold, reinsert into the new ords\n-                if (docCount <= maxDocCount) {\n-                    newBucketOrd = newBucketOrds.add(oldKey);\n-                    LongRareTerms.Bucket bucket = new LongRareTerms.Bucket(oldKey, docCount, null, format);\n-                    bucket.bucketOrd = newBucketOrd;\n-                    buckets.add(bucket);\n-                } else {\n-                    // Make a note when one of the ords has been deleted\n-                    deletionCount += 1;\n-                    filter.add(oldKey);\n+    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        /*\n+         * Collect the list of buckets, populate the filter with terms\n+         * that are too frequent, and figure out how to merge sub-buckets.\n+         */\n+        LongRareTerms.Bucket[][] rarestPerOrd = new LongRareTerms.Bucket[owningBucketOrds.length][];\n+        SetBackedScalingCuckooFilter[] filters = new SetBackedScalingCuckooFilter[owningBucketOrds.length];\n+        long keepCount = 0;\n+        long[] mergeMap = new long[(int) bucketOrds.size()];\n+        Arrays.fill(mergeMap, -1);\n+        long size = 0;\n+        for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) {\n+            try (LongHash ordsToCollect = new LongHash(1, context.bigArrays())) {\n+                filters[ordIdx] = newFilter();\n+                List<LongRareTerms.Bucket> buckets = new ArrayList<>();\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrds[ordIdx]);\n+                while (ordsEnum.next()) {\n+                    long docCount = bucketDocCount(ordsEnum.ord());\n+                    // if the key is below threshold, reinsert into the new ords\n+                    if (docCount <= maxDocCount) {\n+                        LongRareTerms.Bucket bucket = new LongRareTerms.Bucket(ordsEnum.value(), docCount, null, format);\n+                        bucket.bucketOrd = mergeMap[(int) ordsEnum.ord()] = size + ordsToCollect.add(ordsEnum.value());\n+                        buckets.add(bucket);\n+                        keepCount++;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f403c0cf68f9a94db4aaeb609f7855815fcdbb51"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQyODgxMQ==", "bodyText": "I think we need to perform the merge if we don't keep all the buckets. We can remove buckets for two reasons now!\n\nThe key is above the threshold.\nThe owningBucketOrd isn't selected.\n\nThis counter will catch both ways. I couldn't come up with a cleaner way to do it.", "url": "https://github.com/elastic/elasticsearch/pull/57948#discussion_r439428811", "createdAt": "2020-06-12T13:48:00Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongRareTermsAggregator.java", "diffHunk": "@@ -42,111 +43,144 @@\n /**\n  * An aggregator that finds \"rare\" string values (e.g. terms agg that orders ascending)\n  */\n-public class LongRareTermsAggregator extends AbstractRareTermsAggregator<ValuesSource.Numeric, IncludeExclude.LongFilter, Long> {\n-\n-    protected LongHash bucketOrds;\n-\n-    LongRareTermsAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource, DocValueFormat format,\n-                                   SearchContext aggregationContext, Aggregator parent, IncludeExclude.LongFilter longFilter,\n-                                   int maxDocCount, double precision, Map<String, Object> metadata) throws IOException {\n-        super(name, factories, aggregationContext, parent, metadata, maxDocCount, precision, format, valuesSource, longFilter);\n-        this.bucketOrds = new LongHash(1, aggregationContext.bigArrays());\n+public class LongRareTermsAggregator extends AbstractRareTermsAggregator {\n+    private final ValuesSource.Numeric valuesSource;\n+    private final IncludeExclude.LongFilter filter;\n+    private final LongKeyedBucketOrds bucketOrds;\n+\n+    LongRareTermsAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSource.Numeric valuesSource,\n+        DocValueFormat format,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        IncludeExclude.LongFilter filter,\n+        int maxDocCount,\n+        double precision,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        super(\n+            name,\n+            factories,\n+            aggregationContext,\n+            parent,\n+            metadata,\n+            maxDocCount,\n+            precision,\n+            format,\n+            collectsFromSingleBucket\n+        );\n+        this.valuesSource = valuesSource;\n+        this.filter = filter;\n+        this.bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n     }\n \n     protected SortedNumericDocValues getValues(ValuesSource.Numeric valuesSource, LeafReaderContext ctx) throws IOException {\n         return valuesSource.longValues(ctx);\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-                                                final LeafBucketCollector sub) throws IOException {\n-        final SortedNumericDocValues values = getValues(valuesSource, ctx);\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+        SortedNumericDocValues values = getValues(valuesSource, ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n-\n             @Override\n-            public void collect(int docId, long owningBucketOrdinal) throws IOException {\n-                if (values.advanceExact(docId)) {\n-                    final int valuesCount = values.docValueCount();\n-                    long previous = Long.MAX_VALUE;\n-                    for (int i = 0; i < valuesCount; ++i) {\n-                        final long val = values.nextValue();\n-                        if (previous != val || i == 0) {\n-                            if ((includeExclude == null) || (includeExclude.accept(val))) {\n-                                doCollect(sub, val, docId);\n-                            }\n-                            previous = val;\n-                        }\n+            public void collect(int docId, long owningBucketOrd) throws IOException {\n+                if (false == values.advanceExact(docId)) {\n+                    return;\n+                }\n+                int valuesCount = values.docValueCount();\n+                long previous = Long.MAX_VALUE;\n+                for (int i = 0; i < valuesCount; ++i) {\n+                    long val = values.nextValue();\n+                    if (i == 0 && previous == val) {\n+                        continue;\n+                    }\n+                    previous = val;\n+                    if (filter != null && false == filter.accept(val)) {\n+                        continue;\n+                    }\n+                    long bucketOrdinal = bucketOrds.add(owningBucketOrd, val);\n+                    if (bucketOrdinal < 0) { // already seen\n+                        bucketOrdinal = -1 - bucketOrdinal;\n+                        collectExistingBucket(sub, docId, bucketOrdinal);\n+                    } else {\n+                        collectBucket(sub, docId, bucketOrdinal);\n                     }\n                 }\n             }\n         };\n     }\n \n     @Override\n-    long addValueToOrds(Long value) {\n-        return bucketOrds.add(value);\n-    }\n-\n-    /**\n-     * Merges the ordinals to a minimal set, populates the CuckooFilter and\n-     * generates a final set of buckets.\n-     *\n-     * If a term is below the maxDocCount, it is turned into a Bucket.  Otherwise,\n-     * the term is added to the filter, and pruned from the ordinal map.  If\n-     * necessary the ordinal map is merged down to a minimal set to remove deletions\n-     */\n-    private List<LongRareTerms.Bucket> buildSketch() {\n-        long deletionCount = 0;\n-        LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-        List<LongRareTerms.Bucket> buckets = new ArrayList<>();\n-        try (LongHash oldBucketOrds = bucketOrds) {\n-\n-            long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-            for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                long oldKey = oldBucketOrds.get(i);\n-                long newBucketOrd = -1;\n-\n-                long docCount = bucketDocCount(i);\n-                // if the key is below threshold, reinsert into the new ords\n-                if (docCount <= maxDocCount) {\n-                    newBucketOrd = newBucketOrds.add(oldKey);\n-                    LongRareTerms.Bucket bucket = new LongRareTerms.Bucket(oldKey, docCount, null, format);\n-                    bucket.bucketOrd = newBucketOrd;\n-                    buckets.add(bucket);\n-                } else {\n-                    // Make a note when one of the ords has been deleted\n-                    deletionCount += 1;\n-                    filter.add(oldKey);\n+    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        /*\n+         * Collect the list of buckets, populate the filter with terms\n+         * that are too frequent, and figure out how to merge sub-buckets.\n+         */\n+        LongRareTerms.Bucket[][] rarestPerOrd = new LongRareTerms.Bucket[owningBucketOrds.length][];\n+        SetBackedScalingCuckooFilter[] filters = new SetBackedScalingCuckooFilter[owningBucketOrds.length];\n+        long keepCount = 0;\n+        long[] mergeMap = new long[(int) bucketOrds.size()];\n+        Arrays.fill(mergeMap, -1);\n+        long size = 0;\n+        for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) {\n+            try (LongHash ordsToCollect = new LongHash(1, context.bigArrays())) {\n+                filters[ordIdx] = newFilter();\n+                List<LongRareTerms.Bucket> buckets = new ArrayList<>();\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrds[ordIdx]);\n+                while (ordsEnum.next()) {\n+                    long docCount = bucketDocCount(ordsEnum.ord());\n+                    // if the key is below threshold, reinsert into the new ords\n+                    if (docCount <= maxDocCount) {\n+                        LongRareTerms.Bucket bucket = new LongRareTerms.Bucket(ordsEnum.value(), docCount, null, format);\n+                        bucket.bucketOrd = mergeMap[(int) ordsEnum.ord()] = size + ordsToCollect.add(ordsEnum.value());\n+                        buckets.add(bucket);\n+                        keepCount++;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4NTE4Mw=="}, "originalCommit": {"oid": "f403c0cf68f9a94db4aaeb609f7855815fcdbb51"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDA4NzAwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringRareTermsAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo0NTo1OFrOGijh1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxMzo0ODoxOVrOGjEngA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4Njg2OQ==", "bodyText": "Nit: !filter.accept() :)\n(Also I realize the irony since the original code had that and it was my fault :) )", "url": "https://github.com/elastic/elasticsearch/pull/57948#discussion_r438886869", "createdAt": "2020-06-11T15:45:58Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringRareTermsAggregator.java", "diffHunk": "@@ -63,96 +87,106 @@ public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n             final BytesRefBuilder previous = new BytesRefBuilder();\n \n             @Override\n-            public void collect(int docId, long bucket) throws IOException {\n-                assert bucket == 0;\n-                if (values.advanceExact(docId)) {\n-                    final int valuesCount = values.docValueCount();\n-                    previous.clear();\n-\n-                    // SortedBinaryDocValues don't guarantee uniqueness so we\n-                    // need to take care of dups\n-                    for (int i = 0; i < valuesCount; ++i) {\n-                        final BytesRef bytes = values.nextValue();\n-                        if (includeExclude != null && !includeExclude.accept(bytes)) {\n-                            continue;\n-                        }\n-                        if (i > 0 && previous.get().equals(bytes)) {\n-                            continue;\n-                        }\n-\n-                        doCollect(sub, bytes, docId);\n-                        previous.copyBytes(bytes);\n+            public void collect(int docId, long owningBucketOrd) throws IOException {\n+                if (false == values.advanceExact(docId)) {\n+                    return;\n+                }\n+                int valuesCount = values.docValueCount();\n+                previous.clear();\n+\n+                // SortedBinaryDocValues don't guarantee uniqueness so we\n+                // need to take care of dups\n+                for (int i = 0; i < valuesCount; ++i) {\n+                    BytesRef bytes = values.nextValue();\n+                    if (filter != null && !filter.accept(bytes)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f403c0cf68f9a94db4aaeb609f7855815fcdbb51"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQyODk5Mg==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/57948#discussion_r439428992", "createdAt": "2020-06-12T13:48:19Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringRareTermsAggregator.java", "diffHunk": "@@ -63,96 +87,106 @@ public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n             final BytesRefBuilder previous = new BytesRefBuilder();\n \n             @Override\n-            public void collect(int docId, long bucket) throws IOException {\n-                assert bucket == 0;\n-                if (values.advanceExact(docId)) {\n-                    final int valuesCount = values.docValueCount();\n-                    previous.clear();\n-\n-                    // SortedBinaryDocValues don't guarantee uniqueness so we\n-                    // need to take care of dups\n-                    for (int i = 0; i < valuesCount; ++i) {\n-                        final BytesRef bytes = values.nextValue();\n-                        if (includeExclude != null && !includeExclude.accept(bytes)) {\n-                            continue;\n-                        }\n-                        if (i > 0 && previous.get().equals(bytes)) {\n-                            continue;\n-                        }\n-\n-                        doCollect(sub, bytes, docId);\n-                        previous.copyBytes(bytes);\n+            public void collect(int docId, long owningBucketOrd) throws IOException {\n+                if (false == values.advanceExact(docId)) {\n+                    return;\n+                }\n+                int valuesCount = values.docValueCount();\n+                previous.clear();\n+\n+                // SortedBinaryDocValues don't guarantee uniqueness so we\n+                // need to take care of dups\n+                for (int i = 0; i < valuesCount; ++i) {\n+                    BytesRef bytes = values.nextValue();\n+                    if (filter != null && !filter.accept(bytes)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4Njg2OQ=="}, "originalCommit": {"oid": "f403c0cf68f9a94db4aaeb609f7855815fcdbb51"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDEzNDc2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongRareTermsAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo1NDowM1rOGij_TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo1NDowM1rOGij_TA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5NDQxMg==", "bodyText": "General comment about this method:  we have a lot of \"ords\" being referenced and it's hard to keep track of which ord is which.  E.g. we have the bucket ordinals that our parent is requesting we build, and then we have the bucket ordinals from each of those instances that we are collecting into buckets\nNot sure how, but if we could find a way to rename the variables to help identify or disambiguate I think it would help a bunch.", "url": "https://github.com/elastic/elasticsearch/pull/57948#discussion_r438894412", "createdAt": "2020-06-11T15:54:03Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongRareTermsAggregator.java", "diffHunk": "@@ -42,111 +43,144 @@\n /**\n  * An aggregator that finds \"rare\" string values (e.g. terms agg that orders ascending)\n  */\n-public class LongRareTermsAggregator extends AbstractRareTermsAggregator<ValuesSource.Numeric, IncludeExclude.LongFilter, Long> {\n-\n-    protected LongHash bucketOrds;\n-\n-    LongRareTermsAggregator(String name, AggregatorFactories factories, ValuesSource.Numeric valuesSource, DocValueFormat format,\n-                                   SearchContext aggregationContext, Aggregator parent, IncludeExclude.LongFilter longFilter,\n-                                   int maxDocCount, double precision, Map<String, Object> metadata) throws IOException {\n-        super(name, factories, aggregationContext, parent, metadata, maxDocCount, precision, format, valuesSource, longFilter);\n-        this.bucketOrds = new LongHash(1, aggregationContext.bigArrays());\n+public class LongRareTermsAggregator extends AbstractRareTermsAggregator {\n+    private final ValuesSource.Numeric valuesSource;\n+    private final IncludeExclude.LongFilter filter;\n+    private final LongKeyedBucketOrds bucketOrds;\n+\n+    LongRareTermsAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        ValuesSource.Numeric valuesSource,\n+        DocValueFormat format,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        IncludeExclude.LongFilter filter,\n+        int maxDocCount,\n+        double precision,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        super(\n+            name,\n+            factories,\n+            aggregationContext,\n+            parent,\n+            metadata,\n+            maxDocCount,\n+            precision,\n+            format,\n+            collectsFromSingleBucket\n+        );\n+        this.valuesSource = valuesSource;\n+        this.filter = filter;\n+        this.bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n     }\n \n     protected SortedNumericDocValues getValues(ValuesSource.Numeric valuesSource, LeafReaderContext ctx) throws IOException {\n         return valuesSource.longValues(ctx);\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-                                                final LeafBucketCollector sub) throws IOException {\n-        final SortedNumericDocValues values = getValues(valuesSource, ctx);\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+        SortedNumericDocValues values = getValues(valuesSource, ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n-\n             @Override\n-            public void collect(int docId, long owningBucketOrdinal) throws IOException {\n-                if (values.advanceExact(docId)) {\n-                    final int valuesCount = values.docValueCount();\n-                    long previous = Long.MAX_VALUE;\n-                    for (int i = 0; i < valuesCount; ++i) {\n-                        final long val = values.nextValue();\n-                        if (previous != val || i == 0) {\n-                            if ((includeExclude == null) || (includeExclude.accept(val))) {\n-                                doCollect(sub, val, docId);\n-                            }\n-                            previous = val;\n-                        }\n+            public void collect(int docId, long owningBucketOrd) throws IOException {\n+                if (false == values.advanceExact(docId)) {\n+                    return;\n+                }\n+                int valuesCount = values.docValueCount();\n+                long previous = Long.MAX_VALUE;\n+                for (int i = 0; i < valuesCount; ++i) {\n+                    long val = values.nextValue();\n+                    if (i == 0 && previous == val) {\n+                        continue;\n+                    }\n+                    previous = val;\n+                    if (filter != null && false == filter.accept(val)) {\n+                        continue;\n+                    }\n+                    long bucketOrdinal = bucketOrds.add(owningBucketOrd, val);\n+                    if (bucketOrdinal < 0) { // already seen\n+                        bucketOrdinal = -1 - bucketOrdinal;\n+                        collectExistingBucket(sub, docId, bucketOrdinal);\n+                    } else {\n+                        collectBucket(sub, docId, bucketOrdinal);\n                     }\n                 }\n             }\n         };\n     }\n \n     @Override\n-    long addValueToOrds(Long value) {\n-        return bucketOrds.add(value);\n-    }\n-\n-    /**\n-     * Merges the ordinals to a minimal set, populates the CuckooFilter and\n-     * generates a final set of buckets.\n-     *\n-     * If a term is below the maxDocCount, it is turned into a Bucket.  Otherwise,\n-     * the term is added to the filter, and pruned from the ordinal map.  If\n-     * necessary the ordinal map is merged down to a minimal set to remove deletions\n-     */\n-    private List<LongRareTerms.Bucket> buildSketch() {\n-        long deletionCount = 0;\n-        LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-        List<LongRareTerms.Bucket> buckets = new ArrayList<>();\n-        try (LongHash oldBucketOrds = bucketOrds) {\n-\n-            long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-            for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                long oldKey = oldBucketOrds.get(i);\n-                long newBucketOrd = -1;\n-\n-                long docCount = bucketDocCount(i);\n-                // if the key is below threshold, reinsert into the new ords\n-                if (docCount <= maxDocCount) {\n-                    newBucketOrd = newBucketOrds.add(oldKey);\n-                    LongRareTerms.Bucket bucket = new LongRareTerms.Bucket(oldKey, docCount, null, format);\n-                    bucket.bucketOrd = newBucketOrd;\n-                    buckets.add(bucket);\n-                } else {\n-                    // Make a note when one of the ords has been deleted\n-                    deletionCount += 1;\n-                    filter.add(oldKey);\n+    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f403c0cf68f9a94db4aaeb609f7855815fcdbb51"}, "originalPosition": 153}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1680, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}