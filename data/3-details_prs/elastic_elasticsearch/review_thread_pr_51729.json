{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY5NDg1MTM0", "number": 51729, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzozOTo1NlrODcPO0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxNDowMjozMlrODeB7QA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwOTM2MjcyOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzozOTo1NlrOFkLlyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzozOTo1NlrOFkLlyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQ4Mjk1NQ==", "bodyText": "It's a little annoying but if we run this test with more than a single shard, then it might be that we get shards that aren't corrupted because they contain no documents now as well as no physical data files because only the .si and segments_N are uploaded for the empty shards.", "url": "https://github.com/elastic/elasticsearch/pull/51729#discussion_r373482955", "createdAt": "2020-01-31T13:39:56Z", "author": {"login": "original-brownbear"}, "path": "server/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreIT.java", "diffHunk": "@@ -1035,7 +1035,8 @@ public void testUnrestorableFilesDuringRestore() throws Exception {\n         final String indexName = \"unrestorable-files\";\n         final int maxRetries = randomIntBetween(1, 10);\n \n-        Settings createIndexSettings = Settings.builder().put(SETTING_ALLOCATION_MAX_RETRY.getKey(), maxRetries).build();\n+        Settings createIndexSettings = Settings.builder().put(SETTING_ALLOCATION_MAX_RETRY.getKey(), maxRetries)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d557a32cc63edc35db02746df828f9025865169"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwOTM3MjMwOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzo0MzowOFrOFkLrnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzo0MzowOFrOFkLrnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQ4NDQ0NA==", "bodyText": "This simply doesn't match up now and I figured what really matters in these tests is the consistency of the numbers. I didn't want to adjust the results for the file counts to filter out the files that weren't physically uploaded since they are still uploaded as part of the metadata technically.\nWe do have a bunch of other tests that verify all the files are there and incrementality works as expected so I figured this adjustment is good enough.", "url": "https://github.com/elastic/elasticsearch/pull/51729#discussion_r373484444", "createdAt": "2020-01-31T13:43:08Z", "author": {"login": "original-brownbear"}, "path": "server/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java", "diffHunk": "@@ -1132,11 +1132,11 @@ public void testSnapshotTotalAndIncrementalSizes() throws IOException {\n \n         SnapshotStats stats = snapshots.get(0).getStats();\n \n-        assertThat(stats.getTotalFileCount(), is(snapshot0FileCount));\n-        assertThat(stats.getTotalSize(), is(snapshot0FileSize));\n+        assertThat(stats.getTotalFileCount(), greaterThanOrEqualTo(snapshot0FileCount));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d557a32cc63edc35db02746df828f9025865169"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwOTM4Mzk4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzo0NzoxOVrOFkLyyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxMzo0NzoxOVrOFkLyyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzQ4NjI4MQ==", "bodyText": "Still need to have virtual blob here so that the logic/assertions in BlobStoreIndexShardSnapshot(s) works without format change and things stay BwC. Technically we could do without this kind of \"ID\", but then we'd have to go through the whole dance of only updating the format and not uploading the blob once all the snapshots are newer than version X. If we do it this way, we get the benefit of faster restores and snapshots right away since the meta hash works the same way in 6.x already hence even in 7.7 all possible restores should work out.", "url": "https://github.com/elastic/elasticsearch/pull/51729#discussion_r373486281", "createdAt": "2020-01-31T13:47:19Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -179,7 +180,9 @@\n \n     private static final String SNAPSHOT_INDEX_CODEC = \"snapshots\";\n \n-    private static final String DATA_BLOB_PREFIX = \"__\";\n+    private static final String UPLOADED_DATA_BLOB_PREFIX = \"__\";\n+\n+    private static final String VIRTUAL_DATA_BLOB_PREFIX = \"v__\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d557a32cc63edc35db02746df828f9025865169"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyODE0ODk4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxNDowMDo0MFrOFm94Ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxNDowMDo0MFrOFm94Ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQwMzk4Nw==", "bodyText": "can you add some Javadocs here explaining what virtual data blobs are?", "url": "https://github.com/elastic/elasticsearch/pull/51729#discussion_r376403987", "createdAt": "2020-02-07T14:00:40Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -179,7 +180,9 @@\n \n     private static final String SNAPSHOT_INDEX_CODEC = \"snapshots\";\n \n-    private static final String DATA_BLOB_PREFIX = \"__\";\n+    private static final String UPLOADED_DATA_BLOB_PREFIX = \"__\";\n+\n+    private static final String VIRTUAL_DATA_BLOB_PREFIX = \"v__\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c12ed6961ec23b3e8e819308781880c0ee3de4ce"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyODE1NDI0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxNDowMjozMlrOFm97RA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxNzo1NzoyMlrOFnFTjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQwNDgwNA==", "bodyText": "can you assert here that the content is indeed the same? EDIT: happens below. This might be too subtle otherwise. Perhaps also add a method to StoreFileMetaData that marks these files specially, so that it's more obvious to identify the files.", "url": "https://github.com/elastic/elasticsearch/pull/51729#discussion_r376404804", "createdAt": "2020-02-07T14:02:32Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1516,6 +1519,9 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     }\n                 }\n \n+                // We can skip writing blobs where the metadata hash is equal to the blob's contents because we store the hash/contents\n+                // directly in the shard level metadata in this case\n+                final boolean needsWrite = md.hash().length != md.length();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c12ed6961ec23b3e8e819308781880c0ee3de4ce"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjUyNTcxMA==", "bodyText": "Done in 8794541 ... actually found a way to make this even safer via the checksum test just in case some will use this method elsewhere or something changes in the future. Maybe take another look before I merge?", "url": "https://github.com/elastic/elasticsearch/pull/51729#discussion_r376525710", "createdAt": "2020-02-07T17:57:22Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1516,6 +1519,9 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     }\n                 }\n \n+                // We can skip writing blobs where the metadata hash is equal to the blob's contents because we store the hash/contents\n+                // directly in the shard level metadata in this case\n+                final boolean needsWrite = md.hash().length != md.length();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQwNDgwNA=="}, "originalCommit": {"oid": "c12ed6961ec23b3e8e819308781880c0ee3de4ce"}, "originalPosition": 25}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 172, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}