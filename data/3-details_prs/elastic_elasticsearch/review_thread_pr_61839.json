{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc3NzIxMDQ3", "number": 61839, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMDowNToxOVrOEpZ6qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzozNDozM1rOEp0O2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExODUxNjkxOnYy", "diffSide": "RIGHT", "path": "docs/reference/snapshot-restore/apis/clone-snapshot-api.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMDowNToxOVrOHbDaYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMDowNToxOVrOHbDaYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODEyOTUwNw==", "bodyText": "I'm not sure split strings are also trimmed so it's better to not add a white space after the comma\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              \"indices\": \"index_a, index_b\"\n          \n          \n            \n              \"indices\": \"index_a,index_b\"", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498129507", "createdAt": "2020-10-01T10:05:19Z", "author": {"login": "tlrx"}, "path": "docs/reference/snapshot-restore/apis/clone-snapshot-api.asciidoc", "diffHunk": "@@ -0,0 +1,52 @@\n+[[clone-snapshot-api]]\n+=== Clone snapshot API\n+++++\n+<titleabbrev>Clone snapshot</titleabbrev>\n+++++\n+\n+Clones part or all of a snapshot into a new snapshot.\n+\n+[source,console]\n+----\n+PUT /_snapshot/my_repository/source_snapshot/_clone/target_snapshot\n+{\n+  \"indices\": \"index_a, index_b\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExODUxOTk2OnYy", "diffSide": "RIGHT", "path": "docs/reference/snapshot-restore/apis/clone-snapshot-api.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMDowNjowM1rOHbDcGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMDowNjowM1rOHbDcGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODEyOTk0Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Name of the snapshot repository that both source- and target snapshot belong to.\n          \n          \n            \n            Name of the snapshot repository that both source and target snapshot belong to.", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498129947", "createdAt": "2020-10-01T10:06:03Z", "author": {"login": "tlrx"}, "path": "docs/reference/snapshot-restore/apis/clone-snapshot-api.asciidoc", "diffHunk": "@@ -0,0 +1,52 @@\n+[[clone-snapshot-api]]\n+=== Clone snapshot API\n+++++\n+<titleabbrev>Clone snapshot</titleabbrev>\n+++++\n+\n+Clones part or all of a snapshot into a new snapshot.\n+\n+[source,console]\n+----\n+PUT /_snapshot/my_repository/source_snapshot/_clone/target_snapshot\n+{\n+  \"indices\": \"index_a, index_b\"\n+}\n+----\n+// TEST[skip:TODO]\n+\n+[[clone-snapshot-api-request]]\n+==== {api-request-title}\n+\n+`PUT /_snapshot/<repository>/<source_snapshot>/_clone/<target_snapshot>`\n+\n+[[clone-snapshot-api-desc]]\n+==== {api-description-title}\n+\n+The clone snapshot API allows creating a copy of all or part of an existing snapshot\n+within the same repository.\n+\n+[[clone-snapshot-api-params]]\n+==== {api-path-parms-title}\n+\n+`<repository>`::\n+(Required, string)\n+Name of the snapshot repository that both source- and target snapshot belong to.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExODUyMjg2OnYy", "diffSide": "RIGHT", "path": "docs/reference/snapshot-restore/take-snapshot.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMDowNjo1MVrOHbDd2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNDowMTo0NVrOHbL4DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODEzMDM5NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NOTE: You can also create snapshots that are copies of part of an existing snapshot using the <<clone-snapshot-api,clone snapshot API>>.\n          \n          \n            \n            NOTE: You can also create snapshots that are copies or part of an existing snapshot using the <<clone-snapshot-api,clone snapshot API>>.", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498130394", "createdAt": "2020-10-01T10:06:51Z", "author": {"login": "tlrx"}, "path": "docs/reference/snapshot-restore/take-snapshot.asciidoc", "diffHunk": "@@ -124,3 +124,5 @@ PUT /_snapshot/my_backup/<snapshot-{now/d}>\n PUT /_snapshot/my_backup/%3Csnapshot-%7Bnow%2Fd%7D%3E\n -----------------------------------\n // TEST[continued]\n+\n+NOTE: You can also create snapshots that are copies of part of an existing snapshot using the <<clone-snapshot-api,clone snapshot API>>.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI2ODE3Mg==", "bodyText": "This was actually intentional. Cloning is there to copy part of a snapshot, cloning a complete snapshot doesn't really make any sense.", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498268172", "createdAt": "2020-10-01T14:01:45Z", "author": {"login": "original-brownbear"}, "path": "docs/reference/snapshot-restore/take-snapshot.asciidoc", "diffHunk": "@@ -124,3 +124,5 @@ PUT /_snapshot/my_backup/<snapshot-{now/d}>\n PUT /_snapshot/my_backup/%3Csnapshot-%7Bnow%2Fd%7D%3E\n -----------------------------------\n // TEST[continued]\n+\n+NOTE: You can also create snapshots that are copies of part of an existing snapshot using the <<clone-snapshot-api,clone snapshot API>>.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODEzMDM5NA=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExODc3NTQzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMToyMjoxNFrOHbF3TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNDoxMjoyMlrOHbMWRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE2OTY3Ng==", "bodyText": "I think we are silently losing global metadata and data streams information, are we ok with that?", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498169676", "createdAt": "2020-10-01T11:22:14Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -96,18 +99,52 @@ public static Entry startedEntry(Snapshot snapshot, boolean includeGlobalState,\n                 indices, dataStreams, startTime, repositoryStateId, shards, null, userMetadata, version);\n     }\n \n+    /**\n+     * Creates the initial snapshot clone entry\n+     *\n+     * @param snapshot snapshot to clone into\n+     * @param source   snapshot to clone from\n+     * @param indices  indices to clone\n+     * @param startTime start time\n+     * @param repositoryStateId repository state id that this clone is based on\n+     * @param version repository metadata version to write\n+     * @return snapshot clone entry\n+     */\n+    public static Entry startClone(Snapshot snapshot, SnapshotId source, List<IndexId> indices, long startTime,\n+                                   long repositoryStateId, Version version) {\n+        return new SnapshotsInProgress.Entry(snapshot, false, false, State.STARTED, indices, Collections.emptyList(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIyNDMwOQ==", "bodyText": "My understanding would be yes since the motivation here was cloning indices. That said, I could just add the ability to clone global meta as well in a follow up? It's not hard to add but adds yet another step of complexity to snapshot finalization so that going into a separate step might be a good idea if we want it?", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498224309", "createdAt": "2020-10-01T12:59:28Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -96,18 +99,52 @@ public static Entry startedEntry(Snapshot snapshot, boolean includeGlobalState,\n                 indices, dataStreams, startTime, repositoryStateId, shards, null, userMetadata, version);\n     }\n \n+    /**\n+     * Creates the initial snapshot clone entry\n+     *\n+     * @param snapshot snapshot to clone into\n+     * @param source   snapshot to clone from\n+     * @param indices  indices to clone\n+     * @param startTime start time\n+     * @param repositoryStateId repository state id that this clone is based on\n+     * @param version repository metadata version to write\n+     * @return snapshot clone entry\n+     */\n+    public static Entry startClone(Snapshot snapshot, SnapshotId source, List<IndexId> indices, long startTime,\n+                                   long repositoryStateId, Version version) {\n+        return new SnapshotsInProgress.Entry(snapshot, false, false, State.STARTED, indices, Collections.emptyList(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE2OTY3Ng=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIzMDEyOA==", "bodyText": "Again I'm afraid of silent loses. If a snapshot contains the global metadata I'd expect its clone to also contains it (or vice versa). I'm ok for a follow up as long as it lands in the feature before it's backported.", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498230128", "createdAt": "2020-10-01T13:08:34Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -96,18 +99,52 @@ public static Entry startedEntry(Snapshot snapshot, boolean includeGlobalState,\n                 indices, dataStreams, startTime, repositoryStateId, shards, null, userMetadata, version);\n     }\n \n+    /**\n+     * Creates the initial snapshot clone entry\n+     *\n+     * @param snapshot snapshot to clone into\n+     * @param source   snapshot to clone from\n+     * @param indices  indices to clone\n+     * @param startTime start time\n+     * @param repositoryStateId repository state id that this clone is based on\n+     * @param version repository metadata version to write\n+     * @return snapshot clone entry\n+     */\n+    public static Entry startClone(Snapshot snapshot, SnapshotId source, List<IndexId> indices, long startTime,\n+                                   long repositoryStateId, Version version) {\n+        return new SnapshotsInProgress.Entry(snapshot, false, false, State.STARTED, indices, Collections.emptyList(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE2OTY3Ng=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI3NTkwOQ==", "bodyText": "Actually ... turns out bdd6943 is all that's needed to get the global metadata into the clone. The more interesting question is data streams. I'm not sure we would just want to clone those as well every time because of how they interact with indices. That might technically need smarter logic there, I'll look into those in a follow-up.", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498275909", "createdAt": "2020-10-01T14:12:22Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -96,18 +99,52 @@ public static Entry startedEntry(Snapshot snapshot, boolean includeGlobalState,\n                 indices, dataStreams, startTime, repositoryStateId, shards, null, userMetadata, version);\n     }\n \n+    /**\n+     * Creates the initial snapshot clone entry\n+     *\n+     * @param snapshot snapshot to clone into\n+     * @param source   snapshot to clone from\n+     * @param indices  indices to clone\n+     * @param startTime start time\n+     * @param repositoryStateId repository state id that this clone is based on\n+     * @param version repository metadata version to write\n+     * @return snapshot clone entry\n+     */\n+    public static Entry startClone(Snapshot snapshot, SnapshotId source, List<IndexId> indices, long startTime,\n+                                   long repositoryStateId, Version version) {\n+        return new SnapshotsInProgress.Entry(snapshot, false, false, State.STARTED, indices, Collections.emptyList(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE2OTY3Ng=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExODgwNTQxOnYy", "diffSide": "RIGHT", "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/CloneSnapshotIT.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMTozMTo1MVrOHbGJrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQwODozMTowMVrOHblWRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE3NDM4MQ==", "bodyText": "Do you think we could have a test that fails to clone a snapshot with failed indices?", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498174381", "createdAt": "2020-10-01T11:31:51Z", "author": {"login": "tlrx"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/CloneSnapshotIT.java", "diffHunk": "@@ -107,6 +122,365 @@ public void testShardClone() throws Exception {\n         assertEquals(newShardGeneration, newShardGeneration2);\n     }\n \n+    public void testCloneSnapshotIndex() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"fs\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+        if (randomBoolean()) {\n+            assertAcked(admin().indices().prepareDelete(indexName));\n+        }\n+        final String targetSnapshot = \"target-snapshot\";\n+        assertAcked(startClone(repoName, sourceSnapshot, targetSnapshot, indexName).get());\n+\n+        final List<SnapshotStatus> status = clusterAdmin().prepareSnapshotStatus(repoName)\n+                .setSnapshots(sourceSnapshot, targetSnapshot).get().getSnapshots();\n+        assertThat(status, hasSize(2));\n+        final SnapshotIndexStatus status1 = status.get(0).getIndices().get(indexName);\n+        final SnapshotIndexStatus status2 = status.get(1).getIndices().get(indexName);\n+        assertEquals(status1.getStats().getTotalFileCount(), status2.getStats().getTotalFileCount());\n+        assertEquals(status1.getStats().getTotalSize(), status2.getStats().getTotalSize());\n+    }\n+\n+    public void testClonePreventsSnapshotDelete() throws Exception {\n+        final String masterName = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockNodeOnAnyFiles(repoName, masterName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexName);\n+        waitForBlock(masterName, repoName, TimeValue.timeValueSeconds(30L));\n+        assertFalse(cloneFuture.isDone());\n+\n+        ConcurrentSnapshotExecutionException ex = expectThrows(ConcurrentSnapshotExecutionException.class,\n+                () -> startDeleteSnapshot(repoName, sourceSnapshot).actionGet());\n+        assertThat(ex.getMessage(), containsString(\"cannot delete snapshot while it is being cloned\"));\n+\n+        unblockNode(repoName, masterName);\n+        assertAcked(cloneFuture.get());\n+        final List<SnapshotStatus> status = clusterAdmin().prepareSnapshotStatus(repoName)\n+                .setSnapshots(sourceSnapshot, targetSnapshot).get().getSnapshots();\n+        assertThat(status, hasSize(2));\n+        final SnapshotIndexStatus status1 = status.get(0).getIndices().get(indexName);\n+        final SnapshotIndexStatus status2 = status.get(1).getIndices().get(indexName);\n+        assertEquals(status1.getStats().getTotalFileCount(), status2.getStats().getTotalFileCount());\n+        assertEquals(status1.getStats().getTotalSize(), status2.getStats().getTotalSize());\n+    }\n+\n+    public void testConcurrentCloneAndSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        final ActionFuture<CreateSnapshotResponse> snapshot2Future =\n+                startFullSnapshotBlockedOnDataNode(\"snapshot-2\", repoName, dataNode);\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexName);\n+        awaitNumberOfSnapshotsInProgress(2);\n+        unblockNode(repoName, dataNode);\n+        assertAcked(cloneFuture.get());\n+        assertSuccessful(snapshot2Future);\n+    }\n+\n+    public void testLongRunningCloneAllowsConcurrentSnapshot() throws Exception {\n+        // large snapshot pool so blocked snapshot threads from cloning don't prevent concurrent snapshot finalizations\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexSlow = \"index-slow\";\n+        createIndexWithContent(indexSlow);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockMasterOnShardClone(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexSlow);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final String indexFast = \"index-fast\";\n+        createIndexWithRandomDocs(indexFast, randomIntBetween(20, 100));\n+\n+        assertSuccessful(clusterAdmin().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(cloneFuture.isDone(), is(false));\n+        unblockNode(repoName, masterNode);\n+\n+        assertAcked(cloneFuture.get());\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentClone() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexSlow = \"index-slow\";\n+        createIndexWithContent(indexSlow);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        final String indexFast = \"index-fast\";\n+        createIndexWithRandomDocs(indexFast, randomIntBetween(20, 100));\n+\n+        blockDataNode(repoName, dataNode);\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = clusterAdmin()\n+                .prepareCreateSnapshot(repoName, \"fast-snapshot\").setIndices(indexFast).setWaitForCompletion(true).execute();\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        assertAcked(startClone(repoName, sourceSnapshot, targetSnapshot, indexSlow).get());\n+\n+        assertThat(snapshotFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(snapshotFuture);\n+    }\n+\n+    public void testDeletePreventsClone() throws Exception {\n+        final String masterName = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockNodeOnAnyFiles(repoName, masterName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDeleteSnapshot(repoName, sourceSnapshot);\n+        waitForBlock(masterName, repoName, TimeValue.timeValueSeconds(30L));\n+        assertFalse(deleteFuture.isDone());\n+\n+        ConcurrentSnapshotExecutionException ex = expectThrows(ConcurrentSnapshotExecutionException.class, () ->\n+                startClone(repoName, sourceSnapshot, targetSnapshot, indexName).actionGet());\n+        assertThat(ex.getMessage(), containsString(\"cannot clone from snapshot that is being deleted\"));\n+\n+        unblockNode(repoName, masterName);\n+        assertAcked(deleteFuture.get());\n+    }\n+\n+    public void testBackToBackClonesForIndexNotInCluster() throws Exception {\n+        // large snapshot pool so blocked snapshot threads from cloning don't prevent concurrent snapshot finalizations\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexBlocked = \"index-blocked\";\n+        createIndexWithContent(indexBlocked);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        assertAcked(admin().indices().prepareDelete(indexBlocked).get());\n+\n+        final String targetSnapshot1 = \"target-snapshot\";\n+        blockMasterOnShardClone(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture1 = startClone(repoName, sourceSnapshot, targetSnapshot1, indexBlocked);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+        assertThat(cloneFuture1.isDone(), is(false));\n+\n+        final int extraClones = randomIntBetween(1, 5);\n+        final List<ActionFuture<AcknowledgedResponse>> extraCloneFutures = new ArrayList<>(extraClones);\n+        for (int i = 0; i < extraClones; i++) {\n+            extraCloneFutures.add(startClone(repoName, sourceSnapshot, \"target-snapshot-\" + i, indexBlocked));\n+        }\n+        awaitNumberOfSnapshotsInProgress(1 + extraClones);\n+        for (ActionFuture<AcknowledgedResponse> extraCloneFuture : extraCloneFutures) {\n+            assertFalse(extraCloneFuture.isDone());\n+        }\n+\n+        final int extraSnapshots = randomIntBetween(0, 5);\n+        if (extraSnapshots > 0) {\n+            createIndexWithContent(indexBlocked);\n+        }\n+\n+        final List<ActionFuture<CreateSnapshotResponse>> extraSnapshotFutures = new ArrayList<>(extraSnapshots);\n+        for (int i = 0; i < extraSnapshots; i++) {\n+            extraSnapshotFutures.add(startFullSnapshot(repoName, \"extra-snap-\" + i));\n+        }\n+\n+        awaitNumberOfSnapshotsInProgress(1 + extraClones + extraSnapshots);\n+        for (ActionFuture<CreateSnapshotResponse> extraSnapshotFuture : extraSnapshotFutures) {\n+            assertFalse(extraSnapshotFuture.isDone());\n+        }\n+\n+        unblockNode(repoName, masterNode);\n+        assertAcked(cloneFuture1.get());\n+\n+        for (ActionFuture<AcknowledgedResponse> extraCloneFuture : extraCloneFutures) {\n+            assertAcked(extraCloneFuture.get());\n+        }\n+        for (ActionFuture<CreateSnapshotResponse> extraSnapshotFuture : extraSnapshotFutures) {\n+            assertSuccessful(extraSnapshotFuture);\n+        }\n+    }\n+\n+    public void testMasterFailoverDuringCloneStep1() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String testIndex = \"index-test\";\n+        createIndexWithContent(testIndex);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        blockMasterOnReadIndexMeta(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture =\n+                startCloneFromDataNode(repoName, sourceSnapshot, \"target-snapshot\", testIndex);\n+        awaitNumberOfSnapshotsInProgress(1);\n+        final String masterNode = internalCluster().getMasterName();\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+        internalCluster().restartNode(masterNode);\n+        boolean cloneSucceeded = false;\n+        try {\n+            cloneFuture.actionGet(TimeValue.timeValueSeconds(30L));\n+            cloneSucceeded = true;\n+        } catch (SnapshotException sne) {\n+            // ignored, most of the time we will throw here but we could randomly run into a situation where the data node retries the\n+            // snapshot on disconnect slowly enough for it to work out\n+        }\n+\n+        awaitNoMoreRunningOperations(internalCluster().getMasterName());\n+\n+        assertAllSnapshotsSuccessful(getRepositoryData(repoName), cloneSucceeded ? 2 : 1);\n+    }\n+\n+    public void testFailsOnCloneMissingIndices() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 314}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIyMzI1MA==", "bodyText": "Yea we definitely should have one (could've sworn I added one already but apparently not ...), on it.", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498223250", "createdAt": "2020-10-01T12:57:54Z", "author": {"login": "original-brownbear"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/CloneSnapshotIT.java", "diffHunk": "@@ -107,6 +122,365 @@ public void testShardClone() throws Exception {\n         assertEquals(newShardGeneration, newShardGeneration2);\n     }\n \n+    public void testCloneSnapshotIndex() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"fs\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+        if (randomBoolean()) {\n+            assertAcked(admin().indices().prepareDelete(indexName));\n+        }\n+        final String targetSnapshot = \"target-snapshot\";\n+        assertAcked(startClone(repoName, sourceSnapshot, targetSnapshot, indexName).get());\n+\n+        final List<SnapshotStatus> status = clusterAdmin().prepareSnapshotStatus(repoName)\n+                .setSnapshots(sourceSnapshot, targetSnapshot).get().getSnapshots();\n+        assertThat(status, hasSize(2));\n+        final SnapshotIndexStatus status1 = status.get(0).getIndices().get(indexName);\n+        final SnapshotIndexStatus status2 = status.get(1).getIndices().get(indexName);\n+        assertEquals(status1.getStats().getTotalFileCount(), status2.getStats().getTotalFileCount());\n+        assertEquals(status1.getStats().getTotalSize(), status2.getStats().getTotalSize());\n+    }\n+\n+    public void testClonePreventsSnapshotDelete() throws Exception {\n+        final String masterName = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockNodeOnAnyFiles(repoName, masterName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexName);\n+        waitForBlock(masterName, repoName, TimeValue.timeValueSeconds(30L));\n+        assertFalse(cloneFuture.isDone());\n+\n+        ConcurrentSnapshotExecutionException ex = expectThrows(ConcurrentSnapshotExecutionException.class,\n+                () -> startDeleteSnapshot(repoName, sourceSnapshot).actionGet());\n+        assertThat(ex.getMessage(), containsString(\"cannot delete snapshot while it is being cloned\"));\n+\n+        unblockNode(repoName, masterName);\n+        assertAcked(cloneFuture.get());\n+        final List<SnapshotStatus> status = clusterAdmin().prepareSnapshotStatus(repoName)\n+                .setSnapshots(sourceSnapshot, targetSnapshot).get().getSnapshots();\n+        assertThat(status, hasSize(2));\n+        final SnapshotIndexStatus status1 = status.get(0).getIndices().get(indexName);\n+        final SnapshotIndexStatus status2 = status.get(1).getIndices().get(indexName);\n+        assertEquals(status1.getStats().getTotalFileCount(), status2.getStats().getTotalFileCount());\n+        assertEquals(status1.getStats().getTotalSize(), status2.getStats().getTotalSize());\n+    }\n+\n+    public void testConcurrentCloneAndSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        final ActionFuture<CreateSnapshotResponse> snapshot2Future =\n+                startFullSnapshotBlockedOnDataNode(\"snapshot-2\", repoName, dataNode);\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexName);\n+        awaitNumberOfSnapshotsInProgress(2);\n+        unblockNode(repoName, dataNode);\n+        assertAcked(cloneFuture.get());\n+        assertSuccessful(snapshot2Future);\n+    }\n+\n+    public void testLongRunningCloneAllowsConcurrentSnapshot() throws Exception {\n+        // large snapshot pool so blocked snapshot threads from cloning don't prevent concurrent snapshot finalizations\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexSlow = \"index-slow\";\n+        createIndexWithContent(indexSlow);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockMasterOnShardClone(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexSlow);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final String indexFast = \"index-fast\";\n+        createIndexWithRandomDocs(indexFast, randomIntBetween(20, 100));\n+\n+        assertSuccessful(clusterAdmin().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(cloneFuture.isDone(), is(false));\n+        unblockNode(repoName, masterNode);\n+\n+        assertAcked(cloneFuture.get());\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentClone() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexSlow = \"index-slow\";\n+        createIndexWithContent(indexSlow);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        final String indexFast = \"index-fast\";\n+        createIndexWithRandomDocs(indexFast, randomIntBetween(20, 100));\n+\n+        blockDataNode(repoName, dataNode);\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = clusterAdmin()\n+                .prepareCreateSnapshot(repoName, \"fast-snapshot\").setIndices(indexFast).setWaitForCompletion(true).execute();\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        assertAcked(startClone(repoName, sourceSnapshot, targetSnapshot, indexSlow).get());\n+\n+        assertThat(snapshotFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(snapshotFuture);\n+    }\n+\n+    public void testDeletePreventsClone() throws Exception {\n+        final String masterName = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockNodeOnAnyFiles(repoName, masterName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDeleteSnapshot(repoName, sourceSnapshot);\n+        waitForBlock(masterName, repoName, TimeValue.timeValueSeconds(30L));\n+        assertFalse(deleteFuture.isDone());\n+\n+        ConcurrentSnapshotExecutionException ex = expectThrows(ConcurrentSnapshotExecutionException.class, () ->\n+                startClone(repoName, sourceSnapshot, targetSnapshot, indexName).actionGet());\n+        assertThat(ex.getMessage(), containsString(\"cannot clone from snapshot that is being deleted\"));\n+\n+        unblockNode(repoName, masterName);\n+        assertAcked(deleteFuture.get());\n+    }\n+\n+    public void testBackToBackClonesForIndexNotInCluster() throws Exception {\n+        // large snapshot pool so blocked snapshot threads from cloning don't prevent concurrent snapshot finalizations\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexBlocked = \"index-blocked\";\n+        createIndexWithContent(indexBlocked);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        assertAcked(admin().indices().prepareDelete(indexBlocked).get());\n+\n+        final String targetSnapshot1 = \"target-snapshot\";\n+        blockMasterOnShardClone(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture1 = startClone(repoName, sourceSnapshot, targetSnapshot1, indexBlocked);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+        assertThat(cloneFuture1.isDone(), is(false));\n+\n+        final int extraClones = randomIntBetween(1, 5);\n+        final List<ActionFuture<AcknowledgedResponse>> extraCloneFutures = new ArrayList<>(extraClones);\n+        for (int i = 0; i < extraClones; i++) {\n+            extraCloneFutures.add(startClone(repoName, sourceSnapshot, \"target-snapshot-\" + i, indexBlocked));\n+        }\n+        awaitNumberOfSnapshotsInProgress(1 + extraClones);\n+        for (ActionFuture<AcknowledgedResponse> extraCloneFuture : extraCloneFutures) {\n+            assertFalse(extraCloneFuture.isDone());\n+        }\n+\n+        final int extraSnapshots = randomIntBetween(0, 5);\n+        if (extraSnapshots > 0) {\n+            createIndexWithContent(indexBlocked);\n+        }\n+\n+        final List<ActionFuture<CreateSnapshotResponse>> extraSnapshotFutures = new ArrayList<>(extraSnapshots);\n+        for (int i = 0; i < extraSnapshots; i++) {\n+            extraSnapshotFutures.add(startFullSnapshot(repoName, \"extra-snap-\" + i));\n+        }\n+\n+        awaitNumberOfSnapshotsInProgress(1 + extraClones + extraSnapshots);\n+        for (ActionFuture<CreateSnapshotResponse> extraSnapshotFuture : extraSnapshotFutures) {\n+            assertFalse(extraSnapshotFuture.isDone());\n+        }\n+\n+        unblockNode(repoName, masterNode);\n+        assertAcked(cloneFuture1.get());\n+\n+        for (ActionFuture<AcknowledgedResponse> extraCloneFuture : extraCloneFutures) {\n+            assertAcked(extraCloneFuture.get());\n+        }\n+        for (ActionFuture<CreateSnapshotResponse> extraSnapshotFuture : extraSnapshotFutures) {\n+            assertSuccessful(extraSnapshotFuture);\n+        }\n+    }\n+\n+    public void testMasterFailoverDuringCloneStep1() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String testIndex = \"index-test\";\n+        createIndexWithContent(testIndex);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        blockMasterOnReadIndexMeta(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture =\n+                startCloneFromDataNode(repoName, sourceSnapshot, \"target-snapshot\", testIndex);\n+        awaitNumberOfSnapshotsInProgress(1);\n+        final String masterNode = internalCluster().getMasterName();\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+        internalCluster().restartNode(masterNode);\n+        boolean cloneSucceeded = false;\n+        try {\n+            cloneFuture.actionGet(TimeValue.timeValueSeconds(30L));\n+            cloneSucceeded = true;\n+        } catch (SnapshotException sne) {\n+            // ignored, most of the time we will throw here but we could randomly run into a situation where the data node retries the\n+            // snapshot on disconnect slowly enough for it to work out\n+        }\n+\n+        awaitNoMoreRunningOperations(internalCluster().getMasterName());\n+\n+        assertAllSnapshotsSuccessful(getRepositoryData(repoName), cloneSucceeded ? 2 : 1);\n+    }\n+\n+    public void testFailsOnCloneMissingIndices() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE3NDM4MQ=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 314}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODY4NTUxMA==", "bodyText": "Added in 6e5b74b  :)", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498685510", "createdAt": "2020-10-02T08:31:01Z", "author": {"login": "original-brownbear"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/CloneSnapshotIT.java", "diffHunk": "@@ -107,6 +122,365 @@ public void testShardClone() throws Exception {\n         assertEquals(newShardGeneration, newShardGeneration2);\n     }\n \n+    public void testCloneSnapshotIndex() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"fs\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+        if (randomBoolean()) {\n+            assertAcked(admin().indices().prepareDelete(indexName));\n+        }\n+        final String targetSnapshot = \"target-snapshot\";\n+        assertAcked(startClone(repoName, sourceSnapshot, targetSnapshot, indexName).get());\n+\n+        final List<SnapshotStatus> status = clusterAdmin().prepareSnapshotStatus(repoName)\n+                .setSnapshots(sourceSnapshot, targetSnapshot).get().getSnapshots();\n+        assertThat(status, hasSize(2));\n+        final SnapshotIndexStatus status1 = status.get(0).getIndices().get(indexName);\n+        final SnapshotIndexStatus status2 = status.get(1).getIndices().get(indexName);\n+        assertEquals(status1.getStats().getTotalFileCount(), status2.getStats().getTotalFileCount());\n+        assertEquals(status1.getStats().getTotalSize(), status2.getStats().getTotalSize());\n+    }\n+\n+    public void testClonePreventsSnapshotDelete() throws Exception {\n+        final String masterName = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockNodeOnAnyFiles(repoName, masterName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexName);\n+        waitForBlock(masterName, repoName, TimeValue.timeValueSeconds(30L));\n+        assertFalse(cloneFuture.isDone());\n+\n+        ConcurrentSnapshotExecutionException ex = expectThrows(ConcurrentSnapshotExecutionException.class,\n+                () -> startDeleteSnapshot(repoName, sourceSnapshot).actionGet());\n+        assertThat(ex.getMessage(), containsString(\"cannot delete snapshot while it is being cloned\"));\n+\n+        unblockNode(repoName, masterName);\n+        assertAcked(cloneFuture.get());\n+        final List<SnapshotStatus> status = clusterAdmin().prepareSnapshotStatus(repoName)\n+                .setSnapshots(sourceSnapshot, targetSnapshot).get().getSnapshots();\n+        assertThat(status, hasSize(2));\n+        final SnapshotIndexStatus status1 = status.get(0).getIndices().get(indexName);\n+        final SnapshotIndexStatus status2 = status.get(1).getIndices().get(indexName);\n+        assertEquals(status1.getStats().getTotalFileCount(), status2.getStats().getTotalFileCount());\n+        assertEquals(status1.getStats().getTotalSize(), status2.getStats().getTotalSize());\n+    }\n+\n+    public void testConcurrentCloneAndSnapshot() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        final ActionFuture<CreateSnapshotResponse> snapshot2Future =\n+                startFullSnapshotBlockedOnDataNode(\"snapshot-2\", repoName, dataNode);\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexName);\n+        awaitNumberOfSnapshotsInProgress(2);\n+        unblockNode(repoName, dataNode);\n+        assertAcked(cloneFuture.get());\n+        assertSuccessful(snapshot2Future);\n+    }\n+\n+    public void testLongRunningCloneAllowsConcurrentSnapshot() throws Exception {\n+        // large snapshot pool so blocked snapshot threads from cloning don't prevent concurrent snapshot finalizations\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexSlow = \"index-slow\";\n+        createIndexWithContent(indexSlow);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockMasterOnShardClone(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, targetSnapshot, indexSlow);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final String indexFast = \"index-fast\";\n+        createIndexWithRandomDocs(indexFast, randomIntBetween(20, 100));\n+\n+        assertSuccessful(clusterAdmin().prepareCreateSnapshot(repoName, \"fast-snapshot\")\n+                .setIndices(indexFast).setWaitForCompletion(true).execute());\n+\n+        assertThat(cloneFuture.isDone(), is(false));\n+        unblockNode(repoName, masterNode);\n+\n+        assertAcked(cloneFuture.get());\n+    }\n+\n+    public void testLongRunningSnapshotAllowsConcurrentClone() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String dataNode = internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexSlow = \"index-slow\";\n+        createIndexWithContent(indexSlow);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        final String indexFast = \"index-fast\";\n+        createIndexWithRandomDocs(indexFast, randomIntBetween(20, 100));\n+\n+        blockDataNode(repoName, dataNode);\n+        final ActionFuture<CreateSnapshotResponse> snapshotFuture = clusterAdmin()\n+                .prepareCreateSnapshot(repoName, \"fast-snapshot\").setIndices(indexFast).setWaitForCompletion(true).execute();\n+        waitForBlock(dataNode, repoName, TimeValue.timeValueSeconds(30L));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        assertAcked(startClone(repoName, sourceSnapshot, targetSnapshot, indexSlow).get());\n+\n+        assertThat(snapshotFuture.isDone(), is(false));\n+        unblockNode(repoName, dataNode);\n+\n+        assertSuccessful(snapshotFuture);\n+    }\n+\n+    public void testDeletePreventsClone() throws Exception {\n+        final String masterName = internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"repo-name\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"index-1\";\n+        createIndexWithRandomDocs(indexName, randomIntBetween(5, 10));\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        indexRandomDocs(indexName, randomIntBetween(20, 100));\n+\n+        final String targetSnapshot = \"target-snapshot\";\n+        blockNodeOnAnyFiles(repoName, masterName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDeleteSnapshot(repoName, sourceSnapshot);\n+        waitForBlock(masterName, repoName, TimeValue.timeValueSeconds(30L));\n+        assertFalse(deleteFuture.isDone());\n+\n+        ConcurrentSnapshotExecutionException ex = expectThrows(ConcurrentSnapshotExecutionException.class, () ->\n+                startClone(repoName, sourceSnapshot, targetSnapshot, indexName).actionGet());\n+        assertThat(ex.getMessage(), containsString(\"cannot clone from snapshot that is being deleted\"));\n+\n+        unblockNode(repoName, masterName);\n+        assertAcked(deleteFuture.get());\n+    }\n+\n+    public void testBackToBackClonesForIndexNotInCluster() throws Exception {\n+        // large snapshot pool so blocked snapshot threads from cloning don't prevent concurrent snapshot finalizations\n+        final String masterNode = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String indexBlocked = \"index-blocked\";\n+        createIndexWithContent(indexBlocked);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        assertAcked(admin().indices().prepareDelete(indexBlocked).get());\n+\n+        final String targetSnapshot1 = \"target-snapshot\";\n+        blockMasterOnShardClone(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture1 = startClone(repoName, sourceSnapshot, targetSnapshot1, indexBlocked);\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+        assertThat(cloneFuture1.isDone(), is(false));\n+\n+        final int extraClones = randomIntBetween(1, 5);\n+        final List<ActionFuture<AcknowledgedResponse>> extraCloneFutures = new ArrayList<>(extraClones);\n+        for (int i = 0; i < extraClones; i++) {\n+            extraCloneFutures.add(startClone(repoName, sourceSnapshot, \"target-snapshot-\" + i, indexBlocked));\n+        }\n+        awaitNumberOfSnapshotsInProgress(1 + extraClones);\n+        for (ActionFuture<AcknowledgedResponse> extraCloneFuture : extraCloneFutures) {\n+            assertFalse(extraCloneFuture.isDone());\n+        }\n+\n+        final int extraSnapshots = randomIntBetween(0, 5);\n+        if (extraSnapshots > 0) {\n+            createIndexWithContent(indexBlocked);\n+        }\n+\n+        final List<ActionFuture<CreateSnapshotResponse>> extraSnapshotFutures = new ArrayList<>(extraSnapshots);\n+        for (int i = 0; i < extraSnapshots; i++) {\n+            extraSnapshotFutures.add(startFullSnapshot(repoName, \"extra-snap-\" + i));\n+        }\n+\n+        awaitNumberOfSnapshotsInProgress(1 + extraClones + extraSnapshots);\n+        for (ActionFuture<CreateSnapshotResponse> extraSnapshotFuture : extraSnapshotFutures) {\n+            assertFalse(extraSnapshotFuture.isDone());\n+        }\n+\n+        unblockNode(repoName, masterNode);\n+        assertAcked(cloneFuture1.get());\n+\n+        for (ActionFuture<AcknowledgedResponse> extraCloneFuture : extraCloneFutures) {\n+            assertAcked(extraCloneFuture.get());\n+        }\n+        for (ActionFuture<CreateSnapshotResponse> extraSnapshotFuture : extraSnapshotFutures) {\n+            assertSuccessful(extraSnapshotFuture);\n+        }\n+    }\n+\n+    public void testMasterFailoverDuringCloneStep1() throws Exception {\n+        internalCluster().startMasterOnlyNodes(3);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+        final String testIndex = \"index-test\";\n+        createIndexWithContent(testIndex);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        blockMasterOnReadIndexMeta(repoName);\n+        final ActionFuture<AcknowledgedResponse> cloneFuture =\n+                startCloneFromDataNode(repoName, sourceSnapshot, \"target-snapshot\", testIndex);\n+        awaitNumberOfSnapshotsInProgress(1);\n+        final String masterNode = internalCluster().getMasterName();\n+        waitForBlock(masterNode, repoName, TimeValue.timeValueSeconds(30L));\n+        internalCluster().restartNode(masterNode);\n+        boolean cloneSucceeded = false;\n+        try {\n+            cloneFuture.actionGet(TimeValue.timeValueSeconds(30L));\n+            cloneSucceeded = true;\n+        } catch (SnapshotException sne) {\n+            // ignored, most of the time we will throw here but we could randomly run into a situation where the data node retries the\n+            // snapshot on disconnect slowly enough for it to work out\n+        }\n+\n+        awaitNoMoreRunningOperations(internalCluster().getMasterName());\n+\n+        assertAllSnapshotsSuccessful(getRepositoryData(repoName), cloneSucceeded ? 2 : 1);\n+    }\n+\n+    public void testFailsOnCloneMissingIndices() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE3NDM4MQ=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 314}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExODg1NTUwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMTo0NzoyN1rOHbGntQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMTo0NzoyN1rOHbGntQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE4MjA2OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        assert assertShardsConsistent(source, state, indices, shards, this.clones);\n          \n          \n            \n                        assert assertShardsConsistent(this.source, this.state, this.indices, this.shards, this.clones);", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498182069", "createdAt": "2020-10-01T11:47:27Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -124,11 +170,18 @@ public Entry(Snapshot snapshot, boolean includeGlobalState, boolean partial, Sta\n             this.dataStreams = dataStreams;\n             this.startTime = startTime;\n             this.shards = shards;\n-            assert assertShardsConsistent(state, indices, shards);\n             this.repositoryStateId = repositoryStateId;\n             this.failure = failure;\n             this.userMetadata = userMetadata;\n             this.version = version;\n+            this.source = source;\n+            if (source == null) {\n+                assert clones == null || clones.isEmpty() : \"Provided [\" + clones + \"] but no source\";\n+                this.clones = ImmutableOpenMap.of();\n+            } else {\n+                this.clones = clones;\n+            }\n+            assert assertShardsConsistent(source, state, indices, shards, this.clones);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExODkwMjY0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMjowMToxMFrOHbHENA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNDo0NzozMFrOHbOAZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE4OTM2NA==", "bodyText": "Maybe move this few steps up, after final String repositoryName = request.repository();", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498189364", "createdAt": "2020-10-01T12:01:10Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -313,6 +308,275 @@ public TimeValue timeout() {\n         }, \"create_snapshot [\" + snapshotName + ']', listener::onFailure);\n     }\n \n+    private static void ensureSnapshotNameNotRunning(List<SnapshotsInProgress.Entry> runningSnapshots, String repositoryName,\n+                                                     String snapshotName) {\n+        if (runningSnapshots.stream().anyMatch(s -> {\n+            final Snapshot running = s.snapshot();\n+            return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+        })) {\n+            throw new InvalidSnapshotNameException(repositoryName, snapshotName, \"snapshot with the same name is already in-progress\");\n+        }\n+    }\n+\n+    private static Map<String, IndexId> getInFlightIndexIds(List<SnapshotsInProgress.Entry> runningSnapshots, String repositoryName) {\n+        return runningSnapshots.stream().filter(entry -> entry.repository().equals(repositoryName))\n+                .flatMap(entry -> entry.indices().stream()).distinct()\n+                .collect(Collectors.toMap(IndexId::getName, Function.identity()));\n+    }\n+\n+    public void cloneSnapshot(CloneSnapshotRequest request, ActionListener<Void> listener) {\n+        final String repositoryName = request.repository();\n+        final String snapshotName = indexNameExpressionResolver.resolveDateMathExpression(request.target());\n+        validate(repositoryName, snapshotName);\n+        Repository repository = repositoriesService.repository(repositoryName);\n+        if (repository.isReadOnly()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODMwMzA3OA==", "bodyText": "done :)", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498303078", "createdAt": "2020-10-01T14:47:30Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -313,6 +308,275 @@ public TimeValue timeout() {\n         }, \"create_snapshot [\" + snapshotName + ']', listener::onFailure);\n     }\n \n+    private static void ensureSnapshotNameNotRunning(List<SnapshotsInProgress.Entry> runningSnapshots, String repositoryName,\n+                                                     String snapshotName) {\n+        if (runningSnapshots.stream().anyMatch(s -> {\n+            final Snapshot running = s.snapshot();\n+            return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+        })) {\n+            throw new InvalidSnapshotNameException(repositoryName, snapshotName, \"snapshot with the same name is already in-progress\");\n+        }\n+    }\n+\n+    private static Map<String, IndexId> getInFlightIndexIds(List<SnapshotsInProgress.Entry> runningSnapshots, String repositoryName) {\n+        return runningSnapshots.stream().filter(entry -> entry.repository().equals(repositoryName))\n+                .flatMap(entry -> entry.indices().stream()).distinct()\n+                .collect(Collectors.toMap(IndexId::getName, Function.identity()));\n+    }\n+\n+    public void cloneSnapshot(CloneSnapshotRequest request, ActionListener<Void> listener) {\n+        final String repositoryName = request.repository();\n+        final String snapshotName = indexNameExpressionResolver.resolveDateMathExpression(request.target());\n+        validate(repositoryName, snapshotName);\n+        Repository repository = repositoriesService.repository(repositoryName);\n+        if (repository.isReadOnly()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE4OTM2NA=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExODk1MTI4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMjoxNTowOFrOHbHh7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMjoxNTowOFrOHbHh7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODE5Njk3NA==", "bodyText": "nit: indentation messed up here, making the code difficult to read", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498196974", "createdAt": "2020-10-01T12:15:08Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -313,6 +308,275 @@ public TimeValue timeout() {\n         }, \"create_snapshot [\" + snapshotName + ']', listener::onFailure);\n     }\n \n+    private static void ensureSnapshotNameNotRunning(List<SnapshotsInProgress.Entry> runningSnapshots, String repositoryName,\n+                                                     String snapshotName) {\n+        if (runningSnapshots.stream().anyMatch(s -> {\n+            final Snapshot running = s.snapshot();\n+            return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+        })) {\n+            throw new InvalidSnapshotNameException(repositoryName, snapshotName, \"snapshot with the same name is already in-progress\");\n+        }\n+    }\n+\n+    private static Map<String, IndexId> getInFlightIndexIds(List<SnapshotsInProgress.Entry> runningSnapshots, String repositoryName) {\n+        return runningSnapshots.stream().filter(entry -> entry.repository().equals(repositoryName))\n+                .flatMap(entry -> entry.indices().stream()).distinct()\n+                .collect(Collectors.toMap(IndexId::getName, Function.identity()));\n+    }\n+\n+    public void cloneSnapshot(CloneSnapshotRequest request, ActionListener<Void> listener) {\n+        final String repositoryName = request.repository();\n+        final String snapshotName = indexNameExpressionResolver.resolveDateMathExpression(request.target());\n+        validate(repositoryName, snapshotName);\n+        Repository repository = repositoriesService.repository(repositoryName);\n+        if (repository.isReadOnly()) {\n+            listener.onFailure(new RepositoryException(repositoryName, \"cannot create snapshot in a readonly repository\"));\n+            return;\n+        }\n+        final SnapshotId snapshotId = new SnapshotId(snapshotName, UUIDs.randomBase64UUID());\n+        final Snapshot snapshot = new Snapshot(repositoryName, snapshotId);\n+        initializingClones.add(snapshot);\n+        repository.executeConsistentStateUpdate(repositoryData -> new ClusterStateUpdateTask() {\n+\n+            private SnapshotsInProgress.Entry newEntry;\n+\n+            @Override\n+            public ClusterState execute(ClusterState currentState) {\n+                ensureSnapshotNameAvailableInRepo(repositoryData, snapshotName, repository);\n+                ensureNoCleanupInProgress(currentState, repositoryName, snapshotName);\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries();\n+                ensureSnapshotNameNotRunning(runningSnapshots, repositoryName, snapshotName);\n+                validate(repositoryName, snapshotName, currentState);\n+\n+                final SnapshotId sourceSnapshotId = repositoryData.getSnapshotIds()\n+                        .stream()\n+                        .filter(src -> src.getName().equals(request.source()))\n+                        .findAny()\n+                        .orElseThrow(() -> new SnapshotMissingException(repositoryName, request.source()));\n+                final SnapshotDeletionsInProgress deletionsInProgress =\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                if (deletionsInProgress.getEntries().stream().anyMatch(entry -> entry.getSnapshots().contains(sourceSnapshotId))) {\n+                    throw new ConcurrentSnapshotExecutionException(repositoryName, sourceSnapshotId.getName(),\n+                            \"cannot clone from snapshot that is being deleted\");\n+                }\n+                ensureBelowConcurrencyLimit(repositoryName, snapshotName, snapshots, deletionsInProgress);\n+                final List<String> indicesForSnapshot = new ArrayList<>();\n+                for (IndexId indexId : repositoryData.getIndices().values()) {\n+                    if (repositoryData.getSnapshots(indexId).contains(sourceSnapshotId)) {\n+                        indicesForSnapshot.add(indexId.getName());\n+                    }\n+                }\n+                final List<String> matchingIndices =\n+                        SnapshotUtils.filterIndices(indicesForSnapshot, request.indices(), request.indicesOptions());\n+                if (matchingIndices.isEmpty()) {\n+                    throw new SnapshotException(new Snapshot(repositoryName, sourceSnapshotId),\n+                            \"No indices in the source snapshot [\" + sourceSnapshotId + \"] matched requested pattern [\"\n+                                    + Strings.arrayToCommaDelimitedString(request.indices()) + \"]\");\n+                }\n+                newEntry = SnapshotsInProgress.startClone(\n+                        snapshot, sourceSnapshotId,\n+                        repositoryData.resolveIndices(matchingIndices),\n+                        threadPool.absoluteTimeInMillis(), repositoryData.getGenId(),\n+                        minCompatibleVersion(currentState.nodes().getMinNodeVersion(), repositoryData, null));\n+                final List<SnapshotsInProgress.Entry> newEntries = new ArrayList<>(runningSnapshots);\n+                newEntries.add(newEntry);\n+                return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE,\n+                        SnapshotsInProgress.of(List.copyOf(newEntries))).build();\n+            }\n+\n+            @Override\n+            public void onFailure(String source, Exception e) {\n+                initializingClones.remove(snapshot);\n+                logger.warn(() -> new ParameterizedMessage(\"[{}][{}] failed to clone snapshot\", repositoryName, snapshotName), e);\n+                listener.onFailure(e);\n+            }\n+\n+            @Override\n+            public void clusterStateProcessed(String source, ClusterState oldState, final ClusterState newState) {\n+                logger.info(\"snapshot clone [{}] started\", snapshot);\n+                addListener(snapshot, ActionListener.wrap(r -> listener.onResponse(null), listener::onFailure));\n+                startCloning(repository, newEntry);\n+            }\n+\n+            @Override\n+            public TimeValue timeout() {\n+                initializingClones.remove(snapshot);\n+                return request.masterNodeTimeout();\n+            }\n+        }, \"clone_snapshot [\" + request.source() + \"][\" + snapshotName + ']', listener::onFailure);\n+    }\n+\n+    private static void ensureNoCleanupInProgress(ClusterState currentState, String repositoryName, String snapshotName) {\n+        final RepositoryCleanupInProgress repositoryCleanupInProgress =\n+                currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n+        if (repositoryCleanupInProgress.hasCleanupInProgress()) {\n+            throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n+                    \"cannot snapshot while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n+        }\n+    }\n+\n+    private static void ensureSnapshotNameAvailableInRepo(RepositoryData repositoryData, String snapshotName, Repository repository) {\n+        // check if the snapshot name already exists in the repository\n+        if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName))) {\n+            throw new InvalidSnapshotNameException(\n+                    repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n+        }\n+    }\n+\n+    /**\n+     * Determine the number of shards in each index of a clone operation and update the cluster state accordingly.\n+     *\n+     * @param repository     repository to run operation on\n+     * @param cloneEntry     clone operation in the cluster state\n+     */\n+    private void startCloning(Repository repository, SnapshotsInProgress.Entry cloneEntry) {\n+        final List<IndexId> indices = cloneEntry.indices();\n+        final SnapshotId sourceSnapshot = cloneEntry.source();\n+        final Snapshot targetSnapshot = cloneEntry.snapshot();\n+\n+        final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT);\n+        // Exception handler for IO exceptions with loading index and repo metadata\n+        final Consumer<Exception> onFailure = e -> {\n+            initializingClones.remove(targetSnapshot);\n+            logger.info(() -> new ParameterizedMessage(\"Failed to start snapshot clone [{}]\", cloneEntry), e);\n+            removeFailedSnapshotFromClusterState(targetSnapshot, e, null);\n+        };\n+\n+        // 1. step, load SnapshotInfo to make sure that source snapshot was successful for the indices we want to clone\n+        // TODO: we could skip this step for snapshots with state SUCCESS\n+        final StepListener<SnapshotInfo> snapshotInfoListener = new StepListener<>();\n+        executor.execute(ActionRunnable.supply(snapshotInfoListener, () -> repository.getSnapshotInfo(sourceSnapshot)));\n+\n+        final StepListener<Collection<Tuple<IndexId, Integer>>> allShardCountsListener = new StepListener<>();\n+        final GroupedActionListener<Tuple<IndexId, Integer>> shardCountListener =\n+                new GroupedActionListener<>(allShardCountsListener, indices.size());\n+        snapshotInfoListener.whenComplete(snapshotInfo -> {\n+            for (IndexId indexId : indices) {\n+                if (RestoreService.failed(snapshotInfo, indexId.getName())) {\n+                    throw new SnapshotException(targetSnapshot, \"Can't clone index [\" + indexId +\n+                            \"] because its snapshot is was not successful.\");\n+                }\n+            }\n+            // 2. step, load the number of shards we have in each index to be cloned from the index metadata.\n+            repository.getRepositoryData(ActionListener.wrap(repositoryData -> {\n+                for (IndexId index : indices) {\n+                    executor.execute(ActionRunnable.supply(shardCountListener, () -> {\n+                        final IndexMetadata metadata = repository.getSnapshotIndexMetaData(repositoryData, sourceSnapshot, index);\n+                        return Tuple.tuple(index, metadata.getNumberOfShards());\n+                    }));\n+                }\n+            }, onFailure));\n+        }, onFailure);\n+\n+        // 3. step, we have all the shard counts, now update the cluster state to have clone jobs in the snap entry\n+        allShardCountsListener.whenComplete(counts -> repository.executeConsistentStateUpdate(repoData -> new ClusterStateUpdateTask() {\n+\n+            private SnapshotsInProgress.Entry updatedEntry;\n+\n+            @Override\n+            public ClusterState execute(ClusterState currentState) {\n+                final SnapshotsInProgress snapshotsInProgress =\n+                        currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> updatedEntries = new ArrayList<>(snapshotsInProgress.entries());\n+                boolean changed = false;\n+                final String localNodeId = currentState.nodes().getLocalNodeId();\n+                final String repoName = cloneEntry.repository();\n+                final Map<String, IndexId> indexIds = getInFlightIndexIds(updatedEntries, repoName);\n+                final ShardGenerations shardGenerations = repoData.shardGenerations();\n+                for (int i = 0; i < updatedEntries.size(); i++) {\n+                    if (cloneEntry.equals(updatedEntries.get(i))) {\n+                        final ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clonesBuilder =\n+                                ImmutableOpenMap.builder();\n+                        // TODO: could be optimized by just dealing with repo shard id directly\n+                        final Set<RepositoryShardId> busyShardsInRepo =\n+                                busyShardsForRepo(repoName, snapshotsInProgress, currentState.metadata())\n+                                        .stream()\n+                                        .map(shardId -> new RepositoryShardId(indexIds.get(shardId.getIndexName()), shardId.getId()))\n+                                        .collect(Collectors.toSet());\n+                        for (Tuple<IndexId, Integer> count : counts) {\n+                            for (int shardId = 0; shardId < count.v2(); shardId++) {\n+                                final RepositoryShardId repoShardId = new RepositoryShardId(count.v1(), shardId);\n+                                if (busyShardsInRepo.contains(repoShardId)) {\n+                                    clonesBuilder.put(repoShardId, ShardSnapshotStatus.UNASSIGNED_QUEUED);\n+                                } else {\n+                                    clonesBuilder.put(repoShardId,\n+                                            new ShardSnapshotStatus(localNodeId, shardGenerations.getShardGen(count.v1(), shardId)));\n+                                }\n+                            }\n+                        }\n+                        updatedEntry = cloneEntry.withClones(clonesBuilder.build());\n+                        updatedEntries.set(i, updatedEntry);\n+                        changed = true;\n+                        break;\n+                    }\n+                }\n+                return updateWithSnapshots(currentState, changed ? SnapshotsInProgress.of(updatedEntries) : null, null);\n+            }\n+\n+            @Override\n+            public void onFailure(String source, Exception e) {\n+                initializingClones.remove(targetSnapshot);\n+                logger.info(() -> new ParameterizedMessage(\"Failed to start snapshot clone [{}]\", cloneEntry), e);\n+                failAllListenersOnMasterFailOver(e);\n+            }\n+\n+            @Override\n+            public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\n+                initializingClones.remove(targetSnapshot);\n+                if (updatedEntry != null) {\n+                    final Snapshot target = updatedEntry.snapshot();\n+                    final SnapshotId sourceSnapshot = updatedEntry.source();\n+                    for (ObjectObjectCursor<RepositoryShardId, ShardSnapshotStatus> indexClone : updatedEntry.clones()) {\n+                        final ShardSnapshotStatus shardStatusBefore = indexClone.value;\n+                        if (shardStatusBefore.state() != ShardState.INIT) {\n+                            continue;\n+                        }\n+                        final RepositoryShardId repoShardId = indexClone.key;\n+                        runReadyClone(target, sourceSnapshot, shardStatusBefore, repoShardId, repository);\n+                    }\n+                } else {\n+                    // Extremely unlikely corner case of master failing over between between starting the clone and\n+                    // starting shard clones.\n+                    logger.warn(\"Did not find expected entry [{}] in the cluster state\", cloneEntry);\n+                }\n+            }\n+        }, \"start snapshot clone\", onFailure), onFailure);\n+    }\n+\n+    private final Set<RepositoryShardId> currentlyCloning = Collections.synchronizedSet(new HashSet<>());\n+\n+    private void runReadyClone(Snapshot target, SnapshotId sourceSnapshot, ShardSnapshotStatus shardStatusBefore,\n+                               RepositoryShardId repoShardId, Repository repository) {\n+        final SnapshotId targetSnapshot = target.getSnapshotId();\n+        final String localNodeId = clusterService.localNode().getId();\n+        if (currentlyCloning.add(repoShardId)) {\n+            repository.cloneShardSnapshot(sourceSnapshot, targetSnapshot, repoShardId,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 341}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExOTA1MDQ3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMjo0MDo1OFrOHbIeVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMjo0MDo1OFrOHbIeVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIxMjQzNg==", "bodyText": "nit: ident", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498212436", "createdAt": "2020-10-01T12:40:58Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1159,6 +1465,17 @@ public ClusterState execute(ClusterState currentState) {\n                 if (snapshotIds.isEmpty()) {\n                     return currentState;\n                 }\n+                final Set<SnapshotId> activeCloneSources = snapshots.entries()\n+                        .stream()\n+                        .filter(entry -> entry.source() != null)\n+                        .map(SnapshotsInProgress.Entry::source\n+                        ).collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 536}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExOTA1NDgzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMjo0MjowMVrOHbIg-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNTowMjoyOVrOHbOtQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIxMzExMg==", "bodyText": "It's use everywhere, maybe adding a method isClone() returning source != null would be more explicit?", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498213112", "createdAt": "2020-10-01T12:42:01Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1809,16 +2126,32 @@ public static ClusterState updateWithSnapshots(ClusterState state,\n      * @param snapshots    snapshots in progress\n      * @return shard ids that currently have an actively executing shard snapshot on a data node\n      */\n-    private static Set<ShardId> busyShardsForRepo(String repoName, @Nullable SnapshotsInProgress snapshots) {\n+    private static Set<ShardId> busyShardsForRepo(String repoName, @Nullable SnapshotsInProgress snapshots, Metadata metadata) {\n         final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots == null ? List.of() : snapshots.entries();\n         final Set<ShardId> inProgressShards = new HashSet<>();\n         for (SnapshotsInProgress.Entry runningSnapshot : runningSnapshots) {\n             if (runningSnapshot.repository().equals(repoName) == false) {\n                 continue;\n             }\n-            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shard : runningSnapshot.shards()) {\n-                if (shard.value.isActive()) {\n-                    inProgressShards.add(shard.key);\n+            if (runningSnapshot.source() == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 570}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODMxNDU2Mw==", "bodyText": "++  added that, much nicer now :)", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498314563", "createdAt": "2020-10-01T15:02:29Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1809,16 +2126,32 @@ public static ClusterState updateWithSnapshots(ClusterState state,\n      * @param snapshots    snapshots in progress\n      * @return shard ids that currently have an actively executing shard snapshot on a data node\n      */\n-    private static Set<ShardId> busyShardsForRepo(String repoName, @Nullable SnapshotsInProgress snapshots) {\n+    private static Set<ShardId> busyShardsForRepo(String repoName, @Nullable SnapshotsInProgress snapshots, Metadata metadata) {\n         final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots == null ? List.of() : snapshots.entries();\n         final Set<ShardId> inProgressShards = new HashSet<>();\n         for (SnapshotsInProgress.Entry runningSnapshot : runningSnapshots) {\n             if (runningSnapshot.repository().equals(repoName) == false) {\n                 continue;\n             }\n-            for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> shard : runningSnapshot.shards()) {\n-                if (shard.value.isActive()) {\n-                    inProgressShards.add(shard.key);\n+            if (runningSnapshot.source() == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIxMzExMg=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 570}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExOTA1NzYyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMjo0Mjo0NlrOHbIiww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQwODozMzoyMFrOHblatw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIxMzU3MQ==", "bodyText": "Maybe activeShardsForRepo would be more appropriate?", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498213571", "createdAt": "2020-10-01T12:42:46Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1809,16 +2126,32 @@ public static ClusterState updateWithSnapshots(ClusterState state,\n      * @param snapshots    snapshots in progress\n      * @return shard ids that currently have an actively executing shard snapshot on a data node\n      */\n-    private static Set<ShardId> busyShardsForRepo(String repoName, @Nullable SnapshotsInProgress snapshots) {\n+    private static Set<ShardId> busyShardsForRepo(String repoName, @Nullable SnapshotsInProgress snapshots, Metadata metadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 560}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODY4NjY0Nw==", "bodyText": "Maybe I'm indifferent to the naming here, I'd say let's not rename things we don't have to rename in this one to keep the noise down maybe? There's a TODO to refactor the use of this method anyway and we can do better with the naming when we get to that?", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498686647", "createdAt": "2020-10-02T08:33:20Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1809,16 +2126,32 @@ public static ClusterState updateWithSnapshots(ClusterState state,\n      * @param snapshots    snapshots in progress\n      * @return shard ids that currently have an actively executing shard snapshot on a data node\n      */\n-    private static Set<ShardId> busyShardsForRepo(String repoName, @Nullable SnapshotsInProgress snapshots) {\n+    private static Set<ShardId> busyShardsForRepo(String repoName, @Nullable SnapshotsInProgress snapshots, Metadata metadata) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIxMzU3MQ=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 560}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExOTMzMTU0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMzo0NzoxNFrOHbLNLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQwODoxOToxNFrOHbk_rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI1NzE5Ng==", "bodyText": "Given the complexity of this method, I think it deserves its own unit tests.", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498257196", "createdAt": "2020-10-01T13:47:14Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1911,82 +2244,246 @@ public boolean assertAllListenersResolved() {\n         return true;\n     }\n \n+    /**\n+     * Executor that applies {@link ShardSnapshotUpdate}s to the current cluster state. The algorithm implemented below works as described\n+     * below:\n+     * Every shard snapshot or clone state update can result in multiple snapshots being updated. In order to determine whether or not a\n+     * shard update has an effect we use an outer loop over all current executing snapshot operations that iterates over them in the order\n+     * they were started in and an inner loop over the list of shard update tasks.\n+     *\n+     * If the inner loop finds that a shard update task applies to a given snapshot and either a shard-snapshot or shard-clone operation in\n+     * it then it will update the state of the snapshot entry accordingly. If that update was a noop, then the task is removed from the\n+     * iteration as it was already applied before and likely just arrived on the master node again due to retries upstream.\n+     * If the update was not a noop, then it means that the shard it applied to is now available for another snapshot or clone operation\n+     * to be re-assigned if there is another snapshot operation that is waiting for the shard to become available. We therefore record the\n+     * fact that a task was executed by adding it to a collection of executed tasks. If a subsequent execution of the outer loop finds that\n+     * a task in the executed tasks collection applied to a shard it was waiting for to become available, then the shard snapshot operation\n+     * will be started for that snapshot entry and the task removed from the collection of tasks that need to be applied to snapshot\n+     * entries since it can not have any further effects.\n+     */\n     private static final ClusterStateTaskExecutor<ShardSnapshotUpdate> SHARD_STATE_EXECUTOR = (currentState, tasks) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 613}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODY3OTcyNg==", "bodyText": "Added a few of those now with effectively 100% coverage :)", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498679726", "createdAt": "2020-10-02T08:19:14Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1911,82 +2244,246 @@ public boolean assertAllListenersResolved() {\n         return true;\n     }\n \n+    /**\n+     * Executor that applies {@link ShardSnapshotUpdate}s to the current cluster state. The algorithm implemented below works as described\n+     * below:\n+     * Every shard snapshot or clone state update can result in multiple snapshots being updated. In order to determine whether or not a\n+     * shard update has an effect we use an outer loop over all current executing snapshot operations that iterates over them in the order\n+     * they were started in and an inner loop over the list of shard update tasks.\n+     *\n+     * If the inner loop finds that a shard update task applies to a given snapshot and either a shard-snapshot or shard-clone operation in\n+     * it then it will update the state of the snapshot entry accordingly. If that update was a noop, then the task is removed from the\n+     * iteration as it was already applied before and likely just arrived on the master node again due to retries upstream.\n+     * If the update was not a noop, then it means that the shard it applied to is now available for another snapshot or clone operation\n+     * to be re-assigned if there is another snapshot operation that is waiting for the shard to become available. We therefore record the\n+     * fact that a task was executed by adding it to a collection of executed tasks. If a subsequent execution of the outer loop finds that\n+     * a task in the executed tasks collection applied to a shard it was waiting for to become available, then the shard snapshot operation\n+     * will be started for that snapshot entry and the task removed from the collection of tasks that need to be applied to snapshot\n+     * entries since it can not have any further effects.\n+     */\n     private static final ClusterStateTaskExecutor<ShardSnapshotUpdate> SHARD_STATE_EXECUTOR = (currentState, tasks) -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI1NzE5Ng=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 613}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExOTM2MTA3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMzo1MzozNlrOHbLfhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNDo1MjozNFrOHbOPtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI2MTg5NA==", "bodyText": "Can we assert entry.source() != null ?", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498261894", "createdAt": "2020-10-01T13:53:36Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1911,82 +2244,246 @@ public boolean assertAllListenersResolved() {\n         return true;\n     }\n \n+    /**\n+     * Executor that applies {@link ShardSnapshotUpdate}s to the current cluster state. The algorithm implemented below works as described\n+     * below:\n+     * Every shard snapshot or clone state update can result in multiple snapshots being updated. In order to determine whether or not a\n+     * shard update has an effect we use an outer loop over all current executing snapshot operations that iterates over them in the order\n+     * they were started in and an inner loop over the list of shard update tasks.\n+     *\n+     * If the inner loop finds that a shard update task applies to a given snapshot and either a shard-snapshot or shard-clone operation in\n+     * it then it will update the state of the snapshot entry accordingly. If that update was a noop, then the task is removed from the\n+     * iteration as it was already applied before and likely just arrived on the master node again due to retries upstream.\n+     * If the update was not a noop, then it means that the shard it applied to is now available for another snapshot or clone operation\n+     * to be re-assigned if there is another snapshot operation that is waiting for the shard to become available. We therefore record the\n+     * fact that a task was executed by adding it to a collection of executed tasks. If a subsequent execution of the outer loop finds that\n+     * a task in the executed tasks collection applied to a shard it was waiting for to become available, then the shard snapshot operation\n+     * will be started for that snapshot entry and the task removed from the collection of tasks that need to be applied to snapshot\n+     * entries since it can not have any further effects.\n+     */\n     private static final ClusterStateTaskExecutor<ShardSnapshotUpdate> SHARD_STATE_EXECUTOR = (currentState, tasks) -> {\n         int changedCount = 0;\n         int startedCount = 0;\n         final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();\n+        final String localNodeId = currentState.nodes().getLocalNodeId();\n         // Tasks to check for updates for running snapshots.\n         final List<ShardSnapshotUpdate> unconsumedTasks = new ArrayList<>(tasks);\n         // Tasks that were used to complete an existing in-progress shard snapshot\n         final Set<ShardSnapshotUpdate> executedTasks = new HashSet<>();\n+        // Outer loop over all snapshot entries in the order they were created in\n         for (SnapshotsInProgress.Entry entry : currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n             if (entry.state().completed()) {\n+                // completed snapshots do not require any updates so we just add them to the new list and keep going\n                 entries.add(entry);\n                 continue;\n             }\n             ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = null;\n+            ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clones = null;\n+            Map<String, IndexId> indicesLookup = null;\n+            // inner loop over all the shard updates that are potentially applicable to the current snapshot entry\n             for (Iterator<ShardSnapshotUpdate> iterator = unconsumedTasks.iterator(); iterator.hasNext(); ) {\n                 final ShardSnapshotUpdate updateSnapshotState = iterator.next();\n                 final Snapshot updatedSnapshot = updateSnapshotState.snapshot;\n                 final String updatedRepository = updatedSnapshot.getRepository();\n                 if (entry.repository().equals(updatedRepository) == false) {\n+                    // the update applies to a different repository so it is irrelevant here\n                     continue;\n                 }\n-                final ShardId finishedShardId = updateSnapshotState.shardId;\n-                if (entry.snapshot().getSnapshotId().equals(updatedSnapshot.getSnapshotId())) {\n-                    final ShardSnapshotStatus existing = entry.shards().get(finishedShardId);\n-                    if (existing == null) {\n-                        logger.warn(\"Received shard snapshot status update [{}] but this shard is not tracked in [{}]\",\n-                                updateSnapshotState, entry);\n-                        assert false : \"This should never happen, data nodes should only send updates for expected shards\";\n-                        continue;\n-                    }\n-                    if (existing.state().completed()) {\n-                        // No point in doing noop updates that might happen if data nodes resends shard status after a disconnect.\n-                        iterator.remove();\n-                        continue;\n-                    }\n-                    logger.trace(\"[{}] Updating shard [{}] with status [{}]\", updatedSnapshot,\n-                            finishedShardId, updateSnapshotState.updatedState.state());\n-                    if (shards == null) {\n-                        shards = ImmutableOpenMap.builder(entry.shards());\n-                    }\n-                    shards.put(finishedShardId, updateSnapshotState.updatedState);\n-                    executedTasks.add(updateSnapshotState);\n-                    changedCount++;\n-                } else if (executedTasks.contains(updateSnapshotState)) {\n-                    // tasks that completed a shard might allow starting a new shard snapshot for the current snapshot\n-                    final ShardSnapshotStatus existingStatus = entry.shards().get(finishedShardId);\n-                    if (existingStatus == null || existingStatus.state() != ShardState.QUEUED) {\n-                        continue;\n+                if (updateSnapshotState.isClone()) {\n+                    // The update applied to a shard clone operation\n+                    final RepositoryShardId finishedShardId = updateSnapshotState.repoShardId;\n+                    if (entry.snapshot().getSnapshotId().equals(updatedSnapshot.getSnapshotId())) {\n+                        final ShardSnapshotStatus existing = entry.clones().get(finishedShardId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 672}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODMwNjk5OA==", "bodyText": "certainly can, added it :)", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498306998", "createdAt": "2020-10-01T14:52:34Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1911,82 +2244,246 @@ public boolean assertAllListenersResolved() {\n         return true;\n     }\n \n+    /**\n+     * Executor that applies {@link ShardSnapshotUpdate}s to the current cluster state. The algorithm implemented below works as described\n+     * below:\n+     * Every shard snapshot or clone state update can result in multiple snapshots being updated. In order to determine whether or not a\n+     * shard update has an effect we use an outer loop over all current executing snapshot operations that iterates over them in the order\n+     * they were started in and an inner loop over the list of shard update tasks.\n+     *\n+     * If the inner loop finds that a shard update task applies to a given snapshot and either a shard-snapshot or shard-clone operation in\n+     * it then it will update the state of the snapshot entry accordingly. If that update was a noop, then the task is removed from the\n+     * iteration as it was already applied before and likely just arrived on the master node again due to retries upstream.\n+     * If the update was not a noop, then it means that the shard it applied to is now available for another snapshot or clone operation\n+     * to be re-assigned if there is another snapshot operation that is waiting for the shard to become available. We therefore record the\n+     * fact that a task was executed by adding it to a collection of executed tasks. If a subsequent execution of the outer loop finds that\n+     * a task in the executed tasks collection applied to a shard it was waiting for to become available, then the shard snapshot operation\n+     * will be started for that snapshot entry and the task removed from the collection of tasks that need to be applied to snapshot\n+     * entries since it can not have any further effects.\n+     */\n     private static final ClusterStateTaskExecutor<ShardSnapshotUpdate> SHARD_STATE_EXECUTOR = (currentState, tasks) -> {\n         int changedCount = 0;\n         int startedCount = 0;\n         final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();\n+        final String localNodeId = currentState.nodes().getLocalNodeId();\n         // Tasks to check for updates for running snapshots.\n         final List<ShardSnapshotUpdate> unconsumedTasks = new ArrayList<>(tasks);\n         // Tasks that were used to complete an existing in-progress shard snapshot\n         final Set<ShardSnapshotUpdate> executedTasks = new HashSet<>();\n+        // Outer loop over all snapshot entries in the order they were created in\n         for (SnapshotsInProgress.Entry entry : currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n             if (entry.state().completed()) {\n+                // completed snapshots do not require any updates so we just add them to the new list and keep going\n                 entries.add(entry);\n                 continue;\n             }\n             ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = null;\n+            ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clones = null;\n+            Map<String, IndexId> indicesLookup = null;\n+            // inner loop over all the shard updates that are potentially applicable to the current snapshot entry\n             for (Iterator<ShardSnapshotUpdate> iterator = unconsumedTasks.iterator(); iterator.hasNext(); ) {\n                 final ShardSnapshotUpdate updateSnapshotState = iterator.next();\n                 final Snapshot updatedSnapshot = updateSnapshotState.snapshot;\n                 final String updatedRepository = updatedSnapshot.getRepository();\n                 if (entry.repository().equals(updatedRepository) == false) {\n+                    // the update applies to a different repository so it is irrelevant here\n                     continue;\n                 }\n-                final ShardId finishedShardId = updateSnapshotState.shardId;\n-                if (entry.snapshot().getSnapshotId().equals(updatedSnapshot.getSnapshotId())) {\n-                    final ShardSnapshotStatus existing = entry.shards().get(finishedShardId);\n-                    if (existing == null) {\n-                        logger.warn(\"Received shard snapshot status update [{}] but this shard is not tracked in [{}]\",\n-                                updateSnapshotState, entry);\n-                        assert false : \"This should never happen, data nodes should only send updates for expected shards\";\n-                        continue;\n-                    }\n-                    if (existing.state().completed()) {\n-                        // No point in doing noop updates that might happen if data nodes resends shard status after a disconnect.\n-                        iterator.remove();\n-                        continue;\n-                    }\n-                    logger.trace(\"[{}] Updating shard [{}] with status [{}]\", updatedSnapshot,\n-                            finishedShardId, updateSnapshotState.updatedState.state());\n-                    if (shards == null) {\n-                        shards = ImmutableOpenMap.builder(entry.shards());\n-                    }\n-                    shards.put(finishedShardId, updateSnapshotState.updatedState);\n-                    executedTasks.add(updateSnapshotState);\n-                    changedCount++;\n-                } else if (executedTasks.contains(updateSnapshotState)) {\n-                    // tasks that completed a shard might allow starting a new shard snapshot for the current snapshot\n-                    final ShardSnapshotStatus existingStatus = entry.shards().get(finishedShardId);\n-                    if (existingStatus == null || existingStatus.state() != ShardState.QUEUED) {\n-                        continue;\n+                if (updateSnapshotState.isClone()) {\n+                    // The update applied to a shard clone operation\n+                    final RepositoryShardId finishedShardId = updateSnapshotState.repoShardId;\n+                    if (entry.snapshot().getSnapshotId().equals(updatedSnapshot.getSnapshotId())) {\n+                        final ShardSnapshotStatus existing = entry.clones().get(finishedShardId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI2MTg5NA=="}, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 672}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExOTUzNDc4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNDozMDoyMVrOHbNNHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNDozMDoyMVrOHbNNHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI4OTk1MA==", "bodyText": "I've been a bit worried about re-resolving indices here as the cloned shard could reference an index from the snapshot that do not exist anymore, or could have been recreated with a different sharding, but after writing some tests I think it's OK as existingStatus == null later will prevent any difference in the number of shards.", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498289950", "createdAt": "2020-10-01T14:30:21Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1911,82 +2244,246 @@ public boolean assertAllListenersResolved() {\n         return true;\n     }\n \n+    /**\n+     * Executor that applies {@link ShardSnapshotUpdate}s to the current cluster state. The algorithm implemented below works as described\n+     * below:\n+     * Every shard snapshot or clone state update can result in multiple snapshots being updated. In order to determine whether or not a\n+     * shard update has an effect we use an outer loop over all current executing snapshot operations that iterates over them in the order\n+     * they were started in and an inner loop over the list of shard update tasks.\n+     *\n+     * If the inner loop finds that a shard update task applies to a given snapshot and either a shard-snapshot or shard-clone operation in\n+     * it then it will update the state of the snapshot entry accordingly. If that update was a noop, then the task is removed from the\n+     * iteration as it was already applied before and likely just arrived on the master node again due to retries upstream.\n+     * If the update was not a noop, then it means that the shard it applied to is now available for another snapshot or clone operation\n+     * to be re-assigned if there is another snapshot operation that is waiting for the shard to become available. We therefore record the\n+     * fact that a task was executed by adding it to a collection of executed tasks. If a subsequent execution of the outer loop finds that\n+     * a task in the executed tasks collection applied to a shard it was waiting for to become available, then the shard snapshot operation\n+     * will be started for that snapshot entry and the task removed from the collection of tasks that need to be applied to snapshot\n+     * entries since it can not have any further effects.\n+     */\n     private static final ClusterStateTaskExecutor<ShardSnapshotUpdate> SHARD_STATE_EXECUTOR = (currentState, tasks) -> {\n         int changedCount = 0;\n         int startedCount = 0;\n         final List<SnapshotsInProgress.Entry> entries = new ArrayList<>();\n+        final String localNodeId = currentState.nodes().getLocalNodeId();\n         // Tasks to check for updates for running snapshots.\n         final List<ShardSnapshotUpdate> unconsumedTasks = new ArrayList<>(tasks);\n         // Tasks that were used to complete an existing in-progress shard snapshot\n         final Set<ShardSnapshotUpdate> executedTasks = new HashSet<>();\n+        // Outer loop over all snapshot entries in the order they were created in\n         for (SnapshotsInProgress.Entry entry : currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n             if (entry.state().completed()) {\n+                // completed snapshots do not require any updates so we just add them to the new list and keep going\n                 entries.add(entry);\n                 continue;\n             }\n             ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = null;\n+            ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clones = null;\n+            Map<String, IndexId> indicesLookup = null;\n+            // inner loop over all the shard updates that are potentially applicable to the current snapshot entry\n             for (Iterator<ShardSnapshotUpdate> iterator = unconsumedTasks.iterator(); iterator.hasNext(); ) {\n                 final ShardSnapshotUpdate updateSnapshotState = iterator.next();\n                 final Snapshot updatedSnapshot = updateSnapshotState.snapshot;\n                 final String updatedRepository = updatedSnapshot.getRepository();\n                 if (entry.repository().equals(updatedRepository) == false) {\n+                    // the update applies to a different repository so it is irrelevant here\n                     continue;\n                 }\n-                final ShardId finishedShardId = updateSnapshotState.shardId;\n-                if (entry.snapshot().getSnapshotId().equals(updatedSnapshot.getSnapshotId())) {\n-                    final ShardSnapshotStatus existing = entry.shards().get(finishedShardId);\n-                    if (existing == null) {\n-                        logger.warn(\"Received shard snapshot status update [{}] but this shard is not tracked in [{}]\",\n-                                updateSnapshotState, entry);\n-                        assert false : \"This should never happen, data nodes should only send updates for expected shards\";\n-                        continue;\n-                    }\n-                    if (existing.state().completed()) {\n-                        // No point in doing noop updates that might happen if data nodes resends shard status after a disconnect.\n-                        iterator.remove();\n-                        continue;\n-                    }\n-                    logger.trace(\"[{}] Updating shard [{}] with status [{}]\", updatedSnapshot,\n-                            finishedShardId, updateSnapshotState.updatedState.state());\n-                    if (shards == null) {\n-                        shards = ImmutableOpenMap.builder(entry.shards());\n-                    }\n-                    shards.put(finishedShardId, updateSnapshotState.updatedState);\n-                    executedTasks.add(updateSnapshotState);\n-                    changedCount++;\n-                } else if (executedTasks.contains(updateSnapshotState)) {\n-                    // tasks that completed a shard might allow starting a new shard snapshot for the current snapshot\n-                    final ShardSnapshotStatus existingStatus = entry.shards().get(finishedShardId);\n-                    if (existingStatus == null || existingStatus.state() != ShardState.QUEUED) {\n-                        continue;\n+                if (updateSnapshotState.isClone()) {\n+                    // The update applied to a shard clone operation\n+                    final RepositoryShardId finishedShardId = updateSnapshotState.repoShardId;\n+                    if (entry.snapshot().getSnapshotId().equals(updatedSnapshot.getSnapshotId())) {\n+                        final ShardSnapshotStatus existing = entry.clones().get(finishedShardId);\n+                        if (existing == null) {\n+                            logger.warn(\"Received clone shard snapshot status update [{}] but this shard is not tracked in [{}]\",\n+                                    updateSnapshotState, entry);\n+                            assert false : \"This should never happen, master will not submit a state update for a non-existing clone\";\n+                            continue;\n+                        }\n+                        if (existing.state().completed()) {\n+                            // No point in doing noop updates that might happen if data nodes resends shard status after a disconnect.\n+                            iterator.remove();\n+                            continue;\n+                        }\n+                        logger.trace(\"[{}] Updating shard clone [{}] with status [{}]\", updatedSnapshot,\n+                                finishedShardId, updateSnapshotState.updatedState.state());\n+                        if (clones == null) {\n+                            clones = ImmutableOpenMap.builder(entry.clones());\n+                        }\n+                        changedCount++;\n+                        clones.put(finishedShardId, updateSnapshotState.updatedState);\n+                        executedTasks.add(updateSnapshotState);\n+                    } else if (executedTasks.contains(updateSnapshotState)) {\n+                        // the update was already executed on the clone operation it applied to, now we check if it may be possible to\n+                        // start a shard snapshot or clone operation on the current entry\n+                        if (entry.source() == null) {\n+                            // current entry is a snapshot operation so we must translate the repository shard id to a routing shard id\n+                            final IndexMetadata indexMeta = currentState.metadata().index(finishedShardId.indexName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3c5f8cdf4b87ce4211ae8a3cfd6caa7aadae4dec"}, "originalPosition": 697}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMjgyODQzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzozNDozM1rOHbt1vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzozNDozM1rOHbt1vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDYzNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                        \"] because its snapshot is was not successful.\");\n          \n          \n            \n                                        \"] because its snapshot was not successful.\");", "url": "https://github.com/elastic/elasticsearch/pull/61839#discussion_r498824636", "createdAt": "2020-10-02T13:34:33Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -313,6 +308,276 @@ public TimeValue timeout() {\n         }, \"create_snapshot [\" + snapshotName + ']', listener::onFailure);\n     }\n \n+    private static void ensureSnapshotNameNotRunning(List<SnapshotsInProgress.Entry> runningSnapshots, String repositoryName,\n+                                                     String snapshotName) {\n+        if (runningSnapshots.stream().anyMatch(s -> {\n+            final Snapshot running = s.snapshot();\n+            return running.getRepository().equals(repositoryName) && running.getSnapshotId().getName().equals(snapshotName);\n+        })) {\n+            throw new InvalidSnapshotNameException(repositoryName, snapshotName, \"snapshot with the same name is already in-progress\");\n+        }\n+    }\n+\n+    private static Map<String, IndexId> getInFlightIndexIds(List<SnapshotsInProgress.Entry> runningSnapshots, String repositoryName) {\n+        return runningSnapshots.stream().filter(entry -> entry.repository().equals(repositoryName))\n+                .flatMap(entry -> entry.indices().stream()).distinct()\n+                .collect(Collectors.toMap(IndexId::getName, Function.identity()));\n+    }\n+\n+    public void cloneSnapshot(CloneSnapshotRequest request, ActionListener<Void> listener) {\n+        final String repositoryName = request.repository();\n+        Repository repository = repositoriesService.repository(repositoryName);\n+        if (repository.isReadOnly()) {\n+            listener.onFailure(new RepositoryException(repositoryName, \"cannot create snapshot in a readonly repository\"));\n+            return;\n+        }\n+        final String snapshotName = indexNameExpressionResolver.resolveDateMathExpression(request.target());\n+        validate(repositoryName, snapshotName);\n+        final SnapshotId snapshotId = new SnapshotId(snapshotName, UUIDs.randomBase64UUID());\n+        final Snapshot snapshot = new Snapshot(repositoryName, snapshotId);\n+        initializingClones.add(snapshot);\n+        repository.executeConsistentStateUpdate(repositoryData -> new ClusterStateUpdateTask() {\n+\n+            private SnapshotsInProgress.Entry newEntry;\n+\n+            @Override\n+            public ClusterState execute(ClusterState currentState) {\n+                ensureSnapshotNameAvailableInRepo(repositoryData, snapshotName, repository);\n+                ensureNoCleanupInProgress(currentState, repositoryName, snapshotName);\n+                final SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY);\n+                final List<SnapshotsInProgress.Entry> runningSnapshots = snapshots.entries();\n+                ensureSnapshotNameNotRunning(runningSnapshots, repositoryName, snapshotName);\n+                validate(repositoryName, snapshotName, currentState);\n+\n+                final SnapshotId sourceSnapshotId = repositoryData.getSnapshotIds()\n+                        .stream()\n+                        .filter(src -> src.getName().equals(request.source()))\n+                        .findAny()\n+                        .orElseThrow(() -> new SnapshotMissingException(repositoryName, request.source()));\n+                final SnapshotDeletionsInProgress deletionsInProgress =\n+                        currentState.custom(SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY);\n+                if (deletionsInProgress.getEntries().stream().anyMatch(entry -> entry.getSnapshots().contains(sourceSnapshotId))) {\n+                    throw new ConcurrentSnapshotExecutionException(repositoryName, sourceSnapshotId.getName(),\n+                            \"cannot clone from snapshot that is being deleted\");\n+                }\n+                ensureBelowConcurrencyLimit(repositoryName, snapshotName, snapshots, deletionsInProgress);\n+                final List<String> indicesForSnapshot = new ArrayList<>();\n+                for (IndexId indexId : repositoryData.getIndices().values()) {\n+                    if (repositoryData.getSnapshots(indexId).contains(sourceSnapshotId)) {\n+                        indicesForSnapshot.add(indexId.getName());\n+                    }\n+                }\n+                final List<String> matchingIndices =\n+                        SnapshotUtils.filterIndices(indicesForSnapshot, request.indices(), request.indicesOptions());\n+                if (matchingIndices.isEmpty()) {\n+                    throw new SnapshotException(new Snapshot(repositoryName, sourceSnapshotId),\n+                            \"No indices in the source snapshot [\" + sourceSnapshotId + \"] matched requested pattern [\"\n+                                    + Strings.arrayToCommaDelimitedString(request.indices()) + \"]\");\n+                }\n+                newEntry = SnapshotsInProgress.startClone(\n+                        snapshot, sourceSnapshotId,\n+                        repositoryData.resolveIndices(matchingIndices),\n+                        threadPool.absoluteTimeInMillis(), repositoryData.getGenId(),\n+                        minCompatibleVersion(currentState.nodes().getMinNodeVersion(), repositoryData, null));\n+                final List<SnapshotsInProgress.Entry> newEntries = new ArrayList<>(runningSnapshots);\n+                newEntries.add(newEntry);\n+                return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE,\n+                        SnapshotsInProgress.of(List.copyOf(newEntries))).build();\n+            }\n+\n+            @Override\n+            public void onFailure(String source, Exception e) {\n+                initializingClones.remove(snapshot);\n+                logger.warn(() -> new ParameterizedMessage(\"[{}][{}] failed to clone snapshot\", repositoryName, snapshotName), e);\n+                listener.onFailure(e);\n+            }\n+\n+            @Override\n+            public void clusterStateProcessed(String source, ClusterState oldState, final ClusterState newState) {\n+                logger.info(\"snapshot clone [{}] started\", snapshot);\n+                addListener(snapshot, ActionListener.wrap(r -> listener.onResponse(null), listener::onFailure));\n+                startCloning(repository, newEntry);\n+            }\n+\n+            @Override\n+            public TimeValue timeout() {\n+                initializingClones.remove(snapshot);\n+                return request.masterNodeTimeout();\n+            }\n+        }, \"clone_snapshot [\" + request.source() + \"][\" + snapshotName + ']', listener::onFailure);\n+    }\n+\n+    private static void ensureNoCleanupInProgress(ClusterState currentState, String repositoryName, String snapshotName) {\n+        final RepositoryCleanupInProgress repositoryCleanupInProgress =\n+                currentState.custom(RepositoryCleanupInProgress.TYPE, RepositoryCleanupInProgress.EMPTY);\n+        if (repositoryCleanupInProgress.hasCleanupInProgress()) {\n+            throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName,\n+                    \"cannot snapshot while a repository cleanup is in-progress in [\" + repositoryCleanupInProgress + \"]\");\n+        }\n+    }\n+\n+    private static void ensureSnapshotNameAvailableInRepo(RepositoryData repositoryData, String snapshotName, Repository repository) {\n+        // check if the snapshot name already exists in the repository\n+        if (repositoryData.getSnapshotIds().stream().anyMatch(s -> s.getName().equals(snapshotName))) {\n+            throw new InvalidSnapshotNameException(\n+                    repository.getMetadata().name(), snapshotName, \"snapshot with the same name already exists\");\n+        }\n+    }\n+\n+    /**\n+     * Determine the number of shards in each index of a clone operation and update the cluster state accordingly.\n+     *\n+     * @param repository     repository to run operation on\n+     * @param cloneEntry     clone operation in the cluster state\n+     */\n+    private void startCloning(Repository repository, SnapshotsInProgress.Entry cloneEntry) {\n+        final List<IndexId> indices = cloneEntry.indices();\n+        final SnapshotId sourceSnapshot = cloneEntry.source();\n+        final Snapshot targetSnapshot = cloneEntry.snapshot();\n+\n+        final Executor executor = threadPool.executor(ThreadPool.Names.SNAPSHOT);\n+        // Exception handler for IO exceptions with loading index and repo metadata\n+        final Consumer<Exception> onFailure = e -> {\n+            initializingClones.remove(targetSnapshot);\n+            logger.info(() -> new ParameterizedMessage(\"Failed to start snapshot clone [{}]\", cloneEntry), e);\n+            removeFailedSnapshotFromClusterState(targetSnapshot, e, null);\n+        };\n+\n+        // 1. step, load SnapshotInfo to make sure that source snapshot was successful for the indices we want to clone\n+        // TODO: we could skip this step for snapshots with state SUCCESS\n+        final StepListener<SnapshotInfo> snapshotInfoListener = new StepListener<>();\n+        executor.execute(ActionRunnable.supply(snapshotInfoListener, () -> repository.getSnapshotInfo(sourceSnapshot)));\n+\n+        final StepListener<Collection<Tuple<IndexId, Integer>>> allShardCountsListener = new StepListener<>();\n+        final GroupedActionListener<Tuple<IndexId, Integer>> shardCountListener =\n+                new GroupedActionListener<>(allShardCountsListener, indices.size());\n+        snapshotInfoListener.whenComplete(snapshotInfo -> {\n+            for (IndexId indexId : indices) {\n+                if (RestoreService.failed(snapshotInfo, indexId.getName())) {\n+                    throw new SnapshotException(targetSnapshot, \"Can't clone index [\" + indexId +\n+                            \"] because its snapshot is was not successful.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e5b74bfcd431b1192f4aface3739f2026da8268"}, "originalPosition": 245}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1837, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}