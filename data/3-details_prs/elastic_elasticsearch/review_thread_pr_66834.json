{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ2MTI0ODMy", "number": 66834, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNDo1OTowNFrOFJjBEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xMFQwNTozMjo0M1rOGtCuuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTU1MjE5OnYy", "diffSide": "LEFT", "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNDo1OTowNFrOIL7UqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNDo1OTowNFrOIL7UqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM3NzE5Mg==", "bodyText": "This was overridden by all but the adjusted test implementation.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r549377192", "createdAt": "2020-12-28T14:59:04Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "diffHunk": "@@ -173,10 +170,6 @@ public ThreadPool getThreadPool() {\n         return () -> circuitBreakerService.getBreaker(CircuitBreaker.IN_FLIGHT_REQUESTS);\n     }\n \n-    @Override\n-    protected void doStart() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "597e600d698faed1da8c32e49a1e2b8374b0998d"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTU1MzAxOnYy", "diffSide": "LEFT", "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNDo1OToxN1rOIL7VFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNDo1OToxN1rOIL7VFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM3NzMwMQ==", "bodyText": "return was never used", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r549377301", "createdAt": "2020-12-28T14:59:17Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "diffHunk": "@@ -279,8 +272,8 @@ public void openConnection(DiscoveryNode node, ConnectionProfile profile, Action\n         }\n     }\n \n-    private List<TcpChannel> initiateConnection(DiscoveryNode node, ConnectionProfile connectionProfile,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "597e600d698faed1da8c32e49a1e2b8374b0998d"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTU2MTMzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNTowMjo0OFrOIL7Zlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNTowMjo0OFrOIL7Zlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM3ODQ1NQ==", "bodyText": "I added this check since we might dead-lock when coming from a transport thread (more of a docs thing, we couldn't call this from a transport thread anyway due to other assertions). Other than that, there is no point in forking off here as far as I can tell if we then block anyway. Just makes ITs use more threads and could theoretically dead-lock when called from an already maxed out generic pool.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r549378455", "createdAt": "2020-12-28T15:02:48Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "diffHunk": "@@ -550,42 +542,30 @@ protected final void doClose() {\n \n     @Override\n     protected final void doStop() {\n-        final CountDownLatch latch = new CountDownLatch(1);\n-        // make sure we run it on another thread than a possible IO handler thread\n-        assert threadPool.generic().isShutdown() == false : \"Must stop transport before terminating underlying threadpool\";\n-        threadPool.generic().execute(() -> {\n-            closeLock.writeLock().lock();\n-            try {\n-                keepAlive.close();\n+        assert Transports.assertNotTransportThread(\"Must not block transport thread that might be needed for closing channels below\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "597e600d698faed1da8c32e49a1e2b8374b0998d"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTU2NzExOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNTowNToxM1rOIL7cqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNTowNToxM1rOIL7cqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM3OTI0Mg==", "bodyText": "No point in forking off potentially multiple times here when we do all kinds of slow+blocking operations when closing the connection manager etc. above. All this does is potentially have tests fail when these tasks don't run before the threadpool is shut down and they do in fact release resources (which currently just works by accident).", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r549379242", "createdAt": "2020-12-28T15:05:13Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -252,34 +242,15 @@ protected void doStop() {\n         } finally {\n             // in case the transport is not connected to our local node (thus cleaned on node disconnect)\n             // make sure to clean any leftover on going handles\n-            for (final Transport.ResponseContext holderToNotify : responseHandlers.prune(h -> true)) {\n-                // callback that an exception happened, but on a different thread since we don't\n-                // want handlers to worry about stack overflows\n-                getExecutorService().execute(new AbstractRunnable() {\n-                    @Override\n-                    public void onRejection(Exception e) {\n-                        // if we get rejected during node shutdown we don't wanna bubble it up\n-                        logger.debug(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on rejection, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void onFailure(Exception e) {\n-                        logger.warn(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on exception, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void doRun() {\n-                        TransportException ex = new SendRequestTransportException(holderToNotify.connection().getNode(),\n+            for (final Transport.ResponseContext<?> holderToNotify : responseHandlers.prune(h -> true)) {\n+                try {\n+                    TransportException ex = new SendRequestTransportException(holderToNotify.connection().getNode(),\n                             holderToNotify.action(), new NodeClosedException(localNode));\n-                        holderToNotify.handler().handleException(ex);\n-                    }\n-                });\n+                    holderToNotify.handler().handleException(ex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "597e600d698faed1da8c32e49a1e2b8374b0998d"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTU2ODQ4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNTowNTo1MVrOIL7dYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxNTowNTo1MVrOIL7dYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM3OTQyNg==", "bodyText": "Mostly we don't have any open handlers here on close, no point forking off for an empty list.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r549379426", "createdAt": "2020-12-28T15:05:51Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -1036,27 +1007,29 @@ private void checkForTimeout(long requestId) {\n \n     @Override\n     public void onConnectionClosed(Transport.Connection connection) {\n-        try {\n-            List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n+        List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n                 responseHandlers.prune(h -> h.connection().getCacheKey().equals(connection.getCacheKey()));\n+        if (pruned.isEmpty() == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "597e600d698faed1da8c32e49a1e2b8374b0998d"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDQ5NjU5ODMyOnYy", "diffSide": "LEFT", "path": "server/src/main/java/org/elasticsearch/transport/StatsTracker.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0wOVQxNzo0NzowM1rOKg4ZYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xMFQxMDo0NToxM1rOKhX50w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU2NzA3NQ==", "bodyText": "Seems a bit odd to use a MeanMetric for this and not for bytes written. Let's just use LongAdder for them all.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705567075", "createdAt": "2021-09-09T17:47:03Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/StatsTracker.java", "diffHunk": "@@ -38,11 +38,6 @@ public long getMessagesReceived() {\n         return messagesReceived.sum();\n     }\n \n-\n-    public MeanMetric getWriteBytes() {\n-        return writeBytesMetric;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTg5MzM0Nw==", "bodyText": "Huh didn't notice that :) => long adder it is", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705893347", "createdAt": "2021-09-10T04:47:29Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/StatsTracker.java", "diffHunk": "@@ -38,11 +38,6 @@ public long getMessagesReceived() {\n         return messagesReceived.sum();\n     }\n \n-\n-    public MeanMetric getWriteBytes() {\n-        return writeBytesMetric;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU2NzA3NQ=="}, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNjAwMTYwOQ==", "bodyText": "Actually I had to revert this. We're using the sum and count methods for messages sent and bytes written separately on writeBytesMetric. Obviously didn't notice that, because the sum one didn't change when I moved to long adder \ud83e\udd26 :)", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r706001609", "createdAt": "2021-09-10T08:35:48Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/StatsTracker.java", "diffHunk": "@@ -38,11 +38,6 @@ public long getMessagesReceived() {\n         return messagesReceived.sum();\n     }\n \n-\n-    public MeanMetric getWriteBytes() {\n-        return writeBytesMetric;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU2NzA3NQ=="}, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNjA4MzI4Mw==", "bodyText": "Ah sorry I missed this in the review. Still think we should unwrap the MeanMetric and just have a pair of LongAdders, although of course we need to increment the counter on each message as well as recording the bytes.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r706083283", "createdAt": "2021-09-10T10:45:13Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/StatsTracker.java", "diffHunk": "@@ -38,11 +38,6 @@ public long getMessagesReceived() {\n         return messagesReceived.sum();\n     }\n \n-\n-    public MeanMetric getWriteBytes() {\n-        return writeBytesMetric;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU2NzA3NQ=="}, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDQ5NjcyODMxOnYy", "diffSide": "LEFT", "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0wOVQxODoxNTo0N1rOKg5o-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xMFQwNDoyNzoyN1rOKhL47Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU4NzQ0OQ==", "bodyText": "Should we keep this? Sounds like the sort of invariant we should check, we're probably assuming it in other places.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705587449", "createdAt": "2021-09-09T18:15:47Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "diffHunk": "@@ -559,42 +551,30 @@ protected final void doClose() {\n \n     @Override\n     protected final void doStop() {\n-        final CountDownLatch latch = new CountDownLatch(1);\n-        // make sure we run it on another thread than a possible IO handler thread\n-        assert threadPool.generic().isShutdown() == false : \"Must stop transport before terminating underlying threadpool\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTg4NjQ0NQ==", "bodyText": "Yea good point :) lets keep it.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705886445", "createdAt": "2021-09-10T04:27:27Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "diffHunk": "@@ -559,42 +551,30 @@ protected final void doClose() {\n \n     @Override\n     protected final void doStop() {\n-        final CountDownLatch latch = new CountDownLatch(1);\n-        // make sure we run it on another thread than a possible IO handler thread\n-        assert threadPool.generic().isShutdown() == false : \"Must stop transport before terminating underlying threadpool\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU4NzQ0OQ=="}, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDQ5NjczODU5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0wOVQxODoxODowOFrOKg5vRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xMFQwNzo1NjowN1rOKhRVdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU4OTA2MA==", "bodyText": "Related to the previous comment: if we know that the transport service is closed before the threadpool then can this be rejected? If not we should assert false in the rejection handler. Also while we're here let's use an AbstractRunnable rather than catching the EsRejectedExecutionException ourselves and assert that handling the NodeDisconnectedException doesn't throw I guess.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705589060", "createdAt": "2021-09-09T18:18:08Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -1023,12 +993,15 @@ private void checkForTimeout(long requestId) {\n \n     @Override\n     public void onConnectionClosed(Transport.Connection connection) {\n-        try {\n-            List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n+        List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n                 responseHandlers.prune(h -> h.connection().getCacheKey().equals(connection.getCacheKey()));\n+        if (pruned.isEmpty()) {\n+            return;\n+        }\n+        try {\n             // callback that an exception happened, but on a different thread since we don't\n             // want handlers to worry about stack overflows\n-            getExecutorService().execute(new Runnable() {\n+            threadPool.generic().execute(new Runnable() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTg4Nzc5MQ==", "bodyText": "Actually, it's impossible for this to throw because the generic pool never rejects. Let's just assert false on all exceptions via an AbstractRunnable then we cover everything :)", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705887791", "createdAt": "2021-09-10T04:31:28Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -1023,12 +993,15 @@ private void checkForTimeout(long requestId) {\n \n     @Override\n     public void onConnectionClosed(Transport.Connection connection) {\n-        try {\n-            List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n+        List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n                 responseHandlers.prune(h -> h.connection().getCacheKey().equals(connection.getCacheKey()));\n+        if (pruned.isEmpty()) {\n+            return;\n+        }\n+        try {\n             // callback that an exception happened, but on a different thread since we don't\n             // want handlers to worry about stack overflows\n-            getExecutorService().execute(new Runnable() {\n+            threadPool.generic().execute(new Runnable() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU4OTA2MA=="}, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTk3NTY3MQ==", "bodyText": "the generic pool never rejects\n\n... except if shut down (hence the relationship to the previous comment)", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705975671", "createdAt": "2021-09-10T07:56:07Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -1023,12 +993,15 @@ private void checkForTimeout(long requestId) {\n \n     @Override\n     public void onConnectionClosed(Transport.Connection connection) {\n-        try {\n-            List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n+        List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n                 responseHandlers.prune(h -> h.connection().getCacheKey().equals(connection.getCacheKey()));\n+        if (pruned.isEmpty()) {\n+            return;\n+        }\n+        try {\n             // callback that an exception happened, but on a different thread since we don't\n             // want handlers to worry about stack overflows\n-            getExecutorService().execute(new Runnable() {\n+            threadPool.generic().execute(new Runnable() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU4OTA2MA=="}, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDQ5Njc0MDM5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0wOVQxODoxODozM1rOKg5waw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0wOVQxODoxODozM1rOKg5waw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU4OTM1NQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705589355", "createdAt": "2021-09-09T18:18:33Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -1023,12 +993,15 @@ private void checkForTimeout(long requestId) {\n \n     @Override\n     public void onConnectionClosed(Transport.Connection connection) {\n-        try {\n-            List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n+        List<Transport.ResponseContext<? extends TransportResponse>> pruned =\n                 responseHandlers.prune(h -> h.connection().getCacheKey().equals(connection.getCacheKey()));\n+        if (pruned.isEmpty()) {\n+            return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDQ5Njc0MzY4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0wOVQxODoxOToxOFrOKg5yaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xMFQwNDoyODo0NVrOKhL6jQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU4OTg2NQ==", "bodyText": "Can we assert false here too?", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705589865", "createdAt": "2021-09-09T18:19:18Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -247,33 +237,13 @@ protected void doStop() {\n             // in case the transport is not connected to our local node (thus cleaned on node disconnect)\n             // make sure to clean any leftover on going handles\n             for (final Transport.ResponseContext<?> holderToNotify : responseHandlers.prune(h -> true)) {\n-                // callback that an exception happened, but on a different thread since we don't\n-                // want handlers to worry about stack overflows\n-                getExecutorService().execute(new AbstractRunnable() {\n-                    @Override\n-                    public void onRejection(Exception e) {\n-                        // if we get rejected during node shutdown we don't wanna bubble it up\n-                        logger.debug(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on rejection, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void onFailure(Exception e) {\n-                        logger.warn(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on exception, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void doRun() {\n-                        TransportException ex = new SendRequestTransportException(holderToNotify.connection().getNode(),\n-                            holderToNotify.action(), new NodeClosedException(localNode));\n-                        holderToNotify.handler().handleException(ex);\n-                    }\n-                });\n+                try {\n+                    holderToNotify.handler().handleException(new SendRequestTransportException(holderToNotify.connection().getNode(),\n+                            holderToNotify.action(), new NodeClosedException(localNode)));\n+                } catch (Exception e) {\n+                    logger.warn(() -> new ParameterizedMessage(\"failed to notify response handler on exception, action: {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTg4Njg2MQ==", "bodyText": "Yea lets do it, I could see us running into this in some spots but if we do we can+should fix them :)", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705886861", "createdAt": "2021-09-10T04:28:45Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -247,33 +237,13 @@ protected void doStop() {\n             // in case the transport is not connected to our local node (thus cleaned on node disconnect)\n             // make sure to clean any leftover on going handles\n             for (final Transport.ResponseContext<?> holderToNotify : responseHandlers.prune(h -> true)) {\n-                // callback that an exception happened, but on a different thread since we don't\n-                // want handlers to worry about stack overflows\n-                getExecutorService().execute(new AbstractRunnable() {\n-                    @Override\n-                    public void onRejection(Exception e) {\n-                        // if we get rejected during node shutdown we don't wanna bubble it up\n-                        logger.debug(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on rejection, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void onFailure(Exception e) {\n-                        logger.warn(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on exception, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void doRun() {\n-                        TransportException ex = new SendRequestTransportException(holderToNotify.connection().getNode(),\n-                            holderToNotify.action(), new NodeClosedException(localNode));\n-                        holderToNotify.handler().handleException(ex);\n-                    }\n-                });\n+                try {\n+                    holderToNotify.handler().handleException(new SendRequestTransportException(holderToNotify.connection().getNode(),\n+                            holderToNotify.action(), new NodeClosedException(localNode)));\n+                } catch (Exception e) {\n+                    logger.warn(() -> new ParameterizedMessage(\"failed to notify response handler on exception, action: {}\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU4OTg2NQ=="}, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDQ5Njc2NzU4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0wOVQxODoyNTowN1rOKg6Bbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xMFQwNDozOToyN1rOKhMIjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU5MzcxMA==", "bodyText": "I was wondering if we should respect handler().executor() but then I looked at other call sites and it seems that we almost never do. Except sometimes. That might bite us one day.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705593710", "createdAt": "2021-09-09T18:25:07Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -247,33 +237,13 @@ protected void doStop() {\n             // in case the transport is not connected to our local node (thus cleaned on node disconnect)\n             // make sure to clean any leftover on going handles\n             for (final Transport.ResponseContext<?> holderToNotify : responseHandlers.prune(h -> true)) {\n-                // callback that an exception happened, but on a different thread since we don't\n-                // want handlers to worry about stack overflows\n-                getExecutorService().execute(new AbstractRunnable() {\n-                    @Override\n-                    public void onRejection(Exception e) {\n-                        // if we get rejected during node shutdown we don't wanna bubble it up\n-                        logger.debug(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on rejection, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void onFailure(Exception e) {\n-                        logger.warn(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on exception, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void doRun() {\n-                        TransportException ex = new SendRequestTransportException(holderToNotify.connection().getNode(),\n-                            holderToNotify.action(), new NodeClosedException(localNode));\n-                        holderToNotify.handler().handleException(ex);\n-                    }\n-                });\n+                try {\n+                    holderToNotify.handler().handleException(new SendRequestTransportException(holderToNotify.connection().getNode(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTg5MDQ0NQ==", "bodyText": "Yea this one is a mess (it's similar to the threading discussion we had around the internal node client usage I guess) but this seems like the place where we might specifically not want to respect the executor to make the shutdown as safe as possible.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705890445", "createdAt": "2021-09-10T04:39:27Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TransportService.java", "diffHunk": "@@ -247,33 +237,13 @@ protected void doStop() {\n             // in case the transport is not connected to our local node (thus cleaned on node disconnect)\n             // make sure to clean any leftover on going handles\n             for (final Transport.ResponseContext<?> holderToNotify : responseHandlers.prune(h -> true)) {\n-                // callback that an exception happened, but on a different thread since we don't\n-                // want handlers to worry about stack overflows\n-                getExecutorService().execute(new AbstractRunnable() {\n-                    @Override\n-                    public void onRejection(Exception e) {\n-                        // if we get rejected during node shutdown we don't wanna bubble it up\n-                        logger.debug(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on rejection, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void onFailure(Exception e) {\n-                        logger.warn(\n-                            () -> new ParameterizedMessage(\n-                                \"failed to notify response handler on exception, action: {}\",\n-                                holderToNotify.action()),\n-                            e);\n-                    }\n-                    @Override\n-                    public void doRun() {\n-                        TransportException ex = new SendRequestTransportException(holderToNotify.connection().getNode(),\n-                            holderToNotify.action(), new NodeClosedException(localNode));\n-                        holderToNotify.handler().handleException(ex);\n-                    }\n-                });\n+                try {\n+                    holderToNotify.handler().handleException(new SendRequestTransportException(holderToNotify.connection().getNode(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTU5MzcxMA=="}, "originalCommit": {"oid": "5db50c961594fc09ffd53c1a803329f164b4a935"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDQ5ODgzODMyOnYy", "diffSide": "LEFT", "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xMFQwNTozMjo0M1rOKhNIIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wOS0xMFQwODozNzozM1rOKhS_XA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTkwNjcyMg==", "bodyText": "I wonder if this adds risk that a slow shutdown of networking results in a delayed termination of the host in production?", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705906722", "createdAt": "2021-09-10T05:32:43Z", "author": {"login": "henningandersen"}, "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "diffHunk": "@@ -559,42 +551,31 @@ protected final void doClose() {\n \n     @Override\n     protected final void doStop() {\n-        final CountDownLatch latch = new CountDownLatch(1);\n-        // make sure we run it on another thread than a possible IO handler thread\n+        assert Transports.assertNotTransportThread(\"Must not block transport thread that might be needed for closing channels below\");\n         assert threadPool.generic().isShutdown() == false : \"Must stop transport before terminating underlying threadpool\";\n-        threadPool.generic().execute(() -> {\n-            closeLock.writeLock().lock();\n-            try {\n-                keepAlive.close();\n+        closeLock.writeLock().lock();\n+        try {\n+            keepAlive.close();\n \n-                // first stop to accept any incoming connections so nobody can connect to this transport\n-                for (Map.Entry<String, List<TcpServerChannel>> entry : serverChannels.entrySet()) {\n-                    String profile = entry.getKey();\n-                    List<TcpServerChannel> channels = entry.getValue();\n-                    ActionListener<Void> closeFailLogger = ActionListener.wrap(c -> {\n+            // first stop to accept any incoming connections so nobody can connect to this transport\n+            for (Map.Entry<String, List<TcpServerChannel>> entry : serverChannels.entrySet()) {\n+                String profile = entry.getKey();\n+                List<TcpServerChannel> channels = entry.getValue();\n+                ActionListener<Void> closeFailLogger = ActionListener.wrap(c -> {\n                         },\n                         e -> logger.warn(() -> new ParameterizedMessage(\"Error closing serverChannel for profile [{}]\", profile), e));\n-                    channels.forEach(c -> c.addCloseListener(closeFailLogger));\n-                    CloseableChannel.closeChannels(channels, true);\n-                }\n-                serverChannels.clear();\n-\n-                // close all of the incoming channels. The closeChannels method takes a list so we must convert the set.\n-                CloseableChannel.closeChannels(new ArrayList<>(acceptedChannels), true);\n-                acceptedChannels.clear();\n-\n-                stopInternal();\n-            } finally {\n-                closeLock.writeLock().unlock();\n-                latch.countDown();\n+                channels.forEach(c -> c.addCloseListener(closeFailLogger));\n+                CloseableChannel.closeChannels(channels, true);\n             }\n-        });\n+            serverChannels.clear();\n \n-        try {\n-            latch.await(30, TimeUnit.SECONDS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66b9e6f72317d4209c76bc7cff466368ace11058"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTk3MzAzMA==", "bodyText": "I'm curious what could be slow in this process. Closing a channel should complete fairly promptly, we should just be dispatching a close() call to the event loop and waiting for that to run. I didn't dig into Netty to verify and in particular to check that the close() is passed down to the channel even if the channel is otherwise blocked. We're only closing inbound channels and server channels here so there's no response handlers to notify. And then stopping the event loop itself has its own 5-second timeout:\n\n  \n    \n      elasticsearch/modules/transport-netty4/src/main/java/org/elasticsearch/transport/SharedGroupFactory.java\n    \n    \n        Lines 96 to 100\n      in\n      bfcc93a\n    \n    \n    \n    \n\n        \n          \n           Future<?> shutdownFuture = eventLoopGroup.shutdownGracefully(0, 5, TimeUnit.SECONDS); \n        \n\n        \n          \n           shutdownFuture.awaitUninterruptibly(); \n        \n\n        \n          \n           if (shutdownFuture.isSuccess() == false) { \n        \n\n        \n          \n               logger.warn(\"Error closing netty event loop group\", shutdownFuture.cause()); \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nI also pondered whether we might block on the closeLock for any length of time and I think the answer to that is also no but I found other potential issues, not really relevant to this change tho so I spun them out into #77539.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r705973030", "createdAt": "2021-09-10T07:52:00Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "diffHunk": "@@ -559,42 +551,31 @@ protected final void doClose() {\n \n     @Override\n     protected final void doStop() {\n-        final CountDownLatch latch = new CountDownLatch(1);\n-        // make sure we run it on another thread than a possible IO handler thread\n+        assert Transports.assertNotTransportThread(\"Must not block transport thread that might be needed for closing channels below\");\n         assert threadPool.generic().isShutdown() == false : \"Must stop transport before terminating underlying threadpool\";\n-        threadPool.generic().execute(() -> {\n-            closeLock.writeLock().lock();\n-            try {\n-                keepAlive.close();\n+        closeLock.writeLock().lock();\n+        try {\n+            keepAlive.close();\n \n-                // first stop to accept any incoming connections so nobody can connect to this transport\n-                for (Map.Entry<String, List<TcpServerChannel>> entry : serverChannels.entrySet()) {\n-                    String profile = entry.getKey();\n-                    List<TcpServerChannel> channels = entry.getValue();\n-                    ActionListener<Void> closeFailLogger = ActionListener.wrap(c -> {\n+            // first stop to accept any incoming connections so nobody can connect to this transport\n+            for (Map.Entry<String, List<TcpServerChannel>> entry : serverChannels.entrySet()) {\n+                String profile = entry.getKey();\n+                List<TcpServerChannel> channels = entry.getValue();\n+                ActionListener<Void> closeFailLogger = ActionListener.wrap(c -> {\n                         },\n                         e -> logger.warn(() -> new ParameterizedMessage(\"Error closing serverChannel for profile [{}]\", profile), e));\n-                    channels.forEach(c -> c.addCloseListener(closeFailLogger));\n-                    CloseableChannel.closeChannels(channels, true);\n-                }\n-                serverChannels.clear();\n-\n-                // close all of the incoming channels. The closeChannels method takes a list so we must convert the set.\n-                CloseableChannel.closeChannels(new ArrayList<>(acceptedChannels), true);\n-                acceptedChannels.clear();\n-\n-                stopInternal();\n-            } finally {\n-                closeLock.writeLock().unlock();\n-                latch.countDown();\n+                channels.forEach(c -> c.addCloseListener(closeFailLogger));\n+                CloseableChannel.closeChannels(channels, true);\n             }\n-        });\n+            serverChannels.clear();\n \n-        try {\n-            latch.await(30, TimeUnit.SECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTkwNjcyMg=="}, "originalCommit": {"oid": "66b9e6f72317d4209c76bc7cff466368ace11058"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNjAwMjc4MA==", "bodyText": "I think the 30s here was a fairly random choice a long time ago.  I think the closing here should be almost instant as David points out there's an internal 5s timeout there and nothing that could really run for an extended period of time.", "url": "https://github.com/elastic/elasticsearch/pull/66834#discussion_r706002780", "createdAt": "2021-09-10T08:37:33Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/transport/TcpTransport.java", "diffHunk": "@@ -559,42 +551,31 @@ protected final void doClose() {\n \n     @Override\n     protected final void doStop() {\n-        final CountDownLatch latch = new CountDownLatch(1);\n-        // make sure we run it on another thread than a possible IO handler thread\n+        assert Transports.assertNotTransportThread(\"Must not block transport thread that might be needed for closing channels below\");\n         assert threadPool.generic().isShutdown() == false : \"Must stop transport before terminating underlying threadpool\";\n-        threadPool.generic().execute(() -> {\n-            closeLock.writeLock().lock();\n-            try {\n-                keepAlive.close();\n+        closeLock.writeLock().lock();\n+        try {\n+            keepAlive.close();\n \n-                // first stop to accept any incoming connections so nobody can connect to this transport\n-                for (Map.Entry<String, List<TcpServerChannel>> entry : serverChannels.entrySet()) {\n-                    String profile = entry.getKey();\n-                    List<TcpServerChannel> channels = entry.getValue();\n-                    ActionListener<Void> closeFailLogger = ActionListener.wrap(c -> {\n+            // first stop to accept any incoming connections so nobody can connect to this transport\n+            for (Map.Entry<String, List<TcpServerChannel>> entry : serverChannels.entrySet()) {\n+                String profile = entry.getKey();\n+                List<TcpServerChannel> channels = entry.getValue();\n+                ActionListener<Void> closeFailLogger = ActionListener.wrap(c -> {\n                         },\n                         e -> logger.warn(() -> new ParameterizedMessage(\"Error closing serverChannel for profile [{}]\", profile), e));\n-                    channels.forEach(c -> c.addCloseListener(closeFailLogger));\n-                    CloseableChannel.closeChannels(channels, true);\n-                }\n-                serverChannels.clear();\n-\n-                // close all of the incoming channels. The closeChannels method takes a list so we must convert the set.\n-                CloseableChannel.closeChannels(new ArrayList<>(acceptedChannels), true);\n-                acceptedChannels.clear();\n-\n-                stopInternal();\n-            } finally {\n-                closeLock.writeLock().unlock();\n-                latch.countDown();\n+                channels.forEach(c -> c.addCloseListener(closeFailLogger));\n+                CloseableChannel.closeChannels(channels, true);\n             }\n-        });\n+            serverChannels.clear();\n \n-        try {\n-            latch.await(30, TimeUnit.SECONDS);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDcwNTkwNjcyMg=="}, "originalCommit": {"oid": "66b9e6f72317d4209c76bc7cff466368ace11058"}, "originalPosition": 109}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4375, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}