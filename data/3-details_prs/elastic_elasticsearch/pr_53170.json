{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0MzEwNzE2", "number": 53170, "title": "[DOCS] Reformat `word_delimiter_graph` token filter", "bodyText": "Makes the following changes to the word_delimiter_graph token filter docs:\n\nUpdates the Lucene experimental admonition.\nUpdates description\nAdds detailed analyze snippet\nAdds custom analyzer and custom filter snippets\nReorganizes and updates parameter documentation\nExpands and updates section re: differences between word_delimiter and word_delimiter_graph\n\nAlso updates the trim filter docs to note that the trim filter does not change token offsets. Moved to #53220\nPreview\nhttp://elasticsearch_53170.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/analysis-word-delimiter-graph-tokenfilter.html", "createdAt": "2020-03-05T14:33:52Z", "url": "https://github.com/elastic/elasticsearch/pull/53170", "merged": true, "mergeCommit": {"oid": "1c8ab01ee6ea8ae5c086420a2a53fecc2e21ab38"}, "closed": true, "closedAt": "2020-03-09T10:27:42Z", "author": {"login": "jrodewig"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcKsc_mgH2gAyMzg0MzEwNzE2OmY1OWZiMWZmNWQwN2JlNDdiNDViMTA0NDQyOThlNDUxNDBmZmRjNDI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcL7K_PgFqTM3MTA0NTk5MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "f59fb1ff5d07be47b45b10444298e45140ffdc42", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/f59fb1ff5d07be47b45b10444298e45140ffdc42", "committedDate": "2020-03-05T14:32:49Z", "message": "[DOCS] Reformat word delimiter graph token filter\n\nMakes the following changes to the `word_delimiter_graph` token filter\ndocs:\n\n* Updates the Lucene experimental admonition.\n* Updates description\n* Adds analyze snippet\n* Adds custom analyzer and custom filter snippets\n* Reorganizes and updates parameter list\n* Expands and updates section re: differences between `word_delimiter`\n  and `word_delimiter_graph`"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d48caa6fa0732510b8ba1ccb4b1ebfea812d5464", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/d48caa6fa0732510b8ba1ccb4b1ebfea812d5464", "committedDate": "2020-03-05T20:47:14Z", "message": "Tweak wording for boolean parms"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aafad4f0a91272a2cefc74e1361dcc100d82b82a", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/aafad4f0a91272a2cefc74e1361dcc100d82b82a", "committedDate": "2020-03-06T10:40:16Z", "message": "Remove experimental flag per #53217"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a15173a7402a1399cebe6fb64e884618bc14f493", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/a15173a7402a1399cebe6fb64e884618bc14f493", "committedDate": "2020-03-06T10:40:42Z", "message": "Add missing period"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae34f1e1843ed71a1323cb6dd054a13f16c506e8", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/ae34f1e1843ed71a1323cb6dd054a13f16c506e8", "committedDate": "2020-03-06T11:00:01Z", "message": "Add analyze API link"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcwMjM5NjM2", "url": "https://github.com/elastic/elasticsearch/pull/53170#pullrequestreview-370239636", "createdAt": "2020-03-06T10:56:59Z", "commit": {"oid": "a15173a7402a1399cebe6fb64e884618bc14f493"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxMDo1NzowMFrOFy03Yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxMToxOTozM1rOFy1ccw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODgzOTI2Ng==", "bodyText": "Should this be part of a different PR?", "url": "https://github.com/elastic/elasticsearch/pull/53170#discussion_r388839266", "createdAt": "2020-03-06T10:57:00Z", "author": {"login": "romseygeek"}, "path": "docs/reference/analysis/tokenfilters/trim-tokenfilter.asciidoc", "diffHunk": "@@ -4,7 +4,9 @@\n <titleabbrev>Trim</titleabbrev>\n ++++\n \n-Removes leading and trailing whitespace from each token in a stream.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a15173a7402a1399cebe6fb64e884618bc14f493"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg0ODc1NQ==", "bodyText": "I don't think this is true?  catenate_X parameters break phrase searching in general - for example, searching for the exact phrase the wifi is enabled won't match against the token stream above because fi introduces an extra position, so is is indexes as if it were two positions away from wifi.  This is a hard problem in lucene - we don't want to start indexing position lengths because that will make phrase queries much slower.\nThe advantage of the _graph variant is that it produces graphs which can be used at query time to generate several queries, so a query for the wi-fi is enabled will produce two phrase queries, the wi fi is enabled and the wifi is enabled.  All good if you've indexed the phrase the wi-fi is enabled, as the first query will match.  However, searching for the wifi is enabled won't match - it's all lowercase in the query, so the filter doesn't recognise the need to break it up, and in the index wifi is two positions away from is.\nBreaking up words with hyphens in is tricky because of the possibility that people will try and search for the word without the hyphen; I think these are probably better dealt with via synonyms.  A better usecase for removing punctuation is for things like part numbers, where you only really want phrase searching within a multi-part token, so you use WDGF with a keyword tokenizer.", "url": "https://github.com/elastic/elasticsearch/pull/53170#discussion_r388848755", "createdAt": "2020-03-06T11:19:33Z", "author": {"login": "romseygeek"}, "path": "docs/reference/analysis/tokenfilters/word-delimiter-graph-tokenfilter.asciidoc", "diffHunk": "@@ -4,105 +4,376 @@\n <titleabbrev>Word delimiter graph</titleabbrev>\n ++++\n \n-experimental[This functionality is marked as experimental in Lucene]\n+Splits tokens at non-alphanumeric characters. The `word_delimiter_graph` filter\n+also performs optional token normalization based on a set of rules. By default,\n+the filter uses the following rules:\n \n-Named `word_delimiter_graph`, it splits words into subwords and performs\n-optional transformations on subword groups. Words are split into\n-subwords with the following rules:\n+* Split tokens at non-alphanumeric characters.\n+  The filter uses these characters as delimiters.\n+  For example: `Wi-Fi` -> `Wi`, `Fi`\n+* Remove leading or trailing delimiters from each token.\n+  For example: `hello---there, 'dude'` -> `hello`, `there`, `dude`\n+* Split tokens at letter case transitions.\n+  For example: `PowerShot` -> `Power`, `Shot`\n+* Split tokens at letter-number transitions.\n+  For example: `SD500` -> `SD`, `500`\n+* Remove the English possessive (`'s`) from the end of each token.\n+  For example: `Neil's` -> `Neil`\n \n-* split on intra-word delimiters (by default, all non alpha-numeric\n-characters).\n-* \"Wi-Fi\" -> \"Wi\", \"Fi\"\n-* split on case transitions: \"PowerShot\" -> \"Power\", \"Shot\"\n-* split on letter-number transitions: \"SD500\" -> \"SD\", \"500\"\n-* leading and trailing intra-word delimiters on each subword are\n-ignored: \"//hello---there, 'dude'\" -> \"hello\", \"there\", \"dude\"\n-* trailing \"'s\" are removed for each subword: \"O'Neil's\" -> \"O\", \"Neil\"\n+The `word_delimiter_graph` filter uses Lucene's\n+{lucene-analysis-docs}/miscellaneous/WordDelimiterGraphFilter.html[WordDelimiterGraphFilter].\n \n-Unlike the `word_delimiter`, this token filter correctly handles positions for\n-multi terms expansion at search-time when any of the following options\n-are set to true:\n+[[analysis-word-delimiter-graph-tokenfilter-analyze-ex]]\n+==== Example\n \n- * `preserve_original`\n- * `catenate_numbers`\n- * `catenate_words`\n- * `catenate_all`\n+The following analyze API request uses the `word_delimiter_graph` filter to\n+split `Neil's Wi-Fi-enabled PowerShot SD500` into normalized tokens using the\n+filter's default rules:\n \n-Parameters include:\n+[source,console]\n+----\n+GET /_analyze\n+{\n+  \"tokenizer\": \"whitespace\",\n+  \"filter\": [ \"word_delimiter_graph\" ],\n+  \"text\": \"Neil's Wi-Fi-enabled PowerShot SD500\"\n+}\n+----\n \n-`generate_word_parts`::\n-    If `true` causes parts of words to be\n-    generated: \"PowerShot\" -> \"Power\" \"Shot\". Defaults to `true`.\n+The filter produces the following tokens:\n \n-`generate_number_parts`::\n-    If `true` causes number subwords to be\n-    generated: \"500-42\" -> \"500\" \"42\". Defaults to `true`.\n+[source,txt]\n+----\n+[ Neil, Wi, Fi, enabled, Power, Shot, SD, 500 ]\n+----\n \n-`catenate_words`::\n-    If `true` causes maximum runs of word parts to be\n-    catenated: \"wi-fi\" -> \"wifi\". Defaults to `false`.\n+////\n+[source,console-result]\n+----\n+{\n+  \"tokens\" : [\n+    {\n+      \"token\" : \"Neil\",\n+      \"start_offset\" : 0,\n+      \"end_offset\" : 4,\n+      \"type\" : \"word\",\n+      \"position\" : 0\n+    },\n+    {\n+      \"token\" : \"Wi\",\n+      \"start_offset\" : 7,\n+      \"end_offset\" : 9,\n+      \"type\" : \"word\",\n+      \"position\" : 1\n+    },\n+    {\n+      \"token\" : \"Fi\",\n+      \"start_offset\" : 10,\n+      \"end_offset\" : 12,\n+      \"type\" : \"word\",\n+      \"position\" : 2\n+    },\n+    {\n+      \"token\" : \"enabled\",\n+      \"start_offset\" : 13,\n+      \"end_offset\" : 20,\n+      \"type\" : \"word\",\n+      \"position\" : 3\n+    },\n+    {\n+      \"token\" : \"Power\",\n+      \"start_offset\" : 21,\n+      \"end_offset\" : 26,\n+      \"type\" : \"word\",\n+      \"position\" : 4\n+    },\n+    {\n+      \"token\" : \"Shot\",\n+      \"start_offset\" : 26,\n+      \"end_offset\" : 30,\n+      \"type\" : \"word\",\n+      \"position\" : 5\n+    },\n+    {\n+      \"token\" : \"SD\",\n+      \"start_offset\" : 31,\n+      \"end_offset\" : 33,\n+      \"type\" : \"word\",\n+      \"position\" : 6\n+    },\n+    {\n+      \"token\" : \"500\",\n+      \"start_offset\" : 33,\n+      \"end_offset\" : 36,\n+      \"type\" : \"word\",\n+      \"position\" : 7\n+    }\n+  ]\n+}\n+----\n+////\n \n-`catenate_numbers`::\n-    If `true` causes maximum runs of number parts to\n-    be catenated: \"500-42\" -> \"50042\". Defaults to `false`.\n+[analysis-word-delimiter-tokenfilter-analyzer-ex]]\n+==== Add to an analyzer\n+\n+The following <<indices-create-index,create index API>> request uses the\n+`word_delimiter_graph` filter to configure a new\n+<<analysis-custom-analyzer,custom analyzer>>.\n+\n+[source,console]\n+----\n+PUT /my_index\n+{\n+  \"settings\": {\n+    \"analysis\": {\n+      \"analyzer\": {\n+        \"my_analyzer\": {\n+          \"tokenizer\": \"whitespace\",\n+          \"filter\": [ \"word_delimiter_graph\" ]\n+        }\n+      }\n+    }\n+  }\n+}\n+----\n+\n+[WARNING]\n+====\n+Avoid using the `word_delimiter_graph` filter with tokenizers that remove\n+punctuation, such as the <<analysis-standard-tokenizer,`standard`>> tokenizer.\n+This could prevent the `word_delimiter_graph` filter from splitting tokens\n+correctly. It can also interfere with the filter's configurable parameters, such\n+as <<word-delimiter-graph-tokenfilter-catenate-all,`catenate_all`>> or\n+<<word-delimiter-graph-tokenfilter-preserve-original,`preserve_original`>>. We\n+recommend using the <<analysis-whitespace-tokenizer,`whitespace`>> tokenizer\n+instead.\n+====\n \n+[[word-delimiter-graph-tokenfilter-configure-parms]]\n+==== Configurable parameters\n+\n+[[word-delimiter-graph-tokenfilter-adjust-offsets]]\n+`adjust_offsets`::\n++\n+--\n+(Optional, boolean)\n+If `true`, the filter adjusts the offsets of split or catenated tokens to better\n+reflect their actual position in the token stream. Defaults to `true`.\n+\n+[WARNING]\n+====\n+Set `adjust_offsets` to `false` if your analyzer uses filters, such as the\n+<<analysis-trim-tokenfilter,`trim`>> filter, that change the length of tokens\n+without changing their offsets. Otherwise, the `word_delimiter_graph` filter\n+could produce tokens with illegal offsets.\n+====\n+--\n+\n+[[word-delimiter-graph-tokenfilter-catenate-all]]\n `catenate_all`::\n-    If `true` causes all subword parts to be catenated:\n-    \"wi-fi-4000\" -> \"wifi4000\". Defaults to `false`.\n+(Optional, boolean)\n+If `true`, the filter produces catenated tokens for chains of alphanumeric\n+characters separated by non-alphabetic delimiters. For example:\n+`wi-fi-232-enabled` -> [**`wifi232enabled`**, `wi`, `fi`, `232`, `enabled` ].\n+Defaults to `false`.\n \n-`split_on_case_change`::\n-    If `true` causes \"PowerShot\" to be two tokens;\n-    (\"Power-Shot\" remains two parts regards). Defaults to `true`.\n+[[word-delimiter-graph-tokenfilter-catenate-numbers]]\n+`catenate_numbers`::\n+(Optional, boolean)\n+If `true`, the filter produces catenated tokens for chains of numeric characters\n+separated by non-alphabetic delimiters. For example: `01-02-03` ->\n+[**`010203`**, `01`, `02`, `03` ]. Defaults to `false`.\n \n+[[word-delimiter-graph-tokenfilter-catenate-words]]\n+`catenate_words`::\n+(Optional, boolean)\n+If `true`, the filter produces catenated tokens for chains of alphabetical\n+characters separated by non-alphabetic delimiters. For example: `wi-fi-enabled`\n+-> [**`wifienabled`**, `wi`, `fi`, `enabled`]. Defaults to `false`.\n+\n+`generate_number_parts`::\n+(Optional, boolean)\n+If `true`, the filter includes tokens consisting of only numeric characters in\n+the output. If `false`, the filter excludes these tokens from the output.\n+Defaults to `true`.\n+\n+`generate_word_parts`::\n+(Optional, boolean)\n+If `true`, the filter includes tokens consisting of only alphabetical characters\n+in the output. If `false`, the filter excludes these tokens from the output.\n+Defaults to `true`.\n+\n+[[word-delimiter-graph-tokenfilter-preserve-original]]\n `preserve_original`::\n-    If `true` includes original words in subwords:\n-    \"500-42\" -> \"500-42\" \"500\" \"42\". Defaults to `false`.\n+(Optional, boolean)\n+If `true`, the filter includes the original version of any split tokens in the\n+output. This original version includes non-alphanumeric delimiters. For example:\n+`wi-fi-232` -> [**`wi-fi-232`**, `wi`, `fi`, `232` ]. Defaults to `false`.\n+\n+`protected_words`::\n+(Optional, array of strings)\n+Array of tokens the filter won't split.\n+\n+`protected_words_path`::\n++\n+--\n+(Optional, string)\n+Path to a file that contains a list of tokens the filter won't split.\n+\n+This path must be absolute or relative to the `config` location, and the file\n+must be UTF-8 encoded. Each token in the file must be separated by a line\n+break.\n+--\n+\n+`split_on_case_change`::\n+(Optional, boolean)\n+If `true`, the filter splits tokens at letter case transitions. For example:\n+`camelCase` -> [ `camel`, `Case`]. Defaults to `true`.\n \n `split_on_numerics`::\n-    If `true` causes \"j2se\" to be three tokens; \"j\"\n-    \"2\" \"se\". Defaults to `true`.\n+(Optional, boolean)\n+If `true`, the filter splits tokens at letter-number transitions. For example:\n+`j2se` -> [ `j`, `2`, `se` ]. Defaults to `true`.\n \n `stem_english_possessive`::\n-    If `true` causes trailing \"'s\" to be\n-    removed for each subword: \"O'Neil's\" -> \"O\", \"Neil\". Defaults to `true`.\n+(Optional, boolean)\n+If `true`, the filter removes the English possessive (`'s`) from the end of each\n+token. For example: `O'Neil's` -> `[ `O`, `Neil` ]. Defaults to `true`.\n \n-Advance settings include:\n+`type_table`::\n++\n+--\n+(Optional, array of strings)\n+Array of custom type mappings for characters. This allows you to map\n+non-alphanumeric characters as numeric or alphanumeric to avoid splitting on\n+those characters.\n \n-`protected_words`::\n-    A list of protected words from being delimiter.\n-    Either an array, or also can set `protected_words_path` which resolved\n-    to a file configured with protected words (one on each line).\n-    Automatically resolves to `config/` based location if exists.\n+For example, the following array maps the plus (`+`) and hyphen (`-`) characters\n+as alphanumeric, which means they won't be treated as delimiters:\n \n-`adjust_offsets`::\n-    By default, the filter tries to output subtokens with adjusted offsets\n-    to reflect their actual position in the token stream.  However, when\n-    used in combination with other filters that alter the length or starting\n-    position of tokens without changing their offsets\n-    (e.g. <<analysis-trim-tokenfilter,`trim`>>) this can cause tokens with\n-    illegal offsets to be emitted.  Setting `adjust_offsets` to false will\n-    stop `word_delimiter_graph` from adjusting these internal offsets.\n+`[\"+ => ALPHA\", \"- => ALPHA\"]`\n \n-`type_table`::\n-    A custom type mapping table, for example (when configured\n-    using `type_table_path`):\n-\n-[source,type_table]\n---------------------------------------------------\n-    # Map the $, %, '.', and ',' characters to DIGIT\n-    # This might be useful for financial data.\n-    $ => DIGIT\n-    % => DIGIT\n-    . => DIGIT\n-    \\\\u002C => DIGIT\n-\n-    # in some cases you might not want to split on ZWJ\n-    # this also tests the case where we need a bigger byte[]\n-    # see http://en.wikipedia.org/wiki/Zero-width_joiner\n-    \\\\u200D => ALPHANUM\n---------------------------------------------------\n-\n-NOTE: Using a tokenizer like the `standard` tokenizer may interfere with\n-the `catenate_*` and `preserve_original` parameters, as the original\n-string may already have lost punctuation during tokenization.  Instead,\n-you may want to use the `whitespace` tokenizer.\n+Supported types include:\n+\n+* `ALPHA` (Alphabetical)\n+* `ALPHANUM` (Alphanumeric)\n+* `DIGIT` (Numeric)\n+* `LOWER` (Lowercase alphabetical)\n+* `SUBWORD_DELIM` (Non-alphanumeric delimiter)\n+* `UPPER` (Uppercase alphabetical)\n+--\n+\n+`type_table_path`::\n++\n+--\n+(Optional, string)\n+Path to a file that contains custom type mappings for characters. This allows\n+you to map non-alphanumeric characters as numeric or alphanumeric to avoid\n+splitting on those characters.\n+\n+For example, the contents of this file may contain the following:\n+\n+[source,txt]\n+----\n+# Map the $, %, '.', and ',' characters to DIGIT\n+# This might be useful for financial data.\n+$ => DIGIT\n+% => DIGIT\n+. => DIGIT\n+\\\\u002C => DIGIT\n+\n+# in some cases you might not want to split on ZWJ\n+# this also tests the case where we need a bigger byte[]\n+# see http://en.wikipedia.org/wiki/Zero-width_joiner\n+\\\\u200D => ALPHANUM\n+----\n+\n+Supported types include:\n+\n+* `ALPHA` (Alphabetical)\n+* `ALPHANUM` (Alphanumeric)\n+* `DIGIT` (Numeric)\n+* `LOWER` (Lowercase alphabetical)\n+* `SUBWORD_DELIM` (Non-alphanumeric delimiter)\n+* `UPPER` (Uppercase alphabetical)\n+\n+This file path must be absolute or relative to the `config` location, and the\n+file must be UTF-8 encoded. Each mapping in the file must be separated by a line\n+break.\n+--\n+\n+[[analysis-word-delimiter-graph-tokenfilter-customize]]\n+==== Customize\n+\n+To customize the `word_delimiter_graph` filter, duplicate it to create the basis\n+for a new custom token filter. You can modify the filter using its configurable\n+parameters.\n+\n+For example, the following request creates a `word_delimiter_graph`\n+filter that uses the following rules:\n+\n+* Split tokens at non-alphanumeric characters, _except_ the hyphen (`-`)\n+  character.\n+* Remove leading or trailing delimiters from each token.\n+* Do _not_ split tokens at letter case transitions.\n+* Do _not_ split tokens at letter-number transitions.\n+* Remove the English possessive (`'s`) from the end of each token.\n+\n+[source,console]\n+----\n+PUT /my_index\n+{\n+  \"settings\": {\n+    \"analysis\": {\n+      \"analyzer\": {\n+        \"default\": {\n+          \"tokenizer\": \"whitespace\",\n+          \"filter\": [ \"my_custom_word_delimiter_graph_filter\" ]\n+        }\n+      },\n+      \"filter\": {\n+        \"my_custom_word_delimiter_graph_filter\": {\n+          \"type\": \"word_delimiter_graph\",\n+          \"type_table\": [ \"- => ALPHA\" ],\n+          \"split_on_case_change\": false,\n+          \"split_on_numerics\": false,\n+          \"stem_english_possessive\": true\n+        }\n+      }\n+    }\n+  }\n+}\n+----\n+\n+[[analysis-word-delimiter-graph-differences]]\n+==== Differences between `word_delimiter` and `word_delimiter_graph`\n+\n+Both the <<analysis-word-delimiter-tokenfilter,`word_delimiter`>> and\n+`word_delimiter_graph` token filters can produce catenated tokens when any of\n+the following parameters are `true`:\n+\n+ * <<word-delimiter-graph-tokenfilter-catenate-all,`catenate_all`>>\n+ * <<word-delimiter-graph-tokenfilter-catenate-numbers,`catenate_numbers`>>\n+ * <<word-delimiter-graph-tokenfilter-catenate-words,`catenate_words`>>\n+\n+When adding these new tokens to a stream, the `word_delimiter` filter places\n+catenated tokens _after_ the first delimited token. For example, with\n+`catenate_words` set to `true`, the `word_delimiter` filter changes [ `the`,\n+`wi-fi`, `is`, `enabled`]  to [`the`, `wi`, **`wifi`**, `fi`, `is`, `enabled` ].\n+\n+This can cause issues for the <<query-dsl-match-query-phrase,`match_phrase`>>\n+query and other queries that rely on the sequence of token streams for matching.\n+\n+The `word_delimiter_graph` filter places catenated tokens _before_ the first\n+delimited token. For example, with `catenate_words` set to `true`, the\n+`word_delimiter_graph` filter changes [ `the`, `wi-fi`, `is`, `enabled` ] to\n+[ `the`, **`wifi`**, `wi`, `fi`, `is`, `enabled` ].\n+\n+This better preserves the token stream's original sequence and doesn't usually\n+interfere with `match_phrase` or similar queries.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a15173a7402a1399cebe6fb64e884618bc14f493"}, "originalPosition": 450}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "acf8127265c78eab62ee86454058868b7cdc60a8", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/acf8127265c78eab62ee86454058868b7cdc60a8", "committedDate": "2020-03-06T11:44:09Z", "message": "Reset trim filter changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3c1144bdcb72501a93f88ed81f3783e02ce1e70", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/e3c1144bdcb72501a93f88ed81f3783e02ce1e70", "committedDate": "2020-03-09T09:48:09Z", "message": "Address review feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97a13e950a6d765e5d99c5ec586e0fd581fd5c87", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/97a13e950a6d765e5d99c5ec586e0fd581fd5c87", "committedDate": "2020-03-09T09:54:58Z", "message": "Fix formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "932eb892834f7c32d4e963bb4c5629914fbe01bd", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/932eb892834f7c32d4e963bb4c5629914fbe01bd", "committedDate": "2020-03-09T09:57:37Z", "message": "Another formatting fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1885f0218c7a8d9df8895c7ac2239e35cd26d7c9", "author": {"user": {"login": "jrodewig", "name": "James Rodewig"}}, "url": "https://github.com/elastic/elasticsearch/commit/1885f0218c7a8d9df8895c7ac2239e35cd26d7c9", "committedDate": "2020-03-09T09:58:54Z", "message": "Change heading order"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxMDQ1OTkx", "url": "https://github.com/elastic/elasticsearch/pull/53170#pullrequestreview-371045991", "createdAt": "2020-03-09T10:15:39Z", "commit": {"oid": "1885f0218c7a8d9df8895c7ac2239e35cd26d7c9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1800, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}