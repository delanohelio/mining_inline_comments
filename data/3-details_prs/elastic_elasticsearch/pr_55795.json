{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA5Mzk1MTY0", "number": 55795, "title": "Permit searches to be concurrent to prewarming", "bodyText": "Today when prewarming a searchable snapshot we use the SparseFileTracker to\nlock each (part of a) snapshotted blob, blocking any other readers from\naccessing this data until the whole part is available.\nThis commit changes this strategy: instead we optimistically start to download\nthe blob without any locking, and then lock much smaller ranges after each\nindividual read() call. This may mean that some bytes are downloaded twice,\nbut reduces the time that other readers may need to wait before the data they\nneed is available.\nAs a best-effort optimisation we try to request the smallest possible single\nrange of missing bytes in the part by first checking how many of the initial\nand terminal bytes of the part are already present in cache. In particular if\nthe part is already fully cached before prewarming then this check means we\nskip the part entirely.", "createdAt": "2020-04-27T10:10:24Z", "url": "https://github.com/elastic/elasticsearch/pull/55795", "merged": true, "mergeCommit": {"oid": "12d5ad6454bedb6d6be2e40f935b8272442b136d"}, "closed": true, "closedAt": "2020-04-28T09:43:28Z", "author": {"login": "DaveCTurner"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcbsDCtgH2gAyNDA5Mzk1MTY0OmQwYmZkYmJlOWVmOTQzZTAzYjA0YmU4YTRkMTAzOTBjOTYxOTYzMTc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcb_5MwAH2gAyNDA5Mzk1MTY0OmNkNzkzYjNjOWRkNzk3NWEyNzM2ZmI0ZTMyYmZkMTU5NDI3MmZiZGI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "d0bfdbbe9ef943e03b04be8a4d10390c96196317", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/d0bfdbbe9ef943e03b04be8a4d10390c96196317", "committedDate": "2020-04-27T09:41:11Z", "message": "Permit searches to be concurrent to prewarming\n\nToday when prewarming a searchable snapshot we use the `SparseFileTracker` to\nlock each (part of a) snapshotted blob, blocking any other readers from\naccessing this data until the whole part is available.\n\nThis commit changes this strategy: instead we optimistically start to download\nthe blob without any locking, and then lock much smaller ranges after each\nindividual `read()` call. This may mean that some bytes are downloaded twice,\nbut reduces the time that other readers may need to wait before the data they\nneed is available.\n\nAs a best-effort optimisation we try to request the smallest possible single\nrange of missing bytes in the part by first checking how many of the initial\nand terminal bytes of the part are already present in cache. In particular if\nthe part is already fully cached before prewarming then this check means we\nskip the part entirely."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/6fd6f903274cce215808bcc4b14bb2481feb2553", "committedDate": "2020-04-27T10:21:48Z", "message": "Simplify"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwODMxMDM0", "url": "https://github.com/elastic/elasticsearch/pull/55795#pullrequestreview-400831034", "createdAt": "2020-04-27T10:40:06Z", "commit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMDo0MDowNlrOGMcgnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMDo0MDowNlrOGMcgnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTcwMzE5Nw==", "bodyText": "I don't think there's a problem with the very fine-grained locking done here, but if it turns out to be a bottleneck then we can instead lock larger chunks, maybe ~1MB or so, rather than locking for each individual call to read(). That makes it a bit more complicated since we must worry about aligning reads to the chunk size too, but it's still feasible.", "url": "https://github.com/elastic/elasticsearch/pull/55795#discussion_r415703197", "createdAt": "2020-04-27T10:40:06Z", "author": {"login": "DaveCTurner"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -174,29 +174,130 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n     /**\n      * Prefetches a complete part and writes it in cache. This method is used to prewarm the cache.\n      */\n-    public int prefetchPart(final int part) throws IOException {\n+    public void prefetchPart(final int part) throws IOException {\n         ensureContext(ctx -> ctx == CACHE_WARMING_CONTEXT);\n         if (part >= fileInfo.numberOfParts()) {\n             throw new IllegalArgumentException(\"Unexpected part number [\" + part + \"]\");\n         }\n-        final Tuple<Long, Long> range = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n-        assert assertRangeIsAlignedWithPart(range);\n+        final Tuple<Long, Long> partRange = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n+        assert assertRangeIsAlignedWithPart(partRange);\n+\n         try {\n             final CacheFile cacheFile = getCacheFileSafe();\n             try (ReleasableLock ignored = cacheFile.fileLock()) {\n-                final int bytesRead = cacheFile.fetchRange(range.v1(), range.v2(), (start, end) -> {\n-                    logger.trace(\"range [{}-{}] of file [{}] is now available in cache\", start, end, fileInfo.physicalName());\n-                    return Math.toIntExact(end - start);\n-                }, (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)).get();\n \n-                assert bytesRead == (range.v2() - range.v1());\n-                return bytesRead;\n+                final Tuple<Long, Long> range = cacheFile.getAbsentRangeWithin(partRange.v1(), partRange.v2());\n+                if (range == null) {\n+                    logger.trace(\n+                        \"prefetchPart: part [{}] bytes [{}-{}] is already fully available for cache file [{}]\",\n+                        part,\n+                        partRange.v1(),\n+                        partRange.v2(),\n+                        cacheFileReference\n+                    );\n+                    return;\n+                }\n+\n+                final long rangeStart = range.v1();\n+                final long rangeEnd = range.v2();\n+                final long rangeLength = rangeEnd - rangeStart;\n+\n+                logger.trace(\n+                    \"prefetchPart: prewarming part [{}] bytes [{}-{}] by fetching bytes [{}-{}] for cache file [{}]\",\n+                    part,\n+                    partRange.v1(),\n+                    partRange.v2(),\n+                    rangeStart,\n+                    rangeEnd,\n+                    cacheFileReference\n+                );\n+\n+                final FileChannel fc = cacheFile.getChannel();\n+                assert assertFileChannelOpen(fc);\n+                final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, rangeLength))];\n+\n+                long totalBytesRead = 0L;\n+                long remaining = rangeEnd - rangeStart;\n+                final long startTimeNanos = stats.currentTimeNanos();\n+                try (InputStream input = openInputStream(rangeStart, rangeLength)) {\n+                    while (remaining > 0L) {\n+                        assert totalBytesRead + remaining == rangeLength;\n+                        final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remaining, cacheFileReference);\n+                        final long readStart = rangeStart + totalBytesRead;\n+                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553"}, "originalPosition": 63}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwODg2ODQ2", "url": "https://github.com/elastic/elasticsearch/pull/55795#pullrequestreview-400886846", "createdAt": "2020-04-27T12:06:24Z", "commit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMjowNjoyNVrOGMftSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMjoxMzo1MFrOGMf-3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc1NTU5NQ==", "bodyText": "Maybe remove  the \"now\" word. Technically the range might have been already available before this got called?", "url": "https://github.com/elastic/elasticsearch/pull/55795#discussion_r415755595", "createdAt": "2020-04-27T12:06:25Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -174,29 +174,130 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n     /**\n      * Prefetches a complete part and writes it in cache. This method is used to prewarm the cache.\n      */\n-    public int prefetchPart(final int part) throws IOException {\n+    public void prefetchPart(final int part) throws IOException {\n         ensureContext(ctx -> ctx == CACHE_WARMING_CONTEXT);\n         if (part >= fileInfo.numberOfParts()) {\n             throw new IllegalArgumentException(\"Unexpected part number [\" + part + \"]\");\n         }\n-        final Tuple<Long, Long> range = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n-        assert assertRangeIsAlignedWithPart(range);\n+        final Tuple<Long, Long> partRange = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n+        assert assertRangeIsAlignedWithPart(partRange);\n+\n         try {\n             final CacheFile cacheFile = getCacheFileSafe();\n             try (ReleasableLock ignored = cacheFile.fileLock()) {\n-                final int bytesRead = cacheFile.fetchRange(range.v1(), range.v2(), (start, end) -> {\n-                    logger.trace(\"range [{}-{}] of file [{}] is now available in cache\", start, end, fileInfo.physicalName());\n-                    return Math.toIntExact(end - start);\n-                }, (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)).get();\n \n-                assert bytesRead == (range.v2() - range.v1());\n-                return bytesRead;\n+                final Tuple<Long, Long> range = cacheFile.getAbsentRangeWithin(partRange.v1(), partRange.v2());\n+                if (range == null) {\n+                    logger.trace(\n+                        \"prefetchPart: part [{}] bytes [{}-{}] is already fully available for cache file [{}]\",\n+                        part,\n+                        partRange.v1(),\n+                        partRange.v2(),\n+                        cacheFileReference\n+                    );\n+                    return;\n+                }\n+\n+                final long rangeStart = range.v1();\n+                final long rangeEnd = range.v2();\n+                final long rangeLength = rangeEnd - rangeStart;\n+\n+                logger.trace(\n+                    \"prefetchPart: prewarming part [{}] bytes [{}-{}] by fetching bytes [{}-{}] for cache file [{}]\",\n+                    part,\n+                    partRange.v1(),\n+                    partRange.v2(),\n+                    rangeStart,\n+                    rangeEnd,\n+                    cacheFileReference\n+                );\n+\n+                final FileChannel fc = cacheFile.getChannel();\n+                assert assertFileChannelOpen(fc);\n+                final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, rangeLength))];\n+\n+                long totalBytesRead = 0L;\n+                long remaining = rangeEnd - rangeStart;\n+                final long startTimeNanos = stats.currentTimeNanos();\n+                try (InputStream input = openInputStream(rangeStart, rangeLength)) {\n+                    while (remaining > 0L) {\n+                        assert totalBytesRead + remaining == rangeLength;\n+                        final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remaining, cacheFileReference);\n+                        final long readStart = rangeStart + totalBytesRead;\n+                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {\n+                            logger.trace(\n+                                \"prefetchPart: range [{}-{}] of file [{}] is now available in cache\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc1NjI1NQ==", "bodyText": "Can we capture the returned Future here and assert in the next line that future.isDone?", "url": "https://github.com/elastic/elasticsearch/pull/55795#discussion_r415756255", "createdAt": "2020-04-27T12:07:35Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -174,29 +174,130 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n     /**\n      * Prefetches a complete part and writes it in cache. This method is used to prewarm the cache.\n      */\n-    public int prefetchPart(final int part) throws IOException {\n+    public void prefetchPart(final int part) throws IOException {\n         ensureContext(ctx -> ctx == CACHE_WARMING_CONTEXT);\n         if (part >= fileInfo.numberOfParts()) {\n             throw new IllegalArgumentException(\"Unexpected part number [\" + part + \"]\");\n         }\n-        final Tuple<Long, Long> range = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n-        assert assertRangeIsAlignedWithPart(range);\n+        final Tuple<Long, Long> partRange = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n+        assert assertRangeIsAlignedWithPart(partRange);\n+\n         try {\n             final CacheFile cacheFile = getCacheFileSafe();\n             try (ReleasableLock ignored = cacheFile.fileLock()) {\n-                final int bytesRead = cacheFile.fetchRange(range.v1(), range.v2(), (start, end) -> {\n-                    logger.trace(\"range [{}-{}] of file [{}] is now available in cache\", start, end, fileInfo.physicalName());\n-                    return Math.toIntExact(end - start);\n-                }, (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)).get();\n \n-                assert bytesRead == (range.v2() - range.v1());\n-                return bytesRead;\n+                final Tuple<Long, Long> range = cacheFile.getAbsentRangeWithin(partRange.v1(), partRange.v2());\n+                if (range == null) {\n+                    logger.trace(\n+                        \"prefetchPart: part [{}] bytes [{}-{}] is already fully available for cache file [{}]\",\n+                        part,\n+                        partRange.v1(),\n+                        partRange.v2(),\n+                        cacheFileReference\n+                    );\n+                    return;\n+                }\n+\n+                final long rangeStart = range.v1();\n+                final long rangeEnd = range.v2();\n+                final long rangeLength = rangeEnd - rangeStart;\n+\n+                logger.trace(\n+                    \"prefetchPart: prewarming part [{}] bytes [{}-{}] by fetching bytes [{}-{}] for cache file [{}]\",\n+                    part,\n+                    partRange.v1(),\n+                    partRange.v2(),\n+                    rangeStart,\n+                    rangeEnd,\n+                    cacheFileReference\n+                );\n+\n+                final FileChannel fc = cacheFile.getChannel();\n+                assert assertFileChannelOpen(fc);\n+                final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, rangeLength))];\n+\n+                long totalBytesRead = 0L;\n+                long remaining = rangeEnd - rangeStart;\n+                final long startTimeNanos = stats.currentTimeNanos();\n+                try (InputStream input = openInputStream(rangeStart, rangeLength)) {\n+                    while (remaining > 0L) {\n+                        assert totalBytesRead + remaining == rangeLength;\n+                        final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remaining, cacheFileReference);\n+                        final long readStart = rangeStart + totalBytesRead;\n+                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc2MDA5Mg==", "bodyText": "I'm confused by the stat accounting here as I thought that not all totalBytesRead have been written to filechannel (when other searches have filled the bytes e.g.)?", "url": "https://github.com/elastic/elasticsearch/pull/55795#discussion_r415760092", "createdAt": "2020-04-27T12:13:50Z", "author": {"login": "ywelsch"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -174,29 +174,130 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n     /**\n      * Prefetches a complete part and writes it in cache. This method is used to prewarm the cache.\n      */\n-    public int prefetchPart(final int part) throws IOException {\n+    public void prefetchPart(final int part) throws IOException {\n         ensureContext(ctx -> ctx == CACHE_WARMING_CONTEXT);\n         if (part >= fileInfo.numberOfParts()) {\n             throw new IllegalArgumentException(\"Unexpected part number [\" + part + \"]\");\n         }\n-        final Tuple<Long, Long> range = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n-        assert assertRangeIsAlignedWithPart(range);\n+        final Tuple<Long, Long> partRange = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n+        assert assertRangeIsAlignedWithPart(partRange);\n+\n         try {\n             final CacheFile cacheFile = getCacheFileSafe();\n             try (ReleasableLock ignored = cacheFile.fileLock()) {\n-                final int bytesRead = cacheFile.fetchRange(range.v1(), range.v2(), (start, end) -> {\n-                    logger.trace(\"range [{}-{}] of file [{}] is now available in cache\", start, end, fileInfo.physicalName());\n-                    return Math.toIntExact(end - start);\n-                }, (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)).get();\n \n-                assert bytesRead == (range.v2() - range.v1());\n-                return bytesRead;\n+                final Tuple<Long, Long> range = cacheFile.getAbsentRangeWithin(partRange.v1(), partRange.v2());\n+                if (range == null) {\n+                    logger.trace(\n+                        \"prefetchPart: part [{}] bytes [{}-{}] is already fully available for cache file [{}]\",\n+                        part,\n+                        partRange.v1(),\n+                        partRange.v2(),\n+                        cacheFileReference\n+                    );\n+                    return;\n+                }\n+\n+                final long rangeStart = range.v1();\n+                final long rangeEnd = range.v2();\n+                final long rangeLength = rangeEnd - rangeStart;\n+\n+                logger.trace(\n+                    \"prefetchPart: prewarming part [{}] bytes [{}-{}] by fetching bytes [{}-{}] for cache file [{}]\",\n+                    part,\n+                    partRange.v1(),\n+                    partRange.v2(),\n+                    rangeStart,\n+                    rangeEnd,\n+                    cacheFileReference\n+                );\n+\n+                final FileChannel fc = cacheFile.getChannel();\n+                assert assertFileChannelOpen(fc);\n+                final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, rangeLength))];\n+\n+                long totalBytesRead = 0L;\n+                long remaining = rangeEnd - rangeStart;\n+                final long startTimeNanos = stats.currentTimeNanos();\n+                try (InputStream input = openInputStream(rangeStart, rangeLength)) {\n+                    while (remaining > 0L) {\n+                        assert totalBytesRead + remaining == rangeLength;\n+                        final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remaining, cacheFileReference);\n+                        final long readStart = rangeStart + totalBytesRead;\n+                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {\n+                            logger.trace(\n+                                \"prefetchPart: range [{}-{}] of file [{}] is now available in cache\",\n+                                start,\n+                                end,\n+                                fileInfo.physicalName()\n+                            );\n+                            return Math.toIntExact(end - start);\n+                        }, (start, end) -> {\n+                            final ByteBuffer byteBuffer = ByteBuffer.wrap(\n+                                copyBuffer,\n+                                Math.toIntExact(start - readStart),\n+                                Math.toIntExact(end - start)\n+                            );\n+                            final int writtenBytes = positionalWrite(fc, start, byteBuffer);\n+                            logger.trace(\n+                                \"prefetchPart: writing range [{}-{}] of file [{}], [{}] bytes written\",\n+                                start,\n+                                end,\n+                                fileInfo.physicalName(),\n+                                writtenBytes\n+                            );\n+                        });\n+                        totalBytesRead += bytesRead;\n+                        remaining -= bytesRead;\n+                    }\n+                    final long endTimeNanos = stats.currentTimeNanos();\n+                    stats.addCachedBytesWritten(totalBytesRead, endTimeNanos - startTimeNanos);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553"}, "originalPosition": 90}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e1f062c2c5baf373c9416f56b1970fb407c476a", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/3e1f062c2c5baf373c9416f56b1970fb407c476a", "committedDate": "2020-04-27T14:21:16Z", "message": "Reword log"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxMDM3NzU4", "url": "https://github.com/elastic/elasticsearch/pull/55795#pullrequestreview-401037758", "createdAt": "2020-04-27T14:58:24Z", "commit": {"oid": "3e1f062c2c5baf373c9416f56b1970fb407c476a"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDo1ODoyNFrOGMntug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNTowMzo1MlrOGMoAUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg4Njc3OA==", "bodyText": "I think this will have an impact as it introduces more things to do every COPY_BUFFER_SIZE bytes to write, but I'm not sure it will be a big impact. I'm not sure also this is something we want to optimize.\nIf we have a doubt I can spend some time to benchmark this otherwise we could maybe just add a comment in the code.", "url": "https://github.com/elastic/elasticsearch/pull/55795#discussion_r415886778", "createdAt": "2020-04-27T14:58:24Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -174,29 +174,130 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n     /**\n      * Prefetches a complete part and writes it in cache. This method is used to prewarm the cache.\n      */\n-    public int prefetchPart(final int part) throws IOException {\n+    public void prefetchPart(final int part) throws IOException {\n         ensureContext(ctx -> ctx == CACHE_WARMING_CONTEXT);\n         if (part >= fileInfo.numberOfParts()) {\n             throw new IllegalArgumentException(\"Unexpected part number [\" + part + \"]\");\n         }\n-        final Tuple<Long, Long> range = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n-        assert assertRangeIsAlignedWithPart(range);\n+        final Tuple<Long, Long> partRange = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n+        assert assertRangeIsAlignedWithPart(partRange);\n+\n         try {\n             final CacheFile cacheFile = getCacheFileSafe();\n             try (ReleasableLock ignored = cacheFile.fileLock()) {\n-                final int bytesRead = cacheFile.fetchRange(range.v1(), range.v2(), (start, end) -> {\n-                    logger.trace(\"range [{}-{}] of file [{}] is now available in cache\", start, end, fileInfo.physicalName());\n-                    return Math.toIntExact(end - start);\n-                }, (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)).get();\n \n-                assert bytesRead == (range.v2() - range.v1());\n-                return bytesRead;\n+                final Tuple<Long, Long> range = cacheFile.getAbsentRangeWithin(partRange.v1(), partRange.v2());\n+                if (range == null) {\n+                    logger.trace(\n+                        \"prefetchPart: part [{}] bytes [{}-{}] is already fully available for cache file [{}]\",\n+                        part,\n+                        partRange.v1(),\n+                        partRange.v2(),\n+                        cacheFileReference\n+                    );\n+                    return;\n+                }\n+\n+                final long rangeStart = range.v1();\n+                final long rangeEnd = range.v2();\n+                final long rangeLength = rangeEnd - rangeStart;\n+\n+                logger.trace(\n+                    \"prefetchPart: prewarming part [{}] bytes [{}-{}] by fetching bytes [{}-{}] for cache file [{}]\",\n+                    part,\n+                    partRange.v1(),\n+                    partRange.v2(),\n+                    rangeStart,\n+                    rangeEnd,\n+                    cacheFileReference\n+                );\n+\n+                final FileChannel fc = cacheFile.getChannel();\n+                assert assertFileChannelOpen(fc);\n+                final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, rangeLength))];\n+\n+                long totalBytesRead = 0L;\n+                long remaining = rangeEnd - rangeStart;\n+                final long startTimeNanos = stats.currentTimeNanos();\n+                try (InputStream input = openInputStream(rangeStart, rangeLength)) {\n+                    while (remaining > 0L) {\n+                        assert totalBytesRead + remaining == rangeLength;\n+                        final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remaining, cacheFileReference);\n+                        final long readStart = rangeStart + totalBytesRead;\n+                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTcwMzE5Nw=="}, "originalCommit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg5MTUzNg==", "bodyText": "I think that cached_bytes_written should reflect the number of bytes effectively written in cache. It reflected the number of bytes downloaded (only when the cache range size and the readBlobPreferredLength have the same value) but I think we should compute the download stats differently.", "url": "https://github.com/elastic/elasticsearch/pull/55795#discussion_r415891536", "createdAt": "2020-04-27T15:03:52Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/index/store/cache/CachedBlobContainerIndexInput.java", "diffHunk": "@@ -174,29 +174,130 @@ protected void readInternal(final byte[] buffer, final int offset, final int len\n     /**\n      * Prefetches a complete part and writes it in cache. This method is used to prewarm the cache.\n      */\n-    public int prefetchPart(final int part) throws IOException {\n+    public void prefetchPart(final int part) throws IOException {\n         ensureContext(ctx -> ctx == CACHE_WARMING_CONTEXT);\n         if (part >= fileInfo.numberOfParts()) {\n             throw new IllegalArgumentException(\"Unexpected part number [\" + part + \"]\");\n         }\n-        final Tuple<Long, Long> range = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n-        assert assertRangeIsAlignedWithPart(range);\n+        final Tuple<Long, Long> partRange = computeRange(IntStream.range(0, part).mapToLong(fileInfo::partBytes).sum());\n+        assert assertRangeIsAlignedWithPart(partRange);\n+\n         try {\n             final CacheFile cacheFile = getCacheFileSafe();\n             try (ReleasableLock ignored = cacheFile.fileLock()) {\n-                final int bytesRead = cacheFile.fetchRange(range.v1(), range.v2(), (start, end) -> {\n-                    logger.trace(\"range [{}-{}] of file [{}] is now available in cache\", start, end, fileInfo.physicalName());\n-                    return Math.toIntExact(end - start);\n-                }, (start, end) -> writeCacheFile(cacheFile.getChannel(), start, end)).get();\n \n-                assert bytesRead == (range.v2() - range.v1());\n-                return bytesRead;\n+                final Tuple<Long, Long> range = cacheFile.getAbsentRangeWithin(partRange.v1(), partRange.v2());\n+                if (range == null) {\n+                    logger.trace(\n+                        \"prefetchPart: part [{}] bytes [{}-{}] is already fully available for cache file [{}]\",\n+                        part,\n+                        partRange.v1(),\n+                        partRange.v2(),\n+                        cacheFileReference\n+                    );\n+                    return;\n+                }\n+\n+                final long rangeStart = range.v1();\n+                final long rangeEnd = range.v2();\n+                final long rangeLength = rangeEnd - rangeStart;\n+\n+                logger.trace(\n+                    \"prefetchPart: prewarming part [{}] bytes [{}-{}] by fetching bytes [{}-{}] for cache file [{}]\",\n+                    part,\n+                    partRange.v1(),\n+                    partRange.v2(),\n+                    rangeStart,\n+                    rangeEnd,\n+                    cacheFileReference\n+                );\n+\n+                final FileChannel fc = cacheFile.getChannel();\n+                assert assertFileChannelOpen(fc);\n+                final byte[] copyBuffer = new byte[Math.toIntExact(Math.min(COPY_BUFFER_SIZE, rangeLength))];\n+\n+                long totalBytesRead = 0L;\n+                long remaining = rangeEnd - rangeStart;\n+                final long startTimeNanos = stats.currentTimeNanos();\n+                try (InputStream input = openInputStream(rangeStart, rangeLength)) {\n+                    while (remaining > 0L) {\n+                        assert totalBytesRead + remaining == rangeLength;\n+                        final int bytesRead = readSafe(input, copyBuffer, rangeStart, rangeEnd, remaining, cacheFileReference);\n+                        final long readStart = rangeStart + totalBytesRead;\n+                        cacheFile.fetchRange(readStart, readStart + bytesRead, (start, end) -> {\n+                            logger.trace(\n+                                \"prefetchPart: range [{}-{}] of file [{}] is now available in cache\",\n+                                start,\n+                                end,\n+                                fileInfo.physicalName()\n+                            );\n+                            return Math.toIntExact(end - start);\n+                        }, (start, end) -> {\n+                            final ByteBuffer byteBuffer = ByteBuffer.wrap(\n+                                copyBuffer,\n+                                Math.toIntExact(start - readStart),\n+                                Math.toIntExact(end - start)\n+                            );\n+                            final int writtenBytes = positionalWrite(fc, start, byteBuffer);\n+                            logger.trace(\n+                                \"prefetchPart: writing range [{}-{}] of file [{}], [{}] bytes written\",\n+                                start,\n+                                end,\n+                                fileInfo.physicalName(),\n+                                writtenBytes\n+                            );\n+                        });\n+                        totalBytesRead += bytesRead;\n+                        remaining -= bytesRead;\n+                    }\n+                    final long endTimeNanos = stats.currentTimeNanos();\n+                    stats.addCachedBytesWritten(totalBytesRead, endTimeNanos - startTimeNanos);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc2MDA5Mg=="}, "originalCommit": {"oid": "6fd6f903274cce215808bcc4b14bb2481feb2553"}, "originalPosition": 90}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b1f547916c808e22103377b50f1aac2b817a9ad", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/7b1f547916c808e22103377b50f1aac2b817a9ad", "committedDate": "2020-04-28T08:45:48Z", "message": "Merge branch 'master' into 2020-04-27-allow-searches-concurrent-to-prefetching"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd793b3c9dd7975a2736fb4e32bfd1594272fbdb", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/cd793b3c9dd7975a2736fb4e32bfd1594272fbdb", "committedDate": "2020-04-28T08:48:32Z", "message": "Track bytes written, not bytes read"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 429, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}