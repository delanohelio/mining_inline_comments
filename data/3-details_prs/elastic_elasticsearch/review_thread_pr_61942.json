{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc4Nzc4ODQ2", "number": 61942, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMTo1ODozM1rOEgK3nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo1NjowN1rOEkvWfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMTY3OTY1OnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMTo1ODozM1rOHM41Dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNTo1OToxOFrOHSI5bQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI3NjA0Nw==", "bodyText": "Note for reviewers:\nI'm not sure how much we want to emphasize this now that we've reduced heap usage:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.7/release-highlights.html#_significant_reduction_of_heap_usage_of_segments", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r483276047", "createdAt": "2020-09-03T21:58:33Z", "author": {"login": "jrodewig"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,308 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c6c27dd31309e4f0bf833756aae6d1eca8026a5"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4MjE4OQ==", "bodyText": "IMO this section still applies. We fail less easily under many small shards than we used to, but we still like large shards better.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488782189", "createdAt": "2020-09-15T15:59:18Z", "author": {"login": "jpountz"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,308 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI3NjA0Nw=="}, "originalCommit": {"oid": "7c6c27dd31309e4f0bf833756aae6d1eca8026a5"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMTY4MTQ4OnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMTo1OToxNVrOHM42Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMTo1OToxNVrOHM42Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI3NjMxNQ==", "bodyText": "Note for reviewers:\nNot sure if this guideline still hold now that heap usage is reduced:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.7/release-highlights.html#_significant_reduction_of_heap_usage_of_segments\nAny updated guidance is appreciated!", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r483276315", "createdAt": "2020-09-03T21:59:15Z", "author": {"login": "jrodewig"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,308 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+Data streams let you store time series data across multiple, time-based backing\n+indices. You can use <<index-lifecycle-management,{ilm} ({ilm-init})>>\n+lifecycle policies to automatically manage these backing indices.\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase the rollover threshold in your {ilm-init} policy.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the cat shards API to check the number of\n+shards per node.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c6c27dd31309e4f0bf833756aae6d1eca8026a5"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1ODI1NjU3OnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNTo1MDo1M1rOHSIiMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNzoxOTo0NVrOHSL9oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc3NjI0MA==", "bodyText": "I wonder if we should say stability rather than performance, since this is more commonly the problem.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488776240", "createdAt": "2020-09-15T15:50:53Z", "author": {"login": "jpountz"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzMjQxNg==", "bodyText": "Thanks for the feedback. Updated with ce5f86c.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488832416", "createdAt": "2020-09-15T17:19:45Z", "author": {"login": "jrodewig"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc3NjI0MA=="}, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1ODMwNDA1OnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNjowMTo1NVrOHSJAWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMTozMDoxOVrOHSVO3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4Mzk2MQ==", "bodyText": "I wonder if we want to simplify this messaging with the work we're doing on data tiers. cc @dakrone", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488783961", "createdAt": "2020-09-15T16:01:55Z", "author": {"login": "jpountz"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk4NDI4Nw==", "bodyText": "Simplified this a bit with 1f856d2. Would still like feedback from @dakrone  or @debadair. I'll also add a xref when the data tier docs are live.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488984287", "createdAt": "2020-09-15T21:30:19Z", "author": {"login": "jrodewig"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4Mzk2MQ=="}, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1ODMwODYyOnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNjowMzowMFrOHSJDOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNzoyMDoxNVrOHSL-9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4NDY5OA==", "bodyText": "isn't 'lifecycle' redundant here?", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488784698", "createdAt": "2020-09-15T16:03:00Z", "author": {"login": "jpountz"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzMjc1Nw==", "bodyText": "Removed with ce5f86c.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488832757", "createdAt": "2020-09-15T17:20:15Z", "author": {"login": "jrodewig"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4NDY5OA=="}, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1ODMyNjk5OnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNjowNzowNFrOHSJOXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNzoyMToxNVrOHSMC4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4NzU1MA==", "bodyText": "I wouldn't insist on \"primary\" shards since primaries and replicas perform the same amount of work, except in the update case, which is not typical for hot-warm architectures. I'd say something like \"if a single node contains many shards for an index with a high indexing volume\" instead.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488787550", "createdAt": "2020-09-15T16:07:04Z", "author": {"login": "jpountz"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzMzc2Mw==", "bodyText": "Updated the wording with 6a2277b.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488833763", "createdAt": "2020-09-15T17:21:15Z", "author": {"login": "jrodewig"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc4NzU1MA=="}, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1ODM0MzE0OnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNjoxMDo1M1rOHSJYhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNzoyMTozMVrOHSMD-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc5MDE0OQ==", "bodyText": "I suspect we'd want a much lower value in this example, since we're talking about the maximum number of shards of a single index on a node. E.g. 5 or 10.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488790149", "createdAt": "2020-09-15T16:10:53Z", "author": {"login": "jpountz"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the\n+node is likely to have performance issues.\n+\n+To prevent hotspots, use the\n+<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> index\n+setting to explicitly limit the number of shards on a single node. You can\n+configure `index.routing.allocation.total_shards_per_node` using the\n+<<indices-update-settings,update index settings API>>.\n+\n+[source,console]\n+--------------------------------------------------\n+PUT /my-index-000001/_settings\n+{\n+  \"index\" : {\n+    \"routing.allocation.total_shards_per_node\" : 400", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzNDA0MQ==", "bodyText": "Good catch. Updated with ce5f86c", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488834041", "createdAt": "2020-09-15T17:21:31Z", "author": {"login": "jrodewig"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the\n+node is likely to have performance issues.\n+\n+To prevent hotspots, use the\n+<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> index\n+setting to explicitly limit the number of shards on a single node. You can\n+configure `index.routing.allocation.total_shards_per_node` using the\n+<<indices-update-settings,update index settings API>>.\n+\n+[source,console]\n+--------------------------------------------------\n+PUT /my-index-000001/_settings\n+{\n+  \"index\" : {\n+    \"routing.allocation.total_shards_per_node\" : 400", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc5MDE0OQ=="}, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 187}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1ODU2NzcxOnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNzowODo0OFrOHSLlgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNzoyMTo1OVrOHSMF1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgyNjI0Mw==", "bodyText": "This is a slower, more resource-intensive version of shrinking, so I wonder if we should mention it at all.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488826243", "createdAt": "2020-09-15T17:08:48Z", "author": {"login": "jpountz"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the\n+node is likely to have performance issues.\n+\n+To prevent hotspots, use the\n+<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> index\n+setting to explicitly limit the number of shards on a single node. You can\n+configure `index.routing.allocation.total_shards_per_node` using the\n+<<indices-update-settings,update index settings API>>.\n+\n+[source,console]\n+--------------------------------------------------\n+PUT /my-index-000001/_settings\n+{\n+  \"index\" : {\n+    \"routing.allocation.total_shards_per_node\" : 400\n+  }\n+}\n+--------------------------------------------------\n+// TEST[setup:my_index]\n+\n+\n+[discrete]\n+[[fix-an-oversharded-cluster]]\n+=== Fix an oversharded cluster\n+\n+If your cluster is experiencing performance issues due to oversharded indices,\n+you can use one or more of the following methods to fix them.\n+\n+[discrete]\n+[[reindex-indices-from-shorter-periods-into-longer-periods]]\n+==== Create time-based indices that cover longer periods\n+\n+For time series data, you can create indices that cover longer time intervals.\n+For example, instead of daily indices, you can create indices on a monthly or\n+yearly basis.\n+\n+If you're using {ilm-init}, you can do this by increasing the `max_age`\n+threshold for the <<ilm-rollover,rollover action>>.\n+\n+[discrete]\n+[[delete-empty-indices]]\n+==== Delete empty or unneeded indices\n+\n+If you're using {ilm-init} and roll over indices based on a `max_age` threshold,\n+you can inadvertently create indices with no documents. These empty indices\n+provide no benefit but still consume resources.\n+\n+You can find these empty indices using the <<cat-count,cat count API>>.\n+\n+[source,console]\n+----\n+GET /_cat/count/my-index-000001?v\n+----\n+// TEST[setup:my_index]\n+\n+Once you have a list of empty indices, you can delete them using the\n+<<indices-delete-index,delete index API>>. You can also delete any other\n+unneeded indices.\n+\n+[source,console]\n+----\n+DELETE /my-index-*\n+----\n+// TEST[setup:my_index]\n+\n+[discrete]\n+[[force-merge-during-off-peak-hours]]\n+==== Force merge during off-peak hours\n+\n+If you no longer write to an index, you can use the <<indices-forcemerge,force\n+merge API>> to <<index-modules-merge,merge>> smaller segments into larger ones.\n+This can reduce shard overhead and improve search speeds. However, force merges\n+are resource-intensive. If possible, run the force merge during off-peak hours.\n+\n+[source,console]\n+----\n+POST /my-index-000001/_forcemerge\n+----\n+// TEST[setup:my_index]\n+\n+[discrete]\n+[[shrink-existing-index-to-fewer-shards]]\n+==== Shrink an existing index to fewer shards\n+\n+If you no longer write to an index, you can use the\n+<<indices-shrink-index,shrink index API>> to reduce its shard count.\n+\n+[source,console]\n+----\n+POST /my-index-000001/_shrink/my-shrunken-index-000001\n+----\n+// TEST[s/^/PUT my-index-000001\\n{\"settings\":{\"index.number_of_shards\":2,\"blocks.write\":true}}\\n/]\n+\n+{ilm-init} also has a <<ilm-shrink-action,shrink action>> for indices in the\n+warm phase.\n+\n+[discrete]\n+[[reindex-an-existing-index-to-fewer-shards]]\n+==== Reindex to an index with fewer shards\n+\n+You can use the <<docs-reindex,reindex API>> to reindex data from an oversharded\n+index to an index with fewer shards. After the reindex, delete the oversharded\n+index.\n+\n+[source,console]\n+----\n+POST /_reindex\n+{\n+  \"source\": {\n+    \"index\": \"my-oversharded-index-000001\"\n+  },\n+  \"dest\": {\n+    \"index\": \"my-new-index-000001\"\n+  }\n+}\n+----\n+// TEST[s/^/PUT my-oversharded-index-000001\\n/]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgzNDUxNg==", "bodyText": "+1. Removed with ce5f86c.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r488834516", "createdAt": "2020-09-15T17:21:59Z", "author": {"login": "jrodewig"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,313 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards has degraded performance.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related performance issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring] to track your cluster's\n+performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+threadpool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses file handles, memory, and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards across nodes\n+\n+By default, {es} attempts to spread an index's shards across as many nodes as\n+possible. When a node fails or you add a new node, {es} automatically rebalances\n+shards across the remaining nodes. However, this behavior isn't always wanted.\n+If you use hot-warm architecture, you likely want shards for older indices on\n+nodes with less expensive hardware and vice versa. You can use\n+<<shard-allocation-awareness,shard allocation awareness>> to ensure shards for\n+particular indices only move to specified nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleting individual documents often increases shard overhead. Deleted documents\n+aren't immediately removed from the file system. Instead, {es} marks the\n+document as deleted on each related shard. The marked document will continue to\n+use resources until it's removed during a periodic <<index-modules-merge,segment\n+merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> lifecycle policies to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the cluster's shards across its remaining\n+nodes. Shards larger than 50GB can be harder to move across a network and may\n+tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap space\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap space>>. For example, a node with a 30GB heap should have at\n+most 600 shards. The further below this limit you can keep your nodes, the\n+better. If you find your nodes exceeding more than 20 shards per GB, consider\n+adding another node. You can use the <<cat-shards,cat shards API>> to check the\n+number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If you use a hot-warm architecture, you can use\n+<<shard-allocation-awareness,shard allocation awareness>> to assign shards to\n+nodes based on physical hardware. However, if too many shards are allocated to a\n+specific node, the node can become a hotspot. For example, if a single node\n+contains all the primary shards for an index with a high indexing volume, the\n+node is likely to have performance issues.\n+\n+To prevent hotspots, use the\n+<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> index\n+setting to explicitly limit the number of shards on a single node. You can\n+configure `index.routing.allocation.total_shards_per_node` using the\n+<<indices-update-settings,update index settings API>>.\n+\n+[source,console]\n+--------------------------------------------------\n+PUT /my-index-000001/_settings\n+{\n+  \"index\" : {\n+    \"routing.allocation.total_shards_per_node\" : 400\n+  }\n+}\n+--------------------------------------------------\n+// TEST[setup:my_index]\n+\n+\n+[discrete]\n+[[fix-an-oversharded-cluster]]\n+=== Fix an oversharded cluster\n+\n+If your cluster is experiencing performance issues due to oversharded indices,\n+you can use one or more of the following methods to fix them.\n+\n+[discrete]\n+[[reindex-indices-from-shorter-periods-into-longer-periods]]\n+==== Create time-based indices that cover longer periods\n+\n+For time series data, you can create indices that cover longer time intervals.\n+For example, instead of daily indices, you can create indices on a monthly or\n+yearly basis.\n+\n+If you're using {ilm-init}, you can do this by increasing the `max_age`\n+threshold for the <<ilm-rollover,rollover action>>.\n+\n+[discrete]\n+[[delete-empty-indices]]\n+==== Delete empty or unneeded indices\n+\n+If you're using {ilm-init} and roll over indices based on a `max_age` threshold,\n+you can inadvertently create indices with no documents. These empty indices\n+provide no benefit but still consume resources.\n+\n+You can find these empty indices using the <<cat-count,cat count API>>.\n+\n+[source,console]\n+----\n+GET /_cat/count/my-index-000001?v\n+----\n+// TEST[setup:my_index]\n+\n+Once you have a list of empty indices, you can delete them using the\n+<<indices-delete-index,delete index API>>. You can also delete any other\n+unneeded indices.\n+\n+[source,console]\n+----\n+DELETE /my-index-*\n+----\n+// TEST[setup:my_index]\n+\n+[discrete]\n+[[force-merge-during-off-peak-hours]]\n+==== Force merge during off-peak hours\n+\n+If you no longer write to an index, you can use the <<indices-forcemerge,force\n+merge API>> to <<index-modules-merge,merge>> smaller segments into larger ones.\n+This can reduce shard overhead and improve search speeds. However, force merges\n+are resource-intensive. If possible, run the force merge during off-peak hours.\n+\n+[source,console]\n+----\n+POST /my-index-000001/_forcemerge\n+----\n+// TEST[setup:my_index]\n+\n+[discrete]\n+[[shrink-existing-index-to-fewer-shards]]\n+==== Shrink an existing index to fewer shards\n+\n+If you no longer write to an index, you can use the\n+<<indices-shrink-index,shrink index API>> to reduce its shard count.\n+\n+[source,console]\n+----\n+POST /my-index-000001/_shrink/my-shrunken-index-000001\n+----\n+// TEST[s/^/PUT my-index-000001\\n{\"settings\":{\"index.number_of_shards\":2,\"blocks.write\":true}}\\n/]\n+\n+{ilm-init} also has a <<ilm-shrink-action,shrink action>> for indices in the\n+warm phase.\n+\n+[discrete]\n+[[reindex-an-existing-index-to-fewer-shards]]\n+==== Reindex to an index with fewer shards\n+\n+You can use the <<docs-reindex,reindex API>> to reindex data from an oversharded\n+index to an index with fewer shards. After the reindex, delete the oversharded\n+index.\n+\n+[source,console]\n+----\n+POST /_reindex\n+{\n+  \"source\": {\n+    \"index\": \"my-oversharded-index-000001\"\n+  },\n+  \"dest\": {\n+    \"index\": \"my-new-index-000001\"\n+  }\n+}\n+----\n+// TEST[s/^/PUT my-oversharded-index-000001\\n/]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgyNjI0Mw=="}, "originalCommit": {"oid": "5243b9492e79506bff6b1d9d4b879810ff754ef8"}, "originalPosition": 289}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2OTU3NjQxOnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo0NzoyOVrOHT2rEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo0NzoyOVrOHT2rEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4MDc1Mg==", "bodyText": "\"Things to remember\" seems a bit vague. Maybe \"Sizing considerations\" and then use \"things\" in the intro sentence?", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r490580752", "createdAt": "2020-09-17T21:47:29Z", "author": {"login": "debadair"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,284 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards becomes unstable.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring tools] to track your\n+cluster's stability and performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7bf9495ffd4b663acdceecf05e23dc47861b624"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2OTU4NDU1OnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo1MDoyMVrOHT2wHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo1MDoyMVrOHT2wHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4MjA0NQ==", "bodyText": "These feel more like best practices than guidelines.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r490582045", "createdAt": "2020-09-17T21:50:21Z", "author": {"login": "debadair"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,284 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards becomes unstable.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring tools] to track your\n+cluster's stability and performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+CPU thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+thread pool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses memory and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards within a data tier\n+\n+A cluster's nodes are grouped into data tiers. Within each tier, {es}\n+attempts to spread an index's shards across as many nodes as possible. When you\n+add a new node or a node fails, {es} automatically rebalances the index's shards\n+across the tier's remaining nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7bf9495ffd4b663acdceecf05e23dc47861b624"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2OTU5MDYyOnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo1Mjo0NFrOHT20Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo1Mjo0NFrOHT20Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4MzA0Mg==", "bodyText": "I'd lead with the imperative, \"Use data streams...\"", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r490583042", "createdAt": "2020-09-17T21:52:44Z", "author": {"login": "debadair"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,284 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards becomes unstable.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring tools] to track your\n+cluster's stability and performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+CPU thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+thread pool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses memory and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards within a data tier\n+\n+A cluster's nodes are grouped into data tiers. Within each tier, {es}\n+attempts to spread an index's shards across as many nodes as possible. When you\n+add a new node or a node fails, {es} automatically rebalances the index's shards\n+across the tier's remaining nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleted documents aren't immediately removed from {es}'s file system.\n+Instead, {es} marks the document as deleted on each related shard. The marked\n+document will continue to use resources until it's removed during a periodic\n+<<index-modules-merge,segment merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7bf9495ffd4b663acdceecf05e23dc47861b624"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2OTU5OTk4OnYy", "diffSide": "RIGHT", "path": "docs/reference/how-to/size-your-shards.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo1NjowN1rOHT25sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xN1QyMTo1NjowN1rOHT25sQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU4NDQ5Nw==", "bodyText": "Seems like this should recommend using criteria other than the time period.", "url": "https://github.com/elastic/elasticsearch/pull/61942#discussion_r490584497", "createdAt": "2020-09-17T21:56:07Z", "author": {"login": "debadair"}, "path": "docs/reference/how-to/size-your-shards.asciidoc", "diffHunk": "@@ -0,0 +1,284 @@\n+[[size-your-shards]]\n+== How to size your shards\n+++++\n+<titleabbrev>Size your shards</titleabbrev>\n+++++\n+\n+To protect against hardware failure and increase capacity, {es} stores copies of\n+an index\u2019s data across multiple shards on multiple nodes. The number and size of\n+these shards can have a significant impact on your cluster's health. One common\n+problem is _oversharding_, a situation in which a cluster with a large number of\n+shards becomes unstable.\n+\n+[discrete]\n+[[create-a-sharding-strategy]]\n+=== Create a sharding strategy\n+\n+The best way to prevent oversharding and other shard-related issues\n+is to create a sharding strategy. A sharding strategy helps you determine and\n+maintain the optimal number of shards for your cluster while limiting the size\n+of those shards.\n+\n+Unfortunately, there is no one-size-fits-all sharding strategy. A strategy that\n+works in one environment may not scale in another. A good sharding strategy must\n+account for your infrastructure, use case, and performance expectations.\n+\n+The best way to create a sharding strategy is to benchmark your production data\n+on production hardware using the same queries and indexing loads you'd see in\n+production. For our recommended methodology, watch the\n+https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing[quantitative\n+cluster sizing video]. As you test different shard configurations, use {kib}'s\n+{kibana-ref}/elasticsearch-metrics.html[{es} monitoring tools] to track your\n+cluster's stability and performance.\n+\n+The following sections provide some reminders and guidelines you should consider\n+when designing your sharding strategy. If your cluster has shard-related\n+problems, see <<fix-an-oversharded-cluster>>.\n+\n+[discrete]\n+[[shard-size-reminders]]\n+=== Things to remember\n+\n+Keep the following considerations in mind for your sharding strategy.\n+\n+[discrete]\n+[[single-thread-per-shard]]\n+==== Searches run on a single thread per shard\n+\n+Most searches hit multiple shards. Each shard runs the search on a single\n+CPU thread. While a shard can run multiple concurrent searches, searches across a\n+large number of shards can deplete a node's <<modules-threadpool,search\n+thread pool>>. This can result in low throughput and slow search speeds.\n+\n+[discrete]\n+[[each-shard-has-overhead]]\n+==== Each shard has overhead\n+\n+Every shard uses memory and CPU resources. In most cases, a small\n+set of large shards uses fewer resources than many small shards.\n+\n+Segments play a big role in a shard's resource usage. Most shards contain\n+several segments, which store its index data. {es} keeps segment metadata in\n+<<heap-size,heap memory>> so it can be quickly retrieved for searches. As a\n+shard grows, its segments are <<index-modules-merge,merged>> into fewer, larger\n+segments. This decreases the number of segments, which means less metadata is\n+kept in heap memory.\n+\n+[discrete]\n+[[shard-auto-balance]]\n+==== {es} automatically balances shards within a data tier\n+\n+A cluster's nodes are grouped into data tiers. Within each tier, {es}\n+attempts to spread an index's shards across as many nodes as possible. When you\n+add a new node or a node fails, {es} automatically rebalances the index's shards\n+across the tier's remaining nodes.\n+\n+[discrete]\n+[[shard-size-guidelines]]\n+=== Guidelines\n+\n+Where applicable, use the following guidelines as starting points for your\n+sharding strategy.\n+\n+[discrete]\n+[[delete-indices-not-documents]]\n+==== Delete indices, not documents\n+\n+Deleted documents aren't immediately removed from {es}'s file system.\n+Instead, {es} marks the document as deleted on each related shard. The marked\n+document will continue to use resources until it's removed during a periodic\n+<<index-modules-merge,segment merge>>.\n+\n+When possible, delete entire indices instead. {es} can immediately remove\n+deleted indices directly from the file system and free up resources.\n+\n+[discrete]\n+[[use-ds-ilm-for-time-series]]\n+==== For time series data, use data streams and {ilm-init}\n+\n+<<data-streams,Data streams>> let you store time series data across multiple,\n+time-based backing indices. You can use <<index-lifecycle-management,{ilm}\n+({ilm-init})>> to automatically manage these backing indices.\n+\n+[role=\"screenshot\"]\n+image:images/ilm/index-lifecycle-policies.png[]\n+\n+One advantage of this setup is\n+<<getting-started-index-lifecycle-management,automatic rollover>>, which creates\n+a new write index when the current one meets a defined `max_age`, `max_docs`, or\n+`max_size` threshold. You can use these thresholds to create indices based on\n+your retention intervals. When an index is no longer needed, you can use\n+{ilm-init} to automatically delete it and free up resources.\n+\n+{ilm-init} also makes it easy to change your sharding strategy over time:\n+\n+* *Want to decrease the shard count for new indices?* +\n+Change the <<index-number-of-shards,`index.number_of_shards`>> setting in the\n+data stream's <<data-streams-change-mappings-and-settings,matching index\n+template>>.\n+\n+* *Want larger shards?* +\n+Increase your {ilm-init} policy's <<ilm-rollover,rollover threshold>>.\n+\n+* *Need indices that span shorter intervals?* +\n+Offset the increased shard count by deleting older indices sooner. You can do\n+this by lowering the `min_age` threshold for your policy's\n+<<ilm-index-lifecycle,delete phase>>.\n+\n+Every new backing index is an opportunity to further tune your strategy.\n+\n+[discrete]\n+[[shard-size-recommendation]]\n+==== Aim for shard sizes between 10GB and 50GB\n+\n+Shards larger than 50GB may make a cluster less likely to recover from failure.\n+When a node fails, {es} rebalances the node's shards across the data tier's\n+remaining nodes. Shards larger than 50GB can be harder to move across a network\n+and may tax node resources.\n+\n+[discrete]\n+[[shard-count-recommendation]]\n+==== Aim for 20 shards or fewer per GB of heap memory\n+\n+The number of shards a node can hold is proportional to the node's\n+<<heap-size,heap memory>>. For example, a node with 30GB of heap memory should\n+have at most 600 shards. The further below this limit you can keep your nodes,\n+the better. If you find your nodes exceeding more than 20 shards per GB,\n+consider adding another node. You can use the <<cat-shards,cat shards API>> to\n+check the number of shards per node.\n+\n+[source,console]\n+----\n+GET _cat/shards\n+----\n+// TEST[setup:my_index]\n+\n+To use compressed pointers and save memory, we\n+recommend each node have a maximum heap size of 32GB or 50% of the node's\n+available memory, whichever is lower. See <<heap-size>>.\n+\n+\n+[discrete]\n+[[avoid-node-hotspots]]\n+==== Avoid node hotspots\n+\n+If too many shards are allocated to a specific node, the node can become a\n+hotspot. For example, if a single node contains too many shards for an index\n+with a high indexing volume, the node is likely to have issues.\n+\n+To prevent hotspots, use the\n+<<total-shards-per-node,`index.routing.allocation.total_shards_per_node`>> index\n+setting to explicitly limit the number of shards on a single node. You can\n+configure `index.routing.allocation.total_shards_per_node` using the\n+<<indices-update-settings,update index settings API>>.\n+\n+[source,console]\n+--------------------------------------------------\n+PUT /my-index-000001/_settings\n+{\n+  \"index\" : {\n+    \"routing.allocation.total_shards_per_node\" : 5\n+  }\n+}\n+--------------------------------------------------\n+// TEST[setup:my_index]\n+\n+\n+[discrete]\n+[[fix-an-oversharded-cluster]]\n+=== Fix an oversharded cluster\n+\n+If your cluster is experiencing stability issues due to oversharded indices,\n+you can use one or more of the following methods to fix them.\n+\n+[discrete]\n+[[reindex-indices-from-shorter-periods-into-longer-periods]]\n+==== Create time-based indices that cover longer periods\n+\n+For time series data, you can create indices that cover longer time intervals.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7bf9495ffd4b663acdceecf05e23dc47861b624"}, "originalPosition": 198}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1726, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}