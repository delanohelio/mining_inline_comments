{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA2NTI2NjU2", "number": 55506, "title": "Use streaming reads for GCS", "bodyText": "To read from GCS repositories we're currently using Google SDK's official BlobReadChannel, which issues a new request every 2MB (default chunk size for BlobReadChannel) using range requests, and fully downloads the chunk before exposing it to the returned InputStream. This means that the SDK issues an awfully high number of requests to download large blobs. Increasing the chunk size is not an option, as that will mean that an awfully high amount of heap memory will be consumed by the download process.\nThe Google SDK does not provide the right abstractions for a streaming download. This PR uses the lower-level primitives of the SDK to implement a streaming download, similar to what S3's SDK does.\nAlso closes #55505", "createdAt": "2020-04-21T08:38:34Z", "url": "https://github.com/elastic/elasticsearch/pull/55506", "merged": true, "mergeCommit": {"oid": "923343862ec075a0eacc535a0aa82bf2008405e1"}, "closed": true, "closedAt": "2020-04-21T11:13:10Z", "author": {"login": "ywelsch"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcZvcXzgH2gAyNDA2NTI2NjU2OjIwOTdmMmJjZmEzMzNhNzE1NGViYjU5YzRlZDYzZjA4Y2U4NWY0MDY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcZwwGNAH2gAyNDA2NTI2NjU2OjY0ODE5YWRiZDI5YjdiOWEyNGIzYjQ0Y2MxN2MwMzQ3YzAxZWIyMzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2097f2bcfa333a7154ebb59c4ed63f08ce85f406", "author": {"user": {"login": "ywelsch", "name": "Yannick Welsch"}}, "url": "https://github.com/elastic/elasticsearch/commit/2097f2bcfa333a7154ebb59c4ed63f08ce85f406", "committedDate": "2020-04-21T08:30:43Z", "message": "Use streaming reads for GCS"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3MTE5MjE4", "url": "https://github.com/elastic/elasticsearch/pull/55506#pullrequestreview-397119218", "createdAt": "2020-04-21T08:47:49Z", "commit": {"oid": "2097f2bcfa333a7154ebb59c4ed63f08ce85f406"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODo0Nzo0OVrOGI6Ogg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwODo0Nzo0OVrOGI6Ogg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk5NTc3OA==", "bodyText": "NIT: get.getRequestHeaders().setRange(\"bytes=\" + Math.addExact(start, currentOffset) + \"-\" + end);", "url": "https://github.com/elastic/elasticsearch/pull/55506#discussion_r411995778", "createdAt": "2020-04-21T08:47:49Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-gcs/src/main/java/org/elasticsearch/repositories/gcs/GoogleCloudStorageRetryingInputStream.java", "diffHunk": "@@ -51,111 +56,141 @@\n     static final int MAX_SUPPRESSED_EXCEPTIONS = 10;\n \n     private final Storage client;\n+    private final com.google.api.services.storage.Storage storage;\n \n     private final BlobId blobId;\n \n     private final long start;\n-    private final long length;\n+    private final long end;\n \n-    private final int maxRetries;\n+    private final int maxAttempts;\n \n     private InputStream currentStream;\n     private int attempt = 1;\n     private List<StorageException> failures = new ArrayList<>(MAX_SUPPRESSED_EXCEPTIONS);\n     private long currentOffset;\n     private boolean closed;\n \n-    GoogleCloudStorageRetryingInputStream(Storage client, BlobId blobId, long start, long length) throws IOException {\n+    GoogleCloudStorageRetryingInputStream(Storage client, BlobId blobId) throws IOException {\n+        this(client, blobId, 0, Long.MAX_VALUE - 1);\n+    }\n+\n+    // both start and end are inclusive bounds, following the definition in https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.35\n+    GoogleCloudStorageRetryingInputStream(Storage client, BlobId blobId, long start, long end) throws IOException {\n+        if (start < 0L) {\n+            throw new IllegalArgumentException(\"start must be non-negative\");\n+        }\n+        if (end < start || end == Long.MAX_VALUE) {\n+            throw new IllegalArgumentException(\"end must be >= start and not Long.MAX_VALUE\");\n+        }\n         this.client = client;\n         this.blobId = blobId;\n         this.start = start;\n-        this.length = length;\n-        this.maxRetries = client.getOptions().getRetrySettings().getMaxAttempts() + 1;\n+        this.end = end;\n+        this.maxAttempts = client.getOptions().getRetrySettings().getMaxAttempts();\n+        SpecialPermission.check();\n+        storage = getStorage(client);\n         currentStream = openStream();\n     }\n \n-    private static final int DEFAULT_CHUNK_SIZE = 2 * 1024 * 1024;\n+    @SuppressForbidden(reason = \"need access to storage client\")\n+    private static com.google.api.services.storage.Storage getStorage(Storage client) {\n+        return AccessController.doPrivileged((PrivilegedAction<com.google.api.services.storage.Storage>) () -> {\n+            assert client.getOptions().getRpc() instanceof HttpStorageRpc;\n+            assert Stream.of(client.getOptions().getRpc().getClass().getDeclaredFields()).anyMatch(f -> f.getName().equals(\"storage\"));\n+            try {\n+                final Field storageField = client.getOptions().getRpc().getClass().getDeclaredField(\"storage\");\n+                storageField.setAccessible(true);\n+                return (com.google.api.services.storage.Storage) storageField.get(client.getOptions().getRpc());\n+            } catch (Exception e) {\n+                throw new IllegalStateException(\"storage could not be set up\", e);\n+            }\n+        });\n+    }\n \n     private InputStream openStream() throws IOException {\n         try {\n-            final ReadChannel readChannel = SocketAccess.doPrivilegedIOException(() -> client.reader(blobId));\n-            final long end = start + length < 0L ? Long.MAX_VALUE : start + length; // inclusive\n-            final SeekableByteChannel adaptedChannel = new SeekableByteChannel() {\n-\n-                long position;\n-\n-                @SuppressForbidden(reason = \"Channel is based of a socket not a file\")\n-                @Override\n-                public int read(ByteBuffer dst) throws IOException {\n-                    final long remainingBytesToRead = end - position;\n-                    assert remainingBytesToRead >= 0L;\n-                    // The SDK uses the maximum between chunk size and dst.remaining() to determine fetch size\n-                    // We can be smarter here and only fetch what's needed when we know the length\n-                    if (remainingBytesToRead < DEFAULT_CHUNK_SIZE) {\n-                        readChannel.setChunkSize(Math.toIntExact(remainingBytesToRead));\n-                    }\n-                    if (remainingBytesToRead < dst.remaining()) {\n-                        dst.limit(dst.position() + Math.toIntExact(remainingBytesToRead));\n-                    }\n-                    try {\n-                        int read = SocketAccess.doPrivilegedIOException(() -> readChannel.read(dst));\n-                        if (read > 0) {\n-                            position += read;\n-                        }\n-                        return read;\n-                    } catch (StorageException e) {\n-                        if (e.getCode() == HTTP_NOT_FOUND) {\n-                            throw new NoSuchFileException(\"Blob object [\" + blobId.getName() + \"] not found: \" + e.getMessage());\n+            try {\n+                return RetryHelper.runWithRetries(() -> {\n+                        try {\n+                            return SocketAccess.doPrivilegedIOException(() -> {\n+                                final Get get = storage.objects().get(blobId.getBucket(), blobId.getName());\n+                                get.setReturnRawInputStream(true);\n+\n+                                if (currentOffset > 0 || start > 0 || end < Long.MAX_VALUE - 1) {\n+                                    StringBuilder rangeHeader = new StringBuilder();\n+                                    rangeHeader.append(\"bytes=\").append(Math.addExact(start, currentOffset)).append(\"-\").append(end);\n+                                    get.getRequestHeaders().setRange(rangeHeader.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2097f2bcfa333a7154ebb59c4ed63f08ce85f406"}, "originalPosition": 140}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87ff1caae178ac1bcc7266d2c27ddd4de24aa6d1", "author": {"user": {"login": "ywelsch", "name": "Yannick Welsch"}}, "url": "https://github.com/elastic/elasticsearch/commit/87ff1caae178ac1bcc7266d2c27ddd4de24aa6d1", "committedDate": "2020-04-21T09:58:09Z", "message": "shorter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44efd55a0ea42aa2bbb8cb845fc1164b1a7b5064", "author": {"user": {"login": "ywelsch", "name": "Yannick Welsch"}}, "url": "https://github.com/elastic/elasticsearch/commit/44efd55a0ea42aa2bbb8cb845fc1164b1a7b5064", "committedDate": "2020-04-21T10:00:45Z", "message": "Merge remote-tracking branch 'elastic/master' into gcs-read"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64819adbd29b7b9a24b3b44cc17c0347c01eb235", "author": {"user": {"login": "ywelsch", "name": "Yannick Welsch"}}, "url": "https://github.com/elastic/elasticsearch/commit/64819adbd29b7b9a24b3b44cc17c0347c01eb235", "committedDate": "2020-04-21T10:02:10Z", "message": "remove test mute"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 607, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}