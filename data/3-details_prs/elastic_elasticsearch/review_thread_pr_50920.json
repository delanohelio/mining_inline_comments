{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYyMTQ3MzY4", "number": 50920, "reviewThreads": {"totalCount": 108, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMDo0MDoyNlrOEBsIhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTo1Nzo1NFrOEBtlKA==", "hasNextPage": false, "hasPreviousPage": true}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjA3MTExOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMDo0MDoyNlrOGdr-Qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNTozMTowNVrOGgkAwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4MjMzOA==", "bodyText": "I think this is now quadratic in the graveyard size, which defaults to 500, so that's 500x500=250000 checks in a well-established cluster. I think it'd be more efficient to compare the graveyards for equality and then use IndexGraveyardDiff to find the differences.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433782338", "createdAt": "2020-06-02T10:40:26Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java", "diffHunk": "@@ -256,18 +256,37 @@ public boolean isNewCluster() {\n         if (metadataChanged() == false || isNewCluster()) {\n             return Collections.emptyList();\n         }\n-        List<Index> deleted = null;\n-        for (ObjectCursor<IndexMetadata> cursor : previousState.metadata().indices().values()) {\n+        Set<Index> deleted = null;\n+        final Metadata previousMetadata = previousState.metadata();\n+        final Metadata currentMetadata = state.metadata();\n+\n+        for (ObjectCursor<IndexMetadata> cursor : previousMetadata.indices().values()) {\n             IndexMetadata index = cursor.value;\n-            IndexMetadata current = state.metadata().index(index.getIndex());\n+            IndexMetadata current = currentMetadata.index(index.getIndex());\n             if (current == null) {\n                 if (deleted == null) {\n-                    deleted = new ArrayList<>();\n+                    deleted = new HashSet<>();\n                 }\n                 deleted.add(index.getIndex());\n             }\n         }\n-        return deleted == null ? Collections.<Index>emptyList() : deleted;\n+\n+        // Look for new entries in the index graveyard, where there's no corresponding index in the\n+        // previous metadata. This indicates that a dangling index has been explicitly deleted, so\n+        // each node should make sure to delete any related data.\n+        for (IndexGraveyard.Tombstone tombstone : currentMetadata.indexGraveyard().getTombstones()) {\n+            final Index index = tombstone.getIndex();\n+            final boolean isNewTombstone = previousMetadata.hasIndex(index) == false\n+                && previousMetadata.indexGraveyard().containsIndex(index) == false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc5NzYzNA==", "bodyText": "Now using IndexGraveyardDiff.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r436797634", "createdAt": "2020-06-08T15:31:05Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java", "diffHunk": "@@ -256,18 +256,37 @@ public boolean isNewCluster() {\n         if (metadataChanged() == false || isNewCluster()) {\n             return Collections.emptyList();\n         }\n-        List<Index> deleted = null;\n-        for (ObjectCursor<IndexMetadata> cursor : previousState.metadata().indices().values()) {\n+        Set<Index> deleted = null;\n+        final Metadata previousMetadata = previousState.metadata();\n+        final Metadata currentMetadata = state.metadata();\n+\n+        for (ObjectCursor<IndexMetadata> cursor : previousMetadata.indices().values()) {\n             IndexMetadata index = cursor.value;\n-            IndexMetadata current = state.metadata().index(index.getIndex());\n+            IndexMetadata current = currentMetadata.index(index.getIndex());\n             if (current == null) {\n                 if (deleted == null) {\n-                    deleted = new ArrayList<>();\n+                    deleted = new HashSet<>();\n                 }\n                 deleted.add(index.getIndex());\n             }\n         }\n-        return deleted == null ? Collections.<Index>emptyList() : deleted;\n+\n+        // Look for new entries in the index graveyard, where there's no corresponding index in the\n+        // previous metadata. This indicates that a dangling index has been explicitly deleted, so\n+        // each node should make sure to delete any related data.\n+        for (IndexGraveyard.Tombstone tombstone : currentMetadata.indexGraveyard().getTombstones()) {\n+            final Index index = tombstone.getIndex();\n+            final boolean isNewTombstone = previousMetadata.hasIndex(index) == false\n+                && previousMetadata.indexGraveyard().containsIndex(index) == false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4MjMzOA=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjA3NDMwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/import_index/ImportDanglingIndexRequest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMDo0MTozMFrOGdsARw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNTozMjowMVrOGgkDRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4Mjg1NQ==", "bodyText": "I don't think this should be a BaseNodesRequest, we don't broadcast it across nodes.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433782855", "createdAt": "2020-06-02T10:41:30Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/import_index/ImportDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.import_index;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Objects;\n+\n+/**\n+ * Represents a request to import a particular dangling index, specified\n+ * by its UUID. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class ImportDanglingIndexRequest extends BaseNodesRequest<ImportDanglingIndexRequest> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc5ODI3OQ==", "bodyText": "I swapped it for AcknowledgedRequest, which aligns with the delete request and meant I could remove ImportDanglingIndexResponse entirely.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r436798279", "createdAt": "2020-06-08T15:32:01Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/import_index/ImportDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.import_index;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.Locale;\n+import java.util.Objects;\n+\n+/**\n+ * Represents a request to import a particular dangling index, specified\n+ * by its UUID. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class ImportDanglingIndexRequest extends BaseNodesRequest<ImportDanglingIndexRequest> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4Mjg1NQ=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjA4MDkwOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/action/admin/indices/dangling/list/ListDanglingIndicesResponseTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMDo0Mzo0M1rOGdsEUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNTozMjoyNVrOGgkEZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4Mzg4OA==", "bodyText": "Nit: can we use random UUIDs instead?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433783888", "createdAt": "2020-06-02T10:43:43Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/action/admin/indices/dangling/list/ListDanglingIndicesResponseTests.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.list;\n+\n+import org.elasticsearch.action.admin.indices.dangling.DanglingIndexInfo;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse.AggregatedDanglingIndexInfo;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.test.ESTestCase;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse.resultsByIndexUUID;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ListDanglingIndicesResponseTests extends ESTestCase {\n+\n+    /**\n+     * Checks that {@link ListDanglingIndicesResponse#resultsByIndexUUID(List)} handles the\n+     * basic base of empty input.\n+     */\n+    public void testResultsByIndexUUIDWithEmptyListReturnsEmptyMap() {\n+        assertThat(resultsByIndexUUID(emptyList()), empty());\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate a single dangling index\n+     * on a single node.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateASingleResponse() {\n+        final DiscoveryNode node = mock(DiscoveryNode.class);\n+        when(node.getId()).thenReturn(\"some-node-id\");\n+\n+        final var danglingIndexInfo = List.of(new DanglingIndexInfo(\"some-node-id\", \"some-index\", \"deadb33f\", 123456L));\n+        final var nodes = List.of(new NodeListDanglingIndicesResponse(node, danglingIndexInfo));\n+\n+        final var aggregated = new ArrayList<>(resultsByIndexUUID(nodes));\n+        assertThat(aggregated, hasSize(1));\n+\n+        final var expected = new AggregatedDanglingIndexInfo(\"deadb33f\", \"some-index\", 123456L);\n+        expected.getNodeIds().add(\"some-node-id\");\n+        assertThat(aggregated.get(0), equalTo(expected));\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate a single dangling index\n+     * across multiple nodes.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateAcrossMultipleNodes() {\n+        final DiscoveryNode node1 = mock(DiscoveryNode.class);\n+        final DiscoveryNode node2 = mock(DiscoveryNode.class);\n+        when(node1.getId()).thenReturn(\"node-id-1\");\n+        when(node2.getId()).thenReturn(\"node-id-2\");\n+\n+        final var danglingIndexInfo1 = List.of(new DanglingIndexInfo(\"node-id-1\", \"some-index\", \"deadb33f\", 123456L));\n+        final var danglingIndexInfo2 = List.of(new DanglingIndexInfo(\"node-id-2\", \"some-index\", \"deadb33f\", 123456L));\n+        final var nodes = List.of(\n+            new NodeListDanglingIndicesResponse(node1, danglingIndexInfo1),\n+            new NodeListDanglingIndicesResponse(node2, danglingIndexInfo2)\n+        );\n+\n+        final var aggregated = new ArrayList<>(resultsByIndexUUID(nodes));\n+        assertThat(aggregated, hasSize(1));\n+\n+        final var expected = new AggregatedDanglingIndexInfo(\"deadb33f\", \"some-index\", 123456L);\n+        expected.getNodeIds().add(\"node-id-1\");\n+        expected.getNodeIds().add(\"node-id-2\");\n+        assertThat(aggregated.get(0), equalTo(expected));\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate multiple dangling indices\n+     * on a single node.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateMultipleIndicesOnOneNode() {\n+        final DiscoveryNode node1 = mock(DiscoveryNode.class);\n+        when(node1.getId()).thenReturn(\"node-id-1\");\n+\n+        final var danglingIndexInfo = List.of(\n+            new DanglingIndexInfo(\"node-id-1\", \"some-index\", \"deadb33f\", 123456L),\n+            new DanglingIndexInfo(\"node-id-1\", \"some-other-index\", \"cafebabe\", 7891011L)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc5ODU2Nw==", "bodyText": "Now using UUIDs!", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r436798567", "createdAt": "2020-06-08T15:32:25Z", "author": {"login": "pugnascotia"}, "path": "server/src/test/java/org/elasticsearch/action/admin/indices/dangling/list/ListDanglingIndicesResponseTests.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.list;\n+\n+import org.elasticsearch.action.admin.indices.dangling.DanglingIndexInfo;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse.AggregatedDanglingIndexInfo;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.test.ESTestCase;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse.resultsByIndexUUID;\n+import static org.hamcrest.Matchers.containsInAnyOrder;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class ListDanglingIndicesResponseTests extends ESTestCase {\n+\n+    /**\n+     * Checks that {@link ListDanglingIndicesResponse#resultsByIndexUUID(List)} handles the\n+     * basic base of empty input.\n+     */\n+    public void testResultsByIndexUUIDWithEmptyListReturnsEmptyMap() {\n+        assertThat(resultsByIndexUUID(emptyList()), empty());\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate a single dangling index\n+     * on a single node.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateASingleResponse() {\n+        final DiscoveryNode node = mock(DiscoveryNode.class);\n+        when(node.getId()).thenReturn(\"some-node-id\");\n+\n+        final var danglingIndexInfo = List.of(new DanglingIndexInfo(\"some-node-id\", \"some-index\", \"deadb33f\", 123456L));\n+        final var nodes = List.of(new NodeListDanglingIndicesResponse(node, danglingIndexInfo));\n+\n+        final var aggregated = new ArrayList<>(resultsByIndexUUID(nodes));\n+        assertThat(aggregated, hasSize(1));\n+\n+        final var expected = new AggregatedDanglingIndexInfo(\"deadb33f\", \"some-index\", 123456L);\n+        expected.getNodeIds().add(\"some-node-id\");\n+        assertThat(aggregated.get(0), equalTo(expected));\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate a single dangling index\n+     * across multiple nodes.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateAcrossMultipleNodes() {\n+        final DiscoveryNode node1 = mock(DiscoveryNode.class);\n+        final DiscoveryNode node2 = mock(DiscoveryNode.class);\n+        when(node1.getId()).thenReturn(\"node-id-1\");\n+        when(node2.getId()).thenReturn(\"node-id-2\");\n+\n+        final var danglingIndexInfo1 = List.of(new DanglingIndexInfo(\"node-id-1\", \"some-index\", \"deadb33f\", 123456L));\n+        final var danglingIndexInfo2 = List.of(new DanglingIndexInfo(\"node-id-2\", \"some-index\", \"deadb33f\", 123456L));\n+        final var nodes = List.of(\n+            new NodeListDanglingIndicesResponse(node1, danglingIndexInfo1),\n+            new NodeListDanglingIndicesResponse(node2, danglingIndexInfo2)\n+        );\n+\n+        final var aggregated = new ArrayList<>(resultsByIndexUUID(nodes));\n+        assertThat(aggregated, hasSize(1));\n+\n+        final var expected = new AggregatedDanglingIndexInfo(\"deadb33f\", \"some-index\", 123456L);\n+        expected.getNodeIds().add(\"node-id-1\");\n+        expected.getNodeIds().add(\"node-id-2\");\n+        assertThat(aggregated.get(0), equalTo(expected));\n+    }\n+\n+    /**\n+     * Checks that <code>resultsByIndexUUID(List)</code> can aggregate multiple dangling indices\n+     * on a single node.\n+     */\n+    public void testResultsByIndexUUIDCanAggregateMultipleIndicesOnOneNode() {\n+        final DiscoveryNode node1 = mock(DiscoveryNode.class);\n+        when(node1.getId()).thenReturn(\"node-id-1\");\n+\n+        final var danglingIndexInfo = List.of(\n+            new DanglingIndexInfo(\"node-id-1\", \"some-index\", \"deadb33f\", 123456L),\n+            new DanglingIndexInfo(\"node-id-1\", \"some-other-index\", \"cafebabe\", 7891011L)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc4Mzg4OA=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjExOTczOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMDo1NzowM1rOGdsdsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNTo1NzowNFrOGglHtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MDM4NA==", "bodyText": "I'm concerned that this warning will appear if you delete a dangling index and then immediately list the remaining dangling indices, because the async deletion hasn't gone through yet. But then I'm not sure why we don't see this already with async deletion. Could you investigate that?\nI'm wondering if we really need a warning here at all. If there's a tombstone then the index was certainly deleted, so it's something else's problem and we should be ignoring it here. The problem is if we never get around to cleaning it up until the tombstone expires then it'll become dangling again, but I don't think that's our concern here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433790384", "createdAt": "2020-06-02T10:57:03Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);\n+            }\n+        }\n+\n+        danglingIndices.putAll(findNewDanglingIndices(danglingIndices, metadata));\n     }\n \n     /**\n      * Finds new dangling indices by iterating over the indices and trying to find indices\n-     * that have state on disk, but are not part of the provided meta data, or not detected\n+     * that have state on disk, but are not part of the provided metadata, or not detected\n      * as dangled already.\n      */\n-    Map<Index, IndexMetadata> findNewDanglingIndices(final Metadata metadata) {\n+    public Map<Index, IndexMetadata> findNewDanglingIndices(Map<Index, IndexMetadata> existingDanglingIndices, final Metadata metadata) {\n         final Set<String> excludeIndexPathIds = new HashSet<>(metadata.indices().size() + danglingIndices.size());\n         for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n             excludeIndexPathIds.add(cursor.value.getIndex().getUUID());\n         }\n-        excludeIndexPathIds.addAll(danglingIndices.keySet().stream().map(Index::getUUID).collect(Collectors.toList()));\n+        for (Index index : existingDanglingIndices.keySet()) {\n+            excludeIndexPathIds.add(index.getUUID());\n+        }\n         try {\n             final List<IndexMetadata> indexMetadataList = metaStateService.loadIndicesStates(excludeIndexPathIds::contains);\n             Map<Index, IndexMetadata> newIndices = new HashMap<>(indexMetadataList.size());\n             final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n             for (IndexMetadata indexMetadata : indexMetadataList) {\n-                if (metadata.hasIndex(indexMetadata.getIndex().getName())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as index with same name already exists in cluster metadata\",\n-                        indexMetadata.getIndex());\n-                } else if (graveyard.containsIndex(indexMetadata.getIndex())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as an index with the same name and UUID exist in the \" +\n-                                \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n-                                \"that was previously deleted.\", indexMetadata.getIndex());\n+                Index index = indexMetadata.getIndex();\n+                // Although deleting a dangling index through the API adds a tombstone to the graveyard, that process results in the\n+                // dangling index files being deleted, so we don't expect to encounter a dangling index and a tombstone here when\n+                // everything is working normally.\n+                if (graveyard.containsIndex(index)) {\n+                    logger.warn(\"[{}] cannot be imported as a dangling index, as an index with the same name and UUID exist in the \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgxNzc0Mg==", "bodyText": "What do you mean by async deletion here, David? Deletion is normally synchronous on the CS applier thread (i.e. the one that runs IndicesClusterStateService, which runs before findNewDanglingIndices here).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433817742", "createdAt": "2020-06-02T11:52:17Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);\n+            }\n+        }\n+\n+        danglingIndices.putAll(findNewDanglingIndices(danglingIndices, metadata));\n     }\n \n     /**\n      * Finds new dangling indices by iterating over the indices and trying to find indices\n-     * that have state on disk, but are not part of the provided meta data, or not detected\n+     * that have state on disk, but are not part of the provided metadata, or not detected\n      * as dangled already.\n      */\n-    Map<Index, IndexMetadata> findNewDanglingIndices(final Metadata metadata) {\n+    public Map<Index, IndexMetadata> findNewDanglingIndices(Map<Index, IndexMetadata> existingDanglingIndices, final Metadata metadata) {\n         final Set<String> excludeIndexPathIds = new HashSet<>(metadata.indices().size() + danglingIndices.size());\n         for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n             excludeIndexPathIds.add(cursor.value.getIndex().getUUID());\n         }\n-        excludeIndexPathIds.addAll(danglingIndices.keySet().stream().map(Index::getUUID).collect(Collectors.toList()));\n+        for (Index index : existingDanglingIndices.keySet()) {\n+            excludeIndexPathIds.add(index.getUUID());\n+        }\n         try {\n             final List<IndexMetadata> indexMetadataList = metaStateService.loadIndicesStates(excludeIndexPathIds::contains);\n             Map<Index, IndexMetadata> newIndices = new HashMap<>(indexMetadataList.size());\n             final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n             for (IndexMetadata indexMetadata : indexMetadataList) {\n-                if (metadata.hasIndex(indexMetadata.getIndex().getName())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as index with same name already exists in cluster metadata\",\n-                        indexMetadata.getIndex());\n-                } else if (graveyard.containsIndex(indexMetadata.getIndex())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as an index with the same name and UUID exist in the \" +\n-                                \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n-                                \"that was previously deleted.\", indexMetadata.getIndex());\n+                Index index = indexMetadata.getIndex();\n+                // Although deleting a dangling index through the API adds a tombstone to the graveyard, that process results in the\n+                // dangling index files being deleted, so we don't expect to encounter a dangling index and a tombstone here when\n+                // everything is working normally.\n+                if (graveyard.containsIndex(index)) {\n+                    logger.warn(\"[{}] cannot be imported as a dangling index, as an index with the same name and UUID exist in the \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MDM4NA=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg0OTE2Mg==", "bodyText": "I mean the case where it doesn't get deleted synchronously and gets added to pendingDeletes instead.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433849162", "createdAt": "2020-06-02T12:49:55Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);\n+            }\n+        }\n+\n+        danglingIndices.putAll(findNewDanglingIndices(danglingIndices, metadata));\n     }\n \n     /**\n      * Finds new dangling indices by iterating over the indices and trying to find indices\n-     * that have state on disk, but are not part of the provided meta data, or not detected\n+     * that have state on disk, but are not part of the provided metadata, or not detected\n      * as dangled already.\n      */\n-    Map<Index, IndexMetadata> findNewDanglingIndices(final Metadata metadata) {\n+    public Map<Index, IndexMetadata> findNewDanglingIndices(Map<Index, IndexMetadata> existingDanglingIndices, final Metadata metadata) {\n         final Set<String> excludeIndexPathIds = new HashSet<>(metadata.indices().size() + danglingIndices.size());\n         for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n             excludeIndexPathIds.add(cursor.value.getIndex().getUUID());\n         }\n-        excludeIndexPathIds.addAll(danglingIndices.keySet().stream().map(Index::getUUID).collect(Collectors.toList()));\n+        for (Index index : existingDanglingIndices.keySet()) {\n+            excludeIndexPathIds.add(index.getUUID());\n+        }\n         try {\n             final List<IndexMetadata> indexMetadataList = metaStateService.loadIndicesStates(excludeIndexPathIds::contains);\n             Map<Index, IndexMetadata> newIndices = new HashMap<>(indexMetadataList.size());\n             final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n             for (IndexMetadata indexMetadata : indexMetadataList) {\n-                if (metadata.hasIndex(indexMetadata.getIndex().getName())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as index with same name already exists in cluster metadata\",\n-                        indexMetadata.getIndex());\n-                } else if (graveyard.containsIndex(indexMetadata.getIndex())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as an index with the same name and UUID exist in the \" +\n-                                \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n-                                \"that was previously deleted.\", indexMetadata.getIndex());\n+                Index index = indexMetadata.getIndex();\n+                // Although deleting a dangling index through the API adds a tombstone to the graveyard, that process results in the\n+                // dangling index files being deleted, so we don't expect to encounter a dangling index and a tombstone here when\n+                // everything is working normally.\n+                if (graveyard.containsIndex(index)) {\n+                    logger.warn(\"[{}] cannot be imported as a dangling index, as an index with the same name and UUID exist in the \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MDM4NA=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjgxNTc5OQ==", "bodyText": "I'm not very familiar with the mechanics of deletion, though I see you point about the log message and I could certainly remove it.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r436815799", "createdAt": "2020-06-08T15:57:04Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);\n+            }\n+        }\n+\n+        danglingIndices.putAll(findNewDanglingIndices(danglingIndices, metadata));\n     }\n \n     /**\n      * Finds new dangling indices by iterating over the indices and trying to find indices\n-     * that have state on disk, but are not part of the provided meta data, or not detected\n+     * that have state on disk, but are not part of the provided metadata, or not detected\n      * as dangled already.\n      */\n-    Map<Index, IndexMetadata> findNewDanglingIndices(final Metadata metadata) {\n+    public Map<Index, IndexMetadata> findNewDanglingIndices(Map<Index, IndexMetadata> existingDanglingIndices, final Metadata metadata) {\n         final Set<String> excludeIndexPathIds = new HashSet<>(metadata.indices().size() + danglingIndices.size());\n         for (ObjectCursor<IndexMetadata> cursor : metadata.indices().values()) {\n             excludeIndexPathIds.add(cursor.value.getIndex().getUUID());\n         }\n-        excludeIndexPathIds.addAll(danglingIndices.keySet().stream().map(Index::getUUID).collect(Collectors.toList()));\n+        for (Index index : existingDanglingIndices.keySet()) {\n+            excludeIndexPathIds.add(index.getUUID());\n+        }\n         try {\n             final List<IndexMetadata> indexMetadataList = metaStateService.loadIndicesStates(excludeIndexPathIds::contains);\n             Map<Index, IndexMetadata> newIndices = new HashMap<>(indexMetadataList.size());\n             final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n             for (IndexMetadata indexMetadata : indexMetadataList) {\n-                if (metadata.hasIndex(indexMetadata.getIndex().getName())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as index with same name already exists in cluster metadata\",\n-                        indexMetadata.getIndex());\n-                } else if (graveyard.containsIndex(indexMetadata.getIndex())) {\n-                    logger.warn(\"[{}] can not be imported as a dangling index, as an index with the same name and UUID exist in the \" +\n-                                \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n-                                \"that was previously deleted.\", indexMetadata.getIndex());\n+                Index index = indexMetadata.getIndex();\n+                // Although deleting a dangling index through the API adds a tombstone to the graveyard, that process results in the\n+                // dangling index files being deleted, so we don't expect to encounter a dangling index and a tombstone here when\n+                // everything is working normally.\n+                if (graveyard.containsIndex(index)) {\n+                    logger.warn(\"[{}] cannot be imported as a dangling index, as an index with the same name and UUID exist in the \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MDM4NA=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjEzMDk4OnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTowMDozNVrOGdsk0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMzo0MTowNlrOGkb3QA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MjIwOQ==", "bodyText": "I don't think this is what happens when auto-import is enabled as it is here; instead we update the danglingIndices field and pass that in for subsequent calls.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433792209", "createdAt": "2020-06-02T11:00:35Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -124,7 +154,7 @@ public void testDanglingProcessing() throws Exception {\n             // check that several runs when not in the metadata still keep the dangled index around\n             int numberOfChecks = randomIntBetween(1, 10);\n             for (int i = 0; i < numberOfChecks; i++) {\n-                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(metadata);\n+                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(emptyMap(), metadata);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjgwMjcwNA==", "bodyText": "Re: this and the other comment on this file...this is a weird test. The findNewDanglingIndices method doesn't modify anything, and yet it's being called in a loop. It makes more sense to call findNewAndAddDanglingIndices in a loop (which the test also does). So I feel like maybe the test need rewritten? What do you think?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r436802704", "createdAt": "2020-06-08T15:38:29Z", "author": {"login": "pugnascotia"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -124,7 +154,7 @@ public void testDanglingProcessing() throws Exception {\n             // check that several runs when not in the metadata still keep the dangled index around\n             int numberOfChecks = randomIntBetween(1, 10);\n             for (int i = 0; i < numberOfChecks; i++) {\n-                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(metadata);\n+                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(emptyMap(), metadata);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MjIwOQ=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDg1ODQzMg==", "bodyText": "Good point, the calls to findNewAndAddDanglingIndices do what I suggest too. I'm ok with this as it is.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r440858432", "createdAt": "2020-06-16T13:41:06Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -124,7 +154,7 @@ public void testDanglingProcessing() throws Exception {\n             // check that several runs when not in the metadata still keep the dangled index around\n             int numberOfChecks = randomIntBetween(1, 10);\n             for (int i = 0; i < numberOfChecks; i++) {\n-                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(metadata);\n+                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(emptyMap(), metadata);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MjIwOQ=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjEzMjU1OnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTowMTowMlrOGdsluA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTowMTowMlrOGdsluA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc5MjQ0MA==", "bodyText": "Similarly here, we should be using the previous result rather than emptyMap().", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433792440", "createdAt": "2020-06-02T11:01:02Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -142,7 +172,7 @@ public void testDanglingProcessing() throws Exception {\n \n             // check that several runs when in the metadata, but not cleaned yet, still keeps dangled\n             for (int i = 0; i < numberOfChecks; i++) {\n-                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(metadata);\n+                Map<Index, IndexMetadata> newDanglingIndices = danglingState.findNewDanglingIndices(emptyMap(), metadata);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjI2ODYxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTo0NTo1NFrOGdt7ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNTozMjo0MVrOGgkFKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgxNDM3Mw==", "bodyText": "I'm not a fan of iterating over a map and then concurrently deleting entries. This only works because it's a concurrent map. I would prefer the use of danglingIndices.keySet().removeIf(...)", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433814373", "createdAt": "2020-06-02T11:45:54Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc5ODc2MA==", "bodyText": "Changed as you suggest.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r436798760", "createdAt": "2020-06-08T15:32:41Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,38 +152,51 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgxNDM3Mw=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjMwODI0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/delete/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMTo1Nzo1NFrOGduT1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNTozNDowMFrOGgkIjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgyMDYyOA==", "bodyText": "do we need to check that there is not already a tombstone for this index? Otherwise we might be spamming the graveyard with the same entry, which purges other perhaps more important entries to keep around.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r433820628", "createdAt": "2020-06-02T11:57:54Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/delete/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.delete;\n+\n+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.admin.indices.dangling.DanglingIndexInfo;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesAction;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesRequest;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse;\n+import org.elasticsearch.action.admin.indices.dangling.list.NodeListDanglingIndicesResponse;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, AcknowledgedResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected AcknowledgedResponse read(StreamInput in) throws IOException {\n+        return new AcknowledgedResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<AcknowledgedResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+                String indexUUID = indexToDelete.getUUID();\n+\n+                final ActionListener<AcknowledgedResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(AcknowledgedResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [\" + indexName + \"] [\" + indexUUID + \"]\", e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                final String taskSource = \"delete-dangling-index [\" + indexName + \"] [\" + indexUUID + \"]\";\n+\n+                clusterService.submitStateUpdateTask(\n+                    taskSource,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected AcknowledgedResponse newResponse(boolean acknowledged) {\n+                            return new AcknowledgedResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + deleteRequest.getIndexUUID() + \"]\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final Metadata metaData = currentState.getMetadata();\n+\n+        for (ObjectObjectCursor<String, IndexMetadata> each : metaData.indices()) {\n+            if (indexToDelete.getUUID().equals(each.value.getIndexUUID())) {\n+                throw new IllegalArgumentException(\n+                    \"Refusing to delete dangling index \"\n+                        + indexToDelete\n+                        + \" as an index with UUID [\"\n+                        + indexToDelete.getUUID()\n+                        + \"] already exists in the cluster state\"\n+                );\n+            }\n+        }\n+\n+        Metadata.Builder metaDataBuilder = Metadata.builder(metaData);\n+\n+        final IndexGraveyard newGraveyard = IndexGraveyard.builder(metaDataBuilder.indexGraveyard())\n+            .addTombstone(indexToDelete)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc5OTYyOA==", "bodyText": "I believe that in order to get to this point in the code, the index needs to be (1) missing from the metadata, and (2) missing from the graveyard. But, it doesn't hurt to have an extra check for safety, so I've added it.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r436799628", "createdAt": "2020-06-08T15:34:00Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/delete/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling.delete;\n+\n+import com.carrotsearch.hppc.cursors.ObjectObjectCursor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.admin.indices.dangling.DanglingIndexInfo;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesAction;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesRequest;\n+import org.elasticsearch.action.admin.indices.dangling.list.ListDanglingIndicesResponse;\n+import org.elasticsearch.action.admin.indices.dangling.list.NodeListDanglingIndicesResponse;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.AcknowledgedResponse;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, AcknowledgedResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected AcknowledgedResponse read(StreamInput in) throws IOException {\n+        return new AcknowledgedResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<AcknowledgedResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+                String indexUUID = indexToDelete.getUUID();\n+\n+                final ActionListener<AcknowledgedResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(AcknowledgedResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [\" + indexName + \"] [\" + indexUUID + \"]\", e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                final String taskSource = \"delete-dangling-index [\" + indexName + \"] [\" + indexUUID + \"]\";\n+\n+                clusterService.submitStateUpdateTask(\n+                    taskSource,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected AcknowledgedResponse newResponse(boolean acknowledged) {\n+                            return new AcknowledgedResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + deleteRequest.getIndexUUID() + \"]\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final Metadata metaData = currentState.getMetadata();\n+\n+        for (ObjectObjectCursor<String, IndexMetadata> each : metaData.indices()) {\n+            if (indexToDelete.getUUID().equals(each.value.getIndexUUID())) {\n+                throw new IllegalArgumentException(\n+                    \"Refusing to delete dangling index \"\n+                        + indexToDelete\n+                        + \" as an index with UUID [\"\n+                        + indexToDelete.getUUID()\n+                        + \"] already exists in the cluster state\"\n+                );\n+            }\n+        }\n+\n+        Metadata.Builder metaDataBuilder = Metadata.builder(metaData);\n+\n+        final IndexGraveyard newGraveyard = IndexGraveyard.builder(metaDataBuilder.indexGraveyard())\n+            .addTombstone(indexToDelete)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgyMDYyOA=="}, "originalCommit": {"oid": "53611f1a6b1ac3bb8ff106eb586b58ee795a836e"}, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQwOTM4OnYy", "diffSide": "RIGHT", "path": ".eclipseformat.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxMzo1ODowNVrOFdYUcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTozMToyNFrOFf-SRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MTQ3NA==", "bodyText": "unrelated to this PR, please revert", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366351474", "createdAt": "2020-01-14T13:58:05Z", "author": {"login": "ywelsch"}, "path": ".eclipseformat.xml", "diffHunk": "@@ -62,7 +62,7 @@\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_space_after_opening_angle_bracket_in_type_arguments\" value=\"do not insert\"/>\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_new_line_after_annotation_on_method\" value=\"insert\"/>\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_space_after_opening_paren_in_switch\" value=\"do not insert\"/>\n-        <setting id=\"org.eclipse.jdt.core.formatter.alignment_for_parameterized_type_references\" value=\"0\"/>\n+        <setting id=\"org.eclipse.jdt.core.formatter.alignment_for_parameterized_type_references\" value=\"48\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3MDY2Mw==", "bodyText": "This is now in master and will disappear from the diffs.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369070663", "createdAt": "2020-01-21T15:31:24Z", "author": {"login": "pugnascotia"}, "path": ".eclipseformat.xml", "diffHunk": "@@ -62,7 +62,7 @@\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_space_after_opening_angle_bracket_in_type_arguments\" value=\"do not insert\"/>\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_new_line_after_annotation_on_method\" value=\"insert\"/>\n         <setting id=\"org.eclipse.jdt.core.formatter.insert_space_after_opening_paren_in_switch\" value=\"do not insert\"/>\n-        <setting id=\"org.eclipse.jdt.core.formatter.alignment_for_parameterized_type_references\" value=\"0\"/>\n+        <setting id=\"org.eclipse.jdt.core.formatter.alignment_for_parameterized_type_references\" value=\"48\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MTQ3NA=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQxMjQwOnYy", "diffSide": "RIGHT", "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxMzo1ODo1NVrOFdYWPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxMzo1ODo1NVrOFdYWPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MTkzMw==", "bodyText": "should this be dangling_indices.delete?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366351933", "createdAt": "2020-01-14T13:58:55Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQxNDEzOnYy", "diffSide": "RIGHT", "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxMzo1OToyMVrOFdYXRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxMzo1OToyMVrOFdYXRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MjE5OQ==", "bodyText": "indexUUID", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366352199", "createdAt": "2020-01-14T13:59:21Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {\n+    \"documentation\": {\n+      \"description\": \"Deletes the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",\n+    \"url\": {\n+      \"paths\": [\n+        {\n+          \"path\": \"/_dangling/{indexUuid}\",\n+          \"methods\": [\n+            \"DELETE\"\n+          ],\n+          \"parts\": {\n+            \"indexUuid\": {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQxNjY1OnYy", "diffSide": "RIGHT", "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDowMDoxMVrOFdYY9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDowMDoxMVrOFdYY9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MjYyOA==", "bodyText": "why is a body required for deletion?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366352628", "createdAt": "2020-01-14T14:00:11Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {\n+    \"documentation\": {\n+      \"description\": \"Deletes the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",\n+    \"url\": {\n+      \"paths\": [\n+        {\n+          \"path\": \"/_dangling/{indexUuid}\",\n+          \"methods\": [\n+            \"DELETE\"\n+          ],\n+          \"parts\": {\n+            \"indexUuid\": {\n+              \"type\": \"string\",\n+              \"description\": \"The UUID of the dangling index\"\n+            }\n+          }\n+        }\n+      ]\n+    },\n+    \"params\": {\n+      \"accept_data_loss\": {\n+        \"type\": \"boolean\",\n+        \"description\": \"Must be set to true in order to delete the dangling index\"\n+      }\n+    },\n+    \"body\": {\n+      \"description\": \"Supplies options to the request\",\n+      \"required\": true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQyMjExOnYy", "diffSide": "RIGHT", "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.restore.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDowMTo0NFrOFdYcMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDowMTo0NFrOFdYcMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1MzQ1Ng==", "bodyText": "Perhaps use the Import terminology here instead of Restore? That's how we have been describing the reintroduction of dangling indices into the cluster.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366353456", "createdAt": "2020-01-14T14:01:44Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.restore.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {\n+    \"documentation\": {\n+      \"description\": \"Restores the specified dangling index\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQyNzMzOnYy", "diffSide": "RIGHT", "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.restore.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDowMzoxOVrOFdYfZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDowMzoxOVrOFdYfZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1NDI3Nw==", "bodyText": "why is a body required?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366354277", "createdAt": "2020-01-14T14:03:19Z", "author": {"login": "ywelsch"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.restore.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.restore\": {\n+    \"documentation\": {\n+      \"description\": \"Restores the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",\n+    \"url\": {\n+      \"paths\": [\n+        {\n+          \"path\": \"/_dangling/{indexUuid}\",\n+          \"methods\": [\n+            \"POST\"\n+          ],\n+          \"parts\": {\n+            \"indexUuid\": {\n+              \"type\": \"string\",\n+              \"description\": \"The UUID of the dangling index\"\n+            }\n+          }\n+        }\n+      ]\n+    },\n+    \"params\": {\n+      \"accept_data_loss\": {\n+        \"type\": \"boolean\",\n+        \"description\": \"Must be set to true in order to restore the dangling index\"\n+      }\n+    },\n+    \"body\": {\n+      \"description\": \"Supplies options to the request\",\n+      \"required\": true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQ0OTU3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDowOTo1N1rOFdYs_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDowOTo1N1rOFdYs_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1Nzc1OQ==", "bodyText": "let's also add the index.creation_date here, i.e. IndexMetaData.getCreationDate().", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366357759", "createdAt": "2020-01-14T14:09:57Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {\n+    private String indexName;\n+    private String indexUUID;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQ1NzkwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxMTo1OFrOFdYxtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxMTo1OFrOFdYxtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM1ODk2NQ==", "bodyText": "Should we also output node name here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366358965", "createdAt": "2020-01-14T14:11:58Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {\n+    private String indexName;\n+    private String indexUUID;\n+\n+    public DanglingIndexInfo(DiscoveryNode node, String indexName, String indexUUID) {\n+        super(node);\n+        this.indexName = indexName;\n+        this.indexUUID = indexUUID;\n+    }\n+\n+    public DanglingIndexInfo(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexName = in.readString();\n+        this.indexUUID = in.readString();\n+    }\n+\n+    public String getIndexName() {\n+        return indexName;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public String getNodeId() {\n+        return this.getNode().getId();\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.field(\"nodeId\", this.getNodeId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQ2ODgyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxNDozMlrOFdY38w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxNDozMlrOFdY38w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MDU2Mw==", "bodyText": "would be more convenient as a URL parameter. Deleting/Importing dangling indices should not require a body.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366360563", "createdAt": "2020-01-14T14:14:32Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.master.MasterNodeRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+/**\n+ * Represents a request to delete a particular dangling index, specified by its UUID. The {@link #acceptDataLoss}\n+ * flag must also be explicitly set to true, or later validation will fail.\n+ */\n+public class DeleteDanglingIndexRequest extends MasterNodeRequest<DeleteDanglingIndexRequest> {\n+    private String indexUuid;\n+    private boolean acceptDataLoss = false;\n+\n+    public DeleteDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUuid = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+    }\n+\n+    public DeleteDanglingIndexRequest() {\n+        super();\n+    }\n+\n+    public DeleteDanglingIndexRequest(String indexUuid, boolean acceptDataLoss) {\n+        super();\n+        this.indexUuid = indexUuid;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUuid == null) {\n+            ActionRequestValidationException e = new ActionRequestValidationException();\n+            e.addValidationError(\"No index ID specified\");\n+            return e;\n+        }\n+\n+        return null;\n+    }\n+\n+    public String getIndexUuid() {\n+        return indexUuid;\n+    }\n+\n+    public void setIndexUuid(String indexUuid) {\n+        this.indexUuid = indexUuid;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    public void setAcceptDataLoss(boolean acceptDataLoss) {\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"delete dangling index\";\n+    }\n+\n+    public void source(Map<String, Object> source) {\n+        source.forEach((name, value) -> {\n+            if (\"accept_data_loss\".equals(name)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQ3MzI5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxNTo0OVrOFdY6kQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0MTozMlrOFf-qvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTIzMw==", "bodyText": "why the extra \"status\" field?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366361233", "createdAt": "2020-01-14T14:15:49Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3NjkyNg==", "bodyText": "I was trying to keep this the same as something, but I can't remember what. I checked what we send back when we delete a regular index, and replaced this with acknowledged: true. This isn't strictly necessary, but I feel that it's nice to give feedback e.g. to curl users.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369076926", "createdAt": "2020-01-21T15:41:32Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTIzMw=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQ3NTQyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxNjoyM1rOFdY71w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxNjoyM1rOFdY71w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTU1OQ==", "bodyText": "call super here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366361559", "createdAt": "2020-01-14T14:16:23Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQ3NTkzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxNjozMFrOFdY8Hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo1NjozNFrOFf_Qqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTYzMA==", "bodyText": "call super here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366361630", "createdAt": "2020-01-14T14:16:30Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        // no fields to write", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4NjYzNA==", "bodyText": "there is no super method of this - all of the base classes are abstract AFAICT.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369086634", "createdAt": "2020-01-21T15:56:34Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public DeleteDanglingIndexResponse() {\n+    }\n+\n+    public DeleteDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        // no fields to write", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MTYzMA=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQ4NjQ2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxOTo0MFrOFdZC2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoxOTo0MFrOFdZC2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2MzM1NQ==", "bodyText": "might be convenient later to allow selecting subset of nodes here\nPerhaps use (String[]) null for now like the other usages?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366363355", "createdAt": "2020-01-14T14:19:40Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+\n+import java.io.IOException;\n+\n+public class ListDanglingIndicesRequest extends BaseNodesRequest<ListDanglingIndicesRequest> {\n+    public ListDanglingIndicesRequest(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesRequest() {\n+        super(new String[0]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzQ5NjU5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyMjoxNFrOFdZIrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjoyMTo1NFrOFgANQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NDg0Nw==", "bodyText": "Why SERVICE_UNAVAILABLE? Should this not just be a 500?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366364847", "createdAt": "2020-01-14T14:22:14Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3NzU2OA==", "bodyText": "I was trying to differentiate between \"the server couldn't handle the request\", and \"some nodes couldn't handle the request\". What do you think, is that overkill?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369077568", "createdAt": "2020-01-21T15:42:32Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NDg0Nw=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEwMjE0Ng==", "bodyText": "Yes, I think that's overkill, or at least inconsistent with other APIs. HTTP doesn't have a good way to indicate with a status code that something only partly worked, so we prefer 200 OK to indicate that the coordinating node did its thing successfully, and if it encountered node-level failures then they are already included in the response by RestActions#buildNodesHeader.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369102146", "createdAt": "2020-01-21T16:21:54Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NDg0Nw=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzUwNDQxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyNDoxMlrOFdZNYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyNDoxMlrOFdZNYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NjA0OA==", "bodyText": "The full IndexMetaData might be large, and collecting all of these on a coordinator node might lead to a lot of data transfer and make that node blow up. Let's just return what's needed, index uuid and name and creation date.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366366048", "createdAt": "2020-01-14T14:24:12Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesResponse extends BaseNodeResponse {\n+    private final List<IndexMetaData> indexMetaData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzUwNjQ1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyNDo1MVrOFdZOog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyNDo1MVrOFdZOog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NjM3MA==", "bodyText": "use in.readList", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366366370", "createdAt": "2020-01-14T14:24:51Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesResponse extends BaseNodeResponse {\n+    private final List<IndexMetaData> indexMetaData;\n+\n+    public List<IndexMetaData> getDanglingIndices() {\n+        return this.indexMetaData;\n+    }\n+\n+    public NodeDanglingIndicesResponse(DiscoveryNode node, List<IndexMetaData> indexMetaData) {\n+        super(node);\n+        this.indexMetaData = indexMetaData;\n+    }\n+\n+    protected NodeDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+\n+        final int size = in.readInt();\n+        this.indexMetaData = new ArrayList<>(size);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzUwNzE4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyNTowM1rOFdZPEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyNTowM1rOFdZPEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NjQ4Mg==", "bodyText": "use out.writeList", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366366482", "createdAt": "2020-01-14T14:25:03Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesResponse extends BaseNodeResponse {\n+    private final List<IndexMetaData> indexMetaData;\n+\n+    public List<IndexMetaData> getDanglingIndices() {\n+        return this.indexMetaData;\n+    }\n+\n+    public NodeDanglingIndicesResponse(DiscoveryNode node, List<IndexMetaData> indexMetaData) {\n+        super(node);\n+        this.indexMetaData = indexMetaData;\n+    }\n+\n+    protected NodeDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+\n+        final int size = in.readInt();\n+        this.indexMetaData = new ArrayList<>(size);\n+\n+        for (int i = 0; i < size; i++) {\n+            this.indexMetaData.add(IndexMetaData.readFrom(in));\n+        }\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+\n+        out.writeInt(this.indexMetaData.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzUxMzgwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/RestoreDanglingIndexRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyNjo1OFrOFdZTJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyNjo1OFrOFdZTJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2NzUyNQ==", "bodyText": "similar to accept_data_loss flag, I wonder if the node id could just be URL parameter. In this case, it's less clear if we want to add other parameters.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366367525", "createdAt": "2020-01-14T14:26:58Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/RestoreDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+/**\n+ * Represents a request to restore a particular dangling index, specified\n+ * by its UUID and optionally the node ID, if the dangling index exists on\n+ * more than one node. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class RestoreDanglingIndexRequest extends BaseNodesRequest<RestoreDanglingIndexRequest> {\n+    private String indexUuid;\n+    private boolean acceptDataLoss;\n+    private String nodeId;\n+\n+    public RestoreDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUuid = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+        this.nodeId = in.readOptionalString();\n+    }\n+\n+    public RestoreDanglingIndexRequest() {\n+        super(new String[0]);\n+    }\n+\n+    public RestoreDanglingIndexRequest(String indexUuid, boolean acceptDataLoss) {\n+        this();\n+        this.indexUuid = indexUuid;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUuid == null || this.indexUuid.isEmpty()) {\n+            ActionRequestValidationException e = new ActionRequestValidationException();\n+            e.addValidationError(\"No index UUID specified\");\n+            return e;\n+        }\n+\n+        return null;\n+    }\n+\n+    public String getIndexUuid() {\n+        return indexUuid;\n+    }\n+\n+    public void setIndexUuid(String indexUuid) {\n+        this.indexUuid = indexUuid;\n+    }\n+\n+    public String getNodeId() {\n+        return nodeId;\n+    }\n+\n+    public void setNodeId(String nodeId) {\n+        this.nodeId = nodeId;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    public void setAcceptDataLoss(boolean acceptDataLoss) {\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"restore dangling index\";\n+    }\n+\n+    public void source(Map<String, Object> source) {\n+        source.forEach((name, value) -> {\n+            switch (name) {\n+                case \"accept_data_loss\":\n+                    if (value instanceof Boolean) {\n+                        this.acceptDataLoss = (boolean) value;\n+                    } else {\n+                        throw new IllegalArgumentException(\"malformed accept_data_loss\");\n+                    }\n+                    break;\n+\n+                case \"node_id\":", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzUyMTkyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDoyOToxOFrOFdZYKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QxNjoxMToxN1rOFpZ-Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2ODgwOQ==", "bodyText": "this will lead to a lot of duplicated entries, if many nodes contain the same dangling index. I wonder if we should aggregate this somehow?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366368809", "createdAt": "2020-01-14T14:29:18Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEwMzMzOQ==", "bodyText": "++\nAs a user, I am likely to care more about the indices than the nodes on which they live, so I think it would make more sense to aggregate this by index UUID and list the nodes under each index. Note that the index name and creation date are immutable for a given index (UUID) so we only need include them once.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369103339", "createdAt": "2020-01-21T16:23:49Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2ODgwOQ=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODk2MTQzOQ==", "bodyText": "I've reworked the response shape, and added a REST version of the integration test. It's currently living under qa:smoke-test-http, which I appreciate isn't ideal. At least the REST code is now tested.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r378961439", "createdAt": "2020-02-13T16:11:17Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM2ODgwOQ=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzUzNDg1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDozMjozOVrOFdZf7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0MDowNlrOFf-nJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM3MDc5Ng==", "bodyText": "Why tie this into the graveyard? I was wondering if we should just reach out to all the nodes and delete the on-disk index folder, after checking that the index with the given UUID does not exist in the cluster state. The advantage is that this would not need to involve the master. @DaveCTurner thoughts?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366370796", "createdAt": "2020-01-14T14:32:39Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3NjAwNw==", "bodyText": "I think we do indeed want to put it in the index graveyard, so that we don't need to be sure that all the nodes on which the index still exists are in the cluster at the time. I think it would be surprising to delete a dangling index, get a 200 OK response, and yet the dangling index persists because of a network blip or a node that's still starting.\nI don't see a problem with involving the master here - these updates will be pretty low-volume.\nI do, however, see that this will make it tricky to completely remove the index graveyard in future. I think that's ok, we can still stop using the index graveyard for indices that we're certain were deleted.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369076007", "createdAt": "2020-01-21T15:40:06Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM3MDc5Ng=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzU0MzIyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDozNDo0NlrOFdZkuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDozNDo0NlrOFdZkuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM3MjAyNw==", "bodyText": "Isn't the request already validated earlier?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366372027", "createdAt": "2020-01-14T14:34:46Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.WRITE;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);\n+                }\n+\n+                @Override\n+                public ClusterState execute(final ClusterState currentState) {\n+                    return deleteDanglingIndex(currentState, indexMetaDataToDelete);\n+                }\n+            }\n+        );\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, IndexMetaData indexMetaDataToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexMetaDataToDelete.getIndex()).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToDelete(DeleteDanglingIndexRequest request) {\n+        String indexUuid = request.getIndexUuid();\n+\n+        if (indexUuid == null || indexUuid.isEmpty()) {\n+            throw new IllegalArgumentException(\"No index UUID specified in request\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 168}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzYwNTI4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDo1MToyMlrOFdaKFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDo1MToyMlrOFdaKFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4MTU4OQ==", "bodyText": "why the write thread pool? That's usually used for indexing", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366381589", "createdAt": "2020-01-14T14:51:22Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.WRITE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzYwNzYzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDo1MTo1NlrOFdaLhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNDo1MTo1NlrOFdaLhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4MTk1Nw==", "bodyText": "ouch, why do a blocking call here?\nWhy query the full list of dangling indices just to delete one?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366381957", "createdAt": "2020-01-14T14:51:56Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.WRITE;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);\n+                }\n+\n+                @Override\n+                public ClusterState execute(final ClusterState currentState) {\n+                    return deleteDanglingIndex(currentState, indexMetaDataToDelete);\n+                }\n+            }\n+        );\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, IndexMetaData indexMetaDataToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexMetaDataToDelete.getIndex()).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToDelete(DeleteDanglingIndexRequest request) {\n+        String indexUuid = request.getIndexUuid();\n+\n+        if (indexUuid == null || indexUuid.isEmpty()) {\n+            throw new IllegalArgumentException(\"No index UUID specified in request\");\n+        }\n+\n+        List<IndexMetaData> matchingMetaData = new ArrayList<>();\n+\n+        final List<NodeDanglingIndicesResponse> nodes = fetchDanglingIndices().actionGet().getNodes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2MzY0MTcyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNFQxNTowMDoxOFrOFdagQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo1MjoxNFrOFvNsRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4NzI2NQ==", "bodyText": "This means that dangling indices scanning is still done on every cluster state update. One of the benefits of this PR was that this now would only needed to be done on-demand, i.e. when a user requests the list of dangling indices, not on every CS update.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r366387265", "createdAt": "2020-01-14T15:00:18Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -65,24 +69,25 @@\n \n     private final NodeEnvironment nodeEnv;\n     private final MetaStateService metaStateService;\n-    private final LocalAllocateDangledIndices allocateDangledIndices;\n+    private final LocalAllocateDangledIndices danglingIndicesAllocator;\n     private final boolean isAutoImportDanglingIndicesEnabled;\n \n     private final Map<Index, IndexMetaData> danglingIndices = ConcurrentCollections.newConcurrentMap();\n \n     @Inject\n     public DanglingIndicesState(NodeEnvironment nodeEnv, MetaStateService metaStateService,\n-                                LocalAllocateDangledIndices allocateDangledIndices, ClusterService clusterService) {\n+                                LocalAllocateDangledIndices danglingIndicesAllocator, ClusterService clusterService) {\n         this.nodeEnv = nodeEnv;\n         this.metaStateService = metaStateService;\n-        this.allocateDangledIndices = allocateDangledIndices;\n+        this.danglingIndicesAllocator = danglingIndicesAllocator;\n \n         this.isAutoImportDanglingIndicesEnabled = AUTO_IMPORT_DANGLING_INDICES_SETTING.get(clusterService.getSettings());\n \n-        if (this.isAutoImportDanglingIndicesEnabled) {\n-            clusterService.addListener(this);\n-        } else {\n-            logger.warn(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey() + \" is disabled, dangling indices will not be detected or imported\");\n+        clusterService.addListener(this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MTcxNw==", "bodyText": "This still needs addressing. I agree that we can hunt for dangling indices on demand for the List action, and also load just the specific metadata needed for the Find action. The DanglingIndicesState is effectively a cache, but I don't think we need to cache anything.\nWe could follow-up with something that checks for dangling indices at startup or periodically if we feel it's needed.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385051717", "createdAt": "2020-02-27T10:52:14Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -65,24 +69,25 @@\n \n     private final NodeEnvironment nodeEnv;\n     private final MetaStateService metaStateService;\n-    private final LocalAllocateDangledIndices allocateDangledIndices;\n+    private final LocalAllocateDangledIndices danglingIndicesAllocator;\n     private final boolean isAutoImportDanglingIndicesEnabled;\n \n     private final Map<Index, IndexMetaData> danglingIndices = ConcurrentCollections.newConcurrentMap();\n \n     @Inject\n     public DanglingIndicesState(NodeEnvironment nodeEnv, MetaStateService metaStateService,\n-                                LocalAllocateDangledIndices allocateDangledIndices, ClusterService clusterService) {\n+                                LocalAllocateDangledIndices danglingIndicesAllocator, ClusterService clusterService) {\n         this.nodeEnv = nodeEnv;\n         this.metaStateService = metaStateService;\n-        this.allocateDangledIndices = allocateDangledIndices;\n+        this.danglingIndicesAllocator = danglingIndicesAllocator;\n \n         this.isAutoImportDanglingIndicesEnabled = AUTO_IMPORT_DANGLING_INDICES_SETTING.get(clusterService.getSettings());\n \n-        if (this.isAutoImportDanglingIndicesEnabled) {\n-            clusterService.addListener(this);\n-        } else {\n-            logger.warn(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey() + \" is disabled, dangling indices will not be detected or imported\");\n+        clusterService.addListener(this);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjM4NzI2NQ=="}, "originalCommit": {"oid": "a0aa2366a228038d72442514bb80496174ee6d50"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTA1MzgzOnYy", "diffSide": "RIGHT", "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0MjozN1rOFf-tcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0MjozN1rOFf-tcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3NzYxOQ==", "bodyText": "I think we can be less tentative about this API, unless you know of some specific future changes needed to move it out of an experimental state (in which case, what are they?).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369077619", "createdAt": "2020-01-21T15:42:37Z", "author": {"login": "DaveCTurner"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.delete.json", "diffHunk": "@@ -0,0 +1,30 @@\n+{\n+  \"dangling_indices.delete\": {\n+    \"documentation\": {\n+      \"description\": \"Deletes the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTA1NDk0OnYy", "diffSide": "RIGHT", "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.import.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0Mjo1N1rOFf-uPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0Mjo1N1rOFf-uPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3NzgyMw==", "bodyText": "I think we can be less tentative about this API, unless you know of some specific future changes needed to move it out of an experimental state (in which case, what are they?).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369077823", "createdAt": "2020-01-21T15:42:57Z", "author": {"login": "DaveCTurner"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.import.json", "diffHunk": "@@ -0,0 +1,34 @@\n+{\n+  \"dangling_indices.import\": {\n+    \"documentation\": {\n+      \"description\": \"Imports the specified dangling index\"\n+    },\n+    \"stability\": \"experimental\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTA1NzM2OnYy", "diffSide": "RIGHT", "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.list.json", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0MzozM1rOFf-vvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0MzozM1rOFf-vvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA3ODIwNA==", "bodyText": "I think we can be less tentative about this API, unless you know of some specific future changes needed to move it out of an experimental state (in which case, what are they?).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369078204", "createdAt": "2020-01-21T15:43:33Z", "author": {"login": "DaveCTurner"}, "path": "rest-api-spec/src/main/resources/rest-api-spec/api/dangling_indices.list.json", "diffHunk": "@@ -0,0 +1,19 @@\n+{\n+  \"dangling_indices.list\":{\n+    \"documentation\":{\n+      \"description\":\"Returns all dangling indices.\"\n+    },\n+    \"stability\":\"experimental\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTA4MjIyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0OToyOFrOFf--lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo0OToyOFrOFf--lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4MjAwNQ==", "bodyText": "StreamInput#readString and StreamOutput#writeString do not deal in nulls, so request that's this invalid can't even be (de)serialised. Could we drop the default constructor and the setters, make the fields final, use Objects.requireNonNull(indexUUID) in the 2-argument constructor, and avoid dealing with nulls at all?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369082005", "createdAt": "2020-01-21T15:49:28Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.master.MasterNodeRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to delete a particular dangling index, specified by its UUID. The {@link #acceptDataLoss}\n+ * flag must also be explicitly set to true, or later validation will fail.\n+ */\n+public class DeleteDanglingIndexRequest extends MasterNodeRequest<DeleteDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss = false;\n+\n+    public DeleteDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+    }\n+\n+    public DeleteDanglingIndexRequest() {\n+        super();\n+    }\n+\n+    public DeleteDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        super();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTA5NDY3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo1MjozM1rOFf_GOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNTo1MjozM1rOFf_GOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4Mzk2Mw==", "bodyText": "could we include the field values here? An IDE-generated toString() method is normally fine.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369083963", "createdAt": "2020-01-21T15:52:33Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.master.MasterNodeRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to delete a particular dangling index, specified by its UUID. The {@link #acceptDataLoss}\n+ * flag must also be explicitly set to true, or later validation will fail.\n+ */\n+public class DeleteDanglingIndexRequest extends MasterNodeRequest<DeleteDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss = false;\n+\n+    public DeleteDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+    }\n+\n+    public DeleteDanglingIndexRequest() {\n+        super();\n+    }\n+\n+    public DeleteDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        super();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null) {\n+            ActionRequestValidationException e = new ActionRequestValidationException();\n+            e.addValidationError(\"No index UUID specified\");\n+            return e;\n+        }\n+\n+        // acceptDataLoss is validated later in the transport action, so that the API call can\n+        // be made to check that the UUID exists.\n+\n+        return null;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public void setIndexUUID(String indexUUID) {\n+        this.indexUUID = indexUUID;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    public void setAcceptDataLoss(boolean acceptDataLoss) {\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"delete dangling index\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07638ed35ca9f38e56186792e2dd821158347eff"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTEzMjA1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexRequest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjowMToyMFrOFf_ckQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxNToxNjozOFrOFjutuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4OTY4MQ==", "bodyText": "Serialization doesn't work if indexUUID is null, so let's exclude this at construction time. I think we can lose the setters and the default constructor and the non-final fields here as well. I also think we can survive without checking this.indexUUID.isEmpty() - if you pass an empty UUID here we'll just fail to find a matching index later on, right?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369089681", "createdAt": "2020-01-21T16:01:20Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to import a particular dangling index, specified\n+ * by its UUID and optionally the node ID, if the dangling index exists on\n+ * more than one node. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class ImportDanglingIndexRequest extends BaseNodesRequest<ImportDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss;\n+    private String nodeId;\n+\n+    public ImportDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+        this.nodeId = in.readOptionalString();\n+    }\n+\n+    public ImportDanglingIndexRequest() {\n+        super(new String[0]);\n+    }\n+\n+    public ImportDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        this();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null || this.indexUUID.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzAwOTg0OQ==", "bodyText": "Yes, but IMO it would indicate an error somewhere and I'd rather fail fast.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r373009849", "createdAt": "2020-01-30T15:16:38Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to import a particular dangling index, specified\n+ * by its UUID and optionally the node ID, if the dangling index exists on\n+ * more than one node. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class ImportDanglingIndexRequest extends BaseNodesRequest<ImportDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss;\n+    private String nodeId;\n+\n+    public ImportDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+        this.nodeId = in.readOptionalString();\n+    }\n+\n+    public ImportDanglingIndexRequest() {\n+        super(new String[0]);\n+    }\n+\n+    public ImportDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        this();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null || this.indexUUID.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA4OTY4MQ=="}, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTEzOTg1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjowMzoyMlrOFf_hZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjowMzoyMlrOFf_hZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5MDkxNg==", "bodyText": "Could we add the values of the fields too? Whatever your IDE auto-generates for a toString() is probably fine.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369090916", "createdAt": "2020-01-21T16:03:22Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to import a particular dangling index, specified\n+ * by its UUID and optionally the node ID, if the dangling index exists on\n+ * more than one node. The {@link #acceptDataLoss} flag must also be\n+ * explicitly set to true, or later validation will fail.\n+ */\n+public class ImportDanglingIndexRequest extends BaseNodesRequest<ImportDanglingIndexRequest> {\n+    private String indexUUID;\n+    private boolean acceptDataLoss;\n+    private String nodeId;\n+\n+    public ImportDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+        this.nodeId = in.readOptionalString();\n+    }\n+\n+    public ImportDanglingIndexRequest() {\n+        super(new String[0]);\n+    }\n+\n+    public ImportDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        this();\n+        this.indexUUID = indexUUID;\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        if (this.indexUUID == null || this.indexUUID.isEmpty()) {\n+            ActionRequestValidationException e = new ActionRequestValidationException();\n+            e.addValidationError(\"No index UUID specified\");\n+            return e;\n+        }\n+\n+        // acceptDataLoss is validated later in the transport action, so that the API call can\n+        // be made to check that the UUID exists.\n+\n+        return null;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public void setIndexUUID(String indexUUID) {\n+        this.indexUUID = indexUUID;\n+    }\n+\n+    public String getNodeId() {\n+        return nodeId;\n+    }\n+\n+    public void setNodeId(String nodeId) {\n+        this.nodeId = nodeId;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    public void setAcceptDataLoss(boolean acceptDataLoss) {\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"import dangling index\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTE0ODkzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjowNTo0M1rOFf_m-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjowNTo0M1rOFf_m-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5MjM0NQ==", "bodyText": "acknowledged: true would be more consistent with the other APIs I think.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369092345", "createdAt": "2020-01-21T16:05:43Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class ImportDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public ImportDanglingIndexResponse() {\n+    }\n+\n+    public ImportDanglingIndexResponse(StreamInput in) {\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"status\", \"ok\").endObject();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTE0OTk0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjowNjowNFrOFf_nsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjowNjowNFrOFf_nsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5MjUzMA==", "bodyText": "I think this needs a super(in); too.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369092530", "createdAt": "2020-01-21T16:06:04Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class ImportDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public ImportDanglingIndexResponse() {\n+    }\n+\n+    public ImportDanglingIndexResponse(StreamInput in) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTE2ODYyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjoxMDo1N1rOFf_zUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjoxMDo1N1rOFf_zUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5NTUwNA==", "bodyText": "I think I'd prefer this to a null here:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    super((String[]) null);\n          \n          \n            \n                    super(Strings.EMPTY_ARRAY);\n          \n      \n    \n    \n  \n\nnulls get converted to empty arrays when serialised, and this is a little trappy.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369095504", "createdAt": "2020-01-21T16:10:57Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+\n+import java.io.IOException;\n+\n+public class ListDanglingIndicesRequest extends BaseNodesRequest<ListDanglingIndicesRequest> {\n+    public ListDanglingIndicesRequest(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesRequest() {\n+        super((String[]) null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTE3NjA3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjoxMzowM1rOFf_4Iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjoxMzowM1rOFf_4Iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5NjczOQ==", "bodyText": "I don't think we need to override this, the base class already has a no-op validate() method.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369096739", "createdAt": "2020-01-21T16:13:03Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+\n+import java.io.IOException;\n+\n+public class ListDanglingIndicesRequest extends BaseNodesRequest<ListDanglingIndicesRequest> {\n+    public ListDanglingIndicesRequest(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesRequest() {\n+        super((String[]) null);\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTE4NTgwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjoxNTo0NVrOFf_-UQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjoxNTo0NVrOFf_-UQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTA5ODMyMQ==", "bodyText": "Could we use the class name here? I think something that looks a bit too English might catch someone out if it ever appears in a log message or similar.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369098321", "createdAt": "2020-01-21T16:15:45Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.nodes.BaseNodesRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+\n+import java.io.IOException;\n+\n+public class ListDanglingIndicesRequest extends BaseNodesRequest<ListDanglingIndicesRequest> {\n+    public ListDanglingIndicesRequest(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesRequest() {\n+        super((String[]) null);\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        return null;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"list dangling indices\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTIxOTAzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjoyNDozM1rOFgATvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxNjowNjo0N1rOFjwoMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEwMzgwNw==", "bodyText": "I think we already include this automatically thanks to RestActions#buildNodesHeader", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369103807", "createdAt": "2020-01-21T16:24:33Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID(),\n+                    indexMetaData.getCreationDate()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);\n+            }\n+        }\n+        builder.endArray();\n+\n+        if (this.hasFailures()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzAxMDg3NA==", "bodyText": "I couldn't see how that method would be invoked for the new API, but I've swapped my implementation for a call to buildNodesHeader.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r373010874", "createdAt": "2020-01-30T15:18:17Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID(),\n+                    indexMetaData.getCreationDate()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);\n+            }\n+        }\n+        builder.endArray();\n+\n+        if (this.hasFailures()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEwMzgwNw=="}, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA0MTIwMg==", "bodyText": "Oh I see, I think we need this:\ndiff --git a/server/src/main/java/org/elasticsearch/rest/action/admin/cluster/dangling/RestListDanglingIndicesAction.java b/server/src/main/java/org/elasticsearch/rest/action/admin/cluster/dangling/RestListDanglingIndicesAction.java\nindex 55370801e64..4604b558668 100644\n--- a/server/src/main/java/org/elasticsearch/rest/action/admin/cluster/dangling/RestListDanglingIndicesAction.java\n+++ b/server/src/main/java/org/elasticsearch/rest/action/admin/cluster/dangling/RestListDanglingIndicesAction.java\n@@ -43,6 +43,6 @@ public class RestListDanglingIndicesAction extends BaseRestHandler {\n     @Override\n     public BaseRestHandler.RestChannelConsumer prepareRequest(final RestRequest request, NodeClient client) throws IOException {\n         final ListDanglingIndicesRequest danglingIndicesRequest = new ListDanglingIndicesRequest();\n-        return channel -> client.admin().cluster().listDanglingIndices(danglingIndicesRequest, new RestStatusToXContentListener<>(channel));\n+        return channel -> client.admin().cluster().listDanglingIndices(danglingIndicesRequest, new RestActions.NodesResponseRestListener<>(channel));\n     }\n }", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r373041202", "createdAt": "2020-01-30T16:06:47Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"failed_nodes\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.SERVICE_UNAVAILABLE : RestStatus.OK;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+\n+        builder.startArray(\"dangling_indices\");\n+        for (NodeDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (IndexMetaData indexMetaData : nodeResponse.getDanglingIndices()) {\n+                DanglingIndexInfo danglingIndexInfo = new DanglingIndexInfo(\n+                    nodeResponse.getNode(),\n+                    indexMetaData.getIndex().getName(),\n+                    indexMetaData.getIndexUUID(),\n+                    indexMetaData.getCreationDate()\n+                );\n+                danglingIndexInfo.toXContent(builder, params);\n+            }\n+        }\n+        builder.endArray();\n+\n+        if (this.hasFailures()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEwMzgwNw=="}, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTI2MDczOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjozNToxOVrOFgAtyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjozNToxOVrOFgAtyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExMDQ3NA==", "bodyText": "Inventing a new DeleteIndexRequest() seems inappropriate here. I think we should make DeleteDanglingIndexRequest implement AckedRequest and use request instead.\nAlso you can drop the Priority argument, since NORMAL is the default.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369110474", "createdAt": "2020-01-21T16:35:19Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTI3MDE1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjozNzo0NVrOFgAzlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjozNzo0NVrOFgAzlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExMTk1Nw==", "bodyText": "Since we're using an AckedClusterStateUpdateTask we should pass clusterStateUpdateResponse.isAcknowledged() back to the caller. Alternatively, if we don't care about whether all the nodes acked the request or not then we can just use a plain ClusterStateUpdateTask below.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369111957", "createdAt": "2020-01-21T16:37:45Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTI3NjQ3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjozOToyNlrOFgA3hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjozOToyNlrOFgA3hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExMjk2NQ==", "bodyText": "This looks to be parametric in the response type. Can we use DeleteDanglingIndexResponse instead of ClusterStateUpdateResponse?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369112965", "createdAt": "2020-01-21T16:39:26Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTI4MDI3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo0MDozNVrOFgA56w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo0MDozNVrOFgA56w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExMzU3OQ==", "bodyText": "I think we need only return the Index here, not the full IndexMetadata.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369113579", "createdAt": "2020-01-21T16:40:35Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);\n+                }\n+\n+                @Override\n+                public ClusterState execute(final ClusterState currentState) {\n+                    return deleteDanglingIndex(currentState, indexMetaDataToDelete);\n+                }\n+            }\n+        );\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, IndexMetaData indexMetaDataToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexMetaDataToDelete.getIndex()).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToDelete(DeleteDanglingIndexRequest request) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTI5MjY0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo0Mzo1NFrOFgBBuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo0Mzo1NFrOFgBBuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExNTU3OQ==", "bodyText": "Rather than returning an ActionFuture here, pass in an ActionListener<...> and avoid blocking a thread on the response.\nMoreover I think it would be better to use a different action that focusses only on the index UUID we're interested in here, or else adapt the listing action to allow it to focus on a single UUID. Listing everything just to delete the one thing seems like it might cause trouble.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369115579", "createdAt": "2020-01-21T16:43:54Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                listener.onFailure(e);\n+            }\n+        };\n+\n+        // This flag is checked at this point so that we always check that the supplied index ID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.clusterService.submitStateUpdateTask(\n+            \"delete-dangling-index \" + indexName,\n+            new AckedClusterStateUpdateTask<>(Priority.NORMAL, new DeleteIndexRequest(), actionListener) {\n+\n+                @Override\n+                protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\n+                    return new ClusterStateUpdateResponse(acknowledged);\n+                }\n+\n+                @Override\n+                public ClusterState execute(final ClusterState currentState) {\n+                    return deleteDanglingIndex(currentState, indexMetaDataToDelete);\n+                }\n+            }\n+        );\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, IndexMetaData indexMetaDataToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexMetaDataToDelete.getIndex()).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToDelete(DeleteDanglingIndexRequest request) {\n+        String indexUUID = request.getIndexUUID();\n+\n+        List<IndexMetaData> matchingMetaData = new ArrayList<>();\n+\n+        final List<NodeDanglingIndicesResponse> nodes = fetchDanglingIndices().actionGet().getNodes();\n+\n+        for (NodeDanglingIndicesResponse response : nodes) {\n+            for (IndexMetaData danglingIndex : response.getDanglingIndices()) {\n+                if (danglingIndex.getIndexUUID().equals(indexUUID)) {\n+                    matchingMetaData.add(danglingIndex);\n+                }\n+            }\n+        }\n+\n+        if (matchingMetaData.isEmpty()) {\n+            throw new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\");\n+        }\n+\n+        // Although we could find metadata for the same index on multiple nodes, we return the first\n+        // metadata here because only the index part goes into the graveyard, which is basically the\n+        // name and UUID.\n+        return matchingMetaData.get(0);\n+    }\n+\n+    private ActionFuture<ListDanglingIndicesResponse> fetchDanglingIndices() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTI5NjM1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo0NDo1OFrOFgBEOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo0NDo1OFrOFgBEOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExNjIxOA==", "bodyText": "We pass the exception back to the caller, so I think we need log this at no higher than DEBUG.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369116218", "createdAt": "2020-01-21T16:44:58Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+        final String indexName = indexMetaDataToDelete.getIndex().getName();\n+\n+        final ActionListener<ClusterStateUpdateResponse> actionListener = new ActionListener<>() {\n+            @Override\n+            public void onResponse(ClusterStateUpdateResponse clusterStateUpdateResponse) {\n+                listener.onResponse(new DeleteDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.warn(\"Failed to delete dangling index [{}]\" + indexName, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTMxMjY0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo0OToyNFrOFgBOrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo0OToyNFrOFgBOrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTExODg5Mw==", "bodyText": "I think we might as well let exceptions be thrown, since this method is called from within ActionRunnable.wrap which routes exceptions to the listener in any case.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369118893", "createdAt": "2020-01-21T16:49:24Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ack.ClusterStateUpdateResponse;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest request,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> listener\n+    ) throws Exception {\n+        IndexMetaData indexMetaDataToDelete;\n+        try {\n+            indexMetaDataToDelete = getIndexMetaDataToDelete(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTMzNjA4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo1NTo0NFrOFgBdUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo1NTo0NFrOFgBdUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEyMjY0MA==", "bodyText": "Similarly here, we should be doing this in an async fashion with an ActionListener rather than blocking, and should only focus on the index in question.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369122640", "createdAt": "2020-01-21T16:55:44Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+\n+    private final TransportService transportService;\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.transportService = transportService;\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, ImportDanglingIndexRequest request, ActionListener<ImportDanglingIndexResponse> listener) {\n+        IndexMetaData indexMetaDataToImport;\n+        try {\n+            indexMetaDataToImport = getIndexMetaDataToImport(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+\n+        // This flag is checked at this point so that we always check that the supplied index UUID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+            @Override\n+            public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                listener.onResponse(new ImportDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                listener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToImport(ImportDanglingIndexRequest request) {\n+        String indexUUID = request.getIndexUUID();\n+        String nodeId = request.getNodeId();\n+\n+        List<IndexMetaData> matchingMetaData = new ArrayList<>();\n+\n+        for (NodeDanglingIndicesResponse response : fetchDanglingIndices().actionGet().getNodes()) {\n+            for (IndexMetaData danglingIndex : response.getDanglingIndices()) {\n+                if (danglingIndex.getIndexUUID().equals(indexUUID) && (nodeId == null || response.getNode().getId().equals(nodeId))) {\n+                    matchingMetaData.add(danglingIndex);\n+                }\n+            }\n+        }\n+\n+        if (matchingMetaData.isEmpty()) {\n+            throw new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\");\n+        }\n+\n+        if (matchingMetaData.size() > 1) {\n+            throw new IllegalArgumentException(\n+                \"Multiple nodes contain dangling index [\" + indexUUID + \"]. \" + \"Specify a node ID to import a specific dangling index.\"\n+            );\n+        }\n+\n+        return matchingMetaData.get(0);\n+    }\n+\n+    private ActionFuture<ListDanglingIndicesResponse> fetchDanglingIndices() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTM0OTY4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo1OTowOFrOFgBlmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNjo1OTowOFrOFgBlmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEyNDc2MQ==", "bodyText": "I think it will prove helpful to list the nodes (at least their names and IDs) in this message.\nAlso I think it would be better to automatically select the node holding the index metadata with the highest version rather than asking the user to specify a node ID.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369124761", "createdAt": "2020-01-21T16:59:08Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionFuture;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.PlainActionFuture;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+\n+    private final TransportService transportService;\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.transportService = transportService;\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, ImportDanglingIndexRequest request, ActionListener<ImportDanglingIndexResponse> listener) {\n+        IndexMetaData indexMetaDataToImport;\n+        try {\n+            indexMetaDataToImport = getIndexMetaDataToImport(request);\n+        } catch (Exception e) {\n+            listener.onFailure(e);\n+            return;\n+        }\n+\n+        // This flag is checked at this point so that we always check that the supplied index UUID\n+        // does correspond to a dangling index.\n+        if (request.isAcceptDataLoss() == false) {\n+            throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+        }\n+\n+        this.danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+            @Override\n+            public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                listener.onResponse(new ImportDanglingIndexResponse());\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                listener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private IndexMetaData getIndexMetaDataToImport(ImportDanglingIndexRequest request) {\n+        String indexUUID = request.getIndexUUID();\n+        String nodeId = request.getNodeId();\n+\n+        List<IndexMetaData> matchingMetaData = new ArrayList<>();\n+\n+        for (NodeDanglingIndicesResponse response : fetchDanglingIndices().actionGet().getNodes()) {\n+            for (IndexMetaData danglingIndex : response.getDanglingIndices()) {\n+                if (danglingIndex.getIndexUUID().equals(indexUUID) && (nodeId == null || response.getNode().getId().equals(nodeId))) {\n+                    matchingMetaData.add(danglingIndex);\n+                }\n+            }\n+        }\n+\n+        if (matchingMetaData.isEmpty()) {\n+            throw new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\");\n+        }\n+\n+        if (matchingMetaData.size() > 1) {\n+            throw new IllegalArgumentException(\n+                \"Multiple nodes contain dangling index [\" + indexUUID + \"]. \" + \"Specify a node ID to import a specific dangling index.\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTM2NTM3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/package-info.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNzowMzozMlrOFgBvpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNzowMzozMlrOFgBvpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEyNzMzNA==", "bodyText": "\ud83d\udcaf for package-level javadocs \ud83d\ude00", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369127334", "createdAt": "2020-01-21T17:03:32Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/package-info.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+/**\n+ * Dangling indices are indices that exist on disk on one or more nodes but\n+ * which do not currently exist in the cluster state. They arise in a\n+ * number of situations, such as:\n+ *\n+ * <ul>\n+ * <li>A user overflows the index graveyard by deleting more than 500 indices while a node is offline and then the node rejoins the\n+ * cluster</li>\n+ * <li>A node (unsafely) moves from one cluster to another, perhaps because the original cluster lost all its master nodes</li>\n+ * <li>A user (unsafely) meddles with the contents of the data path, maybe restoring an old index folder from a backup</li>\n+ * <li>A disk partially fails and the user has no replicas and no snapshots and wants to (unsafely) recover whatever they can</li>\n+ * <li>A cluster loses all master nodes and those are (unsafely) restored from backup, but the backup does not contain the index</li>\n+ * </ul>\n+ *\n+ * <p>The classes in this package form an API for managing dangling\n+ * indices, allowing them to be listed, imported or deleted.\n+ */\n+package org.elasticsearch.action.admin.indices.dangling;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTM4NjExOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNzowOTo1MFrOFgB8Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNzowOTo1MFrOFgB8Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEzMDU2Nw==", "bodyText": "Still final?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369130567", "createdAt": "2020-01-21T17:09:50Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -191,27 +192,49 @@ public void testDanglingIndicesStripAliases() throws Exception {\n         }\n     }\n \n-    public void testDanglingIndicesAreNotAllocatedWhenDisabled() throws Exception {\n+    /**\n+     * Check that when auto-imports are disabled, then dangling indices are still detected.\n+     */\n+    public void testDanglingIndicesAreDetectedButNotAllocatedWhenDisabled() throws Exception {\n         try (NodeEnvironment env = newNodeEnvironment()) {\n             MetaStateService metaStateService = new MetaStateService(env, xContentRegistry());\n             LocalAllocateDangledIndices localAllocateDangledIndices = mock(LocalAllocateDangledIndices.class);\n-\n             final Settings allocateSettings = Settings.builder().put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false).build();\n \n             final ClusterService clusterServiceMock = mock(ClusterService.class);\n             when(clusterServiceMock.getSettings()).thenReturn(allocateSettings);\n \n-            final DanglingIndicesState danglingIndicesState = new DanglingIndicesState(\n+            DanglingIndicesState danglingIndicesState = new DanglingIndicesState(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4MTM4NzAxOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNzoxMDowOFrOFgB85w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMVQxNzoxMDowOFrOFgB85w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTEzMDcyNw==", "bodyText": "nit: unnecessary whitespace change", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r369130727", "createdAt": "2020-01-21T17:10:08Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/gateway/DanglingIndicesStateTests.java", "diffHunk": "@@ -191,27 +192,49 @@ public void testDanglingIndicesStripAliases() throws Exception {\n         }\n     }\n \n-    public void testDanglingIndicesAreNotAllocatedWhenDisabled() throws Exception {\n+    /**\n+     * Check that when auto-imports are disabled, then dangling indices are still detected.\n+     */\n+    public void testDanglingIndicesAreDetectedButNotAllocatedWhenDisabled() throws Exception {\n         try (NodeEnvironment env = newNodeEnvironment()) {\n             MetaStateService metaStateService = new MetaStateService(env, xContentRegistry());\n             LocalAllocateDangledIndices localAllocateDangledIndices = mock(LocalAllocateDangledIndices.class);\n-\n             final Settings allocateSettings = Settings.builder().put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false).build();\n \n             final ClusterService clusterServiceMock = mock(ClusterService.class);\n             when(clusterServiceMock.getSettings()).thenReturn(allocateSettings);\n \n-            final DanglingIndicesState danglingIndicesState = new DanglingIndicesState(\n+            DanglingIndicesState danglingIndicesState = new DanglingIndicesState(\n                 env,\n                 metaStateService,\n-                localAllocateDangledIndices,\n-                clusterServiceMock\n+                localAllocateDangledIndices, clusterServiceMock", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "820f07e77f57307b839c3cf4356ed56db2d5a796"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NDYwNTkxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0MDo0NFrOFvNTSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0MDo0NFrOFvNTSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NTMyMA==", "bodyText": "Maybe NodeListDanglingIndicesResponse?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385045320", "createdAt": "2020-02-27T10:40:44Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesResponse extends BaseNodeResponse {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NDYxMDYyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeFindDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0MjowMlrOFvNWKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0MjowMlrOFvNWKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NjA1Nw==", "bodyText": "Oh, fun, we can have more than one version of the index metadata if the user has done something really weird involving multiple data paths. I think that's worth a comment.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385046057", "createdAt": "2020-02-27T10:42:02Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeFindDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for a specific dangling index.\n+ */\n+public class NodeFindDanglingIndexResponse extends BaseNodeResponse {\n+    private final List<IndexMetaData> danglingIndexMetaData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NDYxMjkyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0Mjo0NlrOFvNXrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0Mjo0NlrOFvNXrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0NjQ0NQ==", "bodyText": "Maybe NodeListDanglingIndicesRequest?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385046445", "createdAt": "2020-02-27T10:42:46Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeDanglingIndicesRequest.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Used when querying every node in the cluster for dangling indices, in response to a list request.\n+ */\n+public class NodeDanglingIndicesRequest extends TransportRequest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NDYyMjc2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0NToyOVrOFvNdjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0NToyOVrOFvNdjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0Nzk1MA==", "bodyText": "Should we execute this action with the NodeClient rather than going via the transport service?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385047950", "createdAt": "2020-02-27T10:45:29Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.util.CollectionUtils.map;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private void findDanglingIndex(String indexUUID, ActionListener<Index> listener) {\n+        this.transportService.sendRequest(\n+            this.transportService.getLocalNode(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NDYzMTIxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0Nzo0N1rOFvNivw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo0Nzo0N1rOFvNivw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA0OTI3OQ==", "bodyText": "I think this should use a format string: \"Metadata versions {} found for UUID [{}], selecting the highest\"\nAlso since it's only a DEBUG message, let's always log it.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385049279", "createdAt": "2020-02-27T10:47:47Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.util.CollectionUtils.map;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportImportDanglingIndexAction.class);\n+\n+    private final TransportService transportService;\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.transportService = transportService;\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+    }\n+\n+    @Override\n+    protected void doExecute(\n+        Task task,\n+        ImportDanglingIndexRequest importRequest,\n+        ActionListener<ImportDanglingIndexResponse> importListener\n+    ) {\n+        findDanglingIndex(importRequest, new ActionListener<>() {\n+            @Override\n+            public void onResponse(IndexMetaData indexMetaDataToImport) {\n+                // This flag is checked at this point so that we always check that the supplied index UUID\n+                // does correspond to a dangling index.\n+                if (importRequest.isAcceptDataLoss() == false) {\n+                    throw new IllegalArgumentException(\"accept_data_loss must be set to true\");\n+                }\n+\n+                danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                        importListener.onResponse(new ImportDanglingIndexResponse());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to import dangling index [\" + indexMetaDataToImport.getIndexUUID() + \"]\", e);\n+                        importListener.onFailure(e);\n+                    }\n+                });\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + importRequest.getIndexUUID() + \"]\", e);\n+                importListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private void findDanglingIndex(ImportDanglingIndexRequest request, ActionListener<IndexMetaData> listener) {\n+        final String indexUUID = request.getIndexUUID();\n+\n+        this.transportService.sendRequest(\n+            this.transportService.getLocalNode(),\n+            FindDanglingIndexAction.NAME,\n+            new FindDanglingIndexRequest(indexUUID),\n+            new TransportResponseHandler<FindDanglingIndexResponse>() {\n+\n+                @Override\n+                public void handleResponse(FindDanglingIndexResponse response) {\n+                    if (response.hasFailures()) {\n+                        for (FailedNodeException failure : response.failures()) {\n+                            logger.error(\"Failed to query \" + failure.nodeId(), failure);\n+                        }\n+\n+                        listener.onFailure(\n+                            new ElasticsearchException(\"Failed to query nodes: \" + map(response.failures(), FailedNodeException::nodeId))\n+                        );\n+                        return;\n+                    }\n+\n+                    final List<IndexMetaData> metaDataSortedByVersion = new ArrayList<>();\n+                    for (NodeFindDanglingIndexResponse each : response.getNodes()) {\n+                        metaDataSortedByVersion.addAll(each.getDanglingIndexMetaData());\n+                    }\n+                    metaDataSortedByVersion.sort(Comparator.comparingLong(IndexMetaData::getVersion));\n+\n+                    if (metaDataSortedByVersion.isEmpty()) {\n+                        listener.onFailure(new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\"));\n+                        return;\n+                    }\n+\n+                    if (metaDataSortedByVersion.size() > 1) {\n+                        logger.debug(\n+                            \"Metadata versions \"\n+                                + map(metaDataSortedByVersion, IndexMetaData::getVersion)\n+                                + \" found for UUID [\"\n+                                + indexUUID\n+                                + \"], selecting the highest\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NDY1MjUwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo1Mzo0MFrOFvNvNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxMDo1Mzo0MFrOFvNvNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTA1MjQ2OA==", "bodyText": "Maybe hint that they can now be manually managed?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            + \" is disabled, dangling indices will not be automatically imported\");\n          \n          \n            \n                            + \" is disabled, dangling indices will not be automatically detected or imported and must be managed manually\");", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385052468", "createdAt": "2020-02-27T10:53:40Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -65,24 +69,25 @@\n \n     private final NodeEnvironment nodeEnv;\n     private final MetaStateService metaStateService;\n-    private final LocalAllocateDangledIndices allocateDangledIndices;\n+    private final LocalAllocateDangledIndices danglingIndicesAllocator;\n     private final boolean isAutoImportDanglingIndicesEnabled;\n \n     private final Map<Index, IndexMetaData> danglingIndices = ConcurrentCollections.newConcurrentMap();\n \n     @Inject\n     public DanglingIndicesState(NodeEnvironment nodeEnv, MetaStateService metaStateService,\n-                                LocalAllocateDangledIndices allocateDangledIndices, ClusterService clusterService) {\n+                                LocalAllocateDangledIndices danglingIndicesAllocator, ClusterService clusterService) {\n         this.nodeEnv = nodeEnv;\n         this.metaStateService = metaStateService;\n-        this.allocateDangledIndices = allocateDangledIndices;\n+        this.danglingIndicesAllocator = danglingIndicesAllocator;\n \n         this.isAutoImportDanglingIndicesEnabled = AUTO_IMPORT_DANGLING_INDICES_SETTING.get(clusterService.getSettings());\n \n-        if (this.isAutoImportDanglingIndicesEnabled) {\n-            clusterService.addListener(this);\n-        } else {\n-            logger.warn(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey() + \" is disabled, dangling indices will not be detected or imported\");\n+        clusterService.addListener(this);\n+\n+        if (this.isAutoImportDanglingIndicesEnabled == false) {\n+            logger.warn(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey()\n+                + \" is disabled, dangling indices will not be automatically imported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c3569e45a7ab6590eb19133edf7adc1c2c3b5fe8"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTcyNzUwOnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjoyNDozM1rOFvX6Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMjo1MjoyOFrOGHMtEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIxOTEwNg==", "bodyText": "I think we can rely on the framework to manage our cluster more, ideally enough that this annotation isn't needed. We should be able to start our own data node in each test, ensure it has at least one shard of the test index, and then trigger the dangling index by restarting it. Since we'd be reusing the same cluster we'd need to set SETTING_MAX_TOMBSTONES to 1 throughout and then delete two indices, but I think that's better than taking this much control over the cluster.\nMight be worth extracting a method containing the setup that gets us into a state with a dangling index, since I think that's the same for all the tests.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385219106", "createdAt": "2020-02-27T16:24:33Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIwMTM2MA==", "bodyText": "Ok I looked into this and now think I prefer this to the alternative I suggested in which we configure SETTING_MAX_TOMBSTONES to 1 in all the tests and then work our way around that, so no action required here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410201360", "createdAt": "2020-04-17T12:52:28Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIxOTEwNg=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTc1NTk3OnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjozMDo1MFrOFvYLQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjozMDo1MFrOFvYLQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIyMzQ5MQ==", "bodyText": "Suggest importing this statically to be consistent with the other two settings.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385223491", "createdAt": "2020-02-27T16:30:50Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTc1OTI0OnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjozMTozOFrOFvYNPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjozMTozOFrOFvYNPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIyMzk5Nw==", "bodyText": "Now that we list dangling indices on demand, can we drop the assertBusy here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385223997", "createdAt": "2020-02-27T16:31:38Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTc1OTcwOnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjozMTo0N1rOFvYNiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjozMTo0N1rOFvYNiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIyNDA3NQ==", "bodyText": "Now that we list dangling indices on demand, can we drop the assertBusy here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385224075", "createdAt": "2020-02-27T16:31:47Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));\n+            assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+            assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+        });\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTc1OTkyOnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjozMTo1MVrOFvYNug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjozMTo1MVrOFvYNug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIyNDEyMg==", "bodyText": "Now that we list dangling indices on demand, can we drop the assertBusy here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385224122", "createdAt": "2020-02-27T16:31:51Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));\n+            assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+            assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+        });\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {\n+            final List<String> danglingIndexIds = listDanglingIndexIds();\n+            assertThat(danglingIndexIds, hasSize(1));\n+            danglingIndexUUID.set(danglingIndexIds.get(0));\n+        });\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexUUID.get());\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME)));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                deleteIndex(OTHER_INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 201}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTgyNjEzOnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo0Nzo0NlrOFvY1_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo0Nzo0NlrOFvY1_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzNDQzMA==", "bodyText": "This assertBusy is suspicious. I think the index should exist before the import request returns?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385234430", "createdAt": "2020-02-27T16:47:46Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));\n+            assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+            assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+        });\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {\n+            final List<String> danglingIndexIds = listDanglingIndexIds();\n+            assertThat(danglingIndexIds, hasSize(1));\n+            danglingIndexUUID.set(danglingIndexIds.get(0));\n+        });\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexUUID.get());\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTgzMjMzOnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo0OToxNFrOFvY5uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo0OToxNFrOFvY5uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzNTM4Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            stoppedNodeId = elements[0];\n          \n          \n            \n                            return elements[0];\n          \n      \n    \n    \n  \n\nThen I think we can drop stoppedNodeId.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385235387", "createdAt": "2020-02-27T16:49:14Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));\n+            assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+            assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+        });\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {\n+            final List<String> danglingIndexIds = listDanglingIndexIds();\n+            assertThat(danglingIndexIds, hasSize(1));\n+            danglingIndexUUID.set(danglingIndexIds.get(0));\n+        });\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexUUID.get());\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME)));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart a node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                deleteIndex(OTHER_INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        final AtomicReference<String> danglingIndexUUID = new AtomicReference<>();\n+\n+        // Wait for the dangling index to be noticed\n+        assertBusy(() -> {\n+            final List<String> danglingIndexIds = listDanglingIndexIds();\n+            assertThat(danglingIndexIds, hasSize(1));\n+            danglingIndexUUID.set(danglingIndexIds.get(0));\n+        });\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexUUID.get());\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        String stoppedNodeId = null;\n+\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                stoppedNodeId = elements[0];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 252}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTg0NDQwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo1MjowNVrOFvZBbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo1MjowNVrOFvZBbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzNzM1Ng==", "bodyText": "I think we need a test that we really do pick the metadata with the greatest version number.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385237356", "createdAt": "2020-02-27T16:52:05Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.util.CollectionUtils;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportImportDanglingIndexAction.class);\n+\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator,\n+        NodeClient nodeClient\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected void doExecute(\n+        Task task,\n+        ImportDanglingIndexRequest importRequest,\n+        ActionListener<ImportDanglingIndexResponse> importListener\n+    ) {\n+        findDanglingIndex(importRequest, new ActionListener<>() {\n+            @Override\n+            public void onResponse(IndexMetaData indexMetaDataToImport) {\n+                // This flag is checked at this point so that we always check that the supplied index UUID\n+                // does correspond to a dangling index.\n+                if (importRequest.isAcceptDataLoss() == false) {\n+                    importListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                        importListener.onResponse(new ImportDanglingIndexResponse());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to import dangling index [\" + indexMetaDataToImport.getIndexUUID() + \"]\", e);\n+                        importListener.onFailure(e);\n+                    }\n+                });\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + importRequest.getIndexUUID() + \"]\", e);\n+                importListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private void findDanglingIndex(ImportDanglingIndexRequest request, ActionListener<IndexMetaData> listener) {\n+        final String indexUUID = request.getIndexUUID();\n+\n+        this.nodeClient.execute(FindDanglingIndexAction.INSTANCE, new FindDanglingIndexRequest(indexUUID), new ActionListener<>() {\n+            @Override\n+            public void onResponse(FindDanglingIndexResponse response) {\n+                if (response.hasFailures()) {\n+                    for (FailedNodeException failure : response.failures()) {\n+                        logger.error(\"Failed to query \" + failure.nodeId(), failure);\n+                    }\n+\n+                    listener.onFailure(\n+                        new ElasticsearchException(\n+                            \"Failed to query nodes: \" + CollectionUtils.map(response.failures(), FailedNodeException::nodeId)\n+                        )\n+                    );\n+                    return;\n+                }\n+\n+                final List<IndexMetaData> metaDataSortedByVersion = new ArrayList<>();\n+                for (NodeFindDanglingIndexResponse each : response.getNodes()) {\n+                    metaDataSortedByVersion.addAll(each.getDanglingIndexMetaData());\n+                }\n+                metaDataSortedByVersion.sort(Comparator.comparingLong(IndexMetaData::getVersion));\n+\n+                if (metaDataSortedByVersion.isEmpty()) {\n+                    listener.onFailure(new IllegalArgumentException(\"No dangling index found for UUID [\" + indexUUID + \"]\"));\n+                    return;\n+                }\n+\n+                logger.debug(\n+                    \"Metadata versions {}  found for UUID [{}], selecting the highest\",\n+                    CollectionUtils.map(metaDataSortedByVersion, IndexMetaData::getVersion),\n+                    indexUUID\n+                );\n+\n+                listener.onResponse(metaDataSortedByVersion.get(metaDataSortedByVersion.size() - 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTg1ODI5OnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo1NTozNVrOFvZKHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo1NTozNVrOFvZKHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzOTU4Mg==", "bodyText": "Can we get the index UUID from before and then assert that it matches the one we get here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385239582", "createdAt": "2020-02-27T16:55:35Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.emptyString;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.not;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        ensureStableCluster(3);\n+        createIndices(INDEX_NAME);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+        final RestClient restClient = getRestClient();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public Settings onNodeStopped(String nodeName) throws Exception {\n+                ensureClusterSizeConsistency();\n+                stoppedNodeName.set(nodeName);\n+                deleteIndex(INDEX_NAME);\n+                return super.onNodeStopped(nodeName);\n+            }\n+        });\n+\n+        ensureStableCluster(3);\n+\n+        final String stoppedNodeId = mapNodeNameToId(stoppedNodeName.get());\n+\n+        assertBusy(() -> {\n+            final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+            assertOK(listResponse);\n+\n+            final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+            assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+            assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+            List<Object> indices = mapView.get(\"dangling_indices\");\n+            assertThat(indices, hasSize(1));\n+\n+            assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+            assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), not(emptyString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTg2ODA4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo1ODowMFrOFvZQKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo1ODowMFrOFvZQKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0MTEzMQ==", "bodyText": "I have a slight preference for creationDateMillis here, since it's just a bare long and in some places that means nanoseconds.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385241131", "createdAt": "2020-02-27T16:58:00Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {\n+    private String indexName;\n+    private String indexUUID;\n+    private long creationDate;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTg3MTYyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo1ODo1NFrOFvZSaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNjo1ODo1NFrOFvZSaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0MTcwNw==", "bodyText": "I think we can drop the this. qualifier in a bunch of places in this class?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385241707", "createdAt": "2020-02-27T16:58:54Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {\n+    private String indexName;\n+    private String indexUUID;\n+    private long creationDate;\n+\n+    public DanglingIndexInfo(DiscoveryNode node, String indexName, String indexUUID, long creationDate) {\n+        super(node);\n+        this.indexName = indexName;\n+        this.indexUUID = indexUUID;\n+        this.creationDate = creationDate;\n+    }\n+\n+    public DanglingIndexInfo(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexName = in.readString();\n+        this.indexUUID = in.readString();\n+        this.creationDate = in.readLong();\n+    }\n+\n+    public String getIndexName() {\n+        return indexName;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public String getNodeId() {\n+        return this.getNode().getId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTg5MDU3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzowMzo1MFrOFvZeNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzowMzo1MFrOFvZeNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0NDcyNw==", "bodyText": "I think we should not hardcode this timeout - it's inevitable that we find a cluster where the default isn't long enough, so we should accept a ?timeout parameter in line with other APIs.\nSimilarly I think should accept a ?master_timeout parameter in line with other APIs.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385244727", "createdAt": "2020-02-27T17:03:50Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexRequest.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.support.master.AcknowledgedRequest;\n+import org.elasticsearch.action.support.master.MasterNodeRequest;\n+import org.elasticsearch.cluster.ack.AckedRequest;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents a request to delete a particular dangling index, specified by its UUID. The {@link #acceptDataLoss}\n+ * flag must also be explicitly set to true, or later validation will fail.\n+ */\n+public class DeleteDanglingIndexRequest extends MasterNodeRequest<DeleteDanglingIndexRequest> implements AckedRequest {\n+    private final String indexUUID;\n+    private final boolean acceptDataLoss;\n+\n+    public DeleteDanglingIndexRequest(StreamInput in) throws IOException {\n+        super(in);\n+        this.indexUUID = in.readString();\n+        this.acceptDataLoss = in.readBoolean();\n+    }\n+\n+    public DeleteDanglingIndexRequest(String indexUUID, boolean acceptDataLoss) {\n+        super();\n+        this.indexUUID = Strings.requireNonEmpty(indexUUID, \"indexUUID cannot be null or empty\");\n+        this.acceptDataLoss = acceptDataLoss;\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        return null;\n+    }\n+\n+    public String getIndexUUID() {\n+        return indexUUID;\n+    }\n+\n+    public boolean isAcceptDataLoss() {\n+        return acceptDataLoss;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"DeleteDanglingIndexRequest{\" + \"indexUUID='\" + indexUUID + \"', acceptDataLoss=\" + acceptDataLoss + '}';\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(this.indexUUID);\n+        out.writeBoolean(this.acceptDataLoss);\n+    }\n+\n+    @Override\n+    public TimeValue ackTimeout() {\n+        return AcknowledgedRequest.DEFAULT_ACK_TIMEOUT;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTg5NTU1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzowNToyMVrOFvZhYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzowNToyMVrOFvZhYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0NTUzNg==", "bodyText": "Can we use org.elasticsearch.action.support.master.AcknowledgedResponse instead of a specialised class here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385245536", "createdAt": "2020-02-27T17:05:21Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DeleteDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class DeleteDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTkyMjk0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoxMjoxNVrOFvZyvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoxMjoxNVrOFvZyvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI0OTk4Mg==", "bodyText": "Can we drop this acknowledged field? I don't think we wait for all nodes to ack the import, in which case we shouldn't really return true here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385249982", "createdAt": "2020-02-27T17:12:15Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ImportDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+\n+public class ImportDanglingIndexResponse extends ActionResponse implements StatusToXContentObject {\n+    public ImportDanglingIndexResponse() {\n+    }\n+\n+    public ImportDanglingIndexResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return RestStatus.ACCEPTED;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        return builder.startObject().field(\"acknowledged\", true).endObject();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTk0MTg5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoxNzo0MVrOFvZ-rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoxNzo0MVrOFvZ-rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI1MzAzOQ==", "bodyText": "use byIndexUUID.computeIfAbsent or byIndexUUID.putIfAbsent?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385253039", "createdAt": "2020-02-27T17:17:41Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"_nodes.failures\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeListDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeListDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.INTERNAL_SERVER_ERROR : RestStatus.OK;\n+    }\n+\n+    private Collection<AggregatedDanglingIndexInfo> resultsByIndexUUID() {\n+        Map<String, AggregatedDanglingIndexInfo> byIndexUUID = new HashMap<>();\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (DanglingIndexInfo info : nodeResponse.getDanglingIndices()) {\n+                final String indexUUID = info.getIndexUUID();\n+\n+                if (byIndexUUID.containsKey(indexUUID) == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTk2ODUwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoyNTowNlrOFvaPOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoyNTowNlrOFvaPOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI1NzI3Mg==", "bodyText": "Does this need extends BaseNodeResponse? I think the actual per-node response is NodeListDanglingIndicesResponse.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385257272", "createdAt": "2020-02-27T17:25:06Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/DanglingIndexInfo.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Contains information about a dangling index, i.e. an index that Elasticsearch has found\n+ * on-disk but is not present in the cluster state.\n+ */\n+public class DanglingIndexInfo extends BaseNodeResponse implements ToXContentObject {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTk4NTM3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoyODozOVrOFvaZ5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoyODozOVrOFvaZ5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2MDAwNg==", "bodyText": "Simple though it is, I think this aggregation logic deserves a unit test.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385260006", "createdAt": "2020-02-27T17:28:39Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/ListDanglingIndicesResponse.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.nodes.BaseNodesResponse;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.StatusToXContentObject;\n+import org.elasticsearch.common.xcontent.XContent;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.rest.RestStatus;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Models a response to a {@link ListDanglingIndicesRequest}. A list request queries every node in the\n+ * cluster and aggregates their responses. When the aggregated response is converted to {@link XContent},\n+ * information for each dangling index is presented under the \"dangling_indices\" key. If any nodes\n+ * in the cluster failed to answer, the details are presented under the \"_nodes.failures\" key.\n+ */\n+public class ListDanglingIndicesResponse extends BaseNodesResponse<NodeListDanglingIndicesResponse> implements StatusToXContentObject {\n+\n+    public ListDanglingIndicesResponse(StreamInput in) throws IOException {\n+        super(in);\n+    }\n+\n+    public ListDanglingIndicesResponse(\n+        ClusterName clusterName,\n+        List<NodeListDanglingIndicesResponse> nodes,\n+        List<FailedNodeException> failures\n+    ) {\n+        super(clusterName, nodes, failures);\n+    }\n+\n+    @Override\n+    public RestStatus status() {\n+        return this.hasFailures() ? RestStatus.INTERNAL_SERVER_ERROR : RestStatus.OK;\n+    }\n+\n+    private Collection<AggregatedDanglingIndexInfo> resultsByIndexUUID() {\n+        Map<String, AggregatedDanglingIndexInfo> byIndexUUID = new HashMap<>();\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : this.getNodes()) {\n+            for (DanglingIndexInfo info : nodeResponse.getDanglingIndices()) {\n+                final String indexUUID = info.getIndexUUID();\n+\n+                if (byIndexUUID.containsKey(indexUUID) == false) {\n+                    AggregatedDanglingIndexInfo aggregatedInfo = new AggregatedDanglingIndexInfo(\n+                        indexUUID,\n+                        info.getIndexName(),\n+                        info.getCreationDate()\n+                    );\n+\n+                    byIndexUUID.put(indexUUID, aggregatedInfo);\n+                }\n+\n+                byIndexUUID.get(indexUUID).getNodeIds().add(nodeResponse.getNode().getId());\n+            }\n+        }\n+\n+        return byIndexUUID.values();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTk4OTk3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoyOTo1OVrOFvadEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzoyOTo1OVrOFvadEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2MDgxNg==", "bodyText": "Probably a holdover from a previous iteration, but here we're not listing all indices we're finding one of them.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385260816", "createdAt": "2020-02-27T17:29:59Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NTk5NTQ3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzozMToyM1rOFvagNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzozMToyM1rOFvagNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2MTYyMw==", "bodyText": "Optional nit: maybe inline a bunch of these local variables.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385261623", "createdAt": "2020-02-27T17:31:23Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjAwNTQ1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzozNDoxOFrOFvamQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzozNDoxOFrOFvamQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2MzE3MQ==", "bodyText": "I think it might avoid some confusion to include the index UUID and put the whole thing in square brackets too:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                \"delete-dangling-index \" + indexName,\n          \n          \n            \n                                \"delete-dangling-index \" + indexToDelete,", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385263171", "createdAt": "2020-02-27T17:34:18Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjAwOTI4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzozNTozNFrOFvaotw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzozNTozNFrOFvaotw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2Mzc5OQ==", "bodyText": "Worth a test that we bail out on one or more failures here, with appropriate logging.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385263799", "createdAt": "2020-02-27T17:35:34Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private void findDanglingIndex(String indexUUID, ActionListener<Index> listener) {\n+        this.nodeClient.execute(FindDanglingIndexAction.INSTANCE, new FindDanglingIndexRequest(indexUUID), new ActionListener<>() {\n+            @Override\n+            public void onResponse(FindDanglingIndexResponse response) {\n+                if (response.hasFailures()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA0MDY1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo0NTowNlrOFva8ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo0NTowNlrOFva8ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2ODg0Mg==", "bodyText": "This is almost the same as what we do in TransportImportDanglingIndexAction except:\n\nwe expose any errors better here\nwe return the first Index here and the latest IndexMetaData in the import action.\n\nI think we could reasonably extract this common code, bringing in the error handling from here and the find-the-latest-IndexMetaData from the other place. It makes little difference if we use the first or the latest Index here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385268842", "createdAt": "2020-02-27T17:45:06Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);\n+        metaDataBuilder.indexGraveyard(newGraveyard);\n+\n+        return ClusterState.builder(currentState).metaData(metaDataBuilder.build()).build();\n+    }\n+\n+    @Override\n+    protected ClusterBlockException checkBlock(DeleteDanglingIndexRequest request, ClusterState state) {\n+        return null;\n+    }\n+\n+    private void findDanglingIndex(String indexUUID, ActionListener<Index> listener) {\n+        this.nodeClient.execute(FindDanglingIndexAction.INSTANCE, new FindDanglingIndexRequest(indexUUID), new ActionListener<>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA0NTIzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo0NjozMlrOFva_Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo0NjozMlrOFva_Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI2OTU4Ng==", "bodyText": "See earlier comment about re-using the error handling from TransportDeleteDanglingIndexAction.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385269586", "createdAt": "2020-02-27T17:46:32Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportImportDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.util.CollectionUtils;\n+import org.elasticsearch.gateway.LocalAllocateDangledIndices;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+\n+/**\n+ * Implements the import of a dangling index. When handling a {@link ImportDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then calls {@link LocalAllocateDangledIndices}\n+ * to perform the actual allocation.\n+ */\n+public class TransportImportDanglingIndexAction extends HandledTransportAction<ImportDanglingIndexRequest, ImportDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportImportDanglingIndexAction.class);\n+\n+    private final LocalAllocateDangledIndices danglingIndexAllocator;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportImportDanglingIndexAction(\n+        ActionFilters actionFilters,\n+        TransportService transportService,\n+        LocalAllocateDangledIndices danglingIndexAllocator,\n+        NodeClient nodeClient\n+    ) {\n+        super(ImportDanglingIndexAction.NAME, transportService, actionFilters, ImportDanglingIndexRequest::new);\n+        this.danglingIndexAllocator = danglingIndexAllocator;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected void doExecute(\n+        Task task,\n+        ImportDanglingIndexRequest importRequest,\n+        ActionListener<ImportDanglingIndexResponse> importListener\n+    ) {\n+        findDanglingIndex(importRequest, new ActionListener<>() {\n+            @Override\n+            public void onResponse(IndexMetaData indexMetaDataToImport) {\n+                // This flag is checked at this point so that we always check that the supplied index UUID\n+                // does correspond to a dangling index.\n+                if (importRequest.isAcceptDataLoss() == false) {\n+                    importListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                danglingIndexAllocator.allocateDangled(List.of(indexMetaDataToImport), new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse allocateDangledResponse) {\n+                        importListener.onResponse(new ImportDanglingIndexResponse());\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to import dangling index [\" + indexMetaDataToImport.getIndexUUID() + \"]\", e);\n+                        importListener.onFailure(e);\n+                    }\n+                });\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to find dangling index [\" + importRequest.getIndexUUID() + \"]\", e);\n+                importListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private void findDanglingIndex(ImportDanglingIndexRequest request, ActionListener<IndexMetaData> listener) {\n+        final String indexUUID = request.getIndexUUID();\n+\n+        this.nodeClient.execute(FindDanglingIndexAction.INSTANCE, new FindDanglingIndexRequest(indexUUID), new ActionListener<>() {\n+            @Override\n+            public void onResponse(FindDanglingIndexResponse response) {\n+                if (response.hasFailures()) {\n+                    for (FailedNodeException failure : response.failures()) {\n+                        logger.error(\"Failed to query \" + failure.nodeId(), failure);\n+                    }\n+\n+                    listener.onFailure(\n+                        new ElasticsearchException(\n+                            \"Failed to query nodes: \" + CollectionUtils.map(response.failures(), FailedNodeException::nodeId)\n+                        )\n+                    );\n+                    return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA1MzE4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/client/ClusterAdminClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo0OTowMFrOFvbETw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo0OTowMFrOFvbETw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MDg2Mw==", "bodyText": "I think we can treat this action as internal and not expose it in the client. If we are going to expose it, it should be called findDanglingIndex.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385270863", "createdAt": "2020-02-27T17:49:00Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/client/ClusterAdminClient.java", "diffHunk": "@@ -718,4 +726,44 @@\n      * Get a script from the cluster state\n      */\n     ActionFuture<GetStoredScriptResponse> getStoredScript(GetStoredScriptRequest request);\n+\n+    /**\n+     * List dangling indices on all nodes.\n+     */\n+    void listDanglingIndices(ListDanglingIndicesRequest request, ActionListener<ListDanglingIndicesResponse> listener);\n+\n+    /**\n+     * List dangling indices on all nodes.\n+     */\n+    ActionFuture<ListDanglingIndicesResponse> listDanglingIndices(ListDanglingIndicesRequest request);\n+\n+    /**\n+     * Find dangling indices on all nodes.\n+     */\n+    void findDanglingIndices(FindDanglingIndexRequest request, ActionListener<FindDanglingIndexResponse> listener);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA1NDMzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/client/ClusterAdminClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo0OToxOVrOFvbFAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo0OToxOVrOFvbFAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MTA0Mg==", "bodyText": "I think we can treat this action as internal and not expose it in the client. If we are going to expose it, it should be called findDanglingIndex.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385271042", "createdAt": "2020-02-27T17:49:19Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/client/ClusterAdminClient.java", "diffHunk": "@@ -718,4 +726,44 @@\n      * Get a script from the cluster state\n      */\n     ActionFuture<GetStoredScriptResponse> getStoredScript(GetStoredScriptRequest request);\n+\n+    /**\n+     * List dangling indices on all nodes.\n+     */\n+    void listDanglingIndices(ListDanglingIndicesRequest request, ActionListener<ListDanglingIndicesResponse> listener);\n+\n+    /**\n+     * List dangling indices on all nodes.\n+     */\n+    ActionFuture<ListDanglingIndicesResponse> listDanglingIndices(ListDanglingIndicesRequest request);\n+\n+    /**\n+     * Find dangling indices on all nodes.\n+     */\n+    void findDanglingIndices(FindDanglingIndexRequest request, ActionListener<FindDanglingIndexResponse> listener);\n+\n+    /**\n+     * Find dangling indices on all nodes.\n+     */\n+    ActionFuture<FindDanglingIndexResponse> findDanglingIndices(FindDanglingIndexRequest request);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA2MzI4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/common/Strings.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1MTo1MFrOFvbKfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMDo1NDo1N1rOGIOuWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MjQ0Nw==", "bodyText": "I don't think we need this - Objects.requireNonNull is enough IMO, an empty UUID string is kinda valid (although it doesn't match anything).", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385272447", "createdAt": "2020-02-27T17:51:50Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/common/Strings.java", "diffHunk": "@@ -853,4 +853,41 @@ public static String padStart(String s, int minimumLength, char c) {\n             return sb.toString();\n         }\n     }\n+\n+    /**\n+     * Checks that the specified String reference is:\n+     *\n+     * <ul>\n+     * <li>not {@code null} and throws a customized {@link NullPointerException} if it is.\n+     * <li>not empty (i.e. has zero length) and throws an  {@link IllegalArgumentException} if it is.\n+     * </ul>\n+     *\n+     * This method is designed primarily for doing parameter validation in methods and\n+     * constructors with multiple parameters, as demonstrated below:\n+     *\n+     * <blockquote><pre>\n+     * public Foo(String bar, String baz) {\n+     *     this.bar = Strings.requireNonEmpty(bar, \"bar must not be null or empty\");\n+     *     this.baz = Strings.requireNonEmpty(baz, \"baz must not be null or empty\");\n+     * }\n+     * </pre></blockquote>\n+     *\n+     * @param obj     the String reference to check for nullity or zero-length\n+     * @param message detail message to be used in the event that an exception is thrown\n+     * @return {@code obj} if not {@code null} or zero-length\n+     * @throws NullPointerException if {@code obj} is {@code null}\n+     * @throws IllegalArgumentException if {@code obj.isEmpty()} returns {@code true}\n+     */\n+    public static String requireNonEmpty(String obj, String message) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIwNjQ0Nw==", "bodyText": "Still outstanding -- let's remove this and just use Objects.requireNonNull.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410206447", "createdAt": "2020-04-17T13:01:47Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/common/Strings.java", "diffHunk": "@@ -853,4 +853,41 @@ public static String padStart(String s, int minimumLength, char c) {\n             return sb.toString();\n         }\n     }\n+\n+    /**\n+     * Checks that the specified String reference is:\n+     *\n+     * <ul>\n+     * <li>not {@code null} and throws a customized {@link NullPointerException} if it is.\n+     * <li>not empty (i.e. has zero length) and throws an  {@link IllegalArgumentException} if it is.\n+     * </ul>\n+     *\n+     * This method is designed primarily for doing parameter validation in methods and\n+     * constructors with multiple parameters, as demonstrated below:\n+     *\n+     * <blockquote><pre>\n+     * public Foo(String bar, String baz) {\n+     *     this.bar = Strings.requireNonEmpty(bar, \"bar must not be null or empty\");\n+     *     this.baz = Strings.requireNonEmpty(baz, \"baz must not be null or empty\");\n+     * }\n+     * </pre></blockquote>\n+     *\n+     * @param obj     the String reference to check for nullity or zero-length\n+     * @param message detail message to be used in the event that an exception is thrown\n+     * @return {@code obj} if not {@code null} or zero-length\n+     * @throws NullPointerException if {@code obj} is {@code null}\n+     * @throws IllegalArgumentException if {@code obj.isEmpty()} returns {@code true}\n+     */\n+    public static String requireNonEmpty(String obj, String message) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MjQ0Nw=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI4MzAzMg==", "bodyText": "an empty UUID string is kinda valid\n\nI'm not sure I agree with this statement. The API is designed to import a specific UUID. You can't even POST to the REST API without passing a UUID, because the REST layer infers that you ought to sending a GET.\nBut, putting that aside, it's highly unlikely that anything is going to call the transport API but our code, so I've swapped the calls out as suggested.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r411283032", "createdAt": "2020-04-20T10:54:57Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/common/Strings.java", "diffHunk": "@@ -853,4 +853,41 @@ public static String padStart(String s, int minimumLength, char c) {\n             return sb.toString();\n         }\n     }\n+\n+    /**\n+     * Checks that the specified String reference is:\n+     *\n+     * <ul>\n+     * <li>not {@code null} and throws a customized {@link NullPointerException} if it is.\n+     * <li>not empty (i.e. has zero length) and throws an  {@link IllegalArgumentException} if it is.\n+     * </ul>\n+     *\n+     * This method is designed primarily for doing parameter validation in methods and\n+     * constructors with multiple parameters, as demonstrated below:\n+     *\n+     * <blockquote><pre>\n+     * public Foo(String bar, String baz) {\n+     *     this.bar = Strings.requireNonEmpty(bar, \"bar must not be null or empty\");\n+     *     this.baz = Strings.requireNonEmpty(baz, \"baz must not be null or empty\");\n+     * }\n+     * </pre></blockquote>\n+     *\n+     * @param obj     the String reference to check for nullity or zero-length\n+     * @param message detail message to be used in the event that an exception is thrown\n+     * @return {@code obj} if not {@code null} or zero-length\n+     * @throws NullPointerException if {@code obj} is {@code null}\n+     * @throws IllegalArgumentException if {@code obj.isEmpty()} returns {@code true}\n+     */\n+    public static String requireNonEmpty(String obj, String message) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MjQ0Nw=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA3MDg0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1NDowN1rOFvbPUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMDo1NTowNlrOGIOuow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MzY4MA==", "bodyText": "If we're here then we're answering a query from the user about dangling indices, so I think we don't need to log anything additional.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385273680", "createdAt": "2020-02-27T17:54:07Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -161,11 +192,21 @@ void findNewAndAddDanglingIndices(final MetaData metaData) {\n                                 \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n                                 \"that was previously deleted.\", indexMetaData.getIndex());\n                 } else {\n-                    logger.info(\"[{}] dangling index exists on local file system, but not in cluster metadata, \" +\n-                                \"auto import to cluster state\", indexMetaData.getIndex());\n+                    if (this.isAutoImportDanglingIndicesEnabled) {\n+                        logger.info(\n+                            \"[{}] dangling index exists on local file system, but not in cluster metadata, auto import to cluster state\",\n+                            indexMetaData.getIndex()\n+                        );\n+                    } else {\n+                        logger.info(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIwNjk3NQ==", "bodyText": "This is still outstanding -- let's not log things here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410206975", "createdAt": "2020-04-17T13:02:46Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -161,11 +192,21 @@ void findNewAndAddDanglingIndices(final MetaData metaData) {\n                                 \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n                                 \"that was previously deleted.\", indexMetaData.getIndex());\n                 } else {\n-                    logger.info(\"[{}] dangling index exists on local file system, but not in cluster metadata, \" +\n-                                \"auto import to cluster state\", indexMetaData.getIndex());\n+                    if (this.isAutoImportDanglingIndicesEnabled) {\n+                        logger.info(\n+                            \"[{}] dangling index exists on local file system, but not in cluster metadata, auto import to cluster state\",\n+                            indexMetaData.getIndex()\n+                        );\n+                    } else {\n+                        logger.info(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MzY4MA=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI4MzEwNw==", "bodyText": "Removed.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r411283107", "createdAt": "2020-04-20T10:55:06Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -161,11 +192,21 @@ void findNewAndAddDanglingIndices(final MetaData metaData) {\n                                 \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" +\n                                 \"that was previously deleted.\", indexMetaData.getIndex());\n                 } else {\n-                    logger.info(\"[{}] dangling index exists on local file system, but not in cluster metadata, \" +\n-                                \"auto import to cluster state\", indexMetaData.getIndex());\n+                    if (this.isAutoImportDanglingIndicesEnabled) {\n+                        logger.info(\n+                            \"[{}] dangling index exists on local file system, but not in cluster metadata, auto import to cluster state\",\n+                            indexMetaData.getIndex()\n+                        );\n+                    } else {\n+                        logger.info(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MzY4MA=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA3NDQ5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1NToyMlrOFvbRyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1NToyMlrOFvbRyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NDMxNA==", "bodyText": "I think this condition is constant (we assert this.isAutoImportDanglingIndicesEnabled in the only production caller of this method.)", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385274314", "createdAt": "2020-02-27T17:55:22Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -188,15 +229,21 @@ private IndexMetaData stripAliases(IndexMetaData indexMetaData) {\n     }\n \n     /**\n-     * Allocates the provided list of the dangled indices by sending them to the master node\n-     * for allocation.\n+     * Allocates the detected list of dangling indices by sending them to the master node\n+     * for allocation, provided auto-import is enabled via the\n+     * {@link #AUTO_IMPORT_DANGLING_INDICES_SETTING} setting.\n      */\n     void allocateDanglingIndices() {\n+        if (this.isAutoImportDanglingIndicesEnabled == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA3NjIyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1NTo1NVrOFvbS4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1NTo1NVrOFvbS4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NDU5NA==", "bodyText": "Suggest reverting this -- one fewer file in the stats :)", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385274594", "createdAt": "2020-02-27T17:55:55Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/LocalAllocateDangledIndices.java", "diffHunk": "@@ -147,7 +147,8 @@ public ClusterState execute(ClusterState currentState) {\n                             logger.warn(() -> new ParameterizedMessage(\"found dangled index [{}] on node [{}]. This index cannot be \" +\n                                 \"upgraded to the latest version, adding as closed\", indexMetaData.getIndex(), request.fromNode), ex);\n                             upgradedIndexMetaData = IndexMetaData.builder(indexMetaData).state(IndexMetaData.State.CLOSE)\n-                                .version(indexMetaData.getVersion() + 1).build();\n+                                .version(indexMetaData.getVersion() + 1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4NjA4NDM5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/common/util/CollectionUtils.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yN1QxNzo1ODoxM1rOFvbX8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMDo1OToxOVrOGIO4Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NTg5MA==", "bodyText": "I'd sorta prefer not to add these simple utility methods. They're not saving that much noise in their callers.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385275890", "createdAt": "2020-02-27T17:58:13Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/common/util/CollectionUtils.java", "diffHunk": "@@ -348,4 +350,80 @@ public static int sortAndDedup(final BytesRefArray bytes, final int[] indices) {\n \n         return result;\n     }\n+\n+    /**\n+     * Utility method that removes some of the boilerplate around streams and mapping. Instead of writing e.g.\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = indexMetaData\n+     *   .stream()\n+     *   .map(each -&gt; each.getIndexUUID())\n+     *   .collect(Collectors.toList());\n+     * </pre>\n+     *\n+     * You can instead write:\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = map(indexMetaData, each -&gt; each.getIndexUUID());\n+     * </pre>\n+     *\n+     * @param list the list to map\n+     * @param mapper a mapping function to apply to each element of the list\n+     * @param <T1> the type that the supplied list contains\n+     * @param <T2> the type that the returned list contains\n+     * @return a list containing the result of applying the <code>mapper</code> function to each element in <code>list</code>.\n+     */\n+    public static <T1, T2> List<T2> map(List<T1> list, Function<T1, T2> mapper) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIwNjY2Mw==", "bodyText": "This is still outstanding -- let's remove these and inline them at their few callers.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410206663", "createdAt": "2020-04-17T13:02:11Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/common/util/CollectionUtils.java", "diffHunk": "@@ -348,4 +350,80 @@ public static int sortAndDedup(final BytesRefArray bytes, final int[] indices) {\n \n         return result;\n     }\n+\n+    /**\n+     * Utility method that removes some of the boilerplate around streams and mapping. Instead of writing e.g.\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = indexMetaData\n+     *   .stream()\n+     *   .map(each -&gt; each.getIndexUUID())\n+     *   .collect(Collectors.toList());\n+     * </pre>\n+     *\n+     * You can instead write:\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = map(indexMetaData, each -&gt; each.getIndexUUID());\n+     * </pre>\n+     *\n+     * @param list the list to map\n+     * @param mapper a mapping function to apply to each element of the list\n+     * @param <T1> the type that the supplied list contains\n+     * @param <T2> the type that the returned list contains\n+     * @return a list containing the result of applying the <code>mapper</code> function to each element in <code>list</code>.\n+     */\n+    public static <T1, T2> List<T2> map(List<T1> list, Function<T1, T2> mapper) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NTg5MA=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI4NTUzMQ==", "bodyText": "Inlined.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r411285531", "createdAt": "2020-04-20T10:59:19Z", "author": {"login": "pugnascotia"}, "path": "server/src/main/java/org/elasticsearch/common/util/CollectionUtils.java", "diffHunk": "@@ -348,4 +350,80 @@ public static int sortAndDedup(final BytesRefArray bytes, final int[] indices) {\n \n         return result;\n     }\n+\n+    /**\n+     * Utility method that removes some of the boilerplate around streams and mapping. Instead of writing e.g.\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = indexMetaData\n+     *   .stream()\n+     *   .map(each -&gt; each.getIndexUUID())\n+     *   .collect(Collectors.toList());\n+     * </pre>\n+     *\n+     * You can instead write:\n+     *\n+     * <pre>\n+     * List&lt;String&gt; ids = map(indexMetaData, each -&gt; each.getIndexUUID());\n+     * </pre>\n+     *\n+     * @param list the list to map\n+     * @param mapper a mapping function to apply to each element of the list\n+     * @param <T1> the type that the supplied list contains\n+     * @param <T2> the type that the returned list contains\n+     * @return a list containing the result of applying the <code>mapper</code> function to each element in <code>list</code>.\n+     */\n+    public static <T1, T2> List<T2> map(List<T1> list, Function<T1, T2> mapper) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NTg5MA=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4ODYyMTg3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeFindDanglingIndexResponse.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjoyOTowN1rOFvzX-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjo1NDo1N1rOFv0B9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY2OTExNA==", "bodyText": "Can we avoid sending the full index metadata here? This object is potentially very big, and we are only interested in two small fields from that object.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385669114", "createdAt": "2020-02-28T12:29:07Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeFindDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for a specific dangling index.\n+ */\n+public class NodeFindDanglingIndexResponse extends BaseNodeResponse {\n+    /**\n+     * A node could report several dangling indices. This class will contain them all.\n+     * A single node could even multiple different index versions for the same index\n+     * UUID if the situation is really crazy, though perhaps this is more likely\n+     * when collating responses from different nodes.\n+     */\n+    private final List<IndexMetaData> danglingIndexMetaData;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY3ODM3Mw==", "bodyText": "Note that we're only sending the metadata for a single index here. Although it might be quite big it shouldn't be unmanageably so.\nThere's a couple of options for trimming it down further: we could distinguish the find action needed for deletion from the one needed for import, or else we could share just the metadata version here and have the coordinating node pick one node and tells it to send its metadata to the master. Either way we have to add another transport action, and I don't think the savings are really worth it.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385678373", "createdAt": "2020-02-28T12:51:10Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeFindDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for a specific dangling index.\n+ */\n+public class NodeFindDanglingIndexResponse extends BaseNodeResponse {\n+    /**\n+     * A node could report several dangling indices. This class will contain them all.\n+     * A single node could even multiple different index versions for the same index\n+     * UUID if the situation is really crazy, though perhaps this is more likely\n+     * when collating responses from different nodes.\n+     */\n+    private final List<IndexMetaData> danglingIndexMetaData;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY2OTExNA=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY3OTg2MQ==", "bodyText": "ok, following your suggestion here", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385679861", "createdAt": "2020-02-28T12:54:57Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/NodeFindDanglingIndexResponse.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.elasticsearch.action.support.nodes.BaseNodeResponse;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Used when querying every node in the cluster for a specific dangling index.\n+ */\n+public class NodeFindDanglingIndexResponse extends BaseNodeResponse {\n+    /**\n+     * A node could report several dangling indices. This class will contain them all.\n+     * A single node could even multiple different index versions for the same index\n+     * UUID if the situation is really crazy, though perhaps this is more likely\n+     * when collating responses from different nodes.\n+     */\n+    private final List<IndexMetaData> danglingIndexMetaData;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY2OTExNA=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4ODYyOTA2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjozMTo0NFrOFvzcNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjozMTo0NFrOFvzcNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY3MDE5Nw==", "bodyText": "do we need to double-check here that the index does not exist as proper index in the cluster state?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385670197", "createdAt": "2020-02-28T12:31:44Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4ODY2Mjg3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjo0NDowMVrOFvzwfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxMjo1NDo1NFrOFv0B5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY3NTM5MQ==", "bodyText": "By just adding the index to the graveyard, how is that making the nodes actually delete the index when applying the CS update? I might be missing something here but can't find the logic for it.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385675391", "createdAt": "2020-02-28T12:44:01Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY3OTg0Nw==", "bodyText": "I think there's a testing gap here. We're checking that when we delete the index it no longer appears as a dangling index, but not actually checking that it is removed from disk. I think this means if we delete another couple of indices to phase this one out of the graveyard then we would see it reappear.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r385679847", "createdAt": "2020-02-28T12:54:54Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/action/admin/indices/dangling/TransportDeleteDanglingIndexAction.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.action.admin.indices.dangling;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.ElasticsearchException;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.FailedNodeException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.master.TransportMasterNodeAction;\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.cluster.AckedClusterStateUpdateTask;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.metadata.IndexGraveyard;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.metadata.MetaData;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.Index;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements the deletion of a dangling index. When handling a {@link DeleteDanglingIndexAction},\n+ * this class first checks that such a dangling index exists. It then submits a cluster state update\n+ * to add the index to the index graveyard.\n+ */\n+public class TransportDeleteDanglingIndexAction extends TransportMasterNodeAction<DeleteDanglingIndexRequest, DeleteDanglingIndexResponse> {\n+    private static final Logger logger = LogManager.getLogger(TransportDeleteDanglingIndexAction.class);\n+\n+    private final Settings settings;\n+    private final NodeClient nodeClient;\n+\n+    @Inject\n+    public TransportDeleteDanglingIndexAction(\n+        TransportService transportService,\n+        ClusterService clusterService,\n+        ThreadPool threadPool,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver,\n+        Settings settings,\n+        NodeClient nodeClient\n+    ) {\n+        super(\n+            DeleteDanglingIndexAction.NAME,\n+            transportService,\n+            clusterService,\n+            threadPool,\n+            actionFilters,\n+            DeleteDanglingIndexRequest::new,\n+            indexNameExpressionResolver\n+        );\n+        this.settings = settings;\n+        this.nodeClient = nodeClient;\n+    }\n+\n+    @Override\n+    protected String executor() {\n+        return ThreadPool.Names.GENERIC;\n+    }\n+\n+    @Override\n+    protected DeleteDanglingIndexResponse read(StreamInput in) throws IOException {\n+        return new DeleteDanglingIndexResponse(in);\n+    }\n+\n+    @Override\n+    protected void masterOperation(\n+        Task task,\n+        DeleteDanglingIndexRequest deleteRequest,\n+        ClusterState state,\n+        ActionListener<DeleteDanglingIndexResponse> deleteListener\n+    ) throws Exception {\n+        findDanglingIndex(deleteRequest.getIndexUUID(), new ActionListener<>() {\n+\n+            @Override\n+            public void onResponse(Index indexToDelete) {\n+                // This flag is checked at this point so that we always check that the supplied index ID\n+                // does correspond to a dangling index.\n+                if (deleteRequest.isAcceptDataLoss() == false) {\n+                    deleteListener.onFailure(new IllegalArgumentException(\"accept_data_loss must be set to true\"));\n+                    return;\n+                }\n+\n+                String indexName = indexToDelete.getName();\n+\n+                final ActionListener<DeleteDanglingIndexResponse> clusterStateUpdatedListener = new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(DeleteDanglingIndexResponse response) {\n+                        deleteListener.onResponse(response);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.debug(\"Failed to delete dangling index [{}]\" + indexName, e);\n+                        deleteListener.onFailure(e);\n+                    }\n+                };\n+\n+                clusterService.submitStateUpdateTask(\n+                    \"delete-dangling-index \" + indexName,\n+                    new AckedClusterStateUpdateTask<>(deleteRequest, clusterStateUpdatedListener) {\n+\n+                        @Override\n+                        protected DeleteDanglingIndexResponse newResponse(boolean acknowledged) {\n+                            return new DeleteDanglingIndexResponse(acknowledged);\n+                        }\n+\n+                        @Override\n+                        public ClusterState execute(final ClusterState currentState) {\n+                            return deleteDanglingIndex(currentState, indexToDelete);\n+                        }\n+                    }\n+                );\n+            }\n+\n+            @Override\n+            public void onFailure(Exception e) {\n+                logger.debug(\"Failed to list dangling indices\", e);\n+                deleteListener.onFailure(e);\n+            }\n+        });\n+    }\n+\n+    private ClusterState deleteDanglingIndex(ClusterState currentState, Index indexToDelete) {\n+        final MetaData meta = currentState.metaData();\n+\n+        MetaData.Builder metaDataBuilder = MetaData.builder(meta);\n+\n+        final IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaDataBuilder.indexGraveyard());\n+\n+        final IndexGraveyard newGraveyard = graveyardBuilder.addTombstone(indexToDelete).build(settings);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTY3NTM5MQ=="}, "originalCommit": {"oid": "1d7cc02eb19dbe842b81de8af1b2d7bf6942ad8c"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0Njk5MjUzOnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMzoyNjowMFrOGHN3Lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMzoyNjowMFrOGHN3Lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIyMDMzNA==", "bodyText": "I think we also need to set index.routing.allocation.total_shards_per_node: 1 to ensure that every node gets a copy of the shard.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410220334", "createdAt": "2020-04-17T13:26:00Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID\n+     */\n+    private Map<String, String> createIndices(String... indices) throws IOException {\n+        assert indices.length > 0;\n+        for (String index : indices) {\n+            String indexSettings = \"{\"\n+                + \"  \\\"settings\\\": {\"\n+                + \"    \\\"index\\\": {\"\n+                + \"      \\\"number_of_shards\\\": 1,\"\n+                + \"      \\\"number_of_replicas\\\": 2\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 222}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0Njk5Nzc4OnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMzoyNzoyMlrOGHN6bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzoxMjo0NlrOGNTTsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIyMTE2Nw==", "bodyText": "Are you sure that the dangling index metadata was written to disk by this point? I think it happens asynchronously now.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410221167", "createdAt": "2020-04-17T13:27:22Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID\n+     */\n+    private Map<String, String> createIndices(String... indices) throws IOException {\n+        assert indices.length > 0;\n+        for (String index : indices) {\n+            String indexSettings = \"{\"\n+                + \"  \\\"settings\\\": {\"\n+                + \"    \\\"index\\\": {\"\n+                + \"      \\\"number_of_shards\\\": 1,\"\n+                + \"      \\\"number_of_replicas\\\": 2\"\n+                + \"    }\"\n+                + \"  }\"\n+                + \"}\";\n+            Request request = new Request(\"PUT\", \"/\" + index);\n+            request.setJsonEntity(indexSettings);\n+            final Response response = getRestClient().performRequest(request);\n+            assertOK(response);\n+        }\n+        ensureGreen(indices);\n+\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/indices?h=index,uuid\"));\n+        assertOK(catResponse);\n+\n+        final Map<String, String> createdIndexIDs = new HashMap<>();\n+\n+        final List<String> indicesAsList = Arrays.asList(indices);\n+\n+        for (String indexLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = indexLine.split(\" +\");\n+            if (indicesAsList.contains(elements[0])) {\n+                createdIndexIDs.put(elements[0], elements[1]);\n+            }\n+        }\n+\n+        assertThat(\"Expected to find as many index UUIDs as created indices\", createdIndexIDs.size(), equalTo(indices.length));\n+\n+        return createdIndexIDs;\n+    }\n+\n+    private void deleteIndex(String indexName) throws IOException {\n+        Response deleteResponse = getRestClient().performRequest(new Request(\"DELETE\", \"/\" + indexName));\n+        assertOK(deleteResponse);\n+    }\n+\n+    private DanglingIndexDetails createDanglingIndices(String... indices) throws Exception {\n+        ensureStableCluster(3);\n+        final Map<String, String> indexToUUID = createIndices(indices);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTM0NjQxNQ==", "bodyText": "I'm not sure - do you know how I would check? I had assumed that the tests would fail if the metadata wasn't written, because the create test explicitly checks what the dangling list API returns, and the import and delete tests both perform list calls before their respective actions.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r411346415", "createdAt": "2020-04-20T12:43:59Z", "author": {"login": "pugnascotia"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID\n+     */\n+    private Map<String, String> createIndices(String... indices) throws IOException {\n+        assert indices.length > 0;\n+        for (String index : indices) {\n+            String indexSettings = \"{\"\n+                + \"  \\\"settings\\\": {\"\n+                + \"    \\\"index\\\": {\"\n+                + \"      \\\"number_of_shards\\\": 1,\"\n+                + \"      \\\"number_of_replicas\\\": 2\"\n+                + \"    }\"\n+                + \"  }\"\n+                + \"}\";\n+            Request request = new Request(\"PUT\", \"/\" + index);\n+            request.setJsonEntity(indexSettings);\n+            final Response response = getRestClient().performRequest(request);\n+            assertOK(response);\n+        }\n+        ensureGreen(indices);\n+\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/indices?h=index,uuid\"));\n+        assertOK(catResponse);\n+\n+        final Map<String, String> createdIndexIDs = new HashMap<>();\n+\n+        final List<String> indicesAsList = Arrays.asList(indices);\n+\n+        for (String indexLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = indexLine.split(\" +\");\n+            if (indicesAsList.contains(elements[0])) {\n+                createdIndexIDs.put(elements[0], elements[1]);\n+            }\n+        }\n+\n+        assertThat(\"Expected to find as many index UUIDs as created indices\", createdIndexIDs.size(), equalTo(indices.length));\n+\n+        return createdIndexIDs;\n+    }\n+\n+    private void deleteIndex(String indexName) throws IOException {\n+        Response deleteResponse = getRestClient().performRequest(new Request(\"DELETE\", \"/\" + indexName));\n+        assertOK(deleteResponse);\n+    }\n+\n+    private DanglingIndexDetails createDanglingIndices(String... indices) throws Exception {\n+        ensureStableCluster(3);\n+        final Map<String, String> indexToUUID = createIndices(indices);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIyMTE2Nw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk2MTc3Nw==", "bodyText": "Yes I think these tests would fail if the metadata weren't written; I didn't see them fail but it might be pretty rare and I didn't try very hard to make it less rare, but nor could I see a reason why the metadata is definitely written by this point.\nI don't think it's enough to list the dangling indices, or at least I don't see how this guarantees anything about the persistence of metadata of the non-dangling indices.\nI was going to suggest adding a method to PersistedState but then I see that we already have one, see GatewayMetaState#allPendingAsyncStatesWritten. It should be enough to assertBusy that this returns true on all nodes.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r411961777", "createdAt": "2020-04-21T08:00:26Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID\n+     */\n+    private Map<String, String> createIndices(String... indices) throws IOException {\n+        assert indices.length > 0;\n+        for (String index : indices) {\n+            String indexSettings = \"{\"\n+                + \"  \\\"settings\\\": {\"\n+                + \"    \\\"index\\\": {\"\n+                + \"      \\\"number_of_shards\\\": 1,\"\n+                + \"      \\\"number_of_replicas\\\": 2\"\n+                + \"    }\"\n+                + \"  }\"\n+                + \"}\";\n+            Request request = new Request(\"PUT\", \"/\" + index);\n+            request.setJsonEntity(indexSettings);\n+            final Response response = getRestClient().performRequest(request);\n+            assertOK(response);\n+        }\n+        ensureGreen(indices);\n+\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/indices?h=index,uuid\"));\n+        assertOK(catResponse);\n+\n+        final Map<String, String> createdIndexIDs = new HashMap<>();\n+\n+        final List<String> indicesAsList = Arrays.asList(indices);\n+\n+        for (String indexLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = indexLine.split(\" +\");\n+            if (indicesAsList.contains(elements[0])) {\n+                createdIndexIDs.put(elements[0], elements[1]);\n+            }\n+        }\n+\n+        assertThat(\"Expected to find as many index UUIDs as created indices\", createdIndexIDs.size(), equalTo(indices.length));\n+\n+        return createdIndexIDs;\n+    }\n+\n+    private void deleteIndex(String indexName) throws IOException {\n+        Response deleteResponse = getRestClient().performRequest(new Request(\"DELETE\", \"/\" + indexName));\n+        assertOK(deleteResponse);\n+    }\n+\n+    private DanglingIndexDetails createDanglingIndices(String... indices) throws Exception {\n+        ensureStableCluster(3);\n+        final Map<String, String> indexToUUID = createIndices(indices);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIyMTE2Nw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYwMTAwOA==", "bodyText": "I see that you added an assertBusy() on GatewayMetaState#allPendingAsyncStatesWritten below. In fact I think we need to wait for that before we restart the node, not after.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416601008", "createdAt": "2020-04-28T13:12:46Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID\n+     */\n+    private Map<String, String> createIndices(String... indices) throws IOException {\n+        assert indices.length > 0;\n+        for (String index : indices) {\n+            String indexSettings = \"{\"\n+                + \"  \\\"settings\\\": {\"\n+                + \"    \\\"index\\\": {\"\n+                + \"      \\\"number_of_shards\\\": 1,\"\n+                + \"      \\\"number_of_replicas\\\": 2\"\n+                + \"    }\"\n+                + \"  }\"\n+                + \"}\";\n+            Request request = new Request(\"PUT\", \"/\" + index);\n+            request.setJsonEntity(indexSettings);\n+            final Response response = getRestClient().performRequest(request);\n+            assertOK(response);\n+        }\n+        ensureGreen(indices);\n+\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/indices?h=index,uuid\"));\n+        assertOK(catResponse);\n+\n+        final Map<String, String> createdIndexIDs = new HashMap<>();\n+\n+        final List<String> indicesAsList = Arrays.asList(indices);\n+\n+        for (String indexLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = indexLine.split(\" +\");\n+            if (indicesAsList.contains(elements[0])) {\n+                createdIndexIDs.put(elements[0], elements[1]);\n+            }\n+        }\n+\n+        assertThat(\"Expected to find as many index UUIDs as created indices\", createdIndexIDs.size(), equalTo(indices.length));\n+\n+        return createdIndexIDs;\n+    }\n+\n+    private void deleteIndex(String indexName) throws IOException {\n+        Response deleteResponse = getRestClient().performRequest(new Request(\"DELETE\", \"/\" + indexName));\n+        assertOK(deleteResponse);\n+    }\n+\n+    private DanglingIndexDetails createDanglingIndices(String... indices) throws Exception {\n+        ensureStableCluster(3);\n+        final Map<String, String> indexToUUID = createIndices(indices);\n+\n+        final AtomicReference<String> stoppedNodeName = new AtomicReference<>();\n+\n+        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDIyMTE2Nw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 264}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0NzExNTMxOnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QxMzo1NzowM1rOGHPFMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMjo0MjozNFrOGMhHjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI0MDMwNw==", "bodyText": "We can demonstrate that the dangling index was not properly deleted by adding this:\n        createIndex(\"additional\");\n        deleteIndex(\"additional\");\n        assertThat(listDanglingIndexIds(), empty());\n\nThe creation/deletion of this extra index pushes the tombstone out of the graveyard which resurrects it.\n(gruesome nomenclature isn't it? \ud83d\ude01)", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r410240307", "createdAt": "2020-04-17T13:57:03Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTM1OTc2Mg==", "bodyText": "Well that worked, in the sense that it showed that the index isn't being deleted off disk. I'll see if I can figure out how to delete it properly, but if you have any pointers, that'd be great.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r411359762", "createdAt": "2020-04-20T13:04:21Z", "author": {"login": "pugnascotia"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI0MDMwNw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTk2NDM2NQ==", "bodyText": "One idea is for ClusterChangedEvent#indicesDeletedFromClusterState to consider added tombstones as an indication that an index was deleted as well as removed index metadata.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r411964365", "createdAt": "2020-04-21T08:04:14Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI0MDMwNw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIxMjMwOQ==", "bodyText": "@DaveCTurner see commit d02c5ce. The tests now passes, but I'm really not sure about the changes. Can you please take a look?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r412212309", "createdAt": "2020-04-21T14:02:44Z", "author": {"login": "pugnascotia"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI0MDMwNw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTcyMjk4Nw==", "bodyText": "Yep, looks like what we want. Suggest using a Set<Index> rather than a list and a membership check but otherwise fine.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r415722987", "createdAt": "2020-04-27T11:12:41Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI0MDMwNw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc3ODcwMw==", "bodyText": "I switched to a set in b94efdf.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r415778703", "createdAt": "2020-04-27T12:42:34Z", "author": {"login": "pugnascotia"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDI0MDMwNw=="}, "originalCommit": {"oid": "5ce7f09b55fa5c518531347a0e73b13755f0d5cc"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MTk1NzQwOnYy", "diffSide": "RIGHT", "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzoxMzo0N1rOGNTWdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzoxMzo0N1rOGNTWdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYwMTcxOQ==", "bodyText": "Typo\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * @return a mapping from each createad index name to its UUID\n          \n          \n            \n                 * @return a mapping from each created index name to its UUID", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416601719", "createdAt": "2020-04-28T13:13:47Z", "author": {"login": "DaveCTurner"}, "path": "qa/smoke-test-http/src/test/java/org/elasticsearch/http/DanglingIndicesRestIT.java", "diffHunk": "@@ -0,0 +1,312 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.http;\n+\n+import org.elasticsearch.client.Request;\n+import org.elasticsearch.client.Response;\n+import org.elasticsearch.client.RestClient;\n+import org.elasticsearch.common.io.Streams;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.gateway.GatewayMetaState;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.ESIntegTestCase.ClusterScope;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.XContentTestUtils;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static org.elasticsearch.cluster.metadata.IndexGraveyard.SETTING_MAX_TOMBSTONES;\n+import static org.elasticsearch.gateway.DanglingIndicesState.AUTO_IMPORT_DANGLING_INDICES_SETTING;\n+import static org.elasticsearch.indices.IndicesService.WRITE_DANGLING_INDICES_INFO_SETTING;\n+import static org.elasticsearch.rest.RestStatus.ACCEPTED;\n+import static org.elasticsearch.rest.RestStatus.OK;\n+import static org.elasticsearch.test.XContentTestUtils.createJsonMapView;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.instanceOf;\n+import static org.hamcrest.Matchers.is;\n+\n+/**\n+ * This class tests the dangling indices REST API.  These tests are here\n+ * today so they have access to a proper REST client. They cannot be in\n+ * :server:integTest since the REST client needs a proper transport\n+ * implementation, and they cannot be REST tests today since they need to\n+ * restart nodes. Really, though, this test should live elsewhere.\n+ *\n+ * @see org.elasticsearch.action.admin.indices.dangling\n+ */\n+@ClusterScope(numDataNodes = 0, scope = ESIntegTestCase.Scope.TEST, autoManageMasterNodes = false)\n+public class DanglingIndicesRestIT extends HttpSmokeTestCase {\n+    private static final String INDEX_NAME = \"test-idx-1\";\n+    private static final String OTHER_INDEX_NAME = INDEX_NAME + \"-other\";\n+\n+    private Settings buildSettings(int maxTombstones) {\n+        return Settings.builder()\n+            // Limit the indices kept in the graveyard. This can be set to zero, so that\n+            // when we delete an index, it's definitely considered to be dangling.\n+            .put(SETTING_MAX_TOMBSTONES.getKey(), maxTombstones)\n+            .put(WRITE_DANGLING_INDICES_INFO_SETTING.getKey(), true)\n+            .put(AUTO_IMPORT_DANGLING_INDICES_SETTING.getKey(), false)\n+            .build();\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed via the REST API.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        final DanglingIndexDetails danglingIndexDetails = createDanglingIndices(INDEX_NAME);\n+        final String stoppedNodeId = mapNodeNameToId(danglingIndexDetails.stoppedNodeName);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final Response listResponse = restClient.performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(listResponse);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(listResponse.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+        assertThat(indices, hasSize(1));\n+\n+        assertThat(mapView.get(\"dangling_indices.0.index_name\"), equalTo(INDEX_NAME));\n+        assertThat(mapView.get(\"dangling_indices.0.index_uuid\"), equalTo(danglingIndexDetails.indexToUUID.get(INDEX_NAME)));\n+        assertThat(mapView.get(\"dangling_indices.0.creation_date_millis\"), instanceOf(Long.class));\n+        assertThat(mapView.get(\"dangling_indices.0.node_ids.0\"), equalTo(stoppedNodeId));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(0));\n+\n+        createDanglingIndices(INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request importRequest = new Request(\"POST\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        importRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure this parameter is accepted\n+        importRequest.addParameter(\"timeout\", \"20s\");\n+        final Response importResponse = restClient.performRequest(importRequest);\n+        assertThat(importResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(importResponse.getEntity().getContent());\n+        assertThat(mapView.get(\"accepted\"), equalTo(true));\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndicesCanBeDeleted() throws Exception {\n+        internalCluster().setBootstrapMasterNodeIndex(1);\n+        internalCluster().startNodes(3, buildSettings(1));\n+\n+        createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        final RestClient restClient = getRestClient();\n+\n+        final List<String> danglingIndexIds = listDanglingIndexIds();\n+        assertThat(danglingIndexIds, hasSize(1));\n+\n+        final Request deleteRequest = new Request(\"DELETE\", \"/_dangling/\" + danglingIndexIds.get(0));\n+        deleteRequest.addParameter(\"accept_data_loss\", \"true\");\n+        // Ensure these parameters is accepted\n+        deleteRequest.addParameter(\"timeout\", \"20s\");\n+        deleteRequest.addParameter(\"master_timeout\", \"20s\");\n+        final Response deleteResponse = restClient.performRequest(deleteRequest);\n+        assertThat(deleteResponse.getStatusLine().getStatusCode(), equalTo(ACCEPTED.getStatus()));\n+\n+        assertBusy(() -> assertThat(\"Expected dangling index to be deleted\", listDanglingIndexIds(), hasSize(0)));\n+\n+        // The dangling index that we deleted ought to have been removed from disk. Check by\n+        // creating and deleting another index, which creates a new tombstone entry, which should\n+        // not cause the deleted dangling index to be considered \"live\" again, just because its\n+        // tombstone has been pushed out of the graveyard.\n+        createIndex(\"additional\");\n+        deleteIndex(\"additional\");\n+        assertThat(listDanglingIndexIds(), is(empty()));\n+    }\n+\n+    private List<String> listDanglingIndexIds() throws IOException {\n+        final Response response = getRestClient().performRequest(new Request(\"GET\", \"/_dangling\"));\n+        assertOK(response);\n+\n+        final XContentTestUtils.JsonMapView mapView = createJsonMapView(response.getEntity().getContent());\n+\n+        assertThat(mapView.get(\"_nodes.total\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.successful\"), equalTo(3));\n+        assertThat(mapView.get(\"_nodes.failed\"), equalTo(0));\n+\n+        List<Object> indices = mapView.get(\"dangling_indices\");\n+\n+        List<String> danglingIndexIds = new ArrayList<>();\n+\n+        for (int i = 0; i < indices.size(); i++) {\n+            danglingIndexIds.add(mapView.get(\"dangling_indices.\" + i + \".index_uuid\"));\n+        }\n+\n+        return danglingIndexIds;\n+    }\n+\n+    private void assertOK(Response response) {\n+        assertThat(response.getStatusLine().getStatusCode(), equalTo(OK.getStatus()));\n+    }\n+\n+    /**\n+     * Given a node name, finds the corresponding node ID.\n+     */\n+    private String mapNodeNameToId(String nodeName) throws IOException {\n+        final Response catResponse = getRestClient().performRequest(new Request(\"GET\", \"/_cat/nodes?full_id&h=id,name\"));\n+        assertOK(catResponse);\n+\n+        for (String nodeLine : Streams.readAllLines(catResponse.getEntity().getContent())) {\n+            String[] elements = nodeLine.split(\" \");\n+            if (elements[1].equals(nodeName)) {\n+                return elements[0];\n+            }\n+        }\n+\n+        throw new AssertionError(\"Failed to map node name [\" + nodeName + \"] to node ID\");\n+    }\n+\n+    /**\n+     * Helper that creates one or more indices, and importantly,\n+     * checks that they are green before proceeding. This is important\n+     * because the tests in this class stop and restart nodes, assuming\n+     * that each index has a primary or replica shard on every node, and if\n+     * a node is stopped prematurely, this assumption is broken.\n+     *\n+     * @return a mapping from each createad index name to its UUID", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MjAzNTc2OnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMDo1MVrOGNUHgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMDo1MVrOGNUHgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNDI3Mw==", "bodyText": "Could we expectThrows(IllegalArgumentException.class, ...) here instead of this?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416614273", "createdAt": "2020-04-28T13:30:51Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 219}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MjAzOTA4OnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMToyOFrOGNUJbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMToyOFrOGNUJbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNDc2Nw==", "bodyText": "Could we expectThrows() here?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416614767", "createdAt": "2020-04-28T13:31:28Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 244}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MjA0NTcwOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMjo0N1rOGNUNeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMjo0N1rOGNUNeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNTgwMQ==", "bodyText": "WDYT about making the same fix to this test as the REST test to verify that the index really did get deleted?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416615801", "createdAt": "2020-04-28T13:32:47Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;\n+        }\n+\n+        assertNotNull(\"No exception thrown\", caughtException);\n+        assertThat(caughtException.getMessage(), containsString(\"accept_data_loss must be set to true\"));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndexCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        client().admin().cluster().deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndexUUID, true)).actionGet();\n+\n+        assertThat(listDanglingIndices(), is(empty()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 271}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MjA0NzgxOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMzoxOFrOGNUOzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMzoxOFrOGNUOzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNjE0Mw==", "bodyText": "Also here -- WDYT about making the same fix to this test as the REST test to verify that the index really did get deleted?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416616143", "createdAt": "2020-04-28T13:33:18Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;\n+        }\n+\n+        assertNotNull(\"No exception thrown\", caughtException);\n+        assertThat(caughtException.getMessage(), containsString(\"accept_data_loss must be set to true\"));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndexCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        client().admin().cluster().deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndexUUID, true)).actionGet();\n+\n+        assertThat(listDanglingIndices(), is(empty()));\n+    }\n+\n+    /**\n+     * Check that when a index is found to be dangling on more than one node, it can be deleted.\n+     */\n+    public void testDanglingIndexOverMultipleNodesCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        createIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        // Restart 2 nodes, deleting the indices in their absence, so that there is a dangling index to recover\n         internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n \n             @Override\n             public Settings onNodeStopped(String nodeName) throws Exception {\n-                internalCluster().validateClusterFormed();\n-                assertAcked(client().admin().indices().prepareDelete(INDEX_NAME));\n+                internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+                    @Override\n+                    public Settings onNodeStopped(String nodeName) throws Exception {\n+                        internalCluster().validateClusterFormed();\n+                        assertAcked(client().admin().indices().prepareDelete(INDEX_NAME));\n+                        assertAcked(client().admin().indices().prepareDelete(OTHER_INDEX_NAME));\n+                        return super.onNodeStopped(nodeName);\n+                    }\n+                });\n+\n                 return super.onNodeStopped(nodeName);\n             }\n         });\n \n-        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n-        // amount of time\n-        assertFalse(\n-            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n-            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        final AtomicReference<List<DanglingIndexInfo>> danglingIndices = new AtomicReference<>();\n+\n+        final List<DanglingIndexInfo> results = listDanglingIndices();\n+\n+        // Both the stopped nodes should have found a dangling index.\n+        assertThat(results, hasSize(2));\n+        danglingIndices.set(results);\n+\n+        // Try to delete the index - this request should succeed\n+        client().admin()\n+            .cluster()\n+            .deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndices.get().get(0).getIndexUUID(), true))\n+            .actionGet();\n+\n+        assertBusy(() -> assertThat(listDanglingIndices(), empty()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 324}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MjA0OTY0OnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMzo0MVrOGNUP7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozMzo0MVrOGNUP7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxNjQyOA==", "bodyText": "Suggest expectThrows here.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416616428", "createdAt": "2020-04-28T13:33:41Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;\n+        }\n+\n+        assertNotNull(\"No exception thrown\", caughtException);\n+        assertThat(caughtException.getMessage(), containsString(\"accept_data_loss must be set to true\"));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndexCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        client().admin().cluster().deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndexUUID, true)).actionGet();\n+\n+        assertThat(listDanglingIndices(), is(empty()));\n+    }\n+\n+    /**\n+     * Check that when a index is found to be dangling on more than one node, it can be deleted.\n+     */\n+    public void testDanglingIndexOverMultipleNodesCanBeDeleted() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        createIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+\n+        // Restart 2 nodes, deleting the indices in their absence, so that there is a dangling index to recover\n         internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n \n             @Override\n             public Settings onNodeStopped(String nodeName) throws Exception {\n-                internalCluster().validateClusterFormed();\n-                assertAcked(client().admin().indices().prepareDelete(INDEX_NAME));\n+                internalCluster().restartRandomDataNode(new InternalTestCluster.RestartCallback() {\n+\n+                    @Override\n+                    public Settings onNodeStopped(String nodeName) throws Exception {\n+                        internalCluster().validateClusterFormed();\n+                        assertAcked(client().admin().indices().prepareDelete(INDEX_NAME));\n+                        assertAcked(client().admin().indices().prepareDelete(OTHER_INDEX_NAME));\n+                        return super.onNodeStopped(nodeName);\n+                    }\n+                });\n+\n                 return super.onNodeStopped(nodeName);\n             }\n         });\n \n-        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n-        // amount of time\n-        assertFalse(\n-            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n-            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        final AtomicReference<List<DanglingIndexInfo>> danglingIndices = new AtomicReference<>();\n+\n+        final List<DanglingIndexInfo> results = listDanglingIndices();\n+\n+        // Both the stopped nodes should have found a dangling index.\n+        assertThat(results, hasSize(2));\n+        danglingIndices.set(results);\n+\n+        // Try to delete the index - this request should succeed\n+        client().admin()\n+            .cluster()\n+            .deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndices.get().get(0).getIndexUUID(), true))\n+            .actionGet();\n+\n+        assertBusy(() -> assertThat(listDanglingIndices(), empty()));\n+    }\n+\n+    /**\n+     * Check that when deleting a dangling index, it is required that the \"accept_data_loss\" flag is set.\n+     */\n+    public void testDeleteDanglingIndicesRequiresDataLossFlagToBeTrue() throws Exception {\n+        final Settings settings = buildSettings(1, true, false);\n+        internalCluster().startNodes(3, settings);\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME, OTHER_INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().deleteDanglingIndex(new DeleteDanglingIndexRequest(danglingIndexUUID, false)).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 342}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MjA2NjI1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozNzowM1rOGNUaAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzozNzowM1rOGNUaAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYxOTAwOQ==", "bodyText": "Should probably have a unit test that passes something other than emptyMap() to this new parameter.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416619009", "createdAt": "2020-04-28T13:37:03Z", "author": {"login": "DaveCTurner"}, "path": "server/src/main/java/org/elasticsearch/gateway/DanglingIndicesState.java", "diffHunk": "@@ -134,24 +153,37 @@ void cleanupAllocatedDangledIndices(Metadata metadata) {\n      * to the currently tracked dangling indices.\n      */\n     void findNewAndAddDanglingIndices(final Metadata metadata) {\n-        danglingIndices.putAll(findNewDanglingIndices(metadata));\n+        final IndexGraveyard graveyard = metadata.indexGraveyard();\n+\n+        // If a tombstone is created for a dangling index, we need to make sure that the\n+        // index is no longer considered dangling.\n+        for (Index key : danglingIndices.keySet()) {\n+            if (graveyard.containsIndex(key)) {\n+                danglingIndices.remove(key);\n+            }\n+        }\n+\n+        danglingIndices.putAll(findNewDanglingIndices(danglingIndices, metadata));\n     }\n \n     /**\n      * Finds new dangling indices by iterating over the indices and trying to find indices\n      * that have state on disk, but are not part of the provided meta data, or not detected\n      * as dangled already.\n      */\n-    Map<Index, IndexMetadata> findNewDanglingIndices(final Metadata metadata) {\n+    public Map<Index, IndexMetadata> findNewDanglingIndices(Map<Index, IndexMetadata> existingDanglingIndices, final Metadata metadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MjEwMjEzOnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzo0NDo0NVrOGNUw-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzo0NDo0NVrOGNUw-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYyNDg4OQ==", "bodyText": "Should we also test that this works if auto-import is enabled? You can make an un-importable dangling index by deleting it and creating another with the same name while the node is away.", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416624889", "createdAt": "2020-04-28T13:44:45Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5MjEwNjA4OnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzo0NTozOVrOGNUzoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxMzo0NTozOVrOGNUzoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjYyNTU2OA==", "bodyText": "Should we also check that this works if auto-import is enabled?", "url": "https://github.com/elastic/elasticsearch/pull/50920#discussion_r416625568", "createdAt": "2020-04-28T13:45:39Z", "author": {"login": "DaveCTurner"}, "path": "server/src/test/java/org/elasticsearch/indices/recovery/DanglingIndicesIT.java", "diffHunk": "@@ -126,29 +146,291 @@ public Settings onNodeStopped(String nodeName) throws Exception {\n      * Check that when dangling indices are not written, then they cannot be recovered into the cluster.\n      */\n     public void testDanglingIndicesAreNotRecoveredWhenNotWritten() throws Exception {\n-        internalCluster().startNodes(3, buildSettings(false, true));\n+        internalCluster().startNodes(3, buildSettings(0, false, true));\n \n-        createIndex(INDEX_NAME, Settings.builder().put(\"number_of_replicas\", 2).build());\n-        ensureGreen(INDEX_NAME);\n-        internalCluster().getInstances(IndicesService.class).forEach(\n-            indicesService -> assertTrue(indicesService.allPendingDanglingIndicesWritten()));\n+        createDanglingIndices(INDEX_NAME);\n \n-        // Restart node, deleting the index in its absence, so that there is a dangling index to recover\n+        // Since index recovery is async, we can't prove index recovery will never occur, just that it doesn't occur within some reasonable\n+        // amount of time\n+        assertFalse(\n+            \"Did not expect dangling index \" + INDEX_NAME + \" to be recovered\",\n+            waitUntil(() -> indexExists(INDEX_NAME), 1, TimeUnit.SECONDS)\n+        );\n+    }\n+\n+    /**\n+     * Check that when dangling indices are discovered, then they can be listed.\n+     */\n+    public void testDanglingIndicesCanBeListed() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final ListDanglingIndicesResponse response = client().admin()\n+            .cluster()\n+            .listDanglingIndices(new ListDanglingIndicesRequest())\n+            .actionGet();\n+        assertThat(response.status(), equalTo(RestStatus.OK));\n+\n+        final List<NodeListDanglingIndicesResponse> nodeResponses = response.getNodes();\n+        assertThat(\"Didn't get responses from all nodes\", nodeResponses, hasSize(3));\n+\n+        for (NodeListDanglingIndicesResponse nodeResponse : nodeResponses) {\n+            if (nodeResponse.getNode().getName().equals(stoppedNodeName)) {\n+                assertThat(\"Expected node that was stopped to have one dangling index\", nodeResponse.getDanglingIndices(), hasSize(1));\n+\n+                final DanglingIndexInfo danglingIndexInfo = nodeResponse.getDanglingIndices().get(0);\n+                assertThat(danglingIndexInfo.getIndexName(), equalTo(INDEX_NAME));\n+            } else {\n+                assertThat(\"Expected node that was never stopped to have no dangling indices\", nodeResponse.getDanglingIndices(), empty());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Check that dangling indices can be imported.\n+     */\n+    public void testDanglingIndicesCanBeImported() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, true);\n+\n+        client().admin().cluster().importDanglingIndex(request).actionGet();\n+\n+        assertTrue(\"Expected dangling index \" + INDEX_NAME + \" to be recovered\", indexExists(INDEX_NAME));\n+    }\n+\n+    /**\n+     * Check that the when sending an import-dangling-indices request, the specified UUIDs are validated as\n+     * being dangling.\n+     */\n+    public void testDanglingIndicesMustExistToBeImported() {\n+        internalCluster().startNodes(1, buildSettings(0, true, false));\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(\"NonExistentUUID\", true);\n+\n+        boolean noExceptionThrown = false;\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+            noExceptionThrown = true;\n+        } catch (Exception e) {\n+            assertThat(e, instanceOf(IllegalArgumentException.class));\n+            assertThat(e.getMessage(), containsString(\"No dangling index found for UUID [NonExistentUUID]\"));\n+        }\n+\n+        assertFalse(\"No exception thrown\", noExceptionThrown);\n+    }\n+\n+    /**\n+     * Check that a dangling index can only be imported if \"accept_data_loss\" is set to true.\n+     */\n+    public void testMustAcceptDataLossToImportDanglingIndex() throws Exception {\n+        internalCluster().startNodes(3, buildSettings(0, true, false));\n+\n+        final String stoppedNodeName = createDanglingIndices(INDEX_NAME);\n+        final String danglingIndexUUID = findDanglingIndexForNode(stoppedNodeName, INDEX_NAME);\n+\n+        final ImportDanglingIndexRequest request = new ImportDanglingIndexRequest(danglingIndexUUID, false);\n+\n+        Exception caughtException = null;\n+\n+        try {\n+            client().admin().cluster().importDanglingIndex(request).actionGet();\n+        } catch (Exception e) {\n+            caughtException = e;\n+        }\n+\n+        assertNotNull(\"No exception thrown\", caughtException);\n+        assertThat(caughtException.getMessage(), containsString(\"accept_data_loss must be set to true\"));\n+    }\n+\n+    /**\n+     * Check that dangling indices can be deleted. Since this requires that\n+     * we add an entry to the index graveyard, the graveyard size must be\n+     * greater than 1. To test deletes, we set the index graveyard size to\n+     * 1, then create two indices and delete them both while one node in\n+     * the cluster is stopped. The deletion of the second pushes the deletion\n+     * of the first out of the graveyard. When the stopped node is resumed,\n+     * only the second index will be found into the graveyard and the the\n+     * other will be considered dangling, and can therefore be listed and\n+     * deleted through the API\n+     */\n+    public void testDanglingIndexCanBeDeleted() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a2ce053cf3111ba6b637778f08231c71289f0504"}, "originalPosition": 262}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4667, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}