{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg2ODI0NTk5", "number": 53423, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMTozMzo0MFrODnozFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNzozMjoyMlrODn6tgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyODg5NDkyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/SearchService.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMTozMzo0MFrOF1wQxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMTo1NzowMVrOF1xW5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkwOTU3NQ==", "bodyText": "@jimczi, I remember us trying not to hold on to references to the SearchRequest because it could be big. Or something like that. Is that still a thing? It looks like we keep the SearchRequest around for a while during the search right now.", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r391909575", "createdAt": "2020-03-12T21:33:40Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/SearchService.java", "diffHunk": "@@ -1200,9 +1203,31 @@ public IndicesService getIndicesService() {\n         return indicesService;\n     }\n \n-    public InternalAggregation.ReduceContext createReduceContext(boolean finalReduce) {\n-        return new InternalAggregation.ReduceContext(bigArrays, scriptService,\n-            finalReduce ? multiBucketConsumerService.create() : bucketCount -> {}, finalReduce);\n+    /**\n+     * Returns a builder for {@link InternalAggregation.ReduceContext}. This\n+     * builder retains a reference to the provided {@link SearchRequest}.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyNzUyNw==", "bodyText": "That's totally ok since you refer to the original request which is unique per search. You also build the pipeline tree lazily which seems like a nice win to me.", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r391927527", "createdAt": "2020-03-12T21:57:01Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/search/SearchService.java", "diffHunk": "@@ -1200,9 +1203,31 @@ public IndicesService getIndicesService() {\n         return indicesService;\n     }\n \n-    public InternalAggregation.ReduceContext createReduceContext(boolean finalReduce) {\n-        return new InternalAggregation.ReduceContext(bigArrays, scriptService,\n-            finalReduce ? multiBucketConsumerService.create() : bucketCount -> {}, finalReduce);\n+    /**\n+     * Returns a builder for {@link InternalAggregation.ReduceContext}. This\n+     * builder retains a reference to the provided {@link SearchRequest}.\n+     */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkwOTU3NQ=="}, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyODk2OTIxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/action/search/SearchPhaseController.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMTo0NzowMFrOF1w38w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMTo0NzowMFrOF1w38w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkxOTYwMw==", "bodyText": "\u2764\ufe0f", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r391919603", "createdAt": "2020-03-12T21:47:00Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/action/search/SearchPhaseController.java", "diffHunk": "@@ -394,17 +391,30 @@ private SearchHits getHits(ReducedQueryPhase reducedQueryPhase, boolean ignoreFr\n      * @param queryResults a list of non-null query shard results\n      */\n     ReducedQueryPhase reducedScrollQueryPhase(Collection<? extends SearchPhaseResult> queryResults) {\n-        return reducedQueryPhase(queryResults, true, SearchContext.TRACK_TOTAL_HITS_ACCURATE, true);\n+        InternalAggregation.ReduceContextBuilder aggReduceContextBuilder = new InternalAggregation.ReduceContextBuilder() {\n+            @Override\n+            public ReduceContext forPartialReduction() {\n+                throw new UnsupportedOperationException(\"Scroll requests don't have aggs\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyOTA0MjYzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/rollup/src/main/java/org/elasticsearch/xpack/rollup/RollupResponseTranslator.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQyMjowMDoxNlrOF1xeQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNzo0MjowMFrOF2M1og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTQwOQ==", "bodyText": "Good catch, let's open an issue since it seems easy to fix rather than a TODO ?", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r391929409", "createdAt": "2020-03-12T22:00:16Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/rollup/src/main/java/org/elasticsearch/xpack/rollup/RollupResponseTranslator.java", "diffHunk": "@@ -289,14 +292,14 @@ private static SearchResponse doCombineResponse(SearchResponse liveResponse, Lis\n             // Iteratively merge in each new set of unrolled aggs, so that we can identify/fix overlapping doc_counts\n             // in the next round of unrolling\n             InternalAggregations finalUnrolledAggs = new InternalAggregations(unrolledAggs);\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs), finalReduceContext);\n         }\n \n         // Add in the live aggregations if they exist\n         if (liveAggs.asList().size() != 0) {\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            // TODO it looks like this passes the \"final\" reduce context more than once.\n+            // Once here and once in the for above. That is bound to cause trouble.\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs), finalReduceContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkzMzYyMQ==", "bodyText": "++", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r391933621", "createdAt": "2020-03-12T22:11:30Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/rollup/src/main/java/org/elasticsearch/xpack/rollup/RollupResponseTranslator.java", "diffHunk": "@@ -289,14 +292,14 @@ private static SearchResponse doCombineResponse(SearchResponse liveResponse, Lis\n             // Iteratively merge in each new set of unrolled aggs, so that we can identify/fix overlapping doc_counts\n             // in the next round of unrolling\n             InternalAggregations finalUnrolledAggs = new InternalAggregations(unrolledAggs);\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs), finalReduceContext);\n         }\n \n         // Add in the live aggregations if they exist\n         if (liveAggs.asList().size() != 0) {\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            // TODO it looks like this passes the \"final\" reduce context more than once.\n+            // Once here and once in the for above. That is bound to cause trouble.\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs), finalReduceContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTQwOQ=="}, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0NjUzMA==", "bodyText": "#53524", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r391946530", "createdAt": "2020-03-12T22:51:13Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/rollup/src/main/java/org/elasticsearch/xpack/rollup/RollupResponseTranslator.java", "diffHunk": "@@ -289,14 +292,14 @@ private static SearchResponse doCombineResponse(SearchResponse liveResponse, Lis\n             // Iteratively merge in each new set of unrolled aggs, so that we can identify/fix overlapping doc_counts\n             // in the next round of unrolling\n             InternalAggregations finalUnrolledAggs = new InternalAggregations(unrolledAggs);\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs), finalReduceContext);\n         }\n \n         // Add in the live aggregations if they exist\n         if (liveAggs.asList().size() != 0) {\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            // TODO it looks like this passes the \"final\" reduce context more than once.\n+            // Once here and once in the for above. That is bound to cause trouble.\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs), finalReduceContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTQwOQ=="}, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3MTg0Mg==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r391971842", "createdAt": "2020-03-13T00:27:03Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/rollup/src/main/java/org/elasticsearch/xpack/rollup/RollupResponseTranslator.java", "diffHunk": "@@ -289,14 +292,14 @@ private static SearchResponse doCombineResponse(SearchResponse liveResponse, Lis\n             // Iteratively merge in each new set of unrolled aggs, so that we can identify/fix overlapping doc_counts\n             // in the next round of unrolling\n             InternalAggregations finalUnrolledAggs = new InternalAggregations(unrolledAggs);\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs), finalReduceContext);\n         }\n \n         // Add in the live aggregations if they exist\n         if (liveAggs.asList().size() != 0) {\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            // TODO it looks like this passes the \"final\" reduce context more than once.\n+            // Once here and once in the for above. That is bound to cause trouble.\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs), finalReduceContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTQwOQ=="}, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM3Nzc2Mg==", "bodyText": "\ud83d\udc4d indeed.  Rollup doesn't work with pipelines anyhow (mostly due to the serialization issue, with different aggs being sent to rollup vs live indices, it messes up how pipelines operate).... but I could see multiple final reductions potentially hurting accuracy on certain aggs that care like terms", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r392377762", "createdAt": "2020-03-13T17:42:00Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/rollup/src/main/java/org/elasticsearch/xpack/rollup/RollupResponseTranslator.java", "diffHunk": "@@ -289,14 +292,14 @@ private static SearchResponse doCombineResponse(SearchResponse liveResponse, Lis\n             // Iteratively merge in each new set of unrolled aggs, so that we can identify/fix overlapping doc_counts\n             // in the next round of unrolling\n             InternalAggregations finalUnrolledAggs = new InternalAggregations(unrolledAggs);\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, finalUnrolledAggs), finalReduceContext);\n         }\n \n         // Add in the live aggregations if they exist\n         if (liveAggs.asList().size() != 0) {\n-            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs),\n-                    new InternalAggregation.ReduceContext(reduceContext.bigArrays(), reduceContext.scriptService(), true));\n+            // TODO it looks like this passes the \"final\" reduce context more than once.\n+            // Once here and once in the for above. That is bound to cause trouble.\n+            currentTree = InternalAggregations.reduce(Arrays.asList(currentTree, liveAggs), finalReduceContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTQwOQ=="}, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQzMTgyOTc2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNzozMjoyMlrOF2Mhzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QxNzozMjoyMlrOF2Mhzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjM3MjY4Ng==", "bodyText": "While we're here, is it possible to nuke skipResolveOrder too?  I believe it's only used by BasePipelineAggregationTestCase, and that doesn't even invoke build() so this is basically \"dead\" testing code.  I think.\nNot a problem to leave if there's a complication... I just particularly dislike this little tidbit and wouldn't mind seeing it go if we're already touching this :)", "url": "https://github.com/elastic/elasticsearch/pull/53423#discussion_r392372686", "createdAt": "2020-03-13T17:32:22Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java", "diffHunk": "@@ -311,8 +315,10 @@ public AggregatorFactories build(QueryShardContext queryShardContext, Aggregator\n             if (skipResolveOrder) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "abf6fc10a0cd56e55c1215249bae9ed130a6ffc0"}, "originalPosition": 19}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3294, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}