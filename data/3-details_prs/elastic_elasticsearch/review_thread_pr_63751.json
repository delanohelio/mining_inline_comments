{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA0MTY5ODU4", "number": 63751, "reviewThreads": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOTozMzoxMFrOEvr7uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMDo0ODo1NVrOEwL_Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NDM4MzMxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwOTozMzoxMFrOHkzYtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDo0ODoxM1rOHlA63A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MjY5Mw==", "bodyText": "A slash is missing after C:\\Users.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r508352693", "createdAt": "2020-10-20T09:33:10Z", "author": {"login": "astefan"}, "path": "x-pack/plugin/sql/connectors/tableau/README.md", "diffHunk": "@@ -0,0 +1,20 @@\n+# Tableau connector for Elasticsearch\n+\n+The Tableau Connector works in tandem with the Elastic JDBC driver to facilitate the query of Elasticsearch. It gives users a simple way to query Elasticsearch data from Tableau.\n+After providing basic connection and authentication information, users can easily select Elasticsearch indices for use in Tableau Desktop and Tableau Server.\n+\n+## Installation\n+\n+1. Tableau connector for Elasticsearch installation:\n+ - Go to the [Connector Download](https://www.elastic.co/downloads/tableau-connector) page.\n+ - Download the _.taco_ connector file.\n+ - Move the _.taco_ file here:\n+    - Windows: C:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869bbb8d62199f92780c2bcb46ae93f386c6bdb9"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU3NDQyOA==", "bodyText": "Fixed.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r508574428", "createdAt": "2020-10-20T14:48:13Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/README.md", "diffHunk": "@@ -0,0 +1,20 @@\n+# Tableau connector for Elasticsearch\n+\n+The Tableau Connector works in tandem with the Elastic JDBC driver to facilitate the query of Elasticsearch. It gives users a simple way to query Elasticsearch data from Tableau.\n+After providing basic connection and authentication information, users can easily select Elasticsearch indices for use in Tableau Desktop and Tableau Server.\n+\n+## Installation\n+\n+1. Tableau connector for Elasticsearch installation:\n+ - Go to the [Connector Download](https://www.elastic.co/downloads/tableau-connector) page.\n+ - Download the _.taco_ connector file.\n+ - Move the _.taco_ file here:\n+    - Windows: C:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODM1MjY5Mw=="}, "originalCommit": {"oid": "869bbb8d62199f92780c2bcb46ae93f386c6bdb9"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4ODY4NTMzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNjo1MDowMFrOHlcpyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNjo1MDowMFrOHlcpyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAyODgwOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - Windows: C:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors\n          \n          \n            \n                - Windows: C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509028809", "createdAt": "2020-10-21T06:50:00Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/README.md", "diffHunk": "@@ -0,0 +1,20 @@\n+# Tableau connector for Elasticsearch\n+\n+The Tableau Connector works in tandem with the Elastic JDBC driver to facilitate the query of Elasticsearch. It gives users a simple way to query Elasticsearch data from Tableau.\n+After providing basic connection and authentication information, users can easily select Elasticsearch indices for use in Tableau Desktop and Tableau Server.\n+\n+## Installation\n+\n+1. Tableau connector for Elasticsearch installation:\n+ - Go to the [Connector Download](https://www.elastic.co/downloads/tableau-connector) page.\n+ - Download the _.taco_ connector file.\n+ - Move the _.taco_ file here:\n+    - Windows: C:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33a1dd180ac0481e08cd9875221a9ba5c13d70f6"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTAxMTAxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxNjo0NlrOHlfvbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODoxNjo0NlrOHlfvbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA3OTQwNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            3. Relaunch Logstash and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n          \n          \n            \n            3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509079406", "createdAt": "2020-10-21T08:16:46Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,118 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0154788b41719a3b81173453dd7a082fc7b331a2"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEyNTgxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0MzoxMFrOHlg3xQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0MzoxMFrOHlg3xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5NzkyNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n          \n          \n            \n            1. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509097925", "createdAt": "2020-10-21T08:43:10Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEyNzQ0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0MzozNVrOHlg45g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0MzozNVrOHlg45g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODIxNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n          \n          \n            \n            2. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098214", "createdAt": "2020-10-21T08:43:35Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEyODQ5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0Mzo0OVrOHlg5jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0Mzo0OVrOHlg5jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODM4Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n          \n          \n            \n            3. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098383", "createdAt": "2020-10-21T08:43:49Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEyOTA5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NDowMFrOHlg6CQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NDowMFrOHlg6CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODUwNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n          \n          \n            \n            4. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098505", "createdAt": "2020-10-21T08:44:00Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEzMDg4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NDoyNVrOHlg7OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NDoyNVrOHlg7OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODgwOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n          \n          \n            \n            1. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098808", "createdAt": "2020-10-21T08:44:25Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEzMTc1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NDozOFrOHlg7yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NDozOFrOHlg7yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5ODk1Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n          \n          \n            \n            2. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509098952", "createdAt": "2020-10-21T08:44:38Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEzMjgwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NDo1M1rOHlg8ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NDo1M1rOHlg8ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTE0Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n          \n          \n            \n            3. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099146", "createdAt": "2020-10-21T08:44:53Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEzMzg0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NTowNVrOHlg9Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NTowNVrOHlg9Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTMwMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            0. Same as in the automated testing.\n          \n          \n            \n            1. Same as in the automated testing.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099303", "createdAt": "2020-10-21T08:45:05Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEzNDk0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NToxN1rOHlg9yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NToxN1rOHlg9yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTQ2NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n          \n          \n            \n            2. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099464", "createdAt": "2020-10-21T08:45:17Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.\n+\n+1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEzNjEwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NTozMFrOHlg-bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NTozMFrOHlg-bQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTYyOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.\n          \n          \n            \n            3. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099629", "createdAt": "2020-10-21T08:45:30Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.\n+\n+1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n+\tTo set up new sources, launch Tableau from command line with the following parameters (PowerShell example):\n+\t```\n+\t.\\tableau.exe -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true\n+\t```\n+\twhere `<path>` is either the path to the directory containing the `.taco` connector *or* the path to the directory containing the connector directory, if this isn't yet packaged.\n+  \n+\t**Note**: When connecting, make sure you pass the `timezone=Z` parameter into the `Additional settings` field of the connection dialog. This sets the timezone for the time data to UTC; if this isn't set, the JDBC driver will use JVM's/system's time zone, which will then result in some failed tests if the machine's not set to the UTC timezone.\n+\n+\tSave the TDS files as `cast_calcs.elastic.tds` and `Staples.elastic.tds`\n+\n+2. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEzNzUyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NTo0OVrOHlg_WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NTo0OVrOHlg_WQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA5OTg2NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            3. Generate the tests by invoking TDVT as follows:\n          \n          \n            \n            4. Generate the tests by invoking TDVT as follows:", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509099865", "createdAt": "2020-10-21T08:45:49Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.\n+\n+1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n+\tTo set up new sources, launch Tableau from command line with the following parameters (PowerShell example):\n+\t```\n+\t.\\tableau.exe -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true\n+\t```\n+\twhere `<path>` is either the path to the directory containing the `.taco` connector *or* the path to the directory containing the connector directory, if this isn't yet packaged.\n+  \n+\t**Note**: When connecting, make sure you pass the `timezone=Z` parameter into the `Additional settings` field of the connection dialog. This sets the timezone for the time data to UTC; if this isn't set, the JDBC driver will use JVM's/system's time zone, which will then result in some failed tests if the machine's not set to the UTC timezone.\n+\n+\tSave the TDS files as `cast_calcs.elastic.tds` and `Staples.elastic.tds`\n+\n+2. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.\n+\tEither package TDVT and install it as a Python PIP module (recommended, if [working](https://github.com/tableau/connector-plugin-sdk/issues/534)), or simply copy the `tdvt` directory of the repo into the \"workspace\" directory. Invoking the TDVT will then be done as `py -3 -m tdvt.tdvt <params>`, or `py -3 .\\tdvt\\tdvt_launcher.py <params>`, respectively. In the steps below the invokation will be indicated by the `$TDVT` call.\n+\t```\n+\t$TDVT action --setup\n+\t```\n+\n+\tCopy/move the above saved `*.tds` files into the just created `tds` directory in the workspace.\n+\tEdit `config/tdvt/tdvt_override.ini` to update `TAB_CLI_EXE_X64` definition.\n+\n+3. Generate the tests by invoking TDVT as follows:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTEzODY3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NjowNlrOHlhAFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwODo0NjowNlrOHlhAFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEwMDA1Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            4. Run the tests:\n          \n          \n            \n            5. Run the tests:", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509100053", "createdAt": "2020-10-21T08:46:06Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,117 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+0. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+1. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+2. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+3. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+0. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+1. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+2. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+0. Same as in the automated testing.\n+\n+1. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n+\tTo set up new sources, launch Tableau from command line with the following parameters (PowerShell example):\n+\t```\n+\t.\\tableau.exe -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true\n+\t```\n+\twhere `<path>` is either the path to the directory containing the `.taco` connector *or* the path to the directory containing the connector directory, if this isn't yet packaged.\n+  \n+\t**Note**: When connecting, make sure you pass the `timezone=Z` parameter into the `Additional settings` field of the connection dialog. This sets the timezone for the time data to UTC; if this isn't set, the JDBC driver will use JVM's/system's time zone, which will then result in some failed tests if the machine's not set to the UTC timezone.\n+\n+\tSave the TDS files as `cast_calcs.elastic.tds` and `Staples.elastic.tds`\n+\n+2. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.\n+\tEither package TDVT and install it as a Python PIP module (recommended, if [working](https://github.com/tableau/connector-plugin-sdk/issues/534)), or simply copy the `tdvt` directory of the repo into the \"workspace\" directory. Invoking the TDVT will then be done as `py -3 -m tdvt.tdvt <params>`, or `py -3 .\\tdvt\\tdvt_launcher.py <params>`, respectively. In the steps below the invokation will be indicated by the `$TDVT` call.\n+\t```\n+\t$TDVT action --setup\n+\t```\n+\n+\tCopy/move the above saved `*.tds` files into the just created `tds` directory in the workspace.\n+\tEdit `config/tdvt/tdvt_override.ini` to update `TAB_CLI_EXE_X64` definition.\n+\n+3. Generate the tests by invoking TDVT as follows:\n+\n+\t```\n+\t$TDVT action --add_ds elastic\n+\t```\n+\n+\tWhen asked for a password, use the same value as in step \"2.\" above when connecting to Elasticsearch.\n+\tFor the \"logical query config\" use `simple_lower`.\n+\n+\tEdit the `elastic.ini` file in the `config` directory in the workspace and add the following line under the `[Datasource]` section: `CommandLineOverride = -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true`, where `<path>` has the same value as in step \"1.\".\n+\n+4. Run the tests:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3762362fadaafe876b7bae804fa1303fe3ae419f"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTI1NTY2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOToxMjo1MFrOHliIlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOToxMjo1MFrOHliIlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExODYxMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tEdit the `elastic.ini` file in the `config` directory in the workspace and add the following line under the `[Datasource]` section: `CommandLineOverride = -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true`, where `<path>` has the same value as in step \"1.\".\n          \n          \n            \n            \tEdit the `elastic.ini` file in the `config` directory in the workspace and add the following line under the `[Datasource]` section: `CommandLineOverride = -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true`, where `<path>` has the same value as in step \"2.\".", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509118612", "createdAt": "2020-10-21T09:12:50Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/tdvt/README.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Testing the Tableau connector for Elasticsearch\n+\n+This folder contains the files needed to run Tableau's TDVT suite, to validate the connector's applicability.\n+\n+## Testing requirements\n+\n+Running the TDVT tests require a working instance of Elasticsearch (local or remote) and a Windows desktop with:\n+* a Python 3 installation;\n+* Tableau Desktop, licensed (the trial mode won't suffice for a JDBC connector), past or at 2019.4 release.\n+* the Elastic JDBC driver available.\n+\n+All these should be at their latest released version.\n+\n+**Note**: If running Elasticsearch remotely, both machines must be (1) time-synchronized and (2) have the same locale settings (i.e.: the calendars on both must start on the same day of week, same date format etc.).\n+\n+## Elasticsearch setup\n+\n+The Elasticsearch server should be on the latest release, have the SQL plugin installed (i.e. the \"default\" distribution) and a valid license for it (can be a temporary trial license).\n+\n+No other settings need to be changed from default, with one exception: the inline script compilation rate needs to be elevated from the default (current: 75/5m). This dynamic cluster setting is called `script.context.aggs.max_compilations_rate` starting with [7.9](https://www.elastic.co/guide/en/elasticsearch/reference/master/circuit-breaker.html#script-compilation-circuit-breaker) releases (for aggregations) and `script.max_compilations_rate` up to [7.8](https://www.elastic.co/guide/en/elasticsearch/reference/7.8/circuit-breaker.html#script-compilation-circuit-breaker). The minimum high-enough value might be hard to get right, but a fast run of the TDVT tests yield a ~7 tests/sec, so with a generous margin, just for testing, one could set a value of `1000/1m`.\n+\n+The data for running the TDVT suite needs to be loaded into Elasticsearch prior to running the tests. This can be achieved in a few ways:\n+\n+### 1. Import using Elastic's ODBC driver testing suite\n+\n+ODBC's driver test suite is a Python application that, among other things, will load the testing data into a running Elasticsearch instance.\n+It requires a Python 3 installation with [requests](python-requests.org) and [psutils](https://pypi.org/project/psutil/) PIP modules installed.\n+\n+Clone it from its [Github repo](https://github.com/elastic/elasticsearch-sql-odbc/) and run the following from within the top repo directory:\n+```\n+python3 ./test/integration/ites.py -p http://user:password@host:port -tx\n+```\n+where the `user`, `password`, `host` and `port` are the credentials and URL to access the Elasticsearch instance.\n+\n+This will download the TDVT CSV data sources from their [connector-plugin-sdk](https://github.com/tableau/connector-plugin-sdk/tree/master/tests/datasets/TestV1) repository and upload them to the destination test Elasticsearch instance. (It will also install the needed pipelines, templates/mappings to accomodate the source data, along with other test data irrelevant for TDVT testing -- just ignore or delete the other created indices).\n+\n+### 2. Reindex from remote\n+\n+Note: this requires some familiarity with Elasticsearch.\n+\n+If there is an available other Elasticsearch instance running, that has the test data loaded already, this data can be imported through the `_reindex` [API](www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html). Follow the two steps detailed in [Reindex from a remote cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html).\n+\n+<TODO: detailed sequence of steps>\n+\n+### 3. Import using Elastic's Logstash application\n+\n+[Logstash](https://www.elastic.co/guide/en/logstash/current/running-logstash-windows.html) is a Java application that can ingest data from various sources and in various formats and upload them into an Elasticsearch instance.\n+\n+Note that Logstash will add a few extra fields per row (\"document\" in Elasticsearch lingo); these shouldn't interfere with the testing, however.\n+\n+1. Download TDVT's data files [Calcs.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Calcs.csv) and [Staples_utf8.csv](https://raw.githubusercontent.com/tableau/connector-plugin-sdk/tdvt-2.1.9/tests/datasets/TestV1/Staples_utf8.csv) and place them into a directory that will be reachable by Logstash.\n+\n+2. Create the Elasticsearch indices `calcs` and `staples` and use for them the [mappings](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html) under these links: [calcs](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L27) and [stapes](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L87). Create an ingest [pipeline](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html) called `calcs-pipeline` with the definition [here](https://github.com/elastic/elasticsearch-sql-odbc/blob/577cd2fa1ed257e42081a082682c8c089b179565/test/integration/data.py#L62).\n+\n+3. Adapt the [config file](https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#configuration-file-structure) under the `logstash` folder, updating the <path>, <host> and <password> tags in it.\n+\n+4. Relaunch Logstash using the updated config file in previous step and wait until the files have been ingested. `calcs` index will need to have 17 documents and `staples` 54860.\n+\n+\n+## Running TDVT\n+\n+### Automated\n+\n+1. Place Elasticsearch JDBC driver into [Tableau's driver folder](https://help.tableau.com/current/pro/desktop/en-us/examples_otherdatabases_jdbc.htm) under: `C:\\Program Files\\Tableau\\Drivers`.\n+\n+2. Place the `.taco` file either in Tableau's dedicated connectors directory, `C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors`, or a custom one (\"<taco dir path>\").\n+\n+3. Use the `tdvt_run.py` application, that clone the TDVT SDK repo, setup config files and launch the TDVT run:\n+    ```\n+    python3 ./tdvt_run.py -u \"http://user:pass@elastic-host:9200\" -t <taco dir path>\n+    ```\n+\n+### Manually\n+\n+Setting up the TDVT testing involves following the steps detailed in the [official documentation](https://tableau.github.io/connector-plugin-sdk/docs/tdvt). The \"fragment\" in parantheses reference the respective chapters in the documentation. It is recommended to execute each test run starting afresh.\n+\n+1. Same as in the automated testing.\n+\n+2. Create new Tableau data sources for the `calcs` and `Staple` tables (#`Test a new data source`), or, alternatively, use those available already in this repo.\n+\tTo set up new sources, launch Tableau from command line with the following parameters (PowerShell example):\n+\t```\n+\t.\\tableau.exe -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true\n+\t```\n+\twhere `<path>` is either the path to the directory containing the `.taco` connector *or* the path to the directory containing the connector directory, if this isn't yet packaged.\n+  \n+\t**Note**: When connecting, make sure you pass the `timezone=Z` parameter into the `Additional settings` field of the connection dialog. This sets the timezone for the time data to UTC; if this isn't set, the JDBC driver will use JVM's/system's time zone, which will then result in some failed tests if the machine's not set to the UTC timezone.\n+\n+\tSave the TDS files as `cast_calcs.elastic.tds` and `Staples.elastic.tds`\n+\n+3. Setup a TDVT \"workspace\" (#`Set up`), i.e. a directory containing the test files.\n+\tEither package TDVT and install it as a Python PIP module (recommended, if [working](https://github.com/tableau/connector-plugin-sdk/issues/534)), or simply copy the `tdvt` directory of the repo into the \"workspace\" directory. Invoking the TDVT will then be done as `py -3 -m tdvt.tdvt <params>`, or `py -3 .\\tdvt\\tdvt_launcher.py <params>`, respectively. In the steps below the invokation will be indicated by the `$TDVT` call.\n+\t```\n+\t$TDVT action --setup\n+\t```\n+\n+\tCopy/move the above saved `*.tds` files into the just created `tds` directory in the workspace.\n+\tEdit `config/tdvt/tdvt_override.ini` to update `TAB_CLI_EXE_X64` definition.\n+\n+4. Generate the tests by invoking TDVT as follows:\n+\n+\t```\n+\t$TDVT action --add_ds elastic\n+\t```\n+\n+\tWhen asked for a password, use the same value as in step \"2.\" above when connecting to Elasticsearch.\n+\tFor the \"logical query config\" use `simple_lower`.\n+\n+\tEdit the `elastic.ini` file in the `config` directory in the workspace and add the following line under the `[Datasource]` section: `CommandLineOverride = -DConnectPluginsPath=<path> -DDisableVerifyConnectorPluginSignature=true`, where `<path>` has the same value as in step \"1.\".", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad44abdf6626157192acc24da227ef3020f91eeb"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTYzNTA3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/sql/connectors/tableau/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMDo0ODo1NVrOHllzQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMTowNToyNVrOHlmWww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTE3ODY4OA==", "bodyText": "Should we mention something about appropriate versions based on the es node/cluster version?", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509178688", "createdAt": "2020-10-21T10:48:55Z", "author": {"login": "matriv"}, "path": "x-pack/plugin/sql/connectors/tableau/README.md", "diffHunk": "@@ -0,0 +1,19 @@\n+# Tableau connector for Elasticsearch\n+\n+The Tableau Connector works in tandem with the Elastic JDBC driver to facilitate the query of Elasticsearch. It gives users a simple way to query Elasticsearch data from Tableau.\n+After providing basic connection and authentication information, users can easily select Elasticsearch indices for use in Tableau Desktop and Tableau Server.\n+\n+## Installation\n+\n+1. Tableau connector for Elasticsearch installation:\n+ - Go to the [Connector Download](https://www.elastic.co/downloads/tableau-connector) page.\n+ - Download the _.taco_ connector file.\n+ - Move the _.taco_ file here:\n+    - Windows: C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors\n+    -  macOS: /Users/[user]/Documents/My Tableau Repository/Connectors\n+2. Elasticsearch JDBC Driver Installation:\n+ - Go to the [Driver Download](https://www.elastic.co/downloads/jdbc-client) page.\n+ - Download the Elasticsearch JDBC Driver _.jar_ file and move it into the following directory:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09eeb0a9420f0aff108cb0af34463ef3f396757c"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTE4Nzc3OQ==", "bodyText": "That's a good point. I would however follow up on this:\n\nonce the connector releasing is hooked into the RM (and we'll then easily be able to pair the connector with the driver, version-wise);\nwith a doc PR stating our drivers' compatibility policy that we can refer to (from here, among other places).", "url": "https://github.com/elastic/elasticsearch/pull/63751#discussion_r509187779", "createdAt": "2020-10-21T11:05:25Z", "author": {"login": "bpintea"}, "path": "x-pack/plugin/sql/connectors/tableau/README.md", "diffHunk": "@@ -0,0 +1,19 @@\n+# Tableau connector for Elasticsearch\n+\n+The Tableau Connector works in tandem with the Elastic JDBC driver to facilitate the query of Elasticsearch. It gives users a simple way to query Elasticsearch data from Tableau.\n+After providing basic connection and authentication information, users can easily select Elasticsearch indices for use in Tableau Desktop and Tableau Server.\n+\n+## Installation\n+\n+1. Tableau connector for Elasticsearch installation:\n+ - Go to the [Connector Download](https://www.elastic.co/downloads/tableau-connector) page.\n+ - Download the _.taco_ connector file.\n+ - Move the _.taco_ file here:\n+    - Windows: C:\\Users\\[Windows User]\\Documents\\My Tableau Repository\\Connectors\n+    -  macOS: /Users/[user]/Documents/My Tableau Repository/Connectors\n+2. Elasticsearch JDBC Driver Installation:\n+ - Go to the [Driver Download](https://www.elastic.co/downloads/jdbc-client) page.\n+ - Download the Elasticsearch JDBC Driver _.jar_ file and move it into the following directory:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTE3ODY4OA=="}, "originalCommit": {"oid": "09eeb0a9420f0aff108cb0af34463ef3f396757c"}, "originalPosition": 16}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2911, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}