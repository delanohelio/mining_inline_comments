{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI0NjM5NDU0", "number": 57304, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxNDoyOVrOEAsZXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxODo0NjowOVrOEFKFZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTYyODQ0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxNDoyOVrOGcIlcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxNDoyOVrOGcIlcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1Mzk2OA==", "bodyText": "remove the ending \"it\"?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432153968", "createdAt": "2020-05-28T22:14:29Z", "author": {"login": "talevy"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,46 +40,150 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * Initially it uses the most fine grained rounding configuration possible but\n+ * as more data arrives it uses two heuristics to shift to coarser and coarser\n+ * rounding. The first heuristic is the number of buckets, specifically, it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTYzMTA4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxNTo0MVrOGcInKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxNTo0MVrOGcInKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NDQxMQ==", "bodyText": "remove the leading \"it\"?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432154411", "createdAt": "2020-05-28T22:15:41Z", "author": {"login": "talevy"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,46 +40,150 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * Initially it uses the most fine grained rounding configuration possible but\n+ * as more data arrives it uses two heuristics to shift to coarser and coarser\n+ * rounding. The first heuristic is the number of buckets, specifically, it\n+ * when there are more buckets than can \"fit\" in the current rounding it shifts\n+ * to the next rounding. Instead of redoing the rounding, it estimates the\n+ * number of buckets that will \"survive\" at the new rounding and uses\n+ * <strong>that</strong> as the initial value for the bucket count that it\n+ * increments in order to trigger another promotion to another coarser\n+ * rounding. This works fairly well at containing the number of buckets, but\n+ * it the estimate of the number of buckets will be wrong if the buckets are", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTYzMjgyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxNjoyOFrOGcIoYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMjoxNjoyOFrOGcIoYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE1NDcyMQ==", "bodyText": "drop the \"is\"?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432154721", "createdAt": "2020-05-28T22:16:28Z", "author": {"login": "talevy"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,46 +40,150 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * Initially it uses the most fine grained rounding configuration possible but\n+ * as more data arrives it uses two heuristics to shift to coarser and coarser\n+ * rounding. The first heuristic is the number of buckets, specifically, it\n+ * when there are more buckets than can \"fit\" in the current rounding it shifts\n+ * to the next rounding. Instead of redoing the rounding, it estimates the\n+ * number of buckets that will \"survive\" at the new rounding and uses\n+ * <strong>that</strong> as the initial value for the bucket count that it\n+ * increments in order to trigger another promotion to another coarser\n+ * rounding. This works fairly well at containing the number of buckets, but\n+ * it the estimate of the number of buckets will be wrong if the buckets are\n+ * quite a spread out compared to the rounding.\n+ * <p>\n+ * The second heuristic it uses to trigger promotion to a coarser rounding is\n+ * the distance between the min and max bucket. When that distance is greater\n+ * than what the current rounding supports it promotes. This is heuristic", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTg3ODAzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMDoyMToxMlrOGcK9Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMDoyMToxMlrOGcK9Vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE5Mjg1NA==", "bodyText": "should this be AutoDateHistogramAggregator#rebucket? Intellij no-likey", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432192854", "createdAt": "2020-05-29T00:21:12Z", "author": {"login": "talevy"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,46 +40,150 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * Initially it uses the most fine grained rounding configuration possible but\n+ * as more data arrives it uses two heuristics to shift to coarser and coarser\n+ * rounding. The first heuristic is the number of buckets, specifically, it\n+ * when there are more buckets than can \"fit\" in the current rounding it shifts\n+ * to the next rounding. Instead of redoing the rounding, it estimates the\n+ * number of buckets that will \"survive\" at the new rounding and uses\n+ * <strong>that</strong> as the initial value for the bucket count that it\n+ * increments in order to trigger another promotion to another coarser\n+ * rounding. This works fairly well at containing the number of buckets, but\n+ * it the estimate of the number of buckets will be wrong if the buckets are\n+ * quite a spread out compared to the rounding.\n+ * <p>\n+ * The second heuristic it uses to trigger promotion to a coarser rounding is\n+ * the distance between the min and max bucket. When that distance is greater\n+ * than what the current rounding supports it promotes. This is heuristic\n+ * isn't good at limiting the number of buckets but is great when the buckets\n+ * are spread out compared to the rounding. So it should complement the first\n+ * heuristic.\n+ * <p>\n+ * When promoting a rounding we keep the old buckets around because it is\n+ * expensive to call {@link MergingBucketsDeferringCollector#mergeBuckets}.\n+ * In particular it is {@code O(number_of_hits_collected_so_far)}. So if we\n+ * called it frequently we'd end up in {@code O(n^2)} territory. Bad news for\n+ * aggregations! Instead, we keep a \"budget\" of buckets that we're ok\n+ * \"wasting\". When we promote the rounding and our estimate of the number of\n+ * \"dead\" buckets that have data but have yet to be merged into the buckets\n+ * that are valid for the current rounding exceeds the budget then we rebucket\n+ * the entire aggregation and double the budget.\n+ * <p>\n+ * Once we're done collecting and we know exactly which buckets we'll be\n+ * returning we <strong>finally</strong> perform a \"real\", \"perfect bucketing\",\n+ * rounding all of the keys for {@code owningBucketOrd} that we're going to\n+ * collect and picking the rounding based on a real, accurate count and the\n+ * min and max.\n  */\n class AutoDateHistogramAggregator extends DeferableBucketAggregator {\n-\n     private final ValuesSource.Numeric valuesSource;\n     private final DocValueFormat formatter;\n     private final RoundingInfo[] roundingInfos;\n     private final Function<Rounding, Rounding.Prepared> roundingPreparer;\n-    private int roundingIdx = 0;\n-    private Rounding.Prepared preparedRounding;\n-\n-    private LongHash bucketOrds;\n-    private int targetBuckets;\n+    private final int targetBuckets;\n+    private final boolean collectsFromSingleBucket;\n+    /**\n+     * An array of prepared roundings in the same order as\n+     * {@link #roundingInfos}. The 0th entry is prepared initially,\n+     * and other entries are null until first needed.\n+     */\n+    private final Rounding.Prepared[] preparedRoundings;\n+    /**\n+     * Map from {@code owningBucketOrd, roundedDate} to {@code bucketOrdinal}.\n+     */\n+    private LongKeyedBucketOrds bucketOrds;\n+    /**\n+     * The index of the rounding that each {@code owningBucketOrd} is\n+     * currently using.\n+     * <p>\n+     * During collection we use overestimates for how much buckets are save\n+     * by bumping to the next rounding index. So we end up bumping less\n+     * aggressively than a \"perfect\" algorithm. That is fine because we\n+     * correct the error when we merge the buckets together all the way\n+     * up in {@link InternalAutoDateHistogram#reduceBucket}. In particular,\n+     * on final reduce we bump the rounding until it we appropriately\n+     * cover the date range across all of the results returned by all of\n+     * the {@link AutoDateHistogramAggregator}s. \n+     */\n+    private ByteArray roundingIndices;\n+    /**\n+     * The min and max of each bucket's keys. min lives in indices of the form\n+     * {@code 2n} and max in {@code 2n + 1}.\n+     */\n+    private LongArray bounds;\n+    /**\n+     * A reference to the collector so we can\n+     * {@link MergingBucketsDeferringCollector#mergeBuckets(long[])}.\n+     */\n     private MergingBucketsDeferringCollector deferringCollector;\n+    /**\n+     * An underestimate of the number of buckets that are \"live\" in the\n+     * current rounding for each {@code owningBucketOrdinal}. \n+     */\n+    private IntArray liveBucketCountUnderestimate;\n+    /**\n+     * An over estimate of the number of wasted buckets. When this gets\n+     * too high we {@link #rebucket()} which sets it to 0.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTg5NTg1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMDozMjozM1rOGcLIdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMVQxNzoyMDoxN1rOGdTIsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE5NTcwMA==", "bodyText": "should there be unit tests for when:\n(max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()) == false\n&& (newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()))\n\n?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432195700", "createdAt": "2020-05-29T00:32:33Z", "author": {"login": "talevy"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM3NTQxMQ==", "bodyText": "We do cover this case in the test case, but there isn't a unit test for this exactly. do you think it is worth pulling this out somehow?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r433375411", "createdAt": "2020-06-01T17:20:17Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE5NTcwMA=="}, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 289}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NDI4NDU3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNTo1NDo1MlrOGcil6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNTo1NDo1MlrOGcil6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MDA3Mg==", "bodyText": "owningBucketOrd * 2 and owningBucketOrd * 2 + 1 are used a lot here and make it a bit difficult to read... could we just set some lowerBound/upperBound (or whatever name makes sense) variables to make it easier to read?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432580072", "createdAt": "2020-05-29T15:54:52Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 272}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NDI5NTY1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNTo1NzoyNVrOGcis2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxOTozODo0MVrOGcpoLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MTg1MQ==", "bodyText": "I'm not really sure I understand what this is doing?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432581851", "createdAt": "2020-05-29T15:57:25Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY5NTM0Mw==", "bodyText": "Setting the min and max properly. It'd all go away if I make too arrays like you asked above. While I'll do.", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432695343", "createdAt": "2020-05-29T19:38:41Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MTg1MQ=="}, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 268}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NDMxMTI0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjowMTozOVrOGci2xQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjowMTozOVrOGci2xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NDM4OQ==", "bodyText": "I think we can move these two bounds.set() inside the if() { ... } down below, to avoid unnecessary setting when we have to fall through and increase the rounding.  Not that it's terribly expensive, but if we can avoid always good :)", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432584389", "createdAt": "2020-05-29T16:01:39Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 273}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NDMzMjkxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjowNzo0MVrOGcjElw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxOTo0NDozNFrOGcp4_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NzkyNw==", "bodyText": "What's the rationale behind doubling the rebucketing threshold?  Shouldn't it stay constant since the goal of the agg is to produce a relatively constant number of buckets?  As we rebucket and move to coarser intervals, the number of buckets will reduce (and wasted buckets should get cleaned up over time)?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432587927", "createdAt": "2020-05-29T16:07:41Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 298}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY5OTY0Ng==", "bodyText": "My idea was to always produce good buckets in the end but to allow more and more \"wasted\" buckets as we end up cleaning up more and more. With the default as it stands when the agg is at the top level we don't clean up any of the \"wasted\" buckets until right before we return the aggs.\nThis bumping is more to defend against having large numbers of owningBucketOrds that cause us to have to rebucket many many times. To sort of blunt the O(n^2) nature of it all. We'd still to the O(n) rebucketing many times. Just fewer times as you accumulate more and more buckets.\nI did think about making it a constant budget. And that works pretty well too because we naturally rebucket less and less as we promote rounding because the buckets are bigger. But if we have a ton of owningBucketOrds I'm kind of worried.", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432699646", "createdAt": "2020-05-29T19:44:34Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NzkyNw=="}, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 298}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NDM5MTQ2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjoyNTozMVrOGcjqqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxOTo0NTozM1rOGcp74Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NzY3Mg==", "bodyText": "Don't we have the min/max in bounds at this point?  Could we use that to help skip to the correct rounding, so we don't have to potentially re-build the ords several times if max-min > the threshold?", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432597672", "createdAt": "2020-05-29T16:25:31Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;\n+                } else {\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n+                }\n+                return newRounding;\n             }\n         };\n     }\n \n+    private void rebucket() {\n+        rebucketCount++;\n+        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+            bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+            for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                while (ordsEnum.next()) {\n+                    long oldKey = ordsEnum.value();\n+                    long newKey = preparedRounding.round(oldKey);\n+                    long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                    mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n+                }\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n+            }\n+            mergeBuckets(mergeMap, bucketOrds.size());\n+            if (deferringCollector != null) {\n+                deferringCollector.mergeBuckets(mergeMap);\n+            }\n+        }\n+    }\n+\n     @Override\n     public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        correctRounding(owningBucketOrds);\n+        /*\n+         * Now that we have the perfect rounding rebucket everything to merge\n+         * all of the buckets together that we were too lazy to merge while\n+         * collecting.\n+         *\n+         * TODO it'd be faster if we could apply the merging on the fly as we\n+         * replay the hits and build the buckets. How much faster is\n+         * *interesting*. Performance tests with a couple of sub-`stats` aggs\n+         * show `date_histogram` to have about the same performance as\n+         * `auto_date_histogram` so there isn't really much to be gained here.\n+         * But if there is a non-delaying but selectivate aggregation \"above\"\n+         * this one then the performance gain could be substantial.\n+         */\n+        rebucket();\n         return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n                 (bucketValue, docCount, subAggregationResults) ->\n                     new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n+                (owningBucketOrd, buckets) -> {\n                     // the contract of the histogram aggregation is that shards must return\n                     // buckets ordered by key in ascending order\n                     CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n \n                     // value source will be null for unmapped fields\n                     InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n+                            roundingIndexFor(owningBucketOrd), buildEmptySubAggregations());\n \n                     return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n                 });\n     }\n \n+    /**\n+     * Pick the correct rounding for the specifies {@code owningBucketOrds}.\n+     */\n+    private void correctRounding(long[] owningBucketOrds) {\n+        for (long owningBucketOrd : owningBucketOrds) {\n+            byte oldRounding = roundingIndexFor(owningBucketOrd);\n+            if (oldRounding >= roundingInfos.length - 1) {\n+                continue;\n+            }\n+            byte newRounding = (byte)(oldRounding - 1);\n+            long count;\n+            long min = Long.MAX_VALUE;\n+            long max = Long.MIN_VALUE;\n+            do {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 379}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcwMDM4NQ==", "bodyText": "We do. Let me think about that. My instinct is that this check here is largely symbolic because if the min and max would have pushed us up then we'd, well, already be on a bigger rounding anyway.", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432700385", "createdAt": "2020-05-29T19:45:33Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;\n+                } else {\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n+                }\n+                return newRounding;\n             }\n         };\n     }\n \n+    private void rebucket() {\n+        rebucketCount++;\n+        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+            bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+            for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                while (ordsEnum.next()) {\n+                    long oldKey = ordsEnum.value();\n+                    long newKey = preparedRounding.round(oldKey);\n+                    long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                    mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n+                }\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n+            }\n+            mergeBuckets(mergeMap, bucketOrds.size());\n+            if (deferringCollector != null) {\n+                deferringCollector.mergeBuckets(mergeMap);\n+            }\n+        }\n+    }\n+\n     @Override\n     public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        correctRounding(owningBucketOrds);\n+        /*\n+         * Now that we have the perfect rounding rebucket everything to merge\n+         * all of the buckets together that we were too lazy to merge while\n+         * collecting.\n+         *\n+         * TODO it'd be faster if we could apply the merging on the fly as we\n+         * replay the hits and build the buckets. How much faster is\n+         * *interesting*. Performance tests with a couple of sub-`stats` aggs\n+         * show `date_histogram` to have about the same performance as\n+         * `auto_date_histogram` so there isn't really much to be gained here.\n+         * But if there is a non-delaying but selectivate aggregation \"above\"\n+         * this one then the performance gain could be substantial.\n+         */\n+        rebucket();\n         return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n                 (bucketValue, docCount, subAggregationResults) ->\n                     new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n+                (owningBucketOrd, buckets) -> {\n                     // the contract of the histogram aggregation is that shards must return\n                     // buckets ordered by key in ascending order\n                     CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n \n                     // value source will be null for unmapped fields\n                     InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n+                            roundingIndexFor(owningBucketOrd), buildEmptySubAggregations());\n \n                     return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n                 });\n     }\n \n+    /**\n+     * Pick the correct rounding for the specifies {@code owningBucketOrds}.\n+     */\n+    private void correctRounding(long[] owningBucketOrds) {\n+        for (long owningBucketOrd : owningBucketOrds) {\n+            byte oldRounding = roundingIndexFor(owningBucketOrd);\n+            if (oldRounding >= roundingInfos.length - 1) {\n+                continue;\n+            }\n+            byte newRounding = (byte)(oldRounding - 1);\n+            long count;\n+            long min = Long.MAX_VALUE;\n+            long max = Long.MIN_VALUE;\n+            do {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5NzY3Mg=="}, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 379}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NDM5NzkwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjoyNzozMFrOGcju3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQyMjowODozMlrOGctqLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5ODc1MQ==", "bodyText": "Is it safe to add to the bucketOrds while also iterating over it?  Haven't looked super closely at the internals.\nOn that note, do we need to clear out the bucketOrdsEnum before adding new keys?  E.g. I'm thinking of the case where we might need to go around this do...while loop a few times, and it looks like each time through we'll just keep appending to the ordsEnum so we'll end up with multiple levels of intervals?  I might be misreading though", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432598751", "createdAt": "2020-05-29T16:27:30Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;\n+                } else {\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n+                }\n+                return newRounding;\n             }\n         };\n     }\n \n+    private void rebucket() {\n+        rebucketCount++;\n+        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+            bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+            for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                while (ordsEnum.next()) {\n+                    long oldKey = ordsEnum.value();\n+                    long newKey = preparedRounding.round(oldKey);\n+                    long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                    mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n+                }\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n+            }\n+            mergeBuckets(mergeMap, bucketOrds.size());\n+            if (deferringCollector != null) {\n+                deferringCollector.mergeBuckets(mergeMap);\n+            }\n+        }\n+    }\n+\n     @Override\n     public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        correctRounding(owningBucketOrds);\n+        /*\n+         * Now that we have the perfect rounding rebucket everything to merge\n+         * all of the buckets together that we were too lazy to merge while\n+         * collecting.\n+         *\n+         * TODO it'd be faster if we could apply the merging on the fly as we\n+         * replay the hits and build the buckets. How much faster is\n+         * *interesting*. Performance tests with a couple of sub-`stats` aggs\n+         * show `date_histogram` to have about the same performance as\n+         * `auto_date_histogram` so there isn't really much to be gained here.\n+         * But if there is a non-delaying but selectivate aggregation \"above\"\n+         * this one then the performance gain could be substantial.\n+         */\n+        rebucket();\n         return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n                 (bucketValue, docCount, subAggregationResults) ->\n                     new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n+                (owningBucketOrd, buckets) -> {\n                     // the contract of the histogram aggregation is that shards must return\n                     // buckets ordered by key in ascending order\n                     CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n \n                     // value source will be null for unmapped fields\n                     InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n+                            roundingIndexFor(owningBucketOrd), buildEmptySubAggregations());\n \n                     return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n                 });\n     }\n \n+    /**\n+     * Pick the correct rounding for the specifies {@code owningBucketOrds}.\n+     */\n+    private void correctRounding(long[] owningBucketOrds) {\n+        for (long owningBucketOrd : owningBucketOrds) {\n+            byte oldRounding = roundingIndexFor(owningBucketOrd);\n+            if (oldRounding >= roundingInfos.length - 1) {\n+                continue;\n+            }\n+            byte newRounding = (byte)(oldRounding - 1);\n+            long count;\n+            long min = Long.MAX_VALUE;\n+            long max = Long.MIN_VALUE;\n+            do {\n+                newRounding++;\n+                try (LongHash perfect = new LongHash(liveBucketCountUnderestimate.get(owningBucketOrd), context.bigArrays())) {\n+                    LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrd);\n+                    Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                    while (ordsEnum.next()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 384}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcwMTQ4Ng==", "bodyText": "Oh shit! This shld be adding to perfect. How did this ever work?!", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432701486", "createdAt": "2020-05-29T19:47:03Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;\n+                } else {\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n+                }\n+                return newRounding;\n             }\n         };\n     }\n \n+    private void rebucket() {\n+        rebucketCount++;\n+        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+            bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+            for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                while (ordsEnum.next()) {\n+                    long oldKey = ordsEnum.value();\n+                    long newKey = preparedRounding.round(oldKey);\n+                    long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                    mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n+                }\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n+            }\n+            mergeBuckets(mergeMap, bucketOrds.size());\n+            if (deferringCollector != null) {\n+                deferringCollector.mergeBuckets(mergeMap);\n+            }\n+        }\n+    }\n+\n     @Override\n     public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        correctRounding(owningBucketOrds);\n+        /*\n+         * Now that we have the perfect rounding rebucket everything to merge\n+         * all of the buckets together that we were too lazy to merge while\n+         * collecting.\n+         *\n+         * TODO it'd be faster if we could apply the merging on the fly as we\n+         * replay the hits and build the buckets. How much faster is\n+         * *interesting*. Performance tests with a couple of sub-`stats` aggs\n+         * show `date_histogram` to have about the same performance as\n+         * `auto_date_histogram` so there isn't really much to be gained here.\n+         * But if there is a non-delaying but selectivate aggregation \"above\"\n+         * this one then the performance gain could be substantial.\n+         */\n+        rebucket();\n         return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n                 (bucketValue, docCount, subAggregationResults) ->\n                     new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n+                (owningBucketOrd, buckets) -> {\n                     // the contract of the histogram aggregation is that shards must return\n                     // buckets ordered by key in ascending order\n                     CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n \n                     // value source will be null for unmapped fields\n                     InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n+                            roundingIndexFor(owningBucketOrd), buildEmptySubAggregations());\n \n                     return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n                 });\n     }\n \n+    /**\n+     * Pick the correct rounding for the specifies {@code owningBucketOrds}.\n+     */\n+    private void correctRounding(long[] owningBucketOrds) {\n+        for (long owningBucketOrd : owningBucketOrds) {\n+            byte oldRounding = roundingIndexFor(owningBucketOrd);\n+            if (oldRounding >= roundingInfos.length - 1) {\n+                continue;\n+            }\n+            byte newRounding = (byte)(oldRounding - 1);\n+            long count;\n+            long min = Long.MAX_VALUE;\n+            long max = Long.MIN_VALUE;\n+            do {\n+                newRounding++;\n+                try (LongHash perfect = new LongHash(liveBucketCountUnderestimate.get(owningBucketOrd), context.bigArrays())) {\n+                    LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrd);\n+                    Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                    while (ordsEnum.next()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5ODc1MQ=="}, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 384}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcwMTc5MA==", "bodyText": "Wow. Ok then. I wonder if I can make a test case that fails because of this......", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432701790", "createdAt": "2020-05-29T19:47:30Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;\n+                } else {\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n+                }\n+                return newRounding;\n             }\n         };\n     }\n \n+    private void rebucket() {\n+        rebucketCount++;\n+        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+            bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+            for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                while (ordsEnum.next()) {\n+                    long oldKey = ordsEnum.value();\n+                    long newKey = preparedRounding.round(oldKey);\n+                    long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                    mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n+                }\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n+            }\n+            mergeBuckets(mergeMap, bucketOrds.size());\n+            if (deferringCollector != null) {\n+                deferringCollector.mergeBuckets(mergeMap);\n+            }\n+        }\n+    }\n+\n     @Override\n     public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        correctRounding(owningBucketOrds);\n+        /*\n+         * Now that we have the perfect rounding rebucket everything to merge\n+         * all of the buckets together that we were too lazy to merge while\n+         * collecting.\n+         *\n+         * TODO it'd be faster if we could apply the merging on the fly as we\n+         * replay the hits and build the buckets. How much faster is\n+         * *interesting*. Performance tests with a couple of sub-`stats` aggs\n+         * show `date_histogram` to have about the same performance as\n+         * `auto_date_histogram` so there isn't really much to be gained here.\n+         * But if there is a non-delaying but selectivate aggregation \"above\"\n+         * this one then the performance gain could be substantial.\n+         */\n+        rebucket();\n         return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n                 (bucketValue, docCount, subAggregationResults) ->\n                     new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n+                (owningBucketOrd, buckets) -> {\n                     // the contract of the histogram aggregation is that shards must return\n                     // buckets ordered by key in ascending order\n                     CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n \n                     // value source will be null for unmapped fields\n                     InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n+                            roundingIndexFor(owningBucketOrd), buildEmptySubAggregations());\n \n                     return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n                 });\n     }\n \n+    /**\n+     * Pick the correct rounding for the specifies {@code owningBucketOrds}.\n+     */\n+    private void correctRounding(long[] owningBucketOrds) {\n+        for (long owningBucketOrd : owningBucketOrds) {\n+            byte oldRounding = roundingIndexFor(owningBucketOrd);\n+            if (oldRounding >= roundingInfos.length - 1) {\n+                continue;\n+            }\n+            byte newRounding = (byte)(oldRounding - 1);\n+            long count;\n+            long min = Long.MAX_VALUE;\n+            long max = Long.MIN_VALUE;\n+            do {\n+                newRounding++;\n+                try (LongHash perfect = new LongHash(liveBucketCountUnderestimate.get(owningBucketOrd), context.bigArrays())) {\n+                    LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrd);\n+                    Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                    while (ordsEnum.next()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5ODc1MQ=="}, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 384}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc2MTM4OA==", "bodyText": "It turns out that the heuristics we had were doing a pretty ok job making the right buckets anyway. So I can just drop this method entirely!", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r432761388", "createdAt": "2020-05-29T22:08:32Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -99,95 +206,219 @@ public DeferringBucketCollector getDeferringCollector() {\n     }\n \n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n+        SortedNumericDocValues values = valuesSource.longValues(ctx);\n         return new LeafBucketCollectorBase(sub, values) {\n             @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n                 if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+                    int valuesCount = values.docValueCount();\n \n                     long previousRounded = Long.MIN_VALUE;\n+                    byte roundingIdx = roundingIndexFor(owningBucketOrd);\n                     for (int i = 0; i < valuesCount; ++i) {\n                         long value = values.nextValue();\n-                        long rounded = preparedRounding.round(value);\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n                         assert rounded >= previousRounded;\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n-                            }\n-                        }\n+                        roundingIdx = collectValue(sub, owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n             }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n-                    }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+            private byte collectValue(LeafBucketCollector sub, long owningBucketOrd, byte roundingIdx, int doc, long rounded)\n+                    throws IOException {\n+                long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                if (bucketOrd < 0) { // already seen\n+                    bucketOrd = -1 - bucketOrd;\n+                    collectExistingBucket(sub, doc, bucketOrd);\n+                    return roundingIdx;\n+                }\n+                collectBucket(sub, doc, bucketOrd);\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                return increaseRounding(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+            }\n+\n+            /**\n+             * Increase the rounding of {@code owningBucketOrd} using\n+             * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+             * buckets if the estimated number of wasted buckets is too high.\n+             */\n+            private byte increaseRounding(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, byte oldRounding) {\n+                if (oldRounding >= roundingInfos.length - 1) {\n+                    return oldRounding;\n+                }\n+                if (bounds.size() < owningBucketOrd * 2 + 2) {\n+                    long oldSize = bounds.size();\n+                    bounds = context.bigArrays().grow(bounds, owningBucketOrd * 2 + 2);\n+                    for (long b = oldSize; b < bounds.size(); b++) {\n+                        bounds.set(b, (b & 1L) == 0L ? Long.MAX_VALUE : Long.MIN_VALUE);\n                     }\n-                    bucketOrds = newBucketOrds;\n                 }\n+                long min = Math.min(bounds.get(owningBucketOrd * 2), newKey);\n+                bounds.set(owningBucketOrd * 2, min);\n+                long max = Math.max(bounds.get(owningBucketOrd * 2 + 1), newKey);\n+                bounds.set(owningBucketOrd * 2 + 1, max);\n+                if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                    return oldRounding;\n+                }\n+                long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                byte newRounding = oldRounding;\n+                int newEstimatedBucketCount;\n+                do {\n+                    newRounding++;\n+                    double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                    newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                } while (newRounding < roundingInfos.length - 1 && (\n+                    newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                        || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                setRounding(owningBucketOrd, newRounding);\n+                bounds.set(owningBucketOrd * 2, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2)));\n+                bounds.set(owningBucketOrd * 2 + 1, preparedRoundings[newRounding].round(bounds.get(owningBucketOrd * 2 + 1)));\n+                wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                if (wastedBucketsOverestimate > nextRebucketAt) {\n+                    rebucket();\n+                    // Bump the threshold for the next rebucketing\n+                    wastedBucketsOverestimate = 0;\n+                    nextRebucketAt *= 2;\n+                } else {\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n+                }\n+                return newRounding;\n             }\n         };\n     }\n \n+    private void rebucket() {\n+        rebucketCount++;\n+        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+            bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+            for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                while (ordsEnum.next()) {\n+                    long oldKey = ordsEnum.value();\n+                    long newKey = preparedRounding.round(oldKey);\n+                    long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                    mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n+                }\n+                liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n+            }\n+            mergeBuckets(mergeMap, bucketOrds.size());\n+            if (deferringCollector != null) {\n+                deferringCollector.mergeBuckets(mergeMap);\n+            }\n+        }\n+    }\n+\n     @Override\n     public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        correctRounding(owningBucketOrds);\n+        /*\n+         * Now that we have the perfect rounding rebucket everything to merge\n+         * all of the buckets together that we were too lazy to merge while\n+         * collecting.\n+         *\n+         * TODO it'd be faster if we could apply the merging on the fly as we\n+         * replay the hits and build the buckets. How much faster is\n+         * *interesting*. Performance tests with a couple of sub-`stats` aggs\n+         * show `date_histogram` to have about the same performance as\n+         * `auto_date_histogram` so there isn't really much to be gained here.\n+         * But if there is a non-delaying but selectivate aggregation \"above\"\n+         * this one then the performance gain could be substantial.\n+         */\n+        rebucket();\n         return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n                 (bucketValue, docCount, subAggregationResults) ->\n                     new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n+                (owningBucketOrd, buckets) -> {\n                     // the contract of the histogram aggregation is that shards must return\n                     // buckets ordered by key in ascending order\n                     CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n \n                     // value source will be null for unmapped fields\n                     InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n+                            roundingIndexFor(owningBucketOrd), buildEmptySubAggregations());\n \n                     return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n                 });\n     }\n \n+    /**\n+     * Pick the correct rounding for the specifies {@code owningBucketOrds}.\n+     */\n+    private void correctRounding(long[] owningBucketOrds) {\n+        for (long owningBucketOrd : owningBucketOrds) {\n+            byte oldRounding = roundingIndexFor(owningBucketOrd);\n+            if (oldRounding >= roundingInfos.length - 1) {\n+                continue;\n+            }\n+            byte newRounding = (byte)(oldRounding - 1);\n+            long count;\n+            long min = Long.MAX_VALUE;\n+            long max = Long.MIN_VALUE;\n+            do {\n+                newRounding++;\n+                try (LongHash perfect = new LongHash(liveBucketCountUnderestimate.get(owningBucketOrd), context.bigArrays())) {\n+                    LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrd);\n+                    Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                    while (ordsEnum.next()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU5ODc1MQ=="}, "originalCommit": {"oid": "342fb4eb66d453c2ec9ce3360c446123c3cdf9c7"}, "originalPosition": 384}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODMwMDczOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzo1Njo0N1rOGjM7ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzo1Njo0N1rOGjM7ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU2NTE1Nw==", "bodyText": "Huh, TIL.", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r439565157", "createdAt": "2020-06-12T17:56:47Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -37,80 +40,253 @@\n import org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector;\n import org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector;\n import org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.RoundingInfo;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds;\n import org.elasticsearch.search.aggregations.support.ValuesSource;\n import org.elasticsearch.search.internal.SearchContext;\n \n import java.io.IOException;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.function.BiConsumer;\n import java.util.function.Function;\n+import java.util.function.LongToIntFunction;\n \n /**\n  * An aggregator for date values that attempts to return a specific number of\n  * buckets, reconfiguring how it rounds dates to buckets on the fly as new\n- * data arrives. \n+ * data arrives.\n+ * <p>\n+ * This class is abstract because there is a simple implementation for when the\n+ * aggregator only collects from a single bucket and a more complex\n+ * implementation when it doesn't. This ain't great from a test coverage\n+ * standpoint but the simpler implementation is between 7% and 15% faster\n+ * when you can use it. This is an important aggregation and we need that\n+ * performance.\n  */\n-class AutoDateHistogramAggregator extends DeferableBucketAggregator {\n+abstract class AutoDateHistogramAggregator extends DeferableBucketAggregator {\n+    static AutoDateHistogramAggregator build(\n+        String name,\n+        AggregatorFactories factories,\n+        int targetBuckets,\n+        RoundingInfo[] roundingInfos,\n+        Function<Rounding, Rounding.Prepared> roundingPreparer,\n+        @Nullable ValuesSource valuesSource,\n+        DocValueFormat formatter,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n+        return collectsFromSingleBucket\n+            ? new FromSingle(\n+                name,\n+                factories,\n+                targetBuckets,\n+                roundingInfos,\n+                roundingPreparer,\n+                valuesSource,\n+                formatter,\n+                aggregationContext,\n+                parent,\n+                metadata\n+            )\n+            : new FromMany(\n+                name,\n+                factories,\n+                targetBuckets,\n+                roundingInfos,\n+                roundingPreparer,\n+                valuesSource,\n+                formatter,\n+                aggregationContext,\n+                parent,\n+                metadata\n+            );\n+    }\n \n     private final ValuesSource.Numeric valuesSource;\n     private final DocValueFormat formatter;\n-    private final RoundingInfo[] roundingInfos;\n     private final Function<Rounding, Rounding.Prepared> roundingPreparer;\n-    private int roundingIdx = 0;\n-    private Rounding.Prepared preparedRounding;\n-\n-    private LongHash bucketOrds;\n-    private int targetBuckets;\n+    /**\n+     * A reference to the collector so we can\n+     * {@link MergingBucketsDeferringCollector#mergeBuckets(long[])}.\n+     */\n     private MergingBucketsDeferringCollector deferringCollector;\n \n-    AutoDateHistogramAggregator(String name, AggregatorFactories factories, int numBuckets, RoundingInfo[] roundingInfos,\n-        Function<Rounding, Rounding.Prepared> roundingPreparer, @Nullable ValuesSource valuesSource, DocValueFormat formatter,\n-        SearchContext aggregationContext, Aggregator parent, Map<String, Object> metadata) throws IOException {\n+    protected final RoundingInfo[] roundingInfos;\n+    protected final int targetBuckets;\n+\n+    private AutoDateHistogramAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        int targetBuckets,\n+        RoundingInfo[] roundingInfos,\n+        Function<Rounding, Rounding.Prepared> roundingPreparer,\n+        @Nullable ValuesSource valuesSource,\n+        DocValueFormat formatter,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        Map<String, Object> metadata\n+    ) throws IOException {\n \n         super(name, factories, aggregationContext, parent, metadata);\n-        this.targetBuckets = numBuckets;\n+        this.targetBuckets = targetBuckets;\n         this.valuesSource = (ValuesSource.Numeric) valuesSource;\n         this.formatter = formatter;\n         this.roundingInfos = roundingInfos;\n         this.roundingPreparer = roundingPreparer;\n-        preparedRounding = roundingPreparer.apply(roundingInfos[roundingIdx].rounding);\n-\n-        bucketOrds = new LongHash(1, aggregationContext.bigArrays());\n-\n     }\n \n     @Override\n-    public ScoreMode scoreMode() {\n+    public final ScoreMode scoreMode() {\n         if (valuesSource != null && valuesSource.needsScores()) {\n             return ScoreMode.COMPLETE;\n         }\n         return super.scoreMode();\n     }\n \n     @Override\n-    protected boolean shouldDefer(Aggregator aggregator) {\n+    protected final boolean shouldDefer(Aggregator aggregator) {\n         return true;\n     }\n \n     @Override\n-    public DeferringBucketCollector getDeferringCollector() {\n+    public final DeferringBucketCollector getDeferringCollector() {\n         deferringCollector = new MergingBucketsDeferringCollector(context, descendsFromGlobalAggregator(parent()));\n         return deferringCollector;\n     }\n \n+    protected abstract LeafBucketCollector getLeafCollector(SortedNumericDocValues values, LeafBucketCollector sub) throws IOException;\n+\n     @Override\n-    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx,\n-            final LeafBucketCollector sub) throws IOException {\n+    public final LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n         if (valuesSource == null) {\n             return LeafBucketCollector.NO_OP_COLLECTOR;\n         }\n-        final SortedNumericDocValues values = valuesSource.longValues(ctx);\n-        return new LeafBucketCollectorBase(sub, values) {\n-            @Override\n-            public void collect(int doc, long bucket) throws IOException {\n-                assert bucket == 0;\n-                if (values.advanceExact(doc)) {\n-                    final int valuesCount = values.docValueCount();\n+        return getLeafCollector(valuesSource.longValues(ctx), sub);\n+    }\n+\n+    protected final InternalAggregation[] buildAggregations(\n+        LongKeyedBucketOrds bucketOrds,\n+        LongToIntFunction roundingIndexFor,\n+        long[] owningBucketOrds\n+    ) throws IOException {\n+        return buildAggregationsForVariableBuckets(\n+            owningBucketOrds,\n+            bucketOrds,\n+            (bucketValue, docCount, subAggregationResults) -> new InternalAutoDateHistogram.Bucket(\n+                bucketValue,\n+                docCount,\n+                formatter,\n+                subAggregationResults\n+            ),\n+            (owningBucketOrd, buckets) -> {\n+                // the contract of the histogram aggregation is that shards must return\n+                // buckets ordered by key in ascending order\n+                CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n+\n+                // value source will be null for unmapped fields\n+                InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(\n+                    roundingInfos,\n+                    roundingIndexFor.applyAsInt(owningBucketOrd),\n+                    buildEmptySubAggregations()\n+                );\n+\n+                return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n+            }\n+        );\n+    }\n+\n+    @Override\n+    public final InternalAggregation buildEmptyAggregation() {\n+        InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(\n+            roundingInfos,\n+            0,\n+            buildEmptySubAggregations()\n+        );\n+        return new InternalAutoDateHistogram(name, Collections.emptyList(), targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n+    }\n+\n+    protected final Rounding.Prepared prepareRounding(int index) {\n+        return roundingPreparer.apply(roundingInfos[index].rounding);\n+    }\n+\n+    protected final void merge(long[] mergeMap, long newNumBuckets) {\n+        mergeBuckets(mergeMap, newNumBuckets);\n+        if (deferringCollector != null) {\n+            deferringCollector.mergeBuckets(mergeMap);\n+        }\n+    }\n+\n+    /**\n+     * Initially it uses the most fine grained rounding configuration possible\n+     * but as more data arrives it rebuckets the data until it \"fits\" in the\n+     * aggregation rounding. Similar to {@link FromMany} this checks both the\n+     * bucket count and range of the aggregation, but unlike\n+     * {@linkplain FromMany} it keeps an accurate count of the buckets and it\n+     * doesn't delay rebucketing.\n+     * <p>\n+     * Rebucketing is roughly {@code O(number_of_hits_collected_so_far)} but we\n+     * rebucket roughly {@code O(log number_of_hits_collected_so_far)} because\n+     * the \"shape\" of the roundings is <strong>roughly</strong>\n+     * logarithmically increasing.\n+     */\n+    private static class FromSingle extends AutoDateHistogramAggregator {\n+        private int roundingIdx;\n+        private Rounding.Prepared preparedRounding;\n+        /**\n+         * Map from value to bucket ordinals.\n+         * <p>\n+         * It is important that this is the exact subtype of\n+         * {@link LongKeyedBucketOrds} so that the JVM can make a monomorphic\n+         * call to {@link LongKeyedBucketOrds#add(long, long)} in the tight\n+         * inner loop of {@link LeafBucketCollector#collect(int, long)}. You'd\n+         * think that it wouldn't matter, but its seriously 7%-15% performance\n+         * difference for the aggregation. Yikes.\n+         */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "518cdfa7e76b22b676d8339eda92c76000f9a324"}, "originalPosition": 248}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODQxODA0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxODo0MDowMFrOGjOHKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxODo0MDowMFrOGjOHKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NDU1NA==", "bodyText": "Hmm, we might need to guard here in the situation where the agg doesn't actually generate any buckets (none of the collected docs end up having the field, no collected docs, etc), and return an empty agg instead.\nOr alternatively, put the guard in rebucket().\nI think BucketsAggregator#mergeBuckets() would be fine since doc counts would also be empty... but I'm less confident about MergingBucketsDeferringCollector#mergeBuckets().  Might be ok though, only skimmed it :)  We've ran into this kind of issue in the past though where buildAgg broke when the agg runs and is effectively \"unmapped\" after running, so thought I'd mention :)", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r439584554", "createdAt": "2020-06-12T18:40:00Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -120,74 +296,357 @@ public void collect(int doc, long bucket) throws IOException {\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n+                        collectValue(doc, rounded);\n+                        previousRounded = rounded;\n+                    }\n+                }\n+\n+                private void collectValue(int doc, long rounded) throws IOException {\n+                    long bucketOrd = bucketOrds.add(0, rounded);\n+                    if (bucketOrd < 0) { // already seen\n+                        bucketOrd = -1 - bucketOrd;\n+                        collectExistingBucket(sub, doc, bucketOrd);\n+                        return;\n+                    }\n+                    collectBucket(sub, doc, bucketOrd);\n+                    increaseRoundingIfNeeded(rounded);\n+                }\n+\n+                private void increaseRoundingIfNeeded(long rounded) {\n+                    if (roundingIdx >= roundingInfos.length - 1) {\n+                        return;\n+                    }\n+                    min = Math.min(min, rounded);\n+                    max = Math.max(max, rounded);\n+                    if (bucketOrds.size() <= targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()) {\n+                        return;\n+                    }\n+                    do {\n+                        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+                            preparedRounding = prepareRounding(++roundingIdx);\n+                            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+                            bucketOrds = new LongKeyedBucketOrds.FromSingle(context.bigArrays());\n+                            LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(0);\n+                            while (ordsEnum.next()) {\n+                                long oldKey = ordsEnum.value();\n+                                long newKey = preparedRounding.round(oldKey);\n+                                long newBucketOrd = bucketOrds.add(0, newKey);\n+                                mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n                             }\n+                            merge(mergeMap, bucketOrds.size());\n+                        }\n+                    } while (roundingIdx < roundingInfos.length - 1\n+                        && (bucketOrds.size() > targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                            || max - min > targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()));\n+                }\n+            };\n+        }\n+\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            return buildAggregations(bucketOrds, l -> roundingIdx, owningBucketOrds);\n+        }\n+\n+        @Override\n+        public void collectDebugInfo(BiConsumer<String, Object> add) {\n+            super.collectDebugInfo(add);\n+            add.accept(\"surviving_buckets\", bucketOrds.size());\n+        }\n+\n+        @Override\n+        protected void doClose() {\n+            Releasables.close(bucketOrds);\n+        }\n+    }\n+\n+    /**\n+     * Initially it uses the most fine grained rounding configuration possible but\n+     * as more data arrives it uses two heuristics to shift to coarser and coarser\n+     * rounding. The first heuristic is the number of buckets, specifically,\n+     * when there are more buckets than can \"fit\" in the current rounding it shifts\n+     * to the next rounding. Instead of redoing the rounding, it estimates the\n+     * number of buckets that will \"survive\" at the new rounding and uses\n+     * <strong>that</strong> as the initial value for the bucket count that it\n+     * increments in order to trigger another promotion to another coarser\n+     * rounding. This works fairly well at containing the number of buckets, but\n+     * the estimate of the number of buckets will be wrong if the buckets are\n+     * quite a spread out compared to the rounding.\n+     * <p>\n+     * The second heuristic it uses to trigger promotion to a coarser rounding is\n+     * the distance between the min and max bucket. When that distance is greater\n+     * than what the current rounding supports it promotes. This heuristic\n+     * isn't good at limiting the number of buckets but is great when the buckets\n+     * are spread out compared to the rounding. So it should complement the first\n+     * heuristic.\n+     * <p>\n+     * When promoting a rounding we keep the old buckets around because it is\n+     * expensive to call {@link MergingBucketsDeferringCollector#mergeBuckets}.\n+     * In particular it is {@code O(number_of_hits_collected_so_far)}. So if we\n+     * called it frequently we'd end up in {@code O(n^2)} territory. Bad news for\n+     * aggregations! Instead, we keep a \"budget\" of buckets that we're ok\n+     * \"wasting\". When we promote the rounding and our estimate of the number of\n+     * \"dead\" buckets that have data but have yet to be merged into the buckets\n+     * that are valid for the current rounding exceeds the budget then we rebucket\n+     * the entire aggregation and double the budget.\n+     * <p>\n+     * Once we're done collecting and we know exactly which buckets we'll be\n+     * returning we <strong>finally</strong> perform a \"real\", \"perfect bucketing\",\n+     * rounding all of the keys for {@code owningBucketOrd} that we're going to\n+     * collect and picking the rounding based on a real, accurate count and the\n+     * min and max.\n+     */\n+    private static class FromMany extends AutoDateHistogramAggregator {\n+        /**\n+         * An array of prepared roundings in the same order as\n+         * {@link #roundingInfos}. The 0th entry is prepared initially,\n+         * and other entries are null until first needed.\n+         */\n+        private final Rounding.Prepared[] preparedRoundings;\n+        /**\n+         * Map from value to bucket ordinals.\n+         * <p>\n+         * It is important that this is the exact subtype of\n+         * {@link LongKeyedBucketOrds} so that the JVM can make a monomorphic\n+         * call to {@link LongKeyedBucketOrds#add(long, long)} in the tight\n+         * inner loop of {@link LeafBucketCollector#collect(int, long)}.\n+         */\n+        private LongKeyedBucketOrds.FromMany bucketOrds;\n+        /**\n+         * The index of the rounding that each {@code owningBucketOrd} is\n+         * currently using.\n+         * <p>\n+         * During collection we use overestimates for how much buckets are save\n+         * by bumping to the next rounding index. So we end up bumping less\n+         * aggressively than a \"perfect\" algorithm. That is fine because we\n+         * correct the error when we merge the buckets together all the way\n+         * up in {@link InternalAutoDateHistogram#reduceBucket}. In particular,\n+         * on final reduce we bump the rounding until it we appropriately\n+         * cover the date range across all of the results returned by all of\n+         * the {@link AutoDateHistogramAggregator}s. \n+         */\n+        private ByteArray roundingIndices;\n+        /**\n+         * The minimum key per {@code owningBucketOrd}.\n+         */\n+        private LongArray mins;\n+        /**\n+         * The max key per {@code owningBucketOrd}.\n+         */\n+        private LongArray maxes;\n+\n+        /**\n+         * An underestimate of the number of buckets that are \"live\" in the\n+         * current rounding for each {@code owningBucketOrdinal}. \n+         */\n+        private IntArray liveBucketCountUnderestimate;\n+        /**\n+         * An over estimate of the number of wasted buckets. When this gets\n+         * too high we {@link #rebucket} which sets it to 0.\n+         */\n+        private long wastedBucketsOverestimate = 0;\n+        /**\n+         * The next {@link #wastedBucketsOverestimate} that will trigger a\n+         * {@link #rebucket() rebucketing}.\n+         */\n+        private long nextRebucketAt = 1000; // TODO this could almost certainly start higher when asMultiBucketAggregator is gone\n+        /**\n+         * The number of times the aggregator had to {@link #rebucket()} the\n+         * results. We keep this just to report to the profiler.\n+         */\n+        private int rebucketCount = 0;\n+\n+        FromMany(\n+            String name,\n+            AggregatorFactories factories,\n+            int targetBuckets,\n+            RoundingInfo[] roundingInfos,\n+            Function<Rounding, Rounding.Prepared> roundingPreparer,\n+            @Nullable ValuesSource valuesSource,\n+            DocValueFormat formatter,\n+            SearchContext aggregationContext,\n+            Aggregator parent,\n+            Map<String, Object> metadata\n+        ) throws IOException {\n+\n+            super(\n+                name,\n+                factories,\n+                targetBuckets,\n+                roundingInfos,\n+                roundingPreparer,\n+                valuesSource,\n+                formatter,\n+                aggregationContext,\n+                parent,\n+                metadata\n+            );\n+            assert roundingInfos.length < 127 : \"Rounding must fit in a signed byte\";\n+            roundingIndices = context.bigArrays().newByteArray(1, true);\n+            mins = context.bigArrays().newLongArray(1, false);\n+            mins.set(0, Long.MAX_VALUE);\n+            maxes = context.bigArrays().newLongArray(1, false);\n+            maxes.set(0, Long.MIN_VALUE);\n+            preparedRoundings = new Rounding.Prepared[roundingInfos.length];\n+            // Prepare the first rounding because we know we'll need it.\n+            preparedRoundings[0] = roundingPreparer.apply(roundingInfos[0].rounding);\n+            bucketOrds = new LongKeyedBucketOrds.FromMany(context.bigArrays());\n+            liveBucketCountUnderestimate = context.bigArrays().newIntArray(1, true);\n+        }\n+\n+        @Override\n+        protected LeafBucketCollector getLeafCollector(SortedNumericDocValues values, LeafBucketCollector sub) throws IOException {\n+            return new LeafBucketCollectorBase(sub, values) {\n+                @Override\n+                public void collect(int doc, long owningBucketOrd) throws IOException {\n+                    if (false == values.advanceExact(doc)) {\n+                        return;\n+                    }\n+                    int valuesCount = values.docValueCount();\n+\n+                    long previousRounded = Long.MIN_VALUE;\n+                    int roundingIdx = roundingIndexFor(owningBucketOrd);\n+                    for (int i = 0; i < valuesCount; ++i) {\n+                        long value = values.nextValue();\n+                        long rounded = preparedRoundings[roundingIdx].round(value);\n+                        assert rounded >= previousRounded;\n+                        if (rounded == previousRounded) {\n+                            continue;\n                         }\n+                        roundingIdx = collectValue(owningBucketOrd, roundingIdx, doc, rounded);\n                         previousRounded = rounded;\n                     }\n                 }\n-            }\n \n-            private void increaseRounding() {\n-                try (LongHash oldBucketOrds = bucketOrds) {\n-                    LongHash newBucketOrds = new LongHash(1, context.bigArrays());\n-                    long[] mergeMap = new long[(int) oldBucketOrds.size()];\n-                    preparedRounding = roundingPreparer.apply(roundingInfos[++roundingIdx].rounding);\n-                    for (int i = 0; i < oldBucketOrds.size(); i++) {\n-                        long oldKey = oldBucketOrds.get(i);\n-                        long newKey = preparedRounding.round(oldKey);\n-                        long newBucketOrd = newBucketOrds.add(newKey);\n-                        if (newBucketOrd >= 0) {\n-                            mergeMap[i] = newBucketOrd;\n-                        } else {\n-                            mergeMap[i] = -1 - newBucketOrd;\n-                        }\n+                private int collectValue(long owningBucketOrd, int roundingIdx, int doc, long rounded) throws IOException {\n+                    long bucketOrd = bucketOrds.add(owningBucketOrd, rounded);\n+                    if (bucketOrd < 0) { // already seen\n+                        bucketOrd = -1 - bucketOrd;\n+                        collectExistingBucket(sub, doc, bucketOrd);\n+                        return roundingIdx;\n+                    }\n+                    collectBucket(sub, doc, bucketOrd);\n+                    liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                    int estimatedBucketCount = liveBucketCountUnderestimate.increment(owningBucketOrd, 1);\n+                    return increaseRoundingIfNeeded(owningBucketOrd, estimatedBucketCount, rounded, roundingIdx);\n+                }\n+\n+                /**\n+                 * Increase the rounding of {@code owningBucketOrd} using\n+                 * estimated, bucket counts, {@link #rebucket() rebucketing} the all\n+                 * buckets if the estimated number of wasted buckets is too high.\n+                 */\n+                private int increaseRoundingIfNeeded(long owningBucketOrd, int oldEstimatedBucketCount, long newKey, int oldRounding) {\n+                    if (oldRounding >= roundingInfos.length - 1) {\n+                        return oldRounding;\n+                    }\n+                    if (mins.size() < owningBucketOrd + 1) {\n+                        long oldSize = mins.size();\n+                        mins = context.bigArrays().grow(mins, owningBucketOrd + 1);\n+                        mins.fill(oldSize, mins.size(), Long.MAX_VALUE);\n+                    }\n+                    if (maxes.size() < owningBucketOrd + 1) {\n+                        long oldSize = maxes.size();\n+                        maxes = context.bigArrays().grow(maxes, owningBucketOrd + 1);\n+                        maxes.fill(oldSize, maxes.size(), Long.MIN_VALUE);\n+                    }\n+\n+                    long min = Math.min(mins.get(owningBucketOrd), newKey);\n+                    mins.set(owningBucketOrd, min);\n+                    long max = Math.max(maxes.get(owningBucketOrd), newKey);\n+                    maxes.set(owningBucketOrd, max);\n+                    if (oldEstimatedBucketCount <= targetBuckets * roundingInfos[oldRounding].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[oldRounding].getMaximumRoughEstimateDurationMillis()) {\n+                        return oldRounding;\n+                    }\n+                    long oldRoughDuration = roundingInfos[oldRounding].roughEstimateDurationMillis;\n+                    int newRounding = oldRounding;\n+                    int newEstimatedBucketCount;\n+                    do {\n+                        newRounding++;\n+                        double ratio = (double) oldRoughDuration / (double) roundingInfos[newRounding].getRoughEstimateDurationMillis();\n+                        newEstimatedBucketCount = (int) Math.ceil(oldEstimatedBucketCount * ratio);\n+                    } while (newRounding < roundingInfos.length - 1\n+                        && (newEstimatedBucketCount > targetBuckets * roundingInfos[newRounding].getMaximumInnerInterval()\n+                            || max - min > targetBuckets * roundingInfos[newRounding].getMaximumRoughEstimateDurationMillis()));\n+                    setRounding(owningBucketOrd, newRounding);\n+                    mins.set(owningBucketOrd, preparedRoundings[newRounding].round(mins.get(owningBucketOrd)));\n+                    maxes.set(owningBucketOrd, preparedRoundings[newRounding].round(maxes.get(owningBucketOrd)));\n+                    wastedBucketsOverestimate += oldEstimatedBucketCount - newEstimatedBucketCount;\n+                    if (wastedBucketsOverestimate > nextRebucketAt) {\n+                        rebucket();\n+                        // Bump the threshold for the next rebucketing\n+                        wastedBucketsOverestimate = 0;\n+                        nextRebucketAt *= 2;\n+                    } else {\n+                        liveBucketCountUnderestimate.set(owningBucketOrd, newEstimatedBucketCount);\n                     }\n-                    mergeBuckets(mergeMap, newBucketOrds.size());\n-                    if (deferringCollector != null) {\n-                        deferringCollector.mergeBuckets(mergeMap);\n+                    return newRounding;\n+                }\n+            };\n+        }\n+\n+        private void rebucket() {\n+            rebucketCount++;\n+            try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+                long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+                bucketOrds = new LongKeyedBucketOrds.FromMany(context.bigArrays());\n+                for (long owningBucketOrd = 0; owningBucketOrd <= oldOrds.maxOwningBucketOrd(); owningBucketOrd++) {\n+                    LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(owningBucketOrd);\n+                    Rounding.Prepared preparedRounding = preparedRoundings[roundingIndexFor(owningBucketOrd)];\n+                    while (ordsEnum.next()) {\n+                        long oldKey = ordsEnum.value();\n+                        long newKey = preparedRounding.round(oldKey);\n+                        long newBucketOrd = bucketOrds.add(owningBucketOrd, newKey);\n+                        mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n                     }\n-                    bucketOrds = newBucketOrds;\n+                    liveBucketCountUnderestimate = context.bigArrays().grow(liveBucketCountUnderestimate, owningBucketOrd + 1);\n+                    liveBucketCountUnderestimate.set(owningBucketOrd, Math.toIntExact(bucketOrds.bucketsInOrd(owningBucketOrd)));\n                 }\n+                merge(mergeMap, bucketOrds.size());\n             }\n-        };\n-    }\n+        }\n \n-    @Override\n-    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n-        return buildAggregationsForVariableBuckets(owningBucketOrds, bucketOrds,\n-                (bucketValue, docCount, subAggregationResults) ->\n-                    new InternalAutoDateHistogram.Bucket(bucketValue, docCount, formatter, subAggregationResults),\n-                buckets -> {\n-                    // the contract of the histogram aggregation is that shards must return\n-                    // buckets ordered by key in ascending order\n-                    CollectionUtil.introSort(buckets, BucketOrder.key(true).comparator());\n-\n-                    // value source will be null for unmapped fields\n-                    InternalAutoDateHistogram.BucketInfo emptyBucketInfo = new InternalAutoDateHistogram.BucketInfo(roundingInfos,\n-                            roundingIdx, buildEmptySubAggregations());\n-\n-                    return new InternalAutoDateHistogram(name, buckets, targetBuckets, emptyBucketInfo, formatter, metadata(), 1);\n-                });\n-    }\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            /*\n+             * Rebucket before building the aggregation to build as small as result\n+             * as possible.\n+             *\n+             * TODO it'd be faster if we could apply the merging on the fly as we\n+             * replay the hits and build the buckets. How much faster is not clear,\n+             * but it does have the advantage of only touching the buckets that we\n+             * want to collect.\n+             */\n+            rebucket();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "518cdfa7e76b22b676d8339eda92c76000f9a324"}, "originalPosition": 668}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODQzNTU4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxODo0NjowOVrOGjOSEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNjo0NDowNVrOGj6S9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NzM0NQ==", "bodyText": "Exposing rebucketing info (count, rebucketAt, wasted, etc) via collectDebugInfo would be super interesting from a profiling perspective.  Especially if a user comes to us with performance issues, we'd have a better idea if they are hitting some kind of antagonistic scenario or something\nNot super important, just a thought :)", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r439587345", "createdAt": "2020-06-12T18:46:09Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -120,74 +296,357 @@ public void collect(int doc, long bucket) throws IOException {\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n+                        collectValue(doc, rounded);\n+                        previousRounded = rounded;\n+                    }\n+                }\n+\n+                private void collectValue(int doc, long rounded) throws IOException {\n+                    long bucketOrd = bucketOrds.add(0, rounded);\n+                    if (bucketOrd < 0) { // already seen\n+                        bucketOrd = -1 - bucketOrd;\n+                        collectExistingBucket(sub, doc, bucketOrd);\n+                        return;\n+                    }\n+                    collectBucket(sub, doc, bucketOrd);\n+                    increaseRoundingIfNeeded(rounded);\n+                }\n+\n+                private void increaseRoundingIfNeeded(long rounded) {\n+                    if (roundingIdx >= roundingInfos.length - 1) {\n+                        return;\n+                    }\n+                    min = Math.min(min, rounded);\n+                    max = Math.max(max, rounded);\n+                    if (bucketOrds.size() <= targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()) {\n+                        return;\n+                    }\n+                    do {\n+                        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+                            preparedRounding = prepareRounding(++roundingIdx);\n+                            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+                            bucketOrds = new LongKeyedBucketOrds.FromSingle(context.bigArrays());\n+                            LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(0);\n+                            while (ordsEnum.next()) {\n+                                long oldKey = ordsEnum.value();\n+                                long newKey = preparedRounding.round(oldKey);\n+                                long newBucketOrd = bucketOrds.add(0, newKey);\n+                                mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n                             }\n+                            merge(mergeMap, bucketOrds.size());\n+                        }\n+                    } while (roundingIdx < roundingInfos.length - 1\n+                        && (bucketOrds.size() > targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                            || max - min > targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()));\n+                }\n+            };\n+        }\n+\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            return buildAggregations(bucketOrds, l -> roundingIdx, owningBucketOrds);\n+        }\n+\n+        @Override\n+        public void collectDebugInfo(BiConsumer<String, Object> add) {\n+            super.collectDebugInfo(add);\n+            add.accept(\"surviving_buckets\", bucketOrds.size());\n+        }\n+\n+        @Override\n+        protected void doClose() {\n+            Releasables.close(bucketOrds);\n+        }\n+    }\n+\n+    /**\n+     * Initially it uses the most fine grained rounding configuration possible but\n+     * as more data arrives it uses two heuristics to shift to coarser and coarser\n+     * rounding. The first heuristic is the number of buckets, specifically,\n+     * when there are more buckets than can \"fit\" in the current rounding it shifts\n+     * to the next rounding. Instead of redoing the rounding, it estimates the\n+     * number of buckets that will \"survive\" at the new rounding and uses\n+     * <strong>that</strong> as the initial value for the bucket count that it\n+     * increments in order to trigger another promotion to another coarser\n+     * rounding. This works fairly well at containing the number of buckets, but\n+     * the estimate of the number of buckets will be wrong if the buckets are\n+     * quite a spread out compared to the rounding.\n+     * <p>\n+     * The second heuristic it uses to trigger promotion to a coarser rounding is\n+     * the distance between the min and max bucket. When that distance is greater\n+     * than what the current rounding supports it promotes. This heuristic\n+     * isn't good at limiting the number of buckets but is great when the buckets\n+     * are spread out compared to the rounding. So it should complement the first\n+     * heuristic.\n+     * <p>\n+     * When promoting a rounding we keep the old buckets around because it is\n+     * expensive to call {@link MergingBucketsDeferringCollector#mergeBuckets}.\n+     * In particular it is {@code O(number_of_hits_collected_so_far)}. So if we\n+     * called it frequently we'd end up in {@code O(n^2)} territory. Bad news for\n+     * aggregations! Instead, we keep a \"budget\" of buckets that we're ok\n+     * \"wasting\". When we promote the rounding and our estimate of the number of\n+     * \"dead\" buckets that have data but have yet to be merged into the buckets\n+     * that are valid for the current rounding exceeds the budget then we rebucket\n+     * the entire aggregation and double the budget.\n+     * <p>\n+     * Once we're done collecting and we know exactly which buckets we'll be\n+     * returning we <strong>finally</strong> perform a \"real\", \"perfect bucketing\",\n+     * rounding all of the keys for {@code owningBucketOrd} that we're going to\n+     * collect and picking the rounding based on a real, accurate count and the\n+     * min and max.\n+     */\n+    private static class FromMany extends AutoDateHistogramAggregator {\n+        /**\n+         * An array of prepared roundings in the same order as\n+         * {@link #roundingInfos}. The 0th entry is prepared initially,\n+         * and other entries are null until first needed.\n+         */\n+        private final Rounding.Prepared[] preparedRoundings;\n+        /**\n+         * Map from value to bucket ordinals.\n+         * <p>\n+         * It is important that this is the exact subtype of\n+         * {@link LongKeyedBucketOrds} so that the JVM can make a monomorphic\n+         * call to {@link LongKeyedBucketOrds#add(long, long)} in the tight\n+         * inner loop of {@link LeafBucketCollector#collect(int, long)}.\n+         */\n+        private LongKeyedBucketOrds.FromMany bucketOrds;\n+        /**\n+         * The index of the rounding that each {@code owningBucketOrd} is\n+         * currently using.\n+         * <p>\n+         * During collection we use overestimates for how much buckets are save\n+         * by bumping to the next rounding index. So we end up bumping less\n+         * aggressively than a \"perfect\" algorithm. That is fine because we\n+         * correct the error when we merge the buckets together all the way\n+         * up in {@link InternalAutoDateHistogram#reduceBucket}. In particular,\n+         * on final reduce we bump the rounding until it we appropriately\n+         * cover the date range across all of the results returned by all of\n+         * the {@link AutoDateHistogramAggregator}s. \n+         */\n+        private ByteArray roundingIndices;\n+        /**\n+         * The minimum key per {@code owningBucketOrd}.\n+         */\n+        private LongArray mins;\n+        /**\n+         * The max key per {@code owningBucketOrd}.\n+         */\n+        private LongArray maxes;\n+\n+        /**\n+         * An underestimate of the number of buckets that are \"live\" in the\n+         * current rounding for each {@code owningBucketOrdinal}. \n+         */\n+        private IntArray liveBucketCountUnderestimate;\n+        /**\n+         * An over estimate of the number of wasted buckets. When this gets\n+         * too high we {@link #rebucket} which sets it to 0.\n+         */\n+        private long wastedBucketsOverestimate = 0;\n+        /**\n+         * The next {@link #wastedBucketsOverestimate} that will trigger a\n+         * {@link #rebucket() rebucketing}.\n+         */\n+        private long nextRebucketAt = 1000; // TODO this could almost certainly start higher when asMultiBucketAggregator is gone\n+        /**\n+         * The number of times the aggregator had to {@link #rebucket()} the\n+         * results. We keep this just to report to the profiler.\n+         */\n+        private int rebucketCount = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "518cdfa7e76b22b676d8339eda92c76000f9a324"}, "originalPosition": 466}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NzgyNA==", "bodyText": "I already exposed it all! I made this in the first place because I wanted to see how the algorithm did with certain bits of data with the profiler.", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r439597824", "createdAt": "2020-06-12T19:10:31Z", "author": {"login": "nik9000"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -120,74 +296,357 @@ public void collect(int doc, long bucket) throws IOException {\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n+                        collectValue(doc, rounded);\n+                        previousRounded = rounded;\n+                    }\n+                }\n+\n+                private void collectValue(int doc, long rounded) throws IOException {\n+                    long bucketOrd = bucketOrds.add(0, rounded);\n+                    if (bucketOrd < 0) { // already seen\n+                        bucketOrd = -1 - bucketOrd;\n+                        collectExistingBucket(sub, doc, bucketOrd);\n+                        return;\n+                    }\n+                    collectBucket(sub, doc, bucketOrd);\n+                    increaseRoundingIfNeeded(rounded);\n+                }\n+\n+                private void increaseRoundingIfNeeded(long rounded) {\n+                    if (roundingIdx >= roundingInfos.length - 1) {\n+                        return;\n+                    }\n+                    min = Math.min(min, rounded);\n+                    max = Math.max(max, rounded);\n+                    if (bucketOrds.size() <= targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()) {\n+                        return;\n+                    }\n+                    do {\n+                        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+                            preparedRounding = prepareRounding(++roundingIdx);\n+                            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+                            bucketOrds = new LongKeyedBucketOrds.FromSingle(context.bigArrays());\n+                            LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(0);\n+                            while (ordsEnum.next()) {\n+                                long oldKey = ordsEnum.value();\n+                                long newKey = preparedRounding.round(oldKey);\n+                                long newBucketOrd = bucketOrds.add(0, newKey);\n+                                mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n                             }\n+                            merge(mergeMap, bucketOrds.size());\n+                        }\n+                    } while (roundingIdx < roundingInfos.length - 1\n+                        && (bucketOrds.size() > targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                            || max - min > targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()));\n+                }\n+            };\n+        }\n+\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            return buildAggregations(bucketOrds, l -> roundingIdx, owningBucketOrds);\n+        }\n+\n+        @Override\n+        public void collectDebugInfo(BiConsumer<String, Object> add) {\n+            super.collectDebugInfo(add);\n+            add.accept(\"surviving_buckets\", bucketOrds.size());\n+        }\n+\n+        @Override\n+        protected void doClose() {\n+            Releasables.close(bucketOrds);\n+        }\n+    }\n+\n+    /**\n+     * Initially it uses the most fine grained rounding configuration possible but\n+     * as more data arrives it uses two heuristics to shift to coarser and coarser\n+     * rounding. The first heuristic is the number of buckets, specifically,\n+     * when there are more buckets than can \"fit\" in the current rounding it shifts\n+     * to the next rounding. Instead of redoing the rounding, it estimates the\n+     * number of buckets that will \"survive\" at the new rounding and uses\n+     * <strong>that</strong> as the initial value for the bucket count that it\n+     * increments in order to trigger another promotion to another coarser\n+     * rounding. This works fairly well at containing the number of buckets, but\n+     * the estimate of the number of buckets will be wrong if the buckets are\n+     * quite a spread out compared to the rounding.\n+     * <p>\n+     * The second heuristic it uses to trigger promotion to a coarser rounding is\n+     * the distance between the min and max bucket. When that distance is greater\n+     * than what the current rounding supports it promotes. This heuristic\n+     * isn't good at limiting the number of buckets but is great when the buckets\n+     * are spread out compared to the rounding. So it should complement the first\n+     * heuristic.\n+     * <p>\n+     * When promoting a rounding we keep the old buckets around because it is\n+     * expensive to call {@link MergingBucketsDeferringCollector#mergeBuckets}.\n+     * In particular it is {@code O(number_of_hits_collected_so_far)}. So if we\n+     * called it frequently we'd end up in {@code O(n^2)} territory. Bad news for\n+     * aggregations! Instead, we keep a \"budget\" of buckets that we're ok\n+     * \"wasting\". When we promote the rounding and our estimate of the number of\n+     * \"dead\" buckets that have data but have yet to be merged into the buckets\n+     * that are valid for the current rounding exceeds the budget then we rebucket\n+     * the entire aggregation and double the budget.\n+     * <p>\n+     * Once we're done collecting and we know exactly which buckets we'll be\n+     * returning we <strong>finally</strong> perform a \"real\", \"perfect bucketing\",\n+     * rounding all of the keys for {@code owningBucketOrd} that we're going to\n+     * collect and picking the rounding based on a real, accurate count and the\n+     * min and max.\n+     */\n+    private static class FromMany extends AutoDateHistogramAggregator {\n+        /**\n+         * An array of prepared roundings in the same order as\n+         * {@link #roundingInfos}. The 0th entry is prepared initially,\n+         * and other entries are null until first needed.\n+         */\n+        private final Rounding.Prepared[] preparedRoundings;\n+        /**\n+         * Map from value to bucket ordinals.\n+         * <p>\n+         * It is important that this is the exact subtype of\n+         * {@link LongKeyedBucketOrds} so that the JVM can make a monomorphic\n+         * call to {@link LongKeyedBucketOrds#add(long, long)} in the tight\n+         * inner loop of {@link LeafBucketCollector#collect(int, long)}.\n+         */\n+        private LongKeyedBucketOrds.FromMany bucketOrds;\n+        /**\n+         * The index of the rounding that each {@code owningBucketOrd} is\n+         * currently using.\n+         * <p>\n+         * During collection we use overestimates for how much buckets are save\n+         * by bumping to the next rounding index. So we end up bumping less\n+         * aggressively than a \"perfect\" algorithm. That is fine because we\n+         * correct the error when we merge the buckets together all the way\n+         * up in {@link InternalAutoDateHistogram#reduceBucket}. In particular,\n+         * on final reduce we bump the rounding until it we appropriately\n+         * cover the date range across all of the results returned by all of\n+         * the {@link AutoDateHistogramAggregator}s. \n+         */\n+        private ByteArray roundingIndices;\n+        /**\n+         * The minimum key per {@code owningBucketOrd}.\n+         */\n+        private LongArray mins;\n+        /**\n+         * The max key per {@code owningBucketOrd}.\n+         */\n+        private LongArray maxes;\n+\n+        /**\n+         * An underestimate of the number of buckets that are \"live\" in the\n+         * current rounding for each {@code owningBucketOrdinal}. \n+         */\n+        private IntArray liveBucketCountUnderestimate;\n+        /**\n+         * An over estimate of the number of wasted buckets. When this gets\n+         * too high we {@link #rebucket} which sets it to 0.\n+         */\n+        private long wastedBucketsOverestimate = 0;\n+        /**\n+         * The next {@link #wastedBucketsOverestimate} that will trigger a\n+         * {@link #rebucket() rebucketing}.\n+         */\n+        private long nextRebucketAt = 1000; // TODO this could almost certainly start higher when asMultiBucketAggregator is gone\n+        /**\n+         * The number of times the aggregator had to {@link #rebucket()} the\n+         * results. We keep this just to report to the profiler.\n+         */\n+        private int rebucketCount = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NzM0NQ=="}, "originalCommit": {"oid": "518cdfa7e76b22b676d8339eda92c76000f9a324"}, "originalPosition": 466}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMwODQ3MQ==", "bodyText": "Wow I totally overlooked those.  Ignore me! :)", "url": "https://github.com/elastic/elasticsearch/pull/57304#discussion_r440308471", "createdAt": "2020-06-15T16:44:05Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregator.java", "diffHunk": "@@ -120,74 +296,357 @@ public void collect(int doc, long bucket) throws IOException {\n                         if (rounded == previousRounded) {\n                             continue;\n                         }\n-                        long bucketOrd = bucketOrds.add(rounded);\n-                        if (bucketOrd < 0) { // already seen\n-                            bucketOrd = -1 - bucketOrd;\n-                            collectExistingBucket(sub, doc, bucketOrd);\n-                        } else {\n-                            collectBucket(sub, doc, bucketOrd);\n-                            while (roundingIdx < roundingInfos.length - 1\n-                                    && bucketOrds.size() > (targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval())) {\n-                                increaseRounding();\n+                        collectValue(doc, rounded);\n+                        previousRounded = rounded;\n+                    }\n+                }\n+\n+                private void collectValue(int doc, long rounded) throws IOException {\n+                    long bucketOrd = bucketOrds.add(0, rounded);\n+                    if (bucketOrd < 0) { // already seen\n+                        bucketOrd = -1 - bucketOrd;\n+                        collectExistingBucket(sub, doc, bucketOrd);\n+                        return;\n+                    }\n+                    collectBucket(sub, doc, bucketOrd);\n+                    increaseRoundingIfNeeded(rounded);\n+                }\n+\n+                private void increaseRoundingIfNeeded(long rounded) {\n+                    if (roundingIdx >= roundingInfos.length - 1) {\n+                        return;\n+                    }\n+                    min = Math.min(min, rounded);\n+                    max = Math.max(max, rounded);\n+                    if (bucketOrds.size() <= targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                        && max - min <= targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()) {\n+                        return;\n+                    }\n+                    do {\n+                        try (LongKeyedBucketOrds oldOrds = bucketOrds) {\n+                            preparedRounding = prepareRounding(++roundingIdx);\n+                            long[] mergeMap = new long[Math.toIntExact(oldOrds.size())];\n+                            bucketOrds = new LongKeyedBucketOrds.FromSingle(context.bigArrays());\n+                            LongKeyedBucketOrds.BucketOrdsEnum ordsEnum = oldOrds.ordsEnum(0);\n+                            while (ordsEnum.next()) {\n+                                long oldKey = ordsEnum.value();\n+                                long newKey = preparedRounding.round(oldKey);\n+                                long newBucketOrd = bucketOrds.add(0, newKey);\n+                                mergeMap[(int) ordsEnum.ord()] = newBucketOrd >= 0 ? newBucketOrd : -1 - newBucketOrd;\n                             }\n+                            merge(mergeMap, bucketOrds.size());\n+                        }\n+                    } while (roundingIdx < roundingInfos.length - 1\n+                        && (bucketOrds.size() > targetBuckets * roundingInfos[roundingIdx].getMaximumInnerInterval()\n+                            || max - min > targetBuckets * roundingInfos[roundingIdx].getMaximumRoughEstimateDurationMillis()));\n+                }\n+            };\n+        }\n+\n+        @Override\n+        public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            return buildAggregations(bucketOrds, l -> roundingIdx, owningBucketOrds);\n+        }\n+\n+        @Override\n+        public void collectDebugInfo(BiConsumer<String, Object> add) {\n+            super.collectDebugInfo(add);\n+            add.accept(\"surviving_buckets\", bucketOrds.size());\n+        }\n+\n+        @Override\n+        protected void doClose() {\n+            Releasables.close(bucketOrds);\n+        }\n+    }\n+\n+    /**\n+     * Initially it uses the most fine grained rounding configuration possible but\n+     * as more data arrives it uses two heuristics to shift to coarser and coarser\n+     * rounding. The first heuristic is the number of buckets, specifically,\n+     * when there are more buckets than can \"fit\" in the current rounding it shifts\n+     * to the next rounding. Instead of redoing the rounding, it estimates the\n+     * number of buckets that will \"survive\" at the new rounding and uses\n+     * <strong>that</strong> as the initial value for the bucket count that it\n+     * increments in order to trigger another promotion to another coarser\n+     * rounding. This works fairly well at containing the number of buckets, but\n+     * the estimate of the number of buckets will be wrong if the buckets are\n+     * quite a spread out compared to the rounding.\n+     * <p>\n+     * The second heuristic it uses to trigger promotion to a coarser rounding is\n+     * the distance between the min and max bucket. When that distance is greater\n+     * than what the current rounding supports it promotes. This heuristic\n+     * isn't good at limiting the number of buckets but is great when the buckets\n+     * are spread out compared to the rounding. So it should complement the first\n+     * heuristic.\n+     * <p>\n+     * When promoting a rounding we keep the old buckets around because it is\n+     * expensive to call {@link MergingBucketsDeferringCollector#mergeBuckets}.\n+     * In particular it is {@code O(number_of_hits_collected_so_far)}. So if we\n+     * called it frequently we'd end up in {@code O(n^2)} territory. Bad news for\n+     * aggregations! Instead, we keep a \"budget\" of buckets that we're ok\n+     * \"wasting\". When we promote the rounding and our estimate of the number of\n+     * \"dead\" buckets that have data but have yet to be merged into the buckets\n+     * that are valid for the current rounding exceeds the budget then we rebucket\n+     * the entire aggregation and double the budget.\n+     * <p>\n+     * Once we're done collecting and we know exactly which buckets we'll be\n+     * returning we <strong>finally</strong> perform a \"real\", \"perfect bucketing\",\n+     * rounding all of the keys for {@code owningBucketOrd} that we're going to\n+     * collect and picking the rounding based on a real, accurate count and the\n+     * min and max.\n+     */\n+    private static class FromMany extends AutoDateHistogramAggregator {\n+        /**\n+         * An array of prepared roundings in the same order as\n+         * {@link #roundingInfos}. The 0th entry is prepared initially,\n+         * and other entries are null until first needed.\n+         */\n+        private final Rounding.Prepared[] preparedRoundings;\n+        /**\n+         * Map from value to bucket ordinals.\n+         * <p>\n+         * It is important that this is the exact subtype of\n+         * {@link LongKeyedBucketOrds} so that the JVM can make a monomorphic\n+         * call to {@link LongKeyedBucketOrds#add(long, long)} in the tight\n+         * inner loop of {@link LeafBucketCollector#collect(int, long)}.\n+         */\n+        private LongKeyedBucketOrds.FromMany bucketOrds;\n+        /**\n+         * The index of the rounding that each {@code owningBucketOrd} is\n+         * currently using.\n+         * <p>\n+         * During collection we use overestimates for how much buckets are save\n+         * by bumping to the next rounding index. So we end up bumping less\n+         * aggressively than a \"perfect\" algorithm. That is fine because we\n+         * correct the error when we merge the buckets together all the way\n+         * up in {@link InternalAutoDateHistogram#reduceBucket}. In particular,\n+         * on final reduce we bump the rounding until it we appropriately\n+         * cover the date range across all of the results returned by all of\n+         * the {@link AutoDateHistogramAggregator}s. \n+         */\n+        private ByteArray roundingIndices;\n+        /**\n+         * The minimum key per {@code owningBucketOrd}.\n+         */\n+        private LongArray mins;\n+        /**\n+         * The max key per {@code owningBucketOrd}.\n+         */\n+        private LongArray maxes;\n+\n+        /**\n+         * An underestimate of the number of buckets that are \"live\" in the\n+         * current rounding for each {@code owningBucketOrdinal}. \n+         */\n+        private IntArray liveBucketCountUnderestimate;\n+        /**\n+         * An over estimate of the number of wasted buckets. When this gets\n+         * too high we {@link #rebucket} which sets it to 0.\n+         */\n+        private long wastedBucketsOverestimate = 0;\n+        /**\n+         * The next {@link #wastedBucketsOverestimate} that will trigger a\n+         * {@link #rebucket() rebucketing}.\n+         */\n+        private long nextRebucketAt = 1000; // TODO this could almost certainly start higher when asMultiBucketAggregator is gone\n+        /**\n+         * The number of times the aggregator had to {@link #rebucket()} the\n+         * results. We keep this just to report to the profiler.\n+         */\n+        private int rebucketCount = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NzM0NQ=="}, "originalCommit": {"oid": "518cdfa7e76b22b676d8339eda92c76000f9a324"}, "originalPosition": 466}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3862, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}