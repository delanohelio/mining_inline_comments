{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxMjQ0NjEw", "number": 65042, "title": "Fix Two Snapshot Clone State Machine Bugs", "bodyText": "There are two separate but closely related bug fixes in this PR:\n\nWhen two snapshot clones would initialize concurrently we could get into a state\nwhere one is in front of the other in the queue snapshots array but its shard states\nare in fact queued behind the other snapshot or clone. Tightly linked to this,\nsnapshot cloning would not account for snapshot deletes when queuing shard snapshots\nwhich could lead to races where both delete and clone are running concurrently for a shard.\nAs a result of fixing the first issue and writing a test for it, it also became obvious\nthat a finished delete was not properly accounted for when it comes to starting snapshot\nclones that could now queue behind a delete.\n\nThe first bug can be seen in https://gradle-enterprise.elastic.co/s/iii3ntyxbil5i for example, the random blocking on reading metadata is just there to make it highly likely to occur. Unfortunately, a 100% reproducer for this wasn't possible with the current test infrastructure but I think this combined with the very strict asserts is good enough for now and tests can be enhanced when resolving TODOs added by this PR.\nA few notes:\n\nI added TODOs about drying things up instead of going for the perfect solution in spots right away to keep this PR at least somewhat digestible\nA large part of the changes in this PR are just enhancements to asserts (that would have caught the bugs in this right away) and related infrastructure", "createdAt": "2020-11-15T19:14:57Z", "url": "https://github.com/elastic/elasticsearch/pull/65042", "merged": true, "mergeCommit": {"oid": "12d412ebad94405dfc2b8d04ea0007b5c37d2dd0"}, "closed": true, "closedAt": "2020-11-17T16:48:18Z", "author": {"login": "original-brownbear"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdc1SjggH2gAyNTIxMjQ0NjEwOjZmMDIxNGI2MmRiMDNiNGMzMTg5MDE2ZTA2OGNmNzNiNTliOGM2MjY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABddbv8zgH2gAyNTIxMjQ0NjEwOjliMWFkZTZlY2YwZGZhOTVlYjE1MDBhYjY5MTEyNTJlN2M0M2MwYTA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "6f0214b62db03b4c3189016e068cf73b59b8c626", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6f0214b62db03b4c3189016e068cf73b59b8c626", "committedDate": "2020-11-15T19:12:21Z", "message": "Fix Two Snapshot Clone State Machine Bugs\n\nThere are two separate but closely related bug fixes in this PR:\n1. When two snapshot clones would initialize concurrently we could get into a state\nwhere one is in front of the other in the queue snapshots array but its shard states\nare in fact queued behind the other snapshot or clone. Tightly linked to this,\nsnapshot cloning would not account for snapshot deletes when queueing shard snapshots\nwhich could lead to races where both delete and clone are running concurrently for a shard.\n2. As a result of fixing the first issue and writing a test for it, it also became obvious\nthat a finished delete was not properly accounted for when it comes to starting snapshot\nclones that could now queue behind a delete."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d57e5da3cf8466ff06584b89649cb19d88d16c9", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/3d57e5da3cf8466ff06584b89649cb19d88d16c9", "committedDate": "2020-11-15T19:14:45Z", "message": "drier"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyMjYzNTc4", "url": "https://github.com/elastic/elasticsearch/pull/65042#pullrequestreview-532263578", "createdAt": "2020-11-17T11:21:34Z", "commit": {"oid": "3d57e5da3cf8466ff06584b89649cb19d88d16c9"}, "state": "APPROVED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxMToyMTozNFrOH0wRdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxMToyNjoyOFrOH0wchA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3ODkwMA==", "bodyText": "This is the only place where the clone is re-queued right? I was wondering if this could lead to starvation in some really bad scenarios, but I think that's not possible?", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525078900", "createdAt": "2020-11-17T11:21:34Z", "author": {"login": "fcofdez"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -476,25 +476,50 @@ public ClusterState execute(ClusterState currentState) {\n                 final String repoName = cloneEntry.repository();\n                 final ShardGenerations shardGenerations = repoData.shardGenerations();\n                 for (int i = 0; i < updatedEntries.size(); i++) {\n-                    if (cloneEntry.snapshot().equals(updatedEntries.get(i).snapshot())) {\n+                    final SnapshotsInProgress.Entry entry = updatedEntries.get(i);\n+                    if (cloneEntry.repository().equals(entry.repository()) == false) {\n+                        // different repo => just continue without modification\n+                        continue;\n+                    }\n+                    if (cloneEntry.snapshot().getSnapshotId().equals(entry.snapshot().getSnapshotId())) {\n                         final ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clonesBuilder =\n                                 ImmutableOpenMap.builder();\n-                        final InFlightShardSnapshotStates inFlightShardStates =\n-                            InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries());\n+                        final boolean readyToExecute = currentState.custom(\n+                                SnapshotDeletionsInProgress.TYPE, SnapshotDeletionsInProgress.EMPTY).getEntries().stream()\n+                                .noneMatch(e -> e.repository().equals(repoName) && e.state() == SnapshotDeletionsInProgress.State.STARTED);\n+                        final InFlightShardSnapshotStates inFlightShardStates;\n+                        if (readyToExecute) {\n+                            inFlightShardStates = InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries());\n+                        } else {\n+                            // no need to compute these, we'll mark all shards as queued anyway because we wait for the delete\n+                            inFlightShardStates = null;\n+                        }\n+                        boolean queuedShards = false;\n                         for (Tuple<IndexId, Integer> count : counts) {\n                             for (int shardId = 0; shardId < count.v2(); shardId++) {\n                                 final RepositoryShardId repoShardId = new RepositoryShardId(count.v1(), shardId);\n                                 final String indexName = repoShardId.indexName();\n-                                if (inFlightShardStates.isActive(indexName, shardId)) {\n+                                if (readyToExecute == false || inFlightShardStates.isActive(indexName, shardId)) {\n                                     clonesBuilder.put(repoShardId, ShardSnapshotStatus.UNASSIGNED_QUEUED);\n+                                    queuedShards = true;\n                                 } else {\n                                     clonesBuilder.put(repoShardId, new ShardSnapshotStatus(localNodeId,\n                                         inFlightShardStates.generationForShard(repoShardId.index(), shardId, shardGenerations)));\n                                 }\n                             }\n                         }\n                         updatedEntry = cloneEntry.withClones(clonesBuilder.build());\n-                        updatedEntries.set(i, updatedEntry);\n+                        if (queuedShards) {\n+                            // We queued up some shards based on the in-flight operations found in all snapshots for the current\n+                            // repository, so in order to make sure we don't set a shard to QUEUED before (as in before it in the\n+                            // `updatedEntries` list) one that is actively executing we just put it to the back of the list as if we had\n+                            // just created the entry\n+                            // TODO: If we could eventually drop the snapshot clone init phase we don't need this any longer\n+                            updatedEntries.remove(i);\n+                            updatedEntries.add(updatedEntry);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d57e5da3cf8466ff06584b89649cb19d88d16c9"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3OTczNQ==", "bodyText": "Is this related to this pr?", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525079735", "createdAt": "2020-11-17T11:22:54Z", "author": {"login": "fcofdez"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -471,6 +472,25 @@ public XContentBuilder toXContent(XContentBuilder builder, Params params) throws\n                 }\n             }\n             builder.endArray();\n+            if (isClone()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d57e5da3cf8466ff06584b89649cb19d88d16c9"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA4MTczMg==", "bodyText": "Maybe we could assert here that the shard snapshots are queued?", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525081732", "createdAt": "2020-11-17T11:26:28Z", "author": {"login": "fcofdez"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/snapshots/CloneSnapshotIT.java", "diffHunk": "@@ -571,6 +575,40 @@ public void testStartCloneWithSuccessfulShardSnapshotPendingFinalization() throw\n         assertEquals(getSnapshot(repoName, cloneName).state(), SnapshotState.SUCCESS);\n     }\n \n+    public void testStartCloneDuringRunningDelete() throws Exception {\n+        final String masterName = internalCluster().startMasterOnlyNode(LARGE_SNAPSHOT_POOL_SETTINGS);\n+        internalCluster().startDataOnlyNode();\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"mock\");\n+\n+        final String indexName = \"test-idx\";\n+        createIndexWithContent(indexName);\n+\n+        final String sourceSnapshot = \"source-snapshot\";\n+        createFullSnapshot(repoName, sourceSnapshot);\n+\n+        final List<String> snapshotNames = createNSnapshots(repoName, randomIntBetween(1, 5));\n+        blockMasterOnWriteIndexFile(repoName);\n+        final ActionFuture<AcknowledgedResponse> deleteFuture = startDeleteSnapshot(repoName, randomFrom(snapshotNames));\n+        waitForBlock(masterName, repoName);\n+        awaitNDeletionsInProgress(1);\n+\n+        final ActionFuture<AcknowledgedResponse> cloneFuture = startClone(repoName, sourceSnapshot, \"target-snapshot\", indexName);\n+        logger.info(\"--> waiting for snapshot clone to be fully initialized\");\n+        awaitClusterState(state -> {\n+            for (SnapshotsInProgress.Entry entry : state.custom(SnapshotsInProgress.TYPE, SnapshotsInProgress.EMPTY).entries()) {\n+                if (entry.clones().isEmpty() == false) {\n+                    assertEquals(sourceSnapshot, entry.source().getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3d57e5da3cf8466ff06584b89649cb19d88d16c9"}, "originalPosition": 38}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1cca1970528597f0381ec4bdb04199661e0bd44e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/1cca1970528597f0381ec4bdb04199661e0bd44e", "committedDate": "2020-11-17T12:57:45Z", "message": "Merge remote-tracking branch 'elastic/master' into fix-clone-bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f7b19344a1b1c239f6bbd5db749e43189e0efea8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/f7b19344a1b1c239f6bbd5db749e43189e0efea8", "committedDate": "2020-11-17T13:02:22Z", "message": "add assertion"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyNDc1MDQ4", "url": "https://github.com/elastic/elasticsearch/pull/65042#pullrequestreview-532475048", "createdAt": "2020-11-17T15:20:15Z", "commit": {"oid": "f7b19344a1b1c239f6bbd5db749e43189e0efea8"}, "state": "APPROVED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNToyMDoxNlrOH06BSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNTozNzoxMlrOH07RvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzODYwMg==", "bodyText": "That looks strange that Entry implements ToXContent just for rendering assertion error messages? I guess it's fine but I find it strange (and it should implement ToXContentObject)", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525238602", "createdAt": "2020-11-17T15:20:16Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java", "diffHunk": "@@ -471,6 +472,25 @@ public XContentBuilder toXContent(XContentBuilder builder, Params params) throws\n                 }\n             }\n             builder.endArray();\n+            if (isClone()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTA3OTczNQ=="}, "originalCommit": {"oid": "3d57e5da3cf8466ff06584b89649cb19d88d16c9"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI0NTgxMA==", "bodyText": "readyToExecute kind of lack of meaning, maybe startedSnapshotDeletion (+ anyMatch)?", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525245810", "createdAt": "2020-11-17T15:26:43Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -476,25 +476,50 @@ public ClusterState execute(ClusterState currentState) {\n                 final String repoName = cloneEntry.repository();\n                 final ShardGenerations shardGenerations = repoData.shardGenerations();\n                 for (int i = 0; i < updatedEntries.size(); i++) {\n-                    if (cloneEntry.snapshot().equals(updatedEntries.get(i).snapshot())) {\n+                    final SnapshotsInProgress.Entry entry = updatedEntries.get(i);\n+                    if (cloneEntry.repository().equals(entry.repository()) == false) {\n+                        // different repo => just continue without modification\n+                        continue;\n+                    }\n+                    if (cloneEntry.snapshot().getSnapshotId().equals(entry.snapshot().getSnapshotId())) {\n                         final ImmutableOpenMap.Builder<RepositoryShardId, ShardSnapshotStatus> clonesBuilder =\n                                 ImmutableOpenMap.builder();\n-                        final InFlightShardSnapshotStates inFlightShardStates =\n-                            InFlightShardSnapshotStates.forRepo(repoName, snapshotsInProgress.entries());\n+                        final boolean readyToExecute = currentState.custom(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7b19344a1b1c239f6bbd5db749e43189e0efea8"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI0OTQ4MQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525249481", "createdAt": "2020-11-17T15:29:05Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -797,11 +822,15 @@ private static boolean assertNoDanglingSnapshots(ClusterState state) {\n         final Set<String> reposSeen = new HashSet<>();\n         for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n             if (reposSeen.add(entry.repository())) {\n-                for (ObjectCursor<ShardSnapshotStatus> value : entry.shards().values()) {\n+                for (ObjectCursor<ShardSnapshotStatus> value : (entry.isClone() ? entry.clones() : entry.shards()).values()) {\n                     if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_QUEUED)) {\n                         assert reposWithRunningDelete.contains(entry.repository())\n                                 : \"Found shard snapshot waiting to be assigned in [\" + entry +\n                                 \"] but it is not blocked by any running delete\";\n+                    } else if (value.value.isActive()) {\n+                        assert reposWithRunningDelete.contains(entry.repository()) == false\n+                                : \"Found shard snapshot actively executing in [\" + entry +\n+                                \"] when it should be blocked by a running delete\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7b19344a1b1c239f6bbd5db749e43189e0efea8"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1NTYxNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                        // Collect waiting shards that in entry that we can assign now that we are done with the deletion\n          \n          \n            \n                                        // Collect waiting shards from that entry that we can assign now that we are done with the deletion", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525255617", "createdAt": "2020-11-17T15:33:07Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1929,50 +1961,89 @@ private SnapshotsInProgress updatedSnapshotsInProgress(ClusterState currentState\n \n             // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n             // them to multiple snapshots by accident\n-            final Set<ShardId> reassignedShardIds = new HashSet<>();\n+            final Map<String, Set<Integer>> reassignedShardIds = new HashMap<>();\n \n             boolean changed = false;\n \n+            final String localNodeId = currentState.nodes().getLocalNodeId();\n             final String repoName = deleteEntry.repository();\n             // Computing the new assignments can be quite costly, only do it once below if actually needed\n             ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardAssignments = null;\n+            InFlightShardSnapshotStates inFlightShardStates = null;\n             for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n                 if (entry.repository().equals(repoName)) {\n                     if (entry.state().completed() == false) {\n-                        // Collect waiting shards that in entry that we can assign now that we are done with the deletion\n-                        final List<ShardId> canBeUpdated = new ArrayList<>();\n-                        for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> value : entry.shards()) {\n-                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_QUEUED)\n-                                    && reassignedShardIds.contains(value.key) == false) {\n-                                canBeUpdated.add(value.key);\n+                        // TODO: dry up redundant computation and code between clone and non-clone case, in particular reuse\n+                        //  `inFlightShardStates` across both clone and standard snapshot code\n+                        if (entry.isClone()) {\n+                            // Collect waiting shards that in entry that we can assign now that we are done with the deletion", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7b19344a1b1c239f6bbd5db749e43189e0efea8"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1ODAxMw==", "bodyText": "Maybe evaluate canBeUpdated.isEmpty() first and then only compute readyToExecute if needed?", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525258013", "createdAt": "2020-11-17T15:35:50Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1929,50 +1961,89 @@ private SnapshotsInProgress updatedSnapshotsInProgress(ClusterState currentState\n \n             // Keep track of shardIds that we started snapshots for as a result of removing this delete so we don't assign\n             // them to multiple snapshots by accident\n-            final Set<ShardId> reassignedShardIds = new HashSet<>();\n+            final Map<String, Set<Integer>> reassignedShardIds = new HashMap<>();\n \n             boolean changed = false;\n \n+            final String localNodeId = currentState.nodes().getLocalNodeId();\n             final String repoName = deleteEntry.repository();\n             // Computing the new assignments can be quite costly, only do it once below if actually needed\n             ImmutableOpenMap<ShardId, ShardSnapshotStatus> shardAssignments = null;\n+            InFlightShardSnapshotStates inFlightShardStates = null;\n             for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\n                 if (entry.repository().equals(repoName)) {\n                     if (entry.state().completed() == false) {\n-                        // Collect waiting shards that in entry that we can assign now that we are done with the deletion\n-                        final List<ShardId> canBeUpdated = new ArrayList<>();\n-                        for (ObjectObjectCursor<ShardId, ShardSnapshotStatus> value : entry.shards()) {\n-                            if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_QUEUED)\n-                                    && reassignedShardIds.contains(value.key) == false) {\n-                                canBeUpdated.add(value.key);\n+                        // TODO: dry up redundant computation and code between clone and non-clone case, in particular reuse\n+                        //  `inFlightShardStates` across both clone and standard snapshot code\n+                        if (entry.isClone()) {\n+                            // Collect waiting shards that in entry that we can assign now that we are done with the deletion\n+                            final List<RepositoryShardId> canBeUpdated = new ArrayList<>();\n+                            for (ObjectObjectCursor<RepositoryShardId, ShardSnapshotStatus> value : entry.clones()) {\n+                                if (value.value.equals(ShardSnapshotStatus.UNASSIGNED_QUEUED)\n+                                        && alreadyReassigned(value.key.indexName(), value.key.shardId(), reassignedShardIds) == false) {\n+                                    canBeUpdated.add(value.key);\n+                                }\n+                            }\n+                            // TODO: the below logic is very similar to that in #startCloning and both could be dried up against each other\n+                            //       also the code for standard snapshots could make use of this breakout as well\n+                            final boolean readyToExecute = updatedDeletions.getEntries().stream().noneMatch(\n+                                    e -> e.repository().equals(repoName) && e.state() == SnapshotDeletionsInProgress.State.STARTED);\n+                            if (readyToExecute == false || canBeUpdated.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7b19344a1b1c239f6bbd5db749e43189e0efea8"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1OTE5Ng==", "bodyText": "Maybe add some message in case it occurs in tests?", "url": "https://github.com/elastic/elasticsearch/pull/65042#discussion_r525259196", "createdAt": "2020-11-17T15:37:12Z", "author": {"login": "tlrx"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java", "diffHunk": "@@ -1987,6 +2058,15 @@ private SnapshotsInProgress updatedSnapshotsInProgress(ClusterState currentState\n             }\n             return changed ? SnapshotsInProgress.of(snapshotEntries) : null;\n         }\n+\n+        private void markShardReassigned(String indexName, int shardId, Map<String, Set<Integer>> reassignments) {\n+            final boolean added = reassignments.computeIfAbsent(indexName, k -> new HashSet<>()).add(shardId);\n+            assert added;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f7b19344a1b1c239f6bbd5db749e43189e0efea8"}, "originalPosition": 210}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f03a53c0ca5be2fc7a7fa953c4500b3d026ae1e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2f03a53c0ca5be2fc7a7fa953c4500b3d026ae1e", "committedDate": "2020-11-17T15:53:49Z", "message": "Update server/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java\n\nCo-authored-by: Tanguy Leroux <tlrx.dev@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f524930d2f53d368fb1509c47924d2b93b88dfc2", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/f524930d2f53d368fb1509c47924d2b93b88dfc2", "committedDate": "2020-11-17T15:54:46Z", "message": "Merge remote-tracking branch 'elastic/master' into fix-clone-bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b1ade6ecf0dfa95eb1500ab6911252e7c43c0a0", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/9b1ade6ecf0dfa95eb1500ab6911252e7c43c0a0", "committedDate": "2020-11-17T16:00:51Z", "message": "CR: comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1087, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}