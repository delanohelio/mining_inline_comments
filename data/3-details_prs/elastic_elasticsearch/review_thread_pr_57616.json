{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI3MzM3NjY3", "number": 57616, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODozMDowM1rOECRdRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQyMzoxMjoxOFrOEDqEqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODE4NjI4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/cluster/metadata/DataStream.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODozMDowM1rOGeoVMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzozNTozMlrOGfQmzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3MTI0OA==", "bodyText": "We can definitely relax the logic on the naming of backing indices, but I think the write index needs to stay the same. In other words, ILM may trigger a rollover which creates a new write index, but the current write index should always conform to the naming scheme and ILM should not shrink or otherwise modify that one.", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r434771248", "createdAt": "2020-06-03T18:30:03Z", "author": {"login": "danhermann"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/DataStream.java", "diffHunk": "@@ -48,7 +48,7 @@ public DataStream(String name, String timeStampField, List<Index> indices, long\n         this.indices = indices;\n         this.generation = generation;\n         assert indices.size() > 0;\n-        assert indices.get(indices.size() - 1).getName().equals(getBackingIndexName(name, generation));\n+        assert indices.get(indices.size() - 1).getName().endsWith(getBackingIndexName(name, generation));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8937fa4a1980570d8730cd359646da642a14f22"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3NzgxNw==", "bodyText": "I started with that invariant in place but if we want to enforce that we will have to invalidate policies that don't have a hot phase (ie. a rollover action).\nA policy that only has a warm phase (the scenario in the integration test https://github.com/elastic/elasticsearch/pull/57616/files#diff-fd8498322f0015d57152688358ddaa0aR45 ) will attempt to shrink the write index.\nI believe we can fail in this particular case and not allow any index replacement (shrink, restore index) on the write index but then the user will have to manually add another index to the data stream (ie. changing the write index) in order for shrink to eventually be successful and the policy to be able to continue.\nIs this a hard constraint?", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r434777817", "createdAt": "2020-06-03T18:41:56Z", "author": {"login": "andreidan"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/DataStream.java", "diffHunk": "@@ -48,7 +48,7 @@ public DataStream(String name, String timeStampField, List<Index> indices, long\n         this.indices = indices;\n         this.generation = generation;\n         assert indices.size() > 0;\n-        assert indices.get(indices.size() - 1).getName().equals(getBackingIndexName(name, generation));\n+        assert indices.get(indices.size() - 1).getName().endsWith(getBackingIndexName(name, generation));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3MTI0OA=="}, "originalCommit": {"oid": "e8937fa4a1980570d8730cd359646da642a14f22"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMTExNw==", "bodyText": "We decided to not allow the shrink operation on the write index. This means the naming of the write index will remain consistent with the agreed naming scheme for backing indices.", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435431117", "createdAt": "2020-06-04T17:35:32Z", "author": {"login": "andreidan"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/DataStream.java", "diffHunk": "@@ -48,7 +48,7 @@ public DataStream(String name, String timeStampField, List<Index> indices, long\n         this.indices = indices;\n         this.generation = generation;\n         assert indices.size() > 0;\n-        assert indices.get(indices.size() - 1).getName().equals(getBackingIndexName(name, generation));\n+        assert indices.get(indices.size() - 1).getName().endsWith(getBackingIndexName(name, generation));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3MTI0OA=="}, "originalCommit": {"oid": "e8937fa4a1980570d8730cd359646da642a14f22"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjI5Mjk3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "isResolved": true, "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzozNzo1NlrOGfQsew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxMDoyNjozNlrOGgX2Ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ==", "bodyText": "we still need to be lenient here as, in the case where we're replacing a backing index with one that has a prefix, the source backing index will be detected as a conflict (it'll usually be the case, thinking of shrinking and searchable snapshot indices, that we'll promptly delete the source backing index so this is a temporary state)", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435432571", "createdAt": "2020-06-04T17:37:56Z", "author": {"login": "andreidan"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ3Nzk2NQ==", "bodyText": "Wouldn't an index with a prefix not be identified as a potential conflict since it no longer starts with ds.getName()?", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435477965", "createdAt": "2020-06-04T18:47:53Z", "author": {"login": "danhermann"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4MTYzNg==", "bodyText": "This code is a bit difficult to read (in that it's difficult for me to understand what we're validating, and also exactly what we're checking for), I think some comments would be really helpful to signal behavior and intent, can you add comments for this (especially so it doesn't get reverted in the future)?", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435581636", "createdAt": "2020-06-04T22:13:19Z", "author": {"login": "dakrone"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTg0Mjk5NA==", "bodyText": "I agree completely.\nSo the reason why this needed relaxing was that we're setting a new DataStream after we replaced the original index. So from the Metadata's perspective we're trying to add a DataStream with backing indices\n[shrink-foo-000001, foo-000002]\n\nwhile the index we replaced foo-000001 is not part of any DataStream anymore.\n@danhermann the potentialConflicts will contain foo-000001 because the ds.getName in this case is foo.\nIterating a this a bit more I believe we can document it better or question this validation entirely? What is it trying to prevent? Accidentally adding indices to a data stream si not possible so is it so that users don't run into an index already exists error when a backing index is rolled over?\nWhat do you think @danhermann @dakrone ?", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435842994", "createdAt": "2020-06-05T10:49:48Z", "author": {"login": "andreidan"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTg5MDU3OA==", "bodyText": "@andreidan, ah, thanks for the explanation. The original purpose of that validation was to prevent name collisions on rollover as you suggested. I have a PR out (#57721) that changes the naming convention and I believe that will resolve this issue.", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435890578", "createdAt": "2020-06-05T12:33:33Z", "author": {"login": "danhermann"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTg5MzA3Ng==", "bodyText": "Hmm, even with the naming change above, we'll still have the problem since ILM may be removing an index with the new naming scheme. I'll have to think about this one.", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435893076", "createdAt": "2020-06-05T12:38:31Z", "author": {"login": "danhermann"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTg5NTU5NA==", "bodyText": "@danhermann Thanks for pointing out #57721. I believe this problem will persist even with this PR in place.\nSo after 57221 is merged, shrinking a backing index of a DataStream will atempt to create a DataStream with backing indices\n[shrink-.ds-foo-000001, .ds-foo-000002]\n\nwhile, at least temporary, index ds-foo-000001 will still be found in the index lookup (as we're deleting it in a later step).\nUnless I'm missing something I think we still need to keep the updated validation as shown in this PR (or a different/better way? I'm a bit out of ideas on this one)\nI just pushed some documentation for the validation to (hopefully) make reasoning about it easier", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435895594", "createdAt": "2020-06-05T12:43:21Z", "author": {"login": "andreidan"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTkwMDM5OQ==", "bodyText": "Maybe we can change the validation to look at the names in potentialConflicts and flag them only if they would conflict  with a future rollover, i.e., with a number > the current generation.", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435900399", "createdAt": "2020-06-05T12:52:22Z", "author": {"login": "danhermann"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTkxNzE5NQ==", "bodyText": "That's an interesting idea. Shall we do this in a separate PR to keep the scope of this (already rather large) PR manageable?", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435917195", "createdAt": "2020-06-05T13:23:09Z", "author": {"login": "andreidan"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA0NzcyNw==", "bodyText": "Opened #57750 for the validation change", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r436047727", "createdAt": "2020-06-05T17:00:33Z", "author": {"login": "andreidan"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU5ODM2Mw==", "bodyText": "#57750 has been merged to master and I just merged master into this branch", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r436598363", "createdAt": "2020-06-08T10:26:36Z", "author": {"login": "andreidan"}, "path": "server/src/main/java/org/elasticsearch/cluster/metadata/Metadata.java", "diffHunk": "@@ -1463,7 +1463,7 @@ private void validateDataStreams(SortedMap<String, IndexAbstraction> indicesLook\n                         List<String> conflicts = new ArrayList<>();\n                         for (Map.Entry<String, IndexAbstraction> entry : potentialConflicts.entrySet()) {\n                             if (entry.getValue().getType() != IndexAbstraction.Type.CONCRETE_INDEX ||\n-                                indexNames.contains(entry.getKey()) == false) {\n+                                indexNames.stream().noneMatch(name -> name.endsWith(entry.getKey()))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjU3MQ=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMjI5NDI2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/BranchingStep.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzozODoxNlrOGfQtTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNzozODoxNlrOGfQtTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQzMjc4MA==", "bodyText": "this is just formatting", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435432780", "createdAt": "2020-06-04T17:38:16Z", "author": {"login": "andreidan"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/BranchingStep.java", "diffHunk": "@@ -49,17 +49,22 @@ public BranchingStep(StepKey key, StepKey nextStepKeyOnFalse, StepKey nextStepKe\n         this.predicateValue = new SetOnce<>();\n     }\n \n-   @Override\n-   public ClusterState performAction(Index index, ClusterState clusterState) {\n-       IndexMetadata indexMetadata = clusterState.metadata().index(index);\n-       if (indexMetadata == null) {\n-           // Index must have been since deleted, ignore it\n-           logger.debug(\"[{}] lifecycle action for index [{}] executed but index no longer exists\", getKey().getAction(), index.getName());\n-           return clusterState;\n-       }\n-       predicateValue.set(predicate.test(index, clusterState));\n-       return clusterState;\n-   }\n+    @Override\n+    public boolean isRetryable() {\n+        return true;\n+    }\n+\n+    @Override\n+    public ClusterState performAction(Index index, ClusterState clusterState) {\n+        IndexMetadata indexMetadata = clusterState.metadata().index(index);\n+        if (indexMetadata == null) {\n+            // Index must have been since deleted, ignore it\n+            logger.debug(\"[{}] lifecycle action for index [{}] executed but index no longer exists\", getKey().getAction(), index.getName());\n+            return clusterState;\n+        }\n+        predicateValue.set(predicate.test(index, clusterState));\n+        return clusterState;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMzIyNzU2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ReplaceDataStreamBackingIndexStep.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjoxOTozM1rOGfZ7hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxMDowODoxOVrOGfokjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4Mzg3OA==", "bodyText": "I think just to be safe we need a null check for these two here, probably with a better message than an NPE further down the line", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435583878", "createdAt": "2020-06-04T22:19:33Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ReplaceDataStreamBackingIndexStep.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.core.ilm;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.index.Index;\n+\n+import java.util.Locale;\n+import java.util.Objects;\n+\n+/**\n+ * This step replaces a data stream backing index with the target index, as part of the data stream's backing indices.\n+ * Eg. if data stream `foo-stream` is backed by indices [`foo-stream-000001`, `foo-stream-000002`] and we'd like to replace the first\n+ * generation index, `foo-stream-000001`, with `shrink-foo-stream-000001`, after this step the `foo-stream` data stream will contain\n+ * the following indices\n+ * <p>\n+ * [`shrink-foo-stream-000001`, `foo-stream-000002`]\n+ * <p>\n+ * The `foo-stream-000001` index will continue to exist but will not be part of the data stream anymore.\n+ * <p>\n+ * As the last generation is the write index of the data stream, replacing the last generation index is not allowed.\n+ * <p>\n+ * This is useful in scenarios following a restore from snapshot operation where the restored index will take the place of the source\n+ * index in the ILM lifecycle or in the case where we shrink an index and the shrunk index will take the place of the original index.\n+ */\n+public class ReplaceDataStreamBackingIndexStep extends ClusterStateActionStep {\n+    public static final String NAME = \"replace-datastream-backing-index\";\n+    private static final Logger logger = LogManager.getLogger(ReplaceDataStreamBackingIndexStep.class);\n+\n+    private final String targetIndexPrefix;\n+\n+    public ReplaceDataStreamBackingIndexStep(StepKey key, StepKey nextStepKey, String targetIndexPrefix) {\n+        super(key, nextStepKey);\n+        this.targetIndexPrefix = targetIndexPrefix;\n+    }\n+\n+    @Override\n+    public boolean isRetryable() {\n+        return true;\n+    }\n+\n+    public String getTargetIndexPrefix() {\n+        return targetIndexPrefix;\n+    }\n+\n+    @Override\n+    public ClusterState performAction(Index index, ClusterState clusterState) {\n+        String originalIndex = index.getName();\n+        final String targetIndexName = targetIndexPrefix + originalIndex;\n+        IndexMetadata targetIndexMetadata = clusterState.metadata().index(targetIndexName);\n+\n+        IndexMetadata originalIndexMetadata = clusterState.metadata().index(index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4NDc1Nw==", "bodyText": "Ah I see you already have a null check for the target, we should check the original also (and maybe move the existing null check up here so it doesn't accidentally get used and cause an NPE?)", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435584757", "createdAt": "2020-06-04T22:22:04Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ReplaceDataStreamBackingIndexStep.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.core.ilm;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.index.Index;\n+\n+import java.util.Locale;\n+import java.util.Objects;\n+\n+/**\n+ * This step replaces a data stream backing index with the target index, as part of the data stream's backing indices.\n+ * Eg. if data stream `foo-stream` is backed by indices [`foo-stream-000001`, `foo-stream-000002`] and we'd like to replace the first\n+ * generation index, `foo-stream-000001`, with `shrink-foo-stream-000001`, after this step the `foo-stream` data stream will contain\n+ * the following indices\n+ * <p>\n+ * [`shrink-foo-stream-000001`, `foo-stream-000002`]\n+ * <p>\n+ * The `foo-stream-000001` index will continue to exist but will not be part of the data stream anymore.\n+ * <p>\n+ * As the last generation is the write index of the data stream, replacing the last generation index is not allowed.\n+ * <p>\n+ * This is useful in scenarios following a restore from snapshot operation where the restored index will take the place of the source\n+ * index in the ILM lifecycle or in the case where we shrink an index and the shrunk index will take the place of the original index.\n+ */\n+public class ReplaceDataStreamBackingIndexStep extends ClusterStateActionStep {\n+    public static final String NAME = \"replace-datastream-backing-index\";\n+    private static final Logger logger = LogManager.getLogger(ReplaceDataStreamBackingIndexStep.class);\n+\n+    private final String targetIndexPrefix;\n+\n+    public ReplaceDataStreamBackingIndexStep(StepKey key, StepKey nextStepKey, String targetIndexPrefix) {\n+        super(key, nextStepKey);\n+        this.targetIndexPrefix = targetIndexPrefix;\n+    }\n+\n+    @Override\n+    public boolean isRetryable() {\n+        return true;\n+    }\n+\n+    public String getTargetIndexPrefix() {\n+        return targetIndexPrefix;\n+    }\n+\n+    @Override\n+    public ClusterState performAction(Index index, ClusterState clusterState) {\n+        String originalIndex = index.getName();\n+        final String targetIndexName = targetIndexPrefix + originalIndex;\n+        IndexMetadata targetIndexMetadata = clusterState.metadata().index(targetIndexName);\n+\n+        IndexMetadata originalIndexMetadata = clusterState.metadata().index(index);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4Mzg3OA=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxNzA2Ng==", "bodyText": "for ClusterStateActionSteps we don't do this check as the ExecuteStepsUpdateTask performs it before calling performAction.\nI think it's alright to keep this as it is for consistency reasons or otherwise update all ClusterStateActionSteps.\nWhat do you think?", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435817066", "createdAt": "2020-06-05T09:55:49Z", "author": {"login": "andreidan"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ReplaceDataStreamBackingIndexStep.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.core.ilm;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.index.Index;\n+\n+import java.util.Locale;\n+import java.util.Objects;\n+\n+/**\n+ * This step replaces a data stream backing index with the target index, as part of the data stream's backing indices.\n+ * Eg. if data stream `foo-stream` is backed by indices [`foo-stream-000001`, `foo-stream-000002`] and we'd like to replace the first\n+ * generation index, `foo-stream-000001`, with `shrink-foo-stream-000001`, after this step the `foo-stream` data stream will contain\n+ * the following indices\n+ * <p>\n+ * [`shrink-foo-stream-000001`, `foo-stream-000002`]\n+ * <p>\n+ * The `foo-stream-000001` index will continue to exist but will not be part of the data stream anymore.\n+ * <p>\n+ * As the last generation is the write index of the data stream, replacing the last generation index is not allowed.\n+ * <p>\n+ * This is useful in scenarios following a restore from snapshot operation where the restored index will take the place of the source\n+ * index in the ILM lifecycle or in the case where we shrink an index and the shrunk index will take the place of the original index.\n+ */\n+public class ReplaceDataStreamBackingIndexStep extends ClusterStateActionStep {\n+    public static final String NAME = \"replace-datastream-backing-index\";\n+    private static final Logger logger = LogManager.getLogger(ReplaceDataStreamBackingIndexStep.class);\n+\n+    private final String targetIndexPrefix;\n+\n+    public ReplaceDataStreamBackingIndexStep(StepKey key, StepKey nextStepKey, String targetIndexPrefix) {\n+        super(key, nextStepKey);\n+        this.targetIndexPrefix = targetIndexPrefix;\n+    }\n+\n+    @Override\n+    public boolean isRetryable() {\n+        return true;\n+    }\n+\n+    public String getTargetIndexPrefix() {\n+        return targetIndexPrefix;\n+    }\n+\n+    @Override\n+    public ClusterState performAction(Index index, ClusterState clusterState) {\n+        String originalIndex = index.getName();\n+        final String targetIndexName = targetIndexPrefix + originalIndex;\n+        IndexMetadata targetIndexMetadata = clusterState.metadata().index(targetIndexName);\n+\n+        IndexMetadata originalIndexMetadata = clusterState.metadata().index(index);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4Mzg3OA=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgyMzc1Ng==", "bodyText": "Ah, I misread this, I'll add the check to the step. ExecuteStepsUpdateTask does indeed perform the check too, but we do indeed test in every step.", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435823756", "createdAt": "2020-06-05T10:08:19Z", "author": {"login": "andreidan"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ReplaceDataStreamBackingIndexStep.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+package org.elasticsearch.xpack.core.ilm;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.metadata.IndexAbstraction;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.index.Index;\n+\n+import java.util.Locale;\n+import java.util.Objects;\n+\n+/**\n+ * This step replaces a data stream backing index with the target index, as part of the data stream's backing indices.\n+ * Eg. if data stream `foo-stream` is backed by indices [`foo-stream-000001`, `foo-stream-000002`] and we'd like to replace the first\n+ * generation index, `foo-stream-000001`, with `shrink-foo-stream-000001`, after this step the `foo-stream` data stream will contain\n+ * the following indices\n+ * <p>\n+ * [`shrink-foo-stream-000001`, `foo-stream-000002`]\n+ * <p>\n+ * The `foo-stream-000001` index will continue to exist but will not be part of the data stream anymore.\n+ * <p>\n+ * As the last generation is the write index of the data stream, replacing the last generation index is not allowed.\n+ * <p>\n+ * This is useful in scenarios following a restore from snapshot operation where the restored index will take the place of the source\n+ * index in the ILM lifecycle or in the case where we shrink an index and the shrunk index will take the place of the original index.\n+ */\n+public class ReplaceDataStreamBackingIndexStep extends ClusterStateActionStep {\n+    public static final String NAME = \"replace-datastream-backing-index\";\n+    private static final Logger logger = LogManager.getLogger(ReplaceDataStreamBackingIndexStep.class);\n+\n+    private final String targetIndexPrefix;\n+\n+    public ReplaceDataStreamBackingIndexStep(StepKey key, StepKey nextStepKey, String targetIndexPrefix) {\n+        super(key, nextStepKey);\n+        this.targetIndexPrefix = targetIndexPrefix;\n+    }\n+\n+    @Override\n+    public boolean isRetryable() {\n+        return true;\n+    }\n+\n+    public String getTargetIndexPrefix() {\n+        return targetIndexPrefix;\n+    }\n+\n+    @Override\n+    public ClusterState performAction(Index index, ClusterState clusterState) {\n+        String originalIndex = index.getName();\n+        final String targetIndexName = targetIndexPrefix + originalIndex;\n+        IndexMetadata targetIndexMetadata = clusterState.metadata().index(targetIndexName);\n+\n+        IndexMetadata originalIndexMetadata = clusterState.metadata().index(index);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4Mzg3OA=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMzIzODU1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjoyNDo1OVrOGfaCoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQwOTo1MjoyNFrOGfoCzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4NTY5Ng==", "bodyText": "We usually have a null check here for indices that have been deleted immediately prior to this execution", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435585696", "createdAt": "2020-06-04T22:24:59Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "diffHunk": "@@ -85,31 +96,77 @@ public boolean isSafeAction() {\n     public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) {\n         Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build();\n \n-        StepKey branchingKey = new StepKey(phase, NAME, BranchingStep.NAME);\n+        StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP);\n         StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME);\n         StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME);\n         StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME);\n         StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME);\n         StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME);\n         StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME);\n         StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME);\n+        StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY);\n         StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME);\n         StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME);\n+        StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME);\n+        StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME);\n \n-        BranchingStep conditionalSkipShrinkStep = new BranchingStep(branchingKey, waitForNoFollowerStepKey, nextStepKey,\n-            (index, clusterState) -> clusterState.getMetadata().index(index).getNumberOfShards() == numberOfShards);\n+        BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, waitForNoFollowerStepKey, nextStepKey,\n+            getSkipShrinkStepPredicate(numberOfShards));\n         WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client);\n         UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, setSingleNodeKey, client, readOnlySettings);\n         SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client);\n         CheckShrinkReadyStep checkShrinkReadyStep = new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey);\n         ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, SHRUNKEN_INDEX_PREFIX);\n         ShrunkShardsAllocatedStep allocated = new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX);\n-        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, aliasKey, SHRUNKEN_INDEX_PREFIX,\n-            ShrunkenIndexCheckStep.NAME);\n+        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey,\n+            SHRUNKEN_INDEX_PREFIX, ShrunkenIndexCheckStep.NAME);\n+        // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index\n+        // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted).\n+        // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and\n+        // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to\n+        // the shrunken index and delete the source\n+        BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey,\n+            (index, clusterState) -> {\n+                IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName());\n+                assert indexAbstraction != null : \"invalid cluster metadata. index [\" + index.getName() + \"] was not found\";\n+                return indexAbstraction.getParentDataStream() != null;\n+            });\n         ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX);\n+        ReplaceDataStreamBackingIndexStep replaceDataStreamBackingIndex = new ReplaceDataStreamBackingIndexStep(replaceDataStreamIndexKey,\n+            deleteIndexKey, SHRUNKEN_INDEX_PREFIX);\n+        DeleteStep deleteSourceIndexStep = new DeleteStep(deleteIndexKey, isShrunkIndexKey, client);\n         ShrunkenIndexCheckStep waitOnShrinkTakeover = new ShrunkenIndexCheckStep(isShrunkIndexKey, nextStepKey, SHRUNKEN_INDEX_PREFIX);\n         return Arrays.asList(conditionalSkipShrinkStep, waitForNoFollowersStep, readOnlyStep, setSingleNodeStep, checkShrinkReadyStep,\n-            shrink, allocated, copyMetadata, aliasSwapAndDelete, waitOnShrinkTakeover);\n+            shrink, allocated, copyMetadata, isDataStreamBranchingStep, aliasSwapAndDelete, waitOnShrinkTakeover,\n+            replaceDataStreamBackingIndex, deleteSourceIndexStep);\n+    }\n+\n+    static BiPredicate<Index, ClusterState> getSkipShrinkStepPredicate(int targetNumberOfShards) {\n+        return (index, clusterState) -> {\n+            IndexMetadata indexMetadata = clusterState.getMetadata().index(index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxNTExNg==", "bodyText": "the BranchingStep performs this check before testing the predicate. I'll add it here as well for readability purposes and because this predicate has semantics on its own, irrespective of ILM steps", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435815116", "createdAt": "2020-06-05T09:52:24Z", "author": {"login": "andreidan"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "diffHunk": "@@ -85,31 +96,77 @@ public boolean isSafeAction() {\n     public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) {\n         Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build();\n \n-        StepKey branchingKey = new StepKey(phase, NAME, BranchingStep.NAME);\n+        StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP);\n         StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME);\n         StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME);\n         StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME);\n         StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME);\n         StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME);\n         StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME);\n         StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME);\n+        StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY);\n         StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME);\n         StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME);\n+        StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME);\n+        StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME);\n \n-        BranchingStep conditionalSkipShrinkStep = new BranchingStep(branchingKey, waitForNoFollowerStepKey, nextStepKey,\n-            (index, clusterState) -> clusterState.getMetadata().index(index).getNumberOfShards() == numberOfShards);\n+        BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, waitForNoFollowerStepKey, nextStepKey,\n+            getSkipShrinkStepPredicate(numberOfShards));\n         WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client);\n         UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, setSingleNodeKey, client, readOnlySettings);\n         SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client);\n         CheckShrinkReadyStep checkShrinkReadyStep = new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey);\n         ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, SHRUNKEN_INDEX_PREFIX);\n         ShrunkShardsAllocatedStep allocated = new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX);\n-        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, aliasKey, SHRUNKEN_INDEX_PREFIX,\n-            ShrunkenIndexCheckStep.NAME);\n+        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey,\n+            SHRUNKEN_INDEX_PREFIX, ShrunkenIndexCheckStep.NAME);\n+        // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index\n+        // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted).\n+        // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and\n+        // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to\n+        // the shrunken index and delete the source\n+        BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey,\n+            (index, clusterState) -> {\n+                IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName());\n+                assert indexAbstraction != null : \"invalid cluster metadata. index [\" + index.getName() + \"] was not found\";\n+                return indexAbstraction.getParentDataStream() != null;\n+            });\n         ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX);\n+        ReplaceDataStreamBackingIndexStep replaceDataStreamBackingIndex = new ReplaceDataStreamBackingIndexStep(replaceDataStreamIndexKey,\n+            deleteIndexKey, SHRUNKEN_INDEX_PREFIX);\n+        DeleteStep deleteSourceIndexStep = new DeleteStep(deleteIndexKey, isShrunkIndexKey, client);\n         ShrunkenIndexCheckStep waitOnShrinkTakeover = new ShrunkenIndexCheckStep(isShrunkIndexKey, nextStepKey, SHRUNKEN_INDEX_PREFIX);\n         return Arrays.asList(conditionalSkipShrinkStep, waitForNoFollowersStep, readOnlyStep, setSingleNodeStep, checkShrinkReadyStep,\n-            shrink, allocated, copyMetadata, aliasSwapAndDelete, waitOnShrinkTakeover);\n+            shrink, allocated, copyMetadata, isDataStreamBranchingStep, aliasSwapAndDelete, waitOnShrinkTakeover,\n+            replaceDataStreamBackingIndex, deleteSourceIndexStep);\n+    }\n+\n+    static BiPredicate<Index, ClusterState> getSkipShrinkStepPredicate(int targetNumberOfShards) {\n+        return (index, clusterState) -> {\n+            IndexMetadata indexMetadata = clusterState.getMetadata().index(index);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4NTY5Ng=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMzI0MjAyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjoyNjoyN1rOGfaEqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjoyNjoyN1rOGfaEqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4NjIxNg==", "bodyText": "I think rather than wrapping and all that, you can do:\nif (indexMetadata.getNumberOfShards() == targetNumberOfShards) {\n    return true;\n}\n\nAnd then not have to wrap the below parts", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435586216", "createdAt": "2020-06-04T22:26:27Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "diffHunk": "@@ -85,31 +96,77 @@ public boolean isSafeAction() {\n     public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) {\n         Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build();\n \n-        StepKey branchingKey = new StepKey(phase, NAME, BranchingStep.NAME);\n+        StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP);\n         StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME);\n         StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME);\n         StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME);\n         StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME);\n         StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME);\n         StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME);\n         StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME);\n+        StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY);\n         StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME);\n         StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME);\n+        StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME);\n+        StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME);\n \n-        BranchingStep conditionalSkipShrinkStep = new BranchingStep(branchingKey, waitForNoFollowerStepKey, nextStepKey,\n-            (index, clusterState) -> clusterState.getMetadata().index(index).getNumberOfShards() == numberOfShards);\n+        BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, waitForNoFollowerStepKey, nextStepKey,\n+            getSkipShrinkStepPredicate(numberOfShards));\n         WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client);\n         UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, setSingleNodeKey, client, readOnlySettings);\n         SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client);\n         CheckShrinkReadyStep checkShrinkReadyStep = new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey);\n         ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, SHRUNKEN_INDEX_PREFIX);\n         ShrunkShardsAllocatedStep allocated = new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX);\n-        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, aliasKey, SHRUNKEN_INDEX_PREFIX,\n-            ShrunkenIndexCheckStep.NAME);\n+        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey,\n+            SHRUNKEN_INDEX_PREFIX, ShrunkenIndexCheckStep.NAME);\n+        // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index\n+        // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted).\n+        // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and\n+        // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to\n+        // the shrunken index and delete the source\n+        BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey,\n+            (index, clusterState) -> {\n+                IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName());\n+                assert indexAbstraction != null : \"invalid cluster metadata. index [\" + index.getName() + \"] was not found\";\n+                return indexAbstraction.getParentDataStream() != null;\n+            });\n         ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX);\n+        ReplaceDataStreamBackingIndexStep replaceDataStreamBackingIndex = new ReplaceDataStreamBackingIndexStep(replaceDataStreamIndexKey,\n+            deleteIndexKey, SHRUNKEN_INDEX_PREFIX);\n+        DeleteStep deleteSourceIndexStep = new DeleteStep(deleteIndexKey, isShrunkIndexKey, client);\n         ShrunkenIndexCheckStep waitOnShrinkTakeover = new ShrunkenIndexCheckStep(isShrunkIndexKey, nextStepKey, SHRUNKEN_INDEX_PREFIX);\n         return Arrays.asList(conditionalSkipShrinkStep, waitForNoFollowersStep, readOnlyStep, setSingleNodeStep, checkShrinkReadyStep,\n-            shrink, allocated, copyMetadata, aliasSwapAndDelete, waitOnShrinkTakeover);\n+            shrink, allocated, copyMetadata, isDataStreamBranchingStep, aliasSwapAndDelete, waitOnShrinkTakeover,\n+            replaceDataStreamBackingIndex, deleteSourceIndexStep);\n+    }\n+\n+    static BiPredicate<Index, ClusterState> getSkipShrinkStepPredicate(int targetNumberOfShards) {\n+        return (index, clusterState) -> {\n+            IndexMetadata indexMetadata = clusterState.getMetadata().index(index);\n+            boolean skipShrink = indexMetadata.getNumberOfShards() == targetNumberOfShards;\n+\n+            if (skipShrink == false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMzI0NzczOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjoyOTowNFrOGfaILA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxMDozNDoxN1rOGfpVDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4NzExNg==", "bodyText": "I think this can happen if the index was deleted, it would be good to check whether the index's metadata still exists in the cluster state, and just moving on otherwise (we have this check in our other ClusterStateActionSteps)", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435587116", "createdAt": "2020-06-04T22:29:04Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "diffHunk": "@@ -85,31 +96,77 @@ public boolean isSafeAction() {\n     public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) {\n         Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build();\n \n-        StepKey branchingKey = new StepKey(phase, NAME, BranchingStep.NAME);\n+        StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP);\n         StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME);\n         StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME);\n         StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME);\n         StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME);\n         StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME);\n         StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME);\n         StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME);\n+        StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY);\n         StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME);\n         StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME);\n+        StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME);\n+        StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME);\n \n-        BranchingStep conditionalSkipShrinkStep = new BranchingStep(branchingKey, waitForNoFollowerStepKey, nextStepKey,\n-            (index, clusterState) -> clusterState.getMetadata().index(index).getNumberOfShards() == numberOfShards);\n+        BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, waitForNoFollowerStepKey, nextStepKey,\n+            getSkipShrinkStepPredicate(numberOfShards));\n         WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client);\n         UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, setSingleNodeKey, client, readOnlySettings);\n         SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client);\n         CheckShrinkReadyStep checkShrinkReadyStep = new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey);\n         ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, SHRUNKEN_INDEX_PREFIX);\n         ShrunkShardsAllocatedStep allocated = new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX);\n-        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, aliasKey, SHRUNKEN_INDEX_PREFIX,\n-            ShrunkenIndexCheckStep.NAME);\n+        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey,\n+            SHRUNKEN_INDEX_PREFIX, ShrunkenIndexCheckStep.NAME);\n+        // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index\n+        // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted).\n+        // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and\n+        // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to\n+        // the shrunken index and delete the source\n+        BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey,\n+            (index, clusterState) -> {\n+                IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName());\n+                assert indexAbstraction != null : \"invalid cluster metadata. index [\" + index.getName() + \"] was not found\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgzNjE3NQ==", "bodyText": "I think we'll have to rely on the check that is executed in the BranchingStep here as we can't decide if the predicate should be true or false in this case (and throwing an exception would keep ILM on this step and keep retrying it)", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435836175", "createdAt": "2020-06-05T10:34:17Z", "author": {"login": "andreidan"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "diffHunk": "@@ -85,31 +96,77 @@ public boolean isSafeAction() {\n     public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) {\n         Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build();\n \n-        StepKey branchingKey = new StepKey(phase, NAME, BranchingStep.NAME);\n+        StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP);\n         StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME);\n         StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME);\n         StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME);\n         StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME);\n         StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME);\n         StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME);\n         StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME);\n+        StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY);\n         StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME);\n         StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME);\n+        StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME);\n+        StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME);\n \n-        BranchingStep conditionalSkipShrinkStep = new BranchingStep(branchingKey, waitForNoFollowerStepKey, nextStepKey,\n-            (index, clusterState) -> clusterState.getMetadata().index(index).getNumberOfShards() == numberOfShards);\n+        BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, waitForNoFollowerStepKey, nextStepKey,\n+            getSkipShrinkStepPredicate(numberOfShards));\n         WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client);\n         UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, setSingleNodeKey, client, readOnlySettings);\n         SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client);\n         CheckShrinkReadyStep checkShrinkReadyStep = new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey);\n         ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, SHRUNKEN_INDEX_PREFIX);\n         ShrunkShardsAllocatedStep allocated = new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX);\n-        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, aliasKey, SHRUNKEN_INDEX_PREFIX,\n-            ShrunkenIndexCheckStep.NAME);\n+        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey,\n+            SHRUNKEN_INDEX_PREFIX, ShrunkenIndexCheckStep.NAME);\n+        // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index\n+        // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted).\n+        // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and\n+        // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to\n+        // the shrunken index and delete the source\n+        BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey,\n+            (index, clusterState) -> {\n+                IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName());\n+                assert indexAbstraction != null : \"invalid cluster metadata. index [\" + index.getName() + \"] was not found\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4NzExNg=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxMzI1OTQ1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQyMjozMzo1NVrOGfaPHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxMDoxMDoyMFrOGfooYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4ODg5NA==", "bodyText": "I think (but I'm not sure), that this might fail for the alias case, because the index will already have been deleted in the ShrinkSetAliasStep, so we may need changes in DeleteStep to ignore deleting an index that doesn't exist", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435588894", "createdAt": "2020-06-04T22:33:55Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "diffHunk": "@@ -85,31 +96,77 @@ public boolean isSafeAction() {\n     public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) {\n         Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build();\n \n-        StepKey branchingKey = new StepKey(phase, NAME, BranchingStep.NAME);\n+        StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP);\n         StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME);\n         StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME);\n         StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME);\n         StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME);\n         StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME);\n         StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME);\n         StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME);\n+        StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY);\n         StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME);\n         StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME);\n+        StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME);\n+        StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME);\n \n-        BranchingStep conditionalSkipShrinkStep = new BranchingStep(branchingKey, waitForNoFollowerStepKey, nextStepKey,\n-            (index, clusterState) -> clusterState.getMetadata().index(index).getNumberOfShards() == numberOfShards);\n+        BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, waitForNoFollowerStepKey, nextStepKey,\n+            getSkipShrinkStepPredicate(numberOfShards));\n         WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client);\n         UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, setSingleNodeKey, client, readOnlySettings);\n         SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client);\n         CheckShrinkReadyStep checkShrinkReadyStep = new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey);\n         ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, SHRUNKEN_INDEX_PREFIX);\n         ShrunkShardsAllocatedStep allocated = new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX);\n-        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, aliasKey, SHRUNKEN_INDEX_PREFIX,\n-            ShrunkenIndexCheckStep.NAME);\n+        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey,\n+            SHRUNKEN_INDEX_PREFIX, ShrunkenIndexCheckStep.NAME);\n+        // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index\n+        // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted).\n+        // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and\n+        // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to\n+        // the shrunken index and delete the source\n+        BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey,\n+            (index, clusterState) -> {\n+                IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName());\n+                assert indexAbstraction != null : \"invalid cluster metadata. index [\" + index.getName() + \"] was not found\";\n+                return indexAbstraction.getParentDataStream() != null;\n+            });\n         ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX);\n+        ReplaceDataStreamBackingIndexStep replaceDataStreamBackingIndex = new ReplaceDataStreamBackingIndexStep(replaceDataStreamIndexKey,\n+            deleteIndexKey, SHRUNKEN_INDEX_PREFIX);\n+        DeleteStep deleteSourceIndexStep = new DeleteStep(deleteIndexKey, isShrunkIndexKey, client);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgyNDczOQ==", "bodyText": "on the non-data stream branch (ie. the alias handling) we don't try to delete the index using this step as the alias swap will delete it. This step is only used on the data stream handling branch, after the replace-datastream-backing-index step\n        ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX);\n\nThe isShrunkIndexKey step is next on the alias handling branch.", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r435824739", "createdAt": "2020-06-05T10:10:20Z", "author": {"login": "andreidan"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "diffHunk": "@@ -85,31 +96,77 @@ public boolean isSafeAction() {\n     public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) {\n         Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build();\n \n-        StepKey branchingKey = new StepKey(phase, NAME, BranchingStep.NAME);\n+        StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP);\n         StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME);\n         StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME);\n         StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME);\n         StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME);\n         StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME);\n         StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME);\n         StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME);\n+        StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY);\n         StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME);\n         StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME);\n+        StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME);\n+        StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME);\n \n-        BranchingStep conditionalSkipShrinkStep = new BranchingStep(branchingKey, waitForNoFollowerStepKey, nextStepKey,\n-            (index, clusterState) -> clusterState.getMetadata().index(index).getNumberOfShards() == numberOfShards);\n+        BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, waitForNoFollowerStepKey, nextStepKey,\n+            getSkipShrinkStepPredicate(numberOfShards));\n         WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client);\n         UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, setSingleNodeKey, client, readOnlySettings);\n         SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client);\n         CheckShrinkReadyStep checkShrinkReadyStep = new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey);\n         ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, SHRUNKEN_INDEX_PREFIX);\n         ShrunkShardsAllocatedStep allocated = new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX);\n-        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, aliasKey, SHRUNKEN_INDEX_PREFIX,\n-            ShrunkenIndexCheckStep.NAME);\n+        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey,\n+            SHRUNKEN_INDEX_PREFIX, ShrunkenIndexCheckStep.NAME);\n+        // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index\n+        // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted).\n+        // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and\n+        // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to\n+        // the shrunken index and delete the source\n+        BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey,\n+            (index, clusterState) -> {\n+                IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName());\n+                assert indexAbstraction != null : \"invalid cluster metadata. index [\" + index.getName() + \"] was not found\";\n+                return indexAbstraction.getParentDataStream() != null;\n+            });\n         ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX);\n+        ReplaceDataStreamBackingIndexStep replaceDataStreamBackingIndex = new ReplaceDataStreamBackingIndexStep(replaceDataStreamIndexKey,\n+            deleteIndexKey, SHRUNKEN_INDEX_PREFIX);\n+        DeleteStep deleteSourceIndexStep = new DeleteStep(deleteIndexKey, isShrunkIndexKey, client);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU4ODg5NA=="}, "originalCommit": {"oid": "0ab909d960da5e44c78db1bf9d2309df57944b4f"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMjcwNTA2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQyMzoxMjoxOFrOGgzdXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQyMzoxMjoxOFrOGgzdXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA1MDcxNg==", "bodyText": "Super minor, but I think we should add a debug log that shrink is being skipped because the targeted number of shards has already been met", "url": "https://github.com/elastic/elasticsearch/pull/57616#discussion_r437050716", "createdAt": "2020-06-08T23:12:18Z", "author": {"login": "dakrone"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ilm/ShrinkAction.java", "diffHunk": "@@ -85,31 +96,83 @@ public boolean isSafeAction() {\n     public List<Step> toSteps(Client client, String phase, Step.StepKey nextStepKey) {\n         Settings readOnlySettings = Settings.builder().put(IndexMetadata.SETTING_BLOCKS_WRITE, true).build();\n \n-        StepKey branchingKey = new StepKey(phase, NAME, BranchingStep.NAME);\n+        StepKey preShrinkBranchingKey = new StepKey(phase, NAME, CONDITIONAL_SKIP_SHRINK_STEP);\n         StepKey waitForNoFollowerStepKey = new StepKey(phase, NAME, WaitForNoFollowersStep.NAME);\n         StepKey readOnlyKey = new StepKey(phase, NAME, ReadOnlyAction.NAME);\n         StepKey setSingleNodeKey = new StepKey(phase, NAME, SetSingleNodeAllocateStep.NAME);\n         StepKey allocationRoutedKey = new StepKey(phase, NAME, CheckShrinkReadyStep.NAME);\n         StepKey shrinkKey = new StepKey(phase, NAME, ShrinkStep.NAME);\n         StepKey enoughShardsKey = new StepKey(phase, NAME, ShrunkShardsAllocatedStep.NAME);\n         StepKey copyMetadataKey = new StepKey(phase, NAME, CopyExecutionStateStep.NAME);\n+        StepKey dataStreamCheckBranchingKey = new StepKey(phase, NAME, CONDITIONAL_DATASTREAM_CHECK_KEY);\n         StepKey aliasKey = new StepKey(phase, NAME, ShrinkSetAliasStep.NAME);\n         StepKey isShrunkIndexKey = new StepKey(phase, NAME, ShrunkenIndexCheckStep.NAME);\n+        StepKey replaceDataStreamIndexKey = new StepKey(phase, NAME, ReplaceDataStreamBackingIndexStep.NAME);\n+        StepKey deleteIndexKey = new StepKey(phase, NAME, DeleteStep.NAME);\n \n-        BranchingStep conditionalSkipShrinkStep = new BranchingStep(branchingKey, waitForNoFollowerStepKey, nextStepKey,\n-            (index, clusterState) -> clusterState.getMetadata().index(index).getNumberOfShards() == numberOfShards);\n+        BranchingStep conditionalSkipShrinkStep = new BranchingStep(preShrinkBranchingKey, waitForNoFollowerStepKey, nextStepKey,\n+            getSkipShrinkStepPredicate(numberOfShards));\n         WaitForNoFollowersStep waitForNoFollowersStep = new WaitForNoFollowersStep(waitForNoFollowerStepKey, readOnlyKey, client);\n         UpdateSettingsStep readOnlyStep = new UpdateSettingsStep(readOnlyKey, setSingleNodeKey, client, readOnlySettings);\n         SetSingleNodeAllocateStep setSingleNodeStep = new SetSingleNodeAllocateStep(setSingleNodeKey, allocationRoutedKey, client);\n         CheckShrinkReadyStep checkShrinkReadyStep = new CheckShrinkReadyStep(allocationRoutedKey, shrinkKey);\n         ShrinkStep shrink = new ShrinkStep(shrinkKey, enoughShardsKey, client, numberOfShards, SHRUNKEN_INDEX_PREFIX);\n         ShrunkShardsAllocatedStep allocated = new ShrunkShardsAllocatedStep(enoughShardsKey, copyMetadataKey, SHRUNKEN_INDEX_PREFIX);\n-        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, aliasKey, SHRUNKEN_INDEX_PREFIX,\n-            ShrunkenIndexCheckStep.NAME);\n+        CopyExecutionStateStep copyMetadata = new CopyExecutionStateStep(copyMetadataKey, dataStreamCheckBranchingKey,\n+            SHRUNKEN_INDEX_PREFIX, ShrunkenIndexCheckStep.NAME);\n+        // by the time we get to this step we have 2 indices, the source and the shrunken one. we now need to choose an index\n+        // swapping strategy such that the shrunken index takes the place of the source index (which is also deleted).\n+        // if the source index is part of a data stream it's a matter of replacing it with the shrunken index one in the data stream and\n+        // then deleting the source index; otherwise we'll use the alias management api to atomically transfer the aliases from source to\n+        // the shrunken index and delete the source\n+        BranchingStep isDataStreamBranchingStep = new BranchingStep(dataStreamCheckBranchingKey, aliasKey, replaceDataStreamIndexKey,\n+            (index, clusterState) -> {\n+                IndexAbstraction indexAbstraction = clusterState.metadata().getIndicesLookup().get(index.getName());\n+                assert indexAbstraction != null : \"invalid cluster metadata. index [\" + index.getName() + \"] was not found\";\n+                return indexAbstraction.getParentDataStream() != null;\n+            });\n         ShrinkSetAliasStep aliasSwapAndDelete = new ShrinkSetAliasStep(aliasKey, isShrunkIndexKey, client, SHRUNKEN_INDEX_PREFIX);\n+        ReplaceDataStreamBackingIndexStep replaceDataStreamBackingIndex = new ReplaceDataStreamBackingIndexStep(replaceDataStreamIndexKey,\n+            deleteIndexKey, SHRUNKEN_INDEX_PREFIX);\n+        DeleteStep deleteSourceIndexStep = new DeleteStep(deleteIndexKey, isShrunkIndexKey, client);\n         ShrunkenIndexCheckStep waitOnShrinkTakeover = new ShrunkenIndexCheckStep(isShrunkIndexKey, nextStepKey, SHRUNKEN_INDEX_PREFIX);\n         return Arrays.asList(conditionalSkipShrinkStep, waitForNoFollowersStep, readOnlyStep, setSingleNodeStep, checkShrinkReadyStep,\n-            shrink, allocated, copyMetadata, aliasSwapAndDelete, waitOnShrinkTakeover);\n+            shrink, allocated, copyMetadata, isDataStreamBranchingStep, aliasSwapAndDelete, waitOnShrinkTakeover,\n+            replaceDataStreamBackingIndex, deleteSourceIndexStep);\n+    }\n+\n+    static BiPredicate<Index, ClusterState> getSkipShrinkStepPredicate(int targetNumberOfShards) {\n+        return (index, clusterState) -> {\n+            IndexMetadata indexMetadata = clusterState.getMetadata().index(index);\n+            if (indexMetadata == null) {\n+                // Index must have been since deleted, skip the shrink action\n+                logger.debug(\"[{}] lifecycle action for index [{}] executed but index no longer exists\", NAME, index.getName());\n+                return true;\n+            }\n+\n+            if (indexMetadata.getNumberOfShards() == targetNumberOfShards) {\n+                return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac6b0f0509e5d1e81247d78f143f162f1f13b4f0"}, "originalPosition": 104}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3656, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}