{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzczNDg5NDgz", "number": 52182, "title": "Better Incrementality for Snapshots of Unchanged Shards", "bodyText": "Use sequence numbers to determine whether a shard has changed or not instead before falling back to comparing files to get incremental snapshots on primary fail-over.", "createdAt": "2020-02-11T06:09:46Z", "url": "https://github.com/elastic/elasticsearch/pull/52182", "merged": true, "mergeCommit": {"oid": "87c910b36f886d3e3b83c65270cef7d7aa3aaa5a"}, "closed": true, "closedAt": "2020-03-23T13:24:59Z", "author": {"login": "original-brownbear"}, "timelineItems": {"totalCount": 79, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb-ws0jAH2gAyMzczNDg5NDgzOmNiZjM5MDVjOWM3OThjMjAwMjVlMGE3MTU1YjIzODc5MmY0OGM5MTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcQdtXSAFqTM3OTM5MzUxNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "cbf3905c9c798c20025e0a7155b238792f48c913", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cbf3905c9c798c20025e0a7155b238792f48c913", "committedDate": "2020-01-28T12:42:38Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "68a54a21a01ab9cf1143208fef0139d421514ccd", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/68a54a21a01ab9cf1143208fef0139d421514ccd", "committedDate": "2020-01-28T12:47:50Z", "message": "Merge remote-tracking branch 'elastic/master' into fix-incrementality-source-only-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11ac7115df152ffaaacc1b9b1ab4449b77ca3574", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/11ac7115df152ffaaacc1b9b1ab4449b77ca3574", "committedDate": "2020-01-28T14:23:48Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "841216d52af2c119ef4f50c6651fd992c9dc3d1d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/841216d52af2c119ef4f50c6651fd992c9dc3d1d", "committedDate": "2020-01-30T11:06:54Z", "message": "Merge remote-tracking branch 'elastic/master' into fix-incrementality-source-only-snapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a47de87626eb6121e45a3b9fc3cb999973b83de6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a47de87626eb6121e45a3b9fc3cb999973b83de6", "committedDate": "2020-01-30T11:45:03Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e54a400cc53a86b111c84f701727a36ceb1ba89", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/4e54a400cc53a86b111c84f701727a36ceb1ba89", "committedDate": "2020-01-30T11:47:02Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4dcea768648c15bbb3cd54b5df1291fa059d1d9a", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/4dcea768648c15bbb3cd54b5df1291fa059d1d9a", "committedDate": "2020-01-30T12:29:17Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0cf43bbbaafa65618f85e33b323a2f51c2aff5f8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/0cf43bbbaafa65618f85e33b323a2f51c2aff5f8", "committedDate": "2020-01-30T13:44:48Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "12bb77b6b041c8ddf31508f6df82ec932946eef9", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/12bb77b6b041c8ddf31508f6df82ec932946eef9", "committedDate": "2020-01-30T15:09:56Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48171e10c428af5c4e19717ed1c4fa31f2885115", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/48171e10c428af5c4e19717ed1c4fa31f2885115", "committedDate": "2020-01-31T06:48:13Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dec9ddbcab703bb220ce0c04ac44c18adfdacf10", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/dec9ddbcab703bb220ce0c04ac44c18adfdacf10", "committedDate": "2020-02-01T18:09:03Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ab25bcb880318c3d1ab31389edc1bb6c06b4f71", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/1ab25bcb880318c3d1ab31389edc1bb6c06b4f71", "committedDate": "2020-02-01T18:38:18Z", "message": "Stop creating new segments_N needlessly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fead3137e5149322baa858ef994f4f37cf2df7a9", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fead3137e5149322baa858ef994f4f37cf2df7a9", "committedDate": "2020-02-02T09:45:57Z", "message": "reorg"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7cacd744d3e3bee23e86843f1414b4531c51a822", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/7cacd744d3e3bee23e86843f1414b4531c51a822", "committedDate": "2020-02-02T09:56:48Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd052b42642d2ae0af90c8b909642339240a3733", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cd052b42642d2ae0af90c8b909642339240a3733", "committedDate": "2020-02-04T13:12:24Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fcd23dbd587b48ff8541bdca16f07e5d394b6f7b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fcd23dbd587b48ff8541bdca16f07e5d394b6f7b", "committedDate": "2020-02-04T16:31:45Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28e3ae0ae7a76a245aa9ca60c8176e0be5c56769", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/28e3ae0ae7a76a245aa9ca60c8176e0be5c56769", "committedDate": "2020-02-04T17:11:29Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c44644d759d450c546343f9e1bdd6bbf144c6081", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/c44644d759d450c546343f9e1bdd6bbf144c6081", "committedDate": "2020-02-04T17:31:12Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04ee734466e7bfe5a538c6d013847dae7b9d04d8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/04ee734466e7bfe5a538c6d013847dae7b9d04d8", "committedDate": "2020-02-04T18:43:09Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3e71c06d31d6cbab9a120e6819f4200a42496a6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d3e71c06d31d6cbab9a120e6819f4200a42496a6", "committedDate": "2020-02-05T07:58:15Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ced3f0c3ab42c177e2f29bfa641e8dc3208d1b8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/3ced3f0c3ab42c177e2f29bfa641e8dc3208d1b8", "committedDate": "2020-02-05T08:44:37Z", "message": "better"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a6270b85f8bc420e0337be40bb5fc046489d7f6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6a6270b85f8bc420e0337be40bb5fc046489d7f6", "committedDate": "2020-02-05T09:08:58Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4ab9fbf2e546f25e5042577f50fee92ca89a7e0", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d4ab9fbf2e546f25e5042577f50fee92ca89a7e0", "committedDate": "2020-02-05T09:09:36Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "625ab710f1d0bec9353f3bdace7d310b3a324c07", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/625ab710f1d0bec9353f3bdace7d310b3a324c07", "committedDate": "2020-02-05T11:55:34Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82ec5f048e2afae5a984057aa1f9dc443de78472", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/82ec5f048e2afae5a984057aa1f9dc443de78472", "committedDate": "2020-02-05T12:16:54Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4358f1d31dfb0b4030a3b2f79a50b2054865789", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d4358f1d31dfb0b4030a3b2f79a50b2054865789", "committedDate": "2020-02-05T13:42:32Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d607c94d060dad1ef1ad01729d7b91cbd920e7ee", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d607c94d060dad1ef1ad01729d7b91cbd920e7ee", "committedDate": "2020-02-06T03:47:08Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54da54a95b4c576d0fb4cfe19c78c60e37991136", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/54da54a95b4c576d0fb4cfe19c78c60e37991136", "committedDate": "2020-02-06T08:25:23Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fec8d131bc00744d6e12625f542f60a0deabb68f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/fec8d131bc00744d6e12625f542f60a0deabb68f", "committedDate": "2020-02-06T12:36:28Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28a8a515327d86681d92e57c5d37330cc0a73620", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/28a8a515327d86681d92e57c5d37330cc0a73620", "committedDate": "2020-02-08T13:19:51Z", "message": "Merge remote-tracking branch 'elastic/master' into source-only-snapshots-fix-2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2a87a71a078b3ba8596af1cc7debd38899fd7541", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2a87a71a078b3ba8596af1cc7debd38899fd7541", "committedDate": "2020-02-08T13:30:04Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e54d34ddb78071f6d775e84b195e37987a6888eb", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e54d34ddb78071f6d775e84b195e37987a6888eb", "committedDate": "2020-02-08T15:55:57Z", "message": "sorta"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "668890347b002854cb24cf2877ef1e44065fc93d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/668890347b002854cb24cf2877ef1e44065fc93d", "committedDate": "2020-02-10T18:53:17Z", "message": "sorta"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3040e081420cabdf04b63d3cc2ef2963a6cb2714", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/3040e081420cabdf04b63d3cc2ef2963a6cb2714", "committedDate": "2020-02-10T20:23:43Z", "message": "reverts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97ccb91e26508eebae2cab9db9ddfb1933cedb3f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/97ccb91e26508eebae2cab9db9ddfb1933cedb3f", "committedDate": "2020-02-10T20:58:40Z", "message": "shorter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0defec84aa334866d6f387e990f21af669775266", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/0defec84aa334866d6f387e990f21af669775266", "committedDate": "2020-02-10T21:24:16Z", "message": "works so far :)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "392945222f015b72932a818aa171f421111ea30f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/392945222f015b72932a818aa171f421111ea30f", "committedDate": "2020-02-10T21:24:57Z", "message": "nicer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15d186a8753fb6a670d759a5db4e2fe51ab505e3", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/15d186a8753fb6a670d759a5db4e2fe51ab505e3", "committedDate": "2020-02-11T00:50:57Z", "message": "makes sense"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e110f836e2414406b777dcb0dff20ba6f41ec630", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e110f836e2414406b777dcb0dff20ba6f41ec630", "committedDate": "2020-02-11T03:22:47Z", "message": "nicer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0130e097aa281264ff567bbaf17779cd3ab57da", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a0130e097aa281264ff567bbaf17779cd3ab57da", "committedDate": "2020-02-11T03:23:02Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72f83923d7a2ab35d62d7a2d514474afd1e6e95f", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/72f83923d7a2ab35d62d7a2d514474afd1e6e95f", "committedDate": "2020-02-11T03:36:44Z", "message": "nicer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a57ca4318713f5ff5cdc7f07bb1f79d9fd2acd2b", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/a57ca4318713f5ff5cdc7f07bb1f79d9fd2acd2b", "committedDate": "2020-02-11T03:44:45Z", "message": "nicer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82d5bd4296fce816eeeb29ae2f26d3574b44d959", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/82d5bd4296fce816eeeb29ae2f26d3574b44d959", "committedDate": "2020-02-11T04:02:57Z", "message": "doccs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/31c46f8261a5e1570398eaf6b6602ff86766f3a4", "committedDate": "2020-02-11T06:07:19Z", "message": "test and fix delete case"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU3MTY0NTM0", "url": "https://github.com/elastic/elasticsearch/pull/52182#pullrequestreview-357164534", "createdAt": "2020-02-12T03:12:40Z", "commit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzoxMjo0MFrOFoghmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMzozODozOVrOFog20A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMDI1MA==", "bodyText": "nit: sequenceNum -> maxSeqNo", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378020250", "createdAt": "2020-02-12T03:12:40Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1640,6 +1656,72 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    /**\n+     * Extracts an instance of {@link SegmentInfos} for each snapshot tracked in the given {@link BlobStoreIndexShardSnapshots} and\n+     * compares it against the given {@link IndexCommit}.\n+     * If the sequence number, local checkpoint, primary term and history UUID of the given commit and the found shard snapshot are equal\n+     * then we will not snapshot the files in the index commit but rather assign the files in the existing matching snapshot to the new\n+     * shard snapshot.\n+     * Compared to merely comparing files in the index commit and repository, the logic here will be able to identify shards with equal\n+     * content but different segment structure (as may be the case after replica promotion) and avoid redundant snapshot creation in this\n+     * situation.\n+     * Note: This method does not load any blobs from the repository. It instead exploits the fact that segment info files are stored\n+     * alongside their content as hash in the {@link BlobStoreIndexShardSnapshots} metadata to quickly load all {@link SegmentInfos} for\n+     * each shard snapshot.\n+     * Also see {@link StoreFileMetaData#hashEqualsContents()}.\n+     *\n+     * @param snapshotIndexCommit Index commit to snapshot\n+     * @param snapshots shard snapshots already in the repository\n+     * @return List of files already in the repository to use as the given shard's snapshot or {@code null} if no such set of files could\n+     *         be found in the current shard snapshots\n+     */\n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(\n+            IndexCommit snapshotIndexCommit, BlobStoreIndexShardSnapshots snapshots) {\n+        final Map<String, String> userCommitData;\n+        try {\n+            userCommitData = snapshotIndexCommit.getUserData();\n+        } catch (IOException e) {\n+            assert false : new AssertionError(\"Did not expect #getUserData to throw but saw exception\", e);\n+            return null;\n+        }\n+        final String sequenceNum = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 196}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMDQzNg==", "bodyText": "nit: maxSeqNo, localCheckpoint, and historyUUID are not null.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378020436", "createdAt": "2020-02-12T03:13:38Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1640,6 +1656,72 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    /**\n+     * Extracts an instance of {@link SegmentInfos} for each snapshot tracked in the given {@link BlobStoreIndexShardSnapshots} and\n+     * compares it against the given {@link IndexCommit}.\n+     * If the sequence number, local checkpoint, primary term and history UUID of the given commit and the found shard snapshot are equal\n+     * then we will not snapshot the files in the index commit but rather assign the files in the existing matching snapshot to the new\n+     * shard snapshot.\n+     * Compared to merely comparing files in the index commit and repository, the logic here will be able to identify shards with equal\n+     * content but different segment structure (as may be the case after replica promotion) and avoid redundant snapshot creation in this\n+     * situation.\n+     * Note: This method does not load any blobs from the repository. It instead exploits the fact that segment info files are stored\n+     * alongside their content as hash in the {@link BlobStoreIndexShardSnapshots} metadata to quickly load all {@link SegmentInfos} for\n+     * each shard snapshot.\n+     * Also see {@link StoreFileMetaData#hashEqualsContents()}.\n+     *\n+     * @param snapshotIndexCommit Index commit to snapshot\n+     * @param snapshots shard snapshots already in the repository\n+     * @return List of files already in the repository to use as the given shard's snapshot or {@code null} if no such set of files could\n+     *         be found in the current shard snapshots\n+     */\n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(\n+            IndexCommit snapshotIndexCommit, BlobStoreIndexShardSnapshots snapshots) {\n+        final Map<String, String> userCommitData;\n+        try {\n+            userCommitData = snapshotIndexCommit.getUserData();\n+        } catch (IOException e) {\n+            assert false : new AssertionError(\"Did not expect #getUserData to throw but saw exception\", e);\n+            return null;\n+        }\n+        final String sequenceNum = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);\n+        final String localCheckpoint = userCommitData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY);\n+        final String primaryTerm = userCommitData.get(Engine.MAX_PRIMARY_TERM);\n+        final String historyUUID = userCommitData.get(Engine.HISTORY_UUID_KEY);\n+        if (sequenceNum != null && localCheckpoint != null && primaryTerm != null && historyUUID != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTE0Mg==", "bodyText": "Should we add maxPrimaryTerm to SequenceNumbers#CommitInfo then use its equals here?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021142", "createdAt": "2020-02-12T03:17:01Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1640,6 +1656,72 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    /**\n+     * Extracts an instance of {@link SegmentInfos} for each snapshot tracked in the given {@link BlobStoreIndexShardSnapshots} and\n+     * compares it against the given {@link IndexCommit}.\n+     * If the sequence number, local checkpoint, primary term and history UUID of the given commit and the found shard snapshot are equal\n+     * then we will not snapshot the files in the index commit but rather assign the files in the existing matching snapshot to the new\n+     * shard snapshot.\n+     * Compared to merely comparing files in the index commit and repository, the logic here will be able to identify shards with equal\n+     * content but different segment structure (as may be the case after replica promotion) and avoid redundant snapshot creation in this\n+     * situation.\n+     * Note: This method does not load any blobs from the repository. It instead exploits the fact that segment info files are stored\n+     * alongside their content as hash in the {@link BlobStoreIndexShardSnapshots} metadata to quickly load all {@link SegmentInfos} for\n+     * each shard snapshot.\n+     * Also see {@link StoreFileMetaData#hashEqualsContents()}.\n+     *\n+     * @param snapshotIndexCommit Index commit to snapshot\n+     * @param snapshots shard snapshots already in the repository\n+     * @return List of files already in the repository to use as the given shard's snapshot or {@code null} if no such set of files could\n+     *         be found in the current shard snapshots\n+     */\n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(\n+            IndexCommit snapshotIndexCommit, BlobStoreIndexShardSnapshots snapshots) {\n+        final Map<String, String> userCommitData;\n+        try {\n+            userCommitData = snapshotIndexCommit.getUserData();\n+        } catch (IOException e) {\n+            assert false : new AssertionError(\"Did not expect #getUserData to throw but saw exception\", e);\n+            return null;\n+        }\n+        final String sequenceNum = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);\n+        final String localCheckpoint = userCommitData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY);\n+        final String primaryTerm = userCommitData.get(Engine.MAX_PRIMARY_TERM);\n+        final String historyUUID = userCommitData.get(Engine.HISTORY_UUID_KEY);\n+        if (sequenceNum != null && localCheckpoint != null && primaryTerm != null && historyUUID != null) {\n+            for (SnapshotFiles snapshotFileSet : snapshots.snapshots()) {\n+                final List<BlobStoreIndexShardSnapshot.FileInfo> files = snapshotFileSet.indexFiles();\n+                final SegmentInfos segmentInfos;\n+                try {\n+                    final Directory dir = new ByteBuffersDirectory();\n+                    for (BlobStoreIndexShardSnapshot.FileInfo f : files) {\n+                        final StoreFileMetaData m = f.metadata();\n+                        if (m.hashEqualsContents() == false) {\n+                            continue;\n+                        }\n+                        try (IndexOutput indexOutput = dir.createOutput(m.name(), IOContext.DEFAULT)) {\n+                            final BytesRef fileContent = m.hash();\n+                            indexOutput.writeBytes(fileContent.bytes, fileContent.offset, fileContent.length);\n+                        }\n+                    }\n+                    segmentInfos = SegmentInfos.readLatestCommit(dir);\n+                } catch (IOException e) {\n+                    logger.debug(\"Failed to read SegmentInfos from files {}\", files);\n+                    continue;\n+                }\n+                final Map<String, String> snapshotUserCommitData = segmentInfos.getUserData();\n+                if (sequenceNum.equals(snapshotUserCommitData.get(SequenceNumbers.MAX_SEQ_NO)) &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 222}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTI3NQ==", "bodyText": "Should we move this tag to SequenceNumbers.java?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021275", "createdAt": "2020-02-12T03:17:42Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/Engine.java", "diffHunk": "@@ -111,6 +111,7 @@\n \n     public static final String SYNC_COMMIT_ID = \"sync_id\"; // TODO: Remove sync_id in 9.0\n     public static final String HISTORY_UUID_KEY = \"history_uuid\";\n+    public static final String MAX_PRIMARY_TERM = \"max_primary_term\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTM2Ng==", "bodyText": "perhaps call this maxPrimaryTerm?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021366", "createdAt": "2020-02-12T03:18:08Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "diffHunk": "@@ -180,6 +180,8 @@\n     @Nullable\n     private final String historyUUID;\n \n+    private volatile long lastOpPrimaryTerm;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTU5OQ==", "bodyText": "I think we should advance the maxPrimaryTerm using the term of the operation instead of the current term of the replication group.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021599", "createdAt": "2020-02-12T03:19:10Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "diffHunk": "@@ -2366,6 +2375,7 @@ public long getPersistedLocalCheckpoint() {\n      */\n     protected final void markSeqNoAsSeen(long seqNo) {\n         localCheckpointTracker.advanceMaxSeqNo(seqNo);\n+        lastOpPrimaryTerm = engineConfig.getPrimaryTermSupplier().getAsLong();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMTg1MA==", "bodyText": "We don't need to do it here.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378021850", "createdAt": "2020-02-12T03:20:39Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/engine/InternalEngine.java", "diffHunk": "@@ -2602,6 +2612,7 @@ public void advanceMaxSeqNoOfUpdatesOrDeletes(long maxSeqNoOfUpdatesOnPrimary) {\n             throw new IllegalArgumentException(\"max_seq_no_of_updates on primary is unassigned\");\n         }\n         this.maxSeqNoOfUpdatesOrDeletes.updateAndGet(curr -> Math.max(curr, maxSeqNoOfUpdatesOnPrimary));\n+        lastOpPrimaryTerm = engineConfig.getPrimaryTermSupplier().getAsLong();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyMjE0Nw==", "bodyText": "nice javadocs :)", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378022147", "createdAt": "2020-02-12T03:22:13Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1640,6 +1656,72 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    /**\n+     * Extracts an instance of {@link SegmentInfos} for each snapshot tracked in the given {@link BlobStoreIndexShardSnapshots} and\n+     * compares it against the given {@link IndexCommit}.\n+     * If the sequence number, local checkpoint, primary term and history UUID of the given commit and the found shard snapshot are equal\n+     * then we will not snapshot the files in the index commit but rather assign the files in the existing matching snapshot to the new\n+     * shard snapshot.\n+     * Compared to merely comparing files in the index commit and repository, the logic here will be able to identify shards with equal", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyNTY4MA==", "bodyText": "Should we still upload new files after force merge?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r378025680", "createdAt": "2020-02-12T03:38:39Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1482,75 +1487,86 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n \n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n-            }\n+            // First inspect all known SegmentInfos instances to see if we already have an equivalent commit in the repository\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> filesFromSegmentInfos =\n+                findMatchingShardSnapshot(snapshotIndexCommit, snapshots);\n+\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles;\n             int indexIncrementalFileCount = 0;\n             int indexTotalNumberOfFiles = 0;\n             long indexIncrementalSize = 0;\n-            long indexTotalFileCount = 0;\n-            for (String fileName : fileNames) {\n-                if (snapshotStatus.isAborted()) {\n-                    logger.debug(\"[{}] [{}] Aborted on the file [{}], exiting\", shardId, snapshotId, fileName);\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Aborted\");\n+            long indexTotalFileSize = 0;\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n+            // If we did not find a set of files that is equal to the current commit we determine the files to upload by comparing files\n+            // in the commit with files already in the repository", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ffb2de5c193a29cb6ed12d5e4550284569e577c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/6ffb2de5c193a29cb6ed12d5e4550284569e577c", "committedDate": "2020-02-17T14:56:50Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "02733bf9675512348a6c60c439e5aaadb088bb10", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/02733bf9675512348a6c60c439e5aaadb088bb10", "committedDate": "2020-02-17T17:41:07Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c584d99cc1d3feefacd90113102e6063c0912842", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/c584d99cc1d3feefacd90113102e6063c0912842", "committedDate": "2020-02-18T16:20:12Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed0477cf36f74195d6e92dc79eeaa1014966b58e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/ed0477cf36f74195d6e92dc79eeaa1014966b58e", "committedDate": "2020-02-18T17:50:01Z", "message": "works"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5cd66be887e1ae755d0515850881b4a5485e3a2a", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/5cd66be887e1ae755d0515850881b4a5485e3a2a", "committedDate": "2020-02-18T17:50:11Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2aefef323928f1131efbeeddce4d92147d7dd9d2", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2aefef323928f1131efbeeddce4d92147d7dd9d2", "committedDate": "2020-02-18T21:26:08Z", "message": "cleanup useless changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10a14c47e36572c664135c7d43c85e8d5f64ce3e", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/10a14c47e36572c664135c7d43c85e8d5f64ce3e", "committedDate": "2020-02-18T21:26:33Z", "message": "cleanup useless changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "69eb658f139e0356d59d1b454bf73070877daada", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/69eb658f139e0356d59d1b454bf73070877daada", "committedDate": "2020-02-20T09:36:51Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d", "committedDate": "2020-02-20T10:11:00Z", "message": "determine whether commit is safe in snapshot shards service"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYxODM4NzA1", "url": "https://github.com/elastic/elasticsearch/pull/52182#pullrequestreview-361838705", "createdAt": "2020-02-20T11:48:36Z", "commit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxMTo0ODozNlrOFsQa3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxMTo1NDozNVrOFsQlyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MDY4Nw==", "bodyText": "Funny enough with the strange way the serialization worked here, we actually didn't need any BwC logic to keep it readable by older ES versions, that's why there's no version specific serialization logic here.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r381950687", "createdAt": "2020-02-20T11:48:36Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -250,15 +259,19 @@ public static BlobStoreIndexShardSnapshots fromXContent(XContentParser parser) t\n                         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n                             if (token == XContentParser.Token.FIELD_NAME) {\n                                 currentFieldName = parser.currentName();\n-                                if (parser.nextToken() == XContentParser.Token.START_ARRAY) {\n-                                    if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) == false) {\n-                                        throw new ElasticsearchParseException(\"unknown array [{}]\", currentFieldName);\n-                                    }\n+                                if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MTQ4Ng==", "bodyText": "This one is still open, @ywelsch do you have any new thoughts on this.\nPersonally, I'd say we should address force merges explicitly somehow by keeping track of what shards we force-merged in some form and then not doing the sequence num based snapshot if a shard was force merged.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r381951486", "createdAt": "2020-02-20T11:50:20Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1482,75 +1487,86 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n \n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n-            }\n+            // First inspect all known SegmentInfos instances to see if we already have an equivalent commit in the repository\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> filesFromSegmentInfos =\n+                findMatchingShardSnapshot(snapshotIndexCommit, snapshots);\n+\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles;\n             int indexIncrementalFileCount = 0;\n             int indexTotalNumberOfFiles = 0;\n             long indexIncrementalSize = 0;\n-            long indexTotalFileCount = 0;\n-            for (String fileName : fileNames) {\n-                if (snapshotStatus.isAborted()) {\n-                    logger.debug(\"[{}] [{}] Aborted on the file [{}], exiting\", shardId, snapshotId, fileName);\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Aborted\");\n+            long indexTotalFileSize = 0;\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n+            // If we did not find a set of files that is equal to the current commit we determine the files to upload by comparing files\n+            // in the commit with files already in the repository", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyNTY4MA=="}, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MzQ4Mg==", "bodyText": "I'm not sure 100% I like computing this here and passing it as yet another parameter to repository.snapshotShard.\nIn hindsight, the API here would be a lot easier/flexible to deal with had we not removed the IndexShard in #42213 ... maybe in a follow up we could introduce some object that exposes the shard related things like Store etc. to simplify this API.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r381953482", "createdAt": "2020-02-20T11:54:35Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "diffHunk": "@@ -340,8 +342,9 @@ private void snapshot(final ShardId shardId, final Snapshot snapshot, final Inde\n             try {\n                 // we flush first to make sure we get the latest writes snapshotted\n                 snapshotRef = indexShard.acquireLastIndexCommit(true);\n+                final IndexCommit indexCommit = snapshotRef.getIndexCommit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYyMDc4NzU5", "url": "https://github.com/elastic/elasticsearch/pull/52182#pullrequestreview-362078759", "createdAt": "2020-02-20T17:07:48Z", "commit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxNzowNzo0OFrOFsbxJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMFQxNzoxMzowN1rOFsb8NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzNjYxNQ==", "bodyText": "Can we pass the global checkpoint here instead of isSafeCommit? Also, can we always store the global checkpoint and the history UUID even when the commit is not safe? If so, I think we can move all the logic to a single place in \"findMatchingShardSnapshot\"?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382136615", "createdAt": "2020-02-20T17:07:48Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1457,8 +1459,8 @@ private void writeAtomic(final String blobName, final BytesReference bytesRef, b\n \n     @Override\n     public void snapshotShard(Store store, MapperService mapperService, SnapshotId snapshotId, IndexId indexId,\n-                              IndexCommit snapshotIndexCommit, IndexShardSnapshotStatus snapshotStatus, Version repositoryMetaVersion,\n-                              Map<String, Object> userMetadata, ActionListener<String> listener) {\n+                              IndexCommit snapshotIndexCommit, boolean isSafeCommit, IndexShardSnapshotStatus snapshotStatus,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzNzY1Ng==", "bodyText": "We should use the local checkpoint from the commit (not the current local checkpoint) and the persisted global checkpoint.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382137656", "createdAt": "2020-02-20T17:09:45Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/snapshots/SnapshotShardsService.java", "diffHunk": "@@ -352,6 +355,19 @@ private void snapshot(final ShardId shardId, final Snapshot snapshot, final Inde\n         }\n     }\n \n+    /**\n+     * Checks whether the index commit is from a sequence number that is equal to the shards global and local checkpoint.\n+     *\n+     * @param indexShard  shard\n+     * @param indexCommit index commit\n+     * @return true if max sequence number in the index commit is equal to the shard's global and local checkpoint\n+     */\n+    private static boolean isSafeIndexCommit(IndexShard indexShard, IndexCommit indexCommit) throws IOException {\n+        final long localCheckPoint = indexShard.getLocalCheckpoint();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjEzOTQ0NQ==", "bodyText": "I think we should avoid using sequence_num. We should use either global_checkpoint, or local_checkpoint, or max_seq_no.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382139445", "createdAt": "2020-02-20T17:13:07Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -135,6 +136,8 @@ public FileInfo findNameFile(String name) {\n \n     static final class ParseFields {\n         static final ParseField FILES = new ParseField(\"files\");\n+        static final ParseField SEQUENCE_NUM = new ParseField(\"sequence_num\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8dc590af22dc863b3fbc9433a265b8b8b101be3d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/8dc590af22dc863b3fbc9433a265b8b8b101be3d", "committedDate": "2020-02-21T07:38:26Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91048f4cef828db33c5bb4cb254d549857e3b3b1", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/91048f4cef828db33c5bb4cb254d549857e3b3b1", "committedDate": "2020-02-21T08:14:45Z", "message": "bck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d0b19acb155a464beaebc49512db814bb327d68d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d0b19acb155a464beaebc49512db814bb327d68d", "committedDate": "2020-02-21T09:13:59Z", "message": "pass down global checkpoint"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8284af2417f35b1dfc24d028e64eb73dd6fa8e35", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/8284af2417f35b1dfc24d028e64eb73dd6fa8e35", "committedDate": "2020-02-21T10:08:08Z", "message": "review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db5859777189ae4411028dad019fbb25212822c0", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/db5859777189ae4411028dad019fbb25212822c0", "committedDate": "2020-02-21T10:09:06Z", "message": "renaming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf4ff3aa8434d38a6653b39cc9c23c9450d84482", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/bf4ff3aa8434d38a6653b39cc9c23c9450d84482", "committedDate": "2020-02-21T10:26:59Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/624b1fbd846a85770be9cd1c68cb337a91d87db1", "committedDate": "2020-02-21T11:52:36Z", "message": "shorter"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYyNjIwNjY4", "url": "https://github.com/elastic/elasticsearch/pull/52182#pullrequestreview-362620668", "createdAt": "2020-02-21T13:16:59Z", "commit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzoxNjo1OVrOFs2jfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQxMzo1MjozOFrOFs3hjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3NTQ4Nw==", "bodyText": "should this be globalCheckpoints?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382575487", "createdAt": "2020-02-21T13:16:59Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -219,6 +226,8 @@ public static BlobStoreIndexShardSnapshots fromXContent(XContentParser parser) t\n             token = parser.nextToken();\n         }\n         Map<String, List<String>> snapshotsMap = new HashMap<>();\n+        Map<String, String> historyUUIDs = new HashMap<>();\n+        Map<String, Long> sequenceNumbers = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3NjQ1Mg==", "bodyText": "Should we make the parsing non-lenient now so that we are not silently dropping fields in the future?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382576452", "createdAt": "2020-02-21T13:19:13Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshots.java", "diffHunk": "@@ -250,15 +259,19 @@ public static BlobStoreIndexShardSnapshots fromXContent(XContentParser parser) t\n                         while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n                             if (token == XContentParser.Token.FIELD_NAME) {\n                                 currentFieldName = parser.currentName();\n-                                if (parser.nextToken() == XContentParser.Token.START_ARRAY) {\n-                                    if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) == false) {\n-                                        throw new ElasticsearchParseException(\"unknown array [{}]\", currentFieldName);\n-                                    }\n+                                if (ParseFields.FILES.match(currentFieldName, parser.getDeprecationHandler()) &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTk1MDY4Nw=="}, "originalCommit": {"oid": "2b7a19fa6ef8e80e6dbda1e52742f797e9a5629d"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3ODY3NQ==", "bodyText": "I'm not sure whether we should call this the global checkpoint. We're only supposed to record this if global checkpoint == max sequence number. I would rather introduce a new term here.\n(same for other occurrences of \"globalCheckpoint\")", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382578675", "createdAt": "2020-02-21T13:24:26Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/index/snapshots/blobstore/SnapshotFiles.java", "diffHunk": "@@ -48,9 +52,25 @@ public String snapshot() {\n      * @param snapshot   snapshot name\n      * @param indexFiles index files\n      */\n-    public SnapshotFiles(String snapshot, List<FileInfo> indexFiles ) {\n+    public SnapshotFiles(String snapshot, List<FileInfo> indexFiles, long globalCheckpoint, String historyUUID) {\n         this.snapshot = snapshot;\n         this.indexFiles = indexFiles;\n+        this.globalCheckpoint = globalCheckpoint;\n+        this.historyUUID = historyUUID;\n+    }\n+\n+    /**\n+     * Returns the shard's global checkpoint at the time the snapshot was taken\n+     */\n+    public long globalCheckpoint() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3ODk2NA==", "bodyText": "We typically name this maxSeqNo", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382578964", "createdAt": "2020-02-21T13:25:11Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1569,76 +1571,99 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                 throw new IndexShardSnapshotFailedException(shardId,\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n-\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n+            final Map<String, String> userCommitData = snapshotIndexCommit.getUserData();\n+            // We only check the sequence number to see if the shard has changed if we know that the commit is safe,\n+            // otherwise we short-circuit things here by not reading the sequence number from the commit\n+            final String maxSequenceNumString = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);\n+            final long maxSequenceNum;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU3OTgwMw==", "bodyText": "load this using SequenceNumbers.loadSeqNoInfoFromLuceneCommit instead", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382579803", "createdAt": "2020-02-21T13:27:05Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1569,76 +1571,99 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                 throw new IndexShardSnapshotFailedException(shardId,\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n-\n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n+            final Map<String, String> userCommitData = snapshotIndexCommit.getUserData();\n+            // We only check the sequence number to see if the shard has changed if we know that the commit is safe,\n+            // otherwise we short-circuit things here by not reading the sequence number from the commit\n+            final String maxSequenceNumString = userCommitData.get(SequenceNumbers.MAX_SEQ_NO);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU4MjA0Ng==", "bodyText": "I think this logic should live in the caller where we determine whether we want to do any kind of lookup by this special number (need a name for it) that is equal to global checkpoint, local checkpoint, and max sequence number.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382582046", "createdAt": "2020-02-21T13:32:21Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1729,6 +1754,21 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n         }\n     }\n \n+    @Nullable\n+    private static List<BlobStoreIndexShardSnapshot.FileInfo> findMatchingShardSnapshot(long globalCheckpoint, long maxSequenceNum,\n+                                                                                        String historyUUID,\n+                                                                                        BlobStoreIndexShardSnapshots snapshots) {\n+        if (maxSequenceNum == SequenceNumbers.UNASSIGNED_SEQ_NO || globalCheckpoint != maxSequenceNum) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624b1fbd846a85770be9cd1c68cb337a91d87db1"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MjU5MTM3NQ==", "bodyText": "yeah, I think we should do it that way, preferably as a predecessor PR to this. The only way I think we can record this is through the liveCommitData. We could for example set a fresh UUID whenever we force-merge.", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r382591375", "createdAt": "2020-02-21T13:52:38Z", "author": {"login": "ywelsch"}, "path": "server/src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java", "diffHunk": "@@ -1482,75 +1487,86 @@ public void snapshotShard(Store store, MapperService mapperService, SnapshotId s\n                     \"Duplicate snapshot name [\" + snapshotId.getName() + \"] detected, aborting\");\n             }\n \n-            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles = new ArrayList<>();\n-            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n-            store.incRef();\n-            final Collection<String> fileNames;\n-            final Store.MetadataSnapshot metadataFromStore;\n-            try {\n-                // TODO apparently we don't use the MetadataSnapshot#.recoveryDiff(...) here but we should\n-                try {\n-                    logger.trace(\n-                        \"[{}] [{}] Loading store metadata using index commit [{}]\", shardId, snapshotId, snapshotIndexCommit);\n-                    metadataFromStore = store.getMetadata(snapshotIndexCommit);\n-                    fileNames = snapshotIndexCommit.getFileNames();\n-                } catch (IOException e) {\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Failed to get store file metadata\", e);\n-                }\n-            } finally {\n-                store.decRef();\n-            }\n+            // First inspect all known SegmentInfos instances to see if we already have an equivalent commit in the repository\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> filesFromSegmentInfos =\n+                findMatchingShardSnapshot(snapshotIndexCommit, snapshots);\n+\n+            final List<BlobStoreIndexShardSnapshot.FileInfo> indexCommitPointFiles;\n             int indexIncrementalFileCount = 0;\n             int indexTotalNumberOfFiles = 0;\n             long indexIncrementalSize = 0;\n-            long indexTotalFileCount = 0;\n-            for (String fileName : fileNames) {\n-                if (snapshotStatus.isAborted()) {\n-                    logger.debug(\"[{}] [{}] Aborted on the file [{}], exiting\", shardId, snapshotId, fileName);\n-                    throw new IndexShardSnapshotFailedException(shardId, \"Aborted\");\n+            long indexTotalFileSize = 0;\n+            final BlockingQueue<BlobStoreIndexShardSnapshot.FileInfo> filesToSnapshot = new LinkedBlockingQueue<>();\n+            // If we did not find a set of files that is equal to the current commit we determine the files to upload by comparing files\n+            // in the commit with files already in the repository", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAyNTY4MA=="}, "originalCommit": {"oid": "31c46f8261a5e1570398eaf6b6602ff86766f3a4"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e00e0613ac2db55e8a660a83009808f398784e0", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/5e00e0613ac2db55e8a660a83009808f398784e0", "committedDate": "2020-02-21T15:55:39Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d90216a96f13e84426d734cae3f98d1d3047abb6", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/d90216a96f13e84426d734cae3f98d1d3047abb6", "committedDate": "2020-02-21T16:30:00Z", "message": "CR comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ef532866707ab5953812cc9a8da8ebf8212ae62", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/1ef532866707ab5953812cc9a8da8ebf8212ae62", "committedDate": "2020-03-11T15:11:24Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba70d5052dec0d19eef7e5a0d8606d381c1cac6d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/ba70d5052dec0d19eef7e5a0d8606d381c1cac6d", "committedDate": "2020-03-22T09:21:58Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c17c7c6e29dc44e197c0a6ddf46eb6ed48b088bf", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/c17c7c6e29dc44e197c0a6ddf46eb6ed48b088bf", "committedDate": "2020-03-22T11:46:50Z", "message": "CR: use single identifier and pass it to repos"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "280287667f1922c34e61277d1d313f55e7ae2822", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/280287667f1922c34e61277d1d313f55e7ae2822", "committedDate": "2020-03-22T11:53:03Z", "message": "remove pointless commit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c59524e4dea5ee8204c7ac3575ddceaaee07b27", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/4c59524e4dea5ee8204c7ac3575ddceaaee07b27", "committedDate": "2020-03-22T12:03:10Z", "message": "better docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0bfb717ea73b0093289004369ba962128c464179", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/0bfb717ea73b0093289004369ba962128c464179", "committedDate": "2020-03-22T13:46:25Z", "message": "safety"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8ca09d9d584f6f0adb7f1479b3ece7d59b480d0", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/f8ca09d9d584f6f0adb7f1479b3ece7d59b480d0", "committedDate": "2020-03-22T13:55:36Z", "message": "simpler"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b12eb801be2006a047f758e4b012d3ba0dab76a", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2b12eb801be2006a047f758e4b012d3ba0dab76a", "committedDate": "2020-03-22T13:58:18Z", "message": "better wording"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "983eb58aa5b2ba59c2dcaa4548587473e02b5f23", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/983eb58aa5b2ba59c2dcaa4548587473e02b5f23", "committedDate": "2020-03-22T13:59:15Z", "message": "better wording"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5MjkyMzAw", "url": "https://github.com/elastic/elasticsearch/pull/52182#pullrequestreview-379292300", "createdAt": "2020-03-23T10:17:43Z", "commit": {"oid": "983eb58aa5b2ba59c2dcaa4548587473e02b5f23"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxMDoxNzo0NFrOF5-0Jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QxMDoxNzo0NFrOF5-0Jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjM0MjMxMA==", "bodyText": "Can you also add a test that checks that force-merging is leading to another full snapshot?", "url": "https://github.com/elastic/elasticsearch/pull/52182#discussion_r396342310", "createdAt": "2020-03-23T10:17:44Z", "author": {"login": "ywelsch"}, "path": "server/src/test/java/org/elasticsearch/snapshots/BlobStoreIncrementalityIT.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.snapshots;\n+\n+import org.elasticsearch.action.DocWriteResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStats;\n+import org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusResponse;\n+import org.elasticsearch.action.bulk.BulkItemResponse;\n+import org.elasticsearch.action.bulk.BulkRequest;\n+import org.elasticsearch.action.bulk.BulkResponse;\n+import org.elasticsearch.action.index.IndexRequest;\n+import org.elasticsearch.action.search.SearchRequest;\n+import org.elasticsearch.cluster.metadata.IndexMetaData;\n+import org.elasticsearch.cluster.routing.UnassignedInfo;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.test.ESIntegTestCase;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.concurrent.ExecutionException;\n+\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class BlobStoreIncrementalityIT extends AbstractSnapshotIntegTestCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "983eb58aa5b2ba59c2dcaa4548587473e02b5f23"}, "originalPosition": 45}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3ddc56e812293371539171c5685ee989b0fdd6d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/e3ddc56e812293371539171c5685ee989b0fdd6d", "committedDate": "2020-03-23T10:22:53Z", "message": "Merge remote-tracking branch 'elastic/master' into better-incrementality-snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b508fb3f1a64f0e10f988eef78fc405368f51aa8", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/b508fb3f1a64f0e10f988eef78fc405368f51aa8", "committedDate": "2020-03-23T12:10:05Z", "message": "CR: add test for force merge scenario"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5MzkzNTE3", "url": "https://github.com/elastic/elasticsearch/pull/52182#pullrequestreview-379393517", "createdAt": "2020-03-23T12:45:40Z", "commit": {"oid": "b508fb3f1a64f0e10f988eef78fc405368f51aa8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2534, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}