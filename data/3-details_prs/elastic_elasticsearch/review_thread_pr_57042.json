{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxMzY5MjU1", "number": 57042, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNToxMToyNlrOEA7kUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMToxMDoyNlrOEB69mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5NDExNDEwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNToxMToyNlrOGcg6XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNToxMToyNlrOGcg6XQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU1MjU0MQ==", "bodyText": "Hm, I think this isn't quite right.  doc is the doc ID, so that could be all over the place, and also all going into the same bucket.  I think there are two options here:\n\n\nDown below, we do if (docCounts.increment(bucketOrd, 1) == 1) { <breaker stuff> } which I think will work because the increment method returns the count after incrementing.  So if we have a doc count of 1, it's the first doc and a new bucket so we can account it\n\n\nAlternatively, we could just account for it up in collectBucket without a conditional, since theoretically that should only be called on new buckets.  It's not guaranteed by the API but in practice that's how aggs use it.\n\n\nThere are two other issues we need to address though:\n\nThe old breaker logic only checked every 1024 buckets, since checking the real-memory breaker has a certain amount of overhead.  So we should re-implement that somehow\nTrickier situation which I didn't think about when suggesting BucketsAggregator... if we add the 1024 threshold back, it's only a local count so aggs with 1023 buckets will never trigger the breaker even if the overall query has millions of buckets.\n\nPerhaps we continue to use the MultiBucketConsumer service thing, but move the breaker accounting to a different method?  That way it could maintain the global count and BucketsAggregator just calls a method on it or something?  Not sure, we can discuss more offline", "url": "https://github.com/elastic/elasticsearch/pull/57042#discussion_r432552541", "createdAt": "2020-05-29T15:11:26Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java", "diffHunk": "@@ -91,6 +87,9 @@ public final void collectBucket(LeafBucketCollector subCollector, int doc, long\n      * Same as {@link #collectBucket(LeafBucketCollector, int, long)}, but doesn't check if the docCounts needs to be re-sized.\n      */\n     public final void collectExistingBucket(LeafBucketCollector subCollector, int doc, long bucketOrd) throws IOException {\n+        if (doc == 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eed40609db22529c8412ca4a55b18fdf1d30018b"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDQ4NjkxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMTowNTozMFrOGeD8ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMTowNTozMFrOGeD8ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE3NTA4Mg==", "bodyText": "Let's add a comment here why we're using 0, just in case a lost soul stumbles over this and is confused :)", "url": "https://github.com/elastic/elasticsearch/pull/57042#discussion_r434175082", "createdAt": "2020-06-02T21:05:30Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java", "diffHunk": "@@ -91,7 +91,9 @@ public final void collectBucket(LeafBucketCollector subCollector, int doc, long\n      * Same as {@link #collectBucket(LeafBucketCollector, int, long)}, but doesn't check if the docCounts needs to be re-sized.\n      */\n     public final void collectExistingBucket(LeafBucketCollector subCollector, int doc, long bucketOrd) throws IOException {\n-        docCounts.increment(bucketOrd, 1);\n+        if (docCounts.increment(bucketOrd, 1) == 1) {\n+            multiBucketConsumer.accept(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "887092555b86b07df43f8c36d1c6b3f8af92ab17"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNDUwMDc0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/search/aggregations/MultiBucketConsumerService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMToxMDoyNlrOGeEFKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMToxMDoyNlrOGeEFKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE3NzMyMQ==", "bodyText": "\ud83d\udc4d for fixing this behavior", "url": "https://github.com/elastic/elasticsearch/pull/57042#discussion_r434177321", "createdAt": "2020-06-02T21:10:26Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/MultiBucketConsumerService.java", "diffHunk": "@@ -110,15 +111,17 @@ public MultiBucketConsumer(int limit, CircuitBreaker breaker) {\n \n         @Override\n         public void accept(int value) {\n-            count += value;\n-            if (count > limit) {\n-                throw new TooManyBucketsException(\"Trying to create too many buckets. Must be less than or equal to: [\" + limit\n-                    + \"] but was [\" + count + \"]. This limit can be set by changing the [\" +\n-                    MAX_BUCKET_SETTING.getKey() + \"] cluster level setting.\", limit);\n+            if (value != 0) {\n+                count += value;\n+                if (count > limit) {\n+                    throw new TooManyBucketsException(\"Trying to create too many buckets. Must be less than or equal to: [\" + limit\n+                        + \"] but was [\" + count + \"]. This limit can be set by changing the [\" +\n+                        MAX_BUCKET_SETTING.getKey() + \"] cluster level setting.\", limit);\n+                }\n             }\n-\n-            // check parent circuit breaker every 1024 buckets\n-            if (value > 0 && (count & 0x3FF) == 0) {\n+            // check parent circuit breaker every 1024 calls\n+            callCount++;\n+            if ((callCount & 0x3FF) == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "887092555b86b07df43f8c36d1c6b3f8af92ab17"}, "originalPosition": 43}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 279, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}