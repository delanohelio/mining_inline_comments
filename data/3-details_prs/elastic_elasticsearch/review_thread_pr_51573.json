{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY4MjQwNzU4", "number": 51573, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNjo1MTowNVrODbd8mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNjo0MToxM1rODbpEtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTI4Nzk1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/ml/inference_crud.yml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNjo1MTowNlrOFi-G5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNjo1MTowNlrOFi-G5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIxMzQ3OA==", "bodyText": "Should we also have   - length: { trained_model_configs: 2 } assertion here?", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372213478", "createdAt": "2020-01-29T06:51:06Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/ml/inference_crud.yml", "diffHunk": "@@ -119,7 +172,7 @@ setup:\n         model_id: \"*\"\n         from: 0\n         size: 2\n-  - match: { count: 4 }\n+  - match: { count: 6 }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTI4ODA4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/ml/inference_crud.yml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNjo1MToxMVrOFi-G-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxMjowNzo0OFrOFjGKyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIxMzQ5Ng==", "bodyText": "Should we also have   - length: { trained_model_configs: 6 } assertion here?", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372213496", "createdAt": "2020-01-29T06:51:11Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/ml/inference_crud.yml", "diffHunk": "@@ -102,10 +152,13 @@ setup:\n   - do:\n       ml.get_trained_models:\n         model_id: \"*\"\n-  - match: { count: 4 }\n+  - match: { count: 6 }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM0NTU0NQ==", "bodyText": "For sure, I will add the length assertions", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372345545", "createdAt": "2020-01-29T12:07:48Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/ml/inference_crud.yml", "diffHunk": "@@ -102,10 +152,13 @@ setup:\n   - do:\n       ml.get_trained_models:\n         model_id: \"*\"\n-  - match: { count: 4 }\n+  - match: { count: 6 }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIxMzQ5Ng=="}, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTMxODIxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProviderTests.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNzowODo1MFrOFi-YNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNzowODo1MFrOFi-YNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjIxNzkwOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    //NOTE: these test assume that the query pagination results are \"buffered\"\n          \n          \n            \n                    // NOTE: these tests assume that the query pagination results are \"buffered\"", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372217908", "createdAt": "2020-01-29T07:08:50Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/test/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProviderTests.java", "diffHunk": "@@ -86,6 +90,63 @@ public void testExpandIdsQuery() {\n         });\n     }\n \n+    public void testExpandIdsPagination() {\n+        //NOTE: these test assume that the query pagination results are \"buffered\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTQ4MzU3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwODoyMjozMlrOFi_4nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNTowMDoxMFrOFjLhAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0MjU5MQ==", "bodyText": "So, are pageParams guaranteed to be non-null?", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372242591", "createdAt": "2020-01-29T08:22:32Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI5NjY3MQ==", "bodyText": "Is the pageParams parameter really necessary? I'd argue that I would expect expandIds to return all the ids rather than a page as the id small compared to a full document.\nThe default value of PageParams.getSize() is 100. Without setting a page param object you may not realise that limitation is in place and that you are not getting all of your ids. Is there even a mechanism to say you need to get the second page now?\nCompare to JobConfigProvider.expandJobIds  which gives you AnomalyDetectorsIndex.CONFIG_INDEX_MAX_RESULTS_WINDOW = 10_000 ids.\nIt would also simplify this code to remove to the pageParams which is a very good reason to remove it.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372296671", "createdAt": "2020-01-29T10:17:40Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0MjU5MQ=="}, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM0NzA1NA==", "bodyText": "Is the pageParams parameter really necessary?\n\nYes, I think it is. The APIs making calls against this function require paging params (default is from: 0, size: 100). I think this is a sane way of handling larger numbers of items.\n\nCompare to JobConfigProvider.expandJobIds which gives you AnomalyDetectorsIndex.CONFIG_INDEX_MAX_RESULTS_WINDOW = 10_000 ids.\n\nI think that this is a bug and shows that we don't support good pagination at all with anomaly detection jobs.\n\nIs there even a mechanism to say you need to get the second page now?\n\nYes, the total count of all the ids found with id pattern is returned.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372347054", "createdAt": "2020-01-29T12:11:24Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0MjU5MQ=="}, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM1MTc2MQ==", "bodyText": "So, are pageParams guaranteed to be non-null?\n\nIt is not possible to set them as null via the API since you can only set from and size individually.\nIf it is null from a client, they just don't set those parameters (and thus get the default value).\nIf an internal caller is making these calls, they should not explicitly set pageParams to null and if they do, they will receive an error pretty quickly.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372351761", "createdAt": "2020-01-29T12:23:04Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0MjU5MQ=="}, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM5NDAxMg==", "bodyText": "My concern was that with default paging we open ourselves up to bugs such as elastic/kibana#38559.\nI assumed getTrainedModel would take paging parameters in which case paging in expandIds would be redundant but I see from the way the code is used that is not the case and GetTrainedModelsStatsAction explicitly depends on paging ids.\nAD jobs are not paged because when they lived in clusterstate all where returned by the GET API. It would have been a breaking change to require paging on that API when we move to the .ml-config index and we could not make a breaking change in that release.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372394012", "createdAt": "2020-01-29T13:54:11Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0MjU5MQ=="}, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQzMzE1NQ==", "bodyText": "Yes, the apis GET _ml/trained_models and GET _ml/trained_models/_stats accept paging parameters and use those when expanding the IDs.\nSince they are stored in indices, paging parameters are a must (even for expanding IDs) and might as well have it handled now instead of worrying about adding them later when we reach scaling issues.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372433155", "createdAt": "2020-01-29T15:00:10Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0MjU5MQ=="}, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTQ5NTcxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwODoyNzoyMlrOFi__4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwODoyNzoyMlrOFi__4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0NDQ1MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        }\n          \n          \n            \n                        } else {", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372244451", "createdAt": "2020-01-29T08:27:22Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,73 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTUwMTc5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwODoyOTozN1rOFjADfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxMjoxNzo0MlrOFjGajA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0NTM3Mw==", "bodyText": "Not sure if it makes sense to optimize this, but I could imagine having an iterator going through allFoundIds and calling .remove() on the iterator rather than removing the smallest item from the collection directly.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372245373", "createdAt": "2020-01-29T08:29:37Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,73 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM0OTU4MA==", "bodyText": "Ah, popping the iterator, for sure. I actually find it more readable to have a remote(..first()). These sets are usually \"small\" and right now the highest number that we will \"buffer\" is the total number of models stored as resources (which is 1).", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372349580", "createdAt": "2020-01-29T12:17:42Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,73 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0NTM3Mw=="}, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTUwODY2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwODozMjoxMFrOFjAHwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxMjoxMjo1N1rOFjGTLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0NjQ2Ng==", "bodyText": "I found this method somewhat hard to follow.\nShould the logic be similar to merge sort? I.e. we have two sorted collections and take size elements (in total) from whichever collection has the lowest id at the front. from would be per-collection (so we'd have two froms).\nLMK if you think it's possible to rewrite it this way.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372246466", "createdAt": "2020-01-29T08:32:10Z", "author": {"login": "przemekwitek"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,73 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjM0NzY5NQ==", "bodyText": "It could be rewritten like a merge sort, but the issue still resides \"From which side of the page do we remove items?\" And we would not know the ends of the pages until the set is fully merged. So, I am not sure how it would be helpful.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372347695", "createdAt": "2020-01-29T12:12:57Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,73 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjI0NjQ2Ng=="}, "originalCommit": {"oid": "b8c0d2e9d19621c48daadbba1f5f8015b686dca8"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMjc4OTM5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNToyMTo1NVrOFjMY5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNToyMTo1NVrOFjMY5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ0NzQ2MQ==", "bodyText": "Unrelated nit: Can MODELS_STORED_AS_RESOURCE be a Collections.unmodifiableSet then line 520\n return new HashSet<>(MODELS_STORED_AS_RESOURCE);\nin matchedResourceIds wouldn't have to wrap a set in a set", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372447461", "createdAt": "2020-01-29T15:21:55Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,\n                           Set<String> tags,\n                           ActionListener<Tuple<Long, Set<String>>> idsListener) {\n         String[] tokens = Strings.tokenizeToStringArray(idExpression, \",\");\n+        Set<String> foundResourceIds = new HashSet<>();\n+        if (tags.isEmpty()) {\n+            foundResourceIds.addAll(matchedResourceIds(tokens));\n+        } else {\n+            for(String resourceId : matchedResourceIds(tokens)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMjgzNTc4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNTozMjo1MlrOFjM1Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxNDowNTowMFrOFjsCLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ1NDc1OA==", "bodyText": "Could this error in a valid situation?\nfrom = 0, size = 10, idExpression = 'bar*,foo'\nI have the models bar-1, bar-2 ... bar-10  and foo.\nThe first 10 bar-n are returned by the search but foo is unmatched.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372454758", "createdAt": "2020-01-29T15:32:52Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU4MjA3Mw==", "bodyText": "@davidkyle i think this was a problem well before I added this logic. Data frame analytics suffers from this as well.\nI think it is OK to error in this case.\nI may be wrong though (about it being OK) :D.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372582073", "createdAt": "2020-01-29T19:22:07Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ1NDc1OA=="}, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjk2NTkzNA==", "bodyText": "Certainly it's not part of this change but let's follow up on it. I don't think we should be reporting en error when we don't know if it is an error or not, maybe we can relax the check", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372965934", "createdAt": "2020-01-30T14:05:00Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ1NDc1OA=="}, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMjg0MTkxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNTozNDoyMVrOFjM5EQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxODo1MjoxMVrOFjTrhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ1NTY5Nw==", "bodyText": "So the strategy is to get extra then figure out where the resource model ids would fit in that sorted list \ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372455697", "createdAt": "2020-01-29T15:34:21Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,\n                           Set<String> tags,\n                           ActionListener<Tuple<Long, Set<String>>> idsListener) {\n         String[] tokens = Strings.tokenizeToStringArray(idExpression, \",\");\n+        Set<String> foundResourceIds = new HashSet<>();\n+        if (tags.isEmpty()) {\n+            foundResourceIds.addAll(matchedResourceIds(tokens));\n+        } else {\n+            for(String resourceId : matchedResourceIds(tokens)) {\n+                // Does the model as a resource have all the tags?\n+                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n+                    foundResourceIds.add(resourceId);\n+                }\n+            }\n+        }\n         SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()\n             .sort(SortBuilders.fieldSort(TrainedModelConfig.MODEL_ID.getPreferredName())\n                 // If there are no resources, there might be no mapping for the id field.\n                 // This makes sure we don't get an error if that happens.\n                 .unmappedType(\"long\"))\n-            .query(buildExpandIdsQuery(tokens, tags));\n-        if (pageParams != null) {\n-            sourceBuilder.from(pageParams.getFrom()).size(pageParams.getSize());\n-        }\n+            .query(buildExpandIdsQuery(tokens, tags))\n+            // We \"buffer\" the from and size to take into account models stored as resources.\n+            // This is so we handle the edge cases when the model that is stored as a resource is at the start/end of\n+            // a page.\n+            .from(Math.max(0, pageParams.getFrom() - foundResourceIds.size()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU2NjkxOA==", "bodyText": "@davidkyle exactly. If we don't have the buffer, AND the resource model ID is at the start of the list, we don't know if it is ACTUALLY at the start of the page, or the END of the previous page.\nHence the need for a buffer.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372566918", "createdAt": "2020-01-29T18:52:11Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -381,19 +380,32 @@ public void deleteTrainedModel(String modelId, ActionListener<Boolean> listener)\n \n     public void expandIds(String idExpression,\n                           boolean allowNoResources,\n-                          @Nullable PageParams pageParams,\n+                          PageParams pageParams,\n                           Set<String> tags,\n                           ActionListener<Tuple<Long, Set<String>>> idsListener) {\n         String[] tokens = Strings.tokenizeToStringArray(idExpression, \",\");\n+        Set<String> foundResourceIds = new HashSet<>();\n+        if (tags.isEmpty()) {\n+            foundResourceIds.addAll(matchedResourceIds(tokens));\n+        } else {\n+            for(String resourceId : matchedResourceIds(tokens)) {\n+                // Does the model as a resource have all the tags?\n+                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n+                    foundResourceIds.add(resourceId);\n+                }\n+            }\n+        }\n         SearchSourceBuilder sourceBuilder = new SearchSourceBuilder()\n             .sort(SortBuilders.fieldSort(TrainedModelConfig.MODEL_ID.getPreferredName())\n                 // If there are no resources, there might be no mapping for the id field.\n                 // This makes sure we don't get an error if that happens.\n                 .unmappedType(\"long\"))\n-            .query(buildExpandIdsQuery(tokens, tags));\n-        if (pageParams != null) {\n-            sourceBuilder.from(pageParams.getFrom()).size(pageParams.getSize());\n-        }\n+            .query(buildExpandIdsQuery(tokens, tags))\n+            // We \"buffer\" the from and size to take into account models stored as resources.\n+            // This is so we handle the edge cases when the model that is stored as a resource is at the start/end of\n+            // a page.\n+            .from(Math.max(0, pageParams.getFrom() - foundResourceIds.size()))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ1NTY5Nw=="}, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMjg5MzMzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNTo0Njo0MVrOFjNYNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxOToyMDozM1rOFjUjmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ2MzY2OA==", "bodyText": "If the number of search hits and and resource model ids are < pageParams.getSize() then that is the size limit. from isn't a factor here", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372463668", "createdAt": "2020-01-29T15:46:41Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU4MTI3Mg==", "bodyText": "I think from is a factor here. totalMatchedIds is the total hits, which means the total number of ids that matched the query which could be WAY bigger than the size.\nIf there are a total of 20 IDs, and the user is doing from: 19, size:200, we should give them the rest of the IDs. Our size limit in this case shouldn't be 200, it should be 1", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372581272", "createdAt": "2020-01-29T19:20:33Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ2MzY2OA=="}, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMjk1NTUzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNjowMTozOVrOFjN-nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNjowMTozOVrOFjN-nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ3MzUwMQ==", "bodyText": "Maybe shortcut:\nif (foundFromResources.size() == 0) { return foundFromDocs }\nfoundFromDocs will be the right size as you won't have changed the from and size parameters to the query by adding 0 to them. (lines 407 & 408)", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372473501", "createdAt": "2020-01-29T16:01:39Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMzExMDk0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQxNjo0MToxM1rOFjPfKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMFQxNjozNzowOVrOFjxw8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ5ODIxNg==", "bodyText": "My way of looking at it is this: we have a request for 10 ids and 2 models stored as resource. We query for 14 ids so we can figure out where the resource model ids would fit. We have a result like\nAAOOOOOOOOOOBB\nwhere the As and Bs are the padding.\nIf the model resource Ids come before the As then they are outside the ordering and all the Os are returned. Same if the model resource ids come after the Bs.\nIf they are inserted middle (ids M & N) we have an ordering like\nAAOOOOOMOOONOOBB\nwe we take the first 10 after the As.\nSame for\nAAMOOOOOONOOOOBB\nMNAAOOOOOOOOOOBB\nAMAOOOOOONOOOOBB\nAAOOOOOOOOOOBBMN\nI think we have to track the insertion position of the model resource Ids. Collections.binarySearch() would give the insert position.\nThere is a further complication when the search does not return size hits and you have\nAAOOOOOOOOOO\nor\nAAOOOO", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372498216", "createdAt": "2020-01-29T16:41:13Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            } else {\n+                // If we have removed all items belonging on the previous page, but are still over sized, this means we should\n+                // remove items that belong on the next page.\n+                allFoundIds.remove(allFoundIds.last());\n+            }\n+        }\n+        return allFoundIds;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU2OTQ0MQ==", "bodyText": "There are no BB at the end. moving back from by 2 and increasing size by 2 simply adds two new items to the start because it moves the page view back but it will still include the end of the initially desired page.\nIf that makes sense.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372569441", "createdAt": "2020-01-29T18:57:03Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            } else {\n+                // If we have removed all items belonging on the previous page, but are still over sized, this means we should\n+                // remove items that belong on the next page.\n+                allFoundIds.remove(allFoundIds.last());\n+            }\n+        }\n+        return allFoundIds;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ5ODIxNg=="}, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjU3OTkwMA==", "bodyText": "Let me take your example and run with it. Assuming size is 10.\n10 ids and 2 models\nSince we decrease from by two, we need to increase the size by two to fill out the page. If we didn't we would end up with\nAAOOOOOOOO (2 As, 8 Os)\nBut, we increase the size to capture those two Os and our search from the docs returns\nAAOOOOOOOOOO\nIf the resource models are inserted middle (ids M & N) we have an ordering like:\nAAOOOOOMOOONOO\nsince our size is 14 in this case, we trim the first two As and then the last two O\nReturning to the user\nOOOOOMOOON\nHere are the results for the rest of your example insertion points\n\nMOOOOOONOO\nAAOOOOOOOO\nAOOOOOONOO\nOOOOOOOOOO\n\nI think 1 and 4 are obvious OK situations.\n2 and 3 may seem weird off hand, but they are OK as well, since M or N would have been at the END of the previous page and \"pushed\" these two buffered IDS onto this page.\n\nI think we have to track the insertion position of the model resource Ids.\n\nI don't think this is possible since we don't know all the page boundries (without the buffers) and knowing the insertion point does not add anything.", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372579900", "createdAt": "2020-01-29T19:17:41Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            } else {\n+                // If we have removed all items belonging on the previous page, but are still over sized, this means we should\n+                // remove items that belong on the next page.\n+                allFoundIds.remove(allFoundIds.last());\n+            }\n+        }\n+        return allFoundIds;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ5ODIxNg=="}, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjk3OTIwOQ==", "bodyText": "Thanks for the explanation. From those examples it seems once you have taken the 2 sorted sets and merged them together there are 2 conditions to consider.\n\nfrom == 0 in which case we remove from the back of the set until allFoundIds.size <= pageparams.size\n\nIn this case there are no As to remove so we trim the set to size from the back\n\nfrom > 0,  remove the number of resource models from the front of the set - this is the As or another id that displaced the As. Then trim the set to size from the back.\n\nWith those observations the code could be simplified to\n    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs) {\n        // If there are no matching resource models, there was no buffering and the models from the docs\n        // are paginated correctly.\n        if (foundFromResources.isEmpty()) {\n            return foundFromDocs;\n        }\n\n        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n        allFoundIds.addAll(foundFromResources);\n\n        if (pageParams.getFrom() > 0) {\n            // not the first page so there will be extra results at the front to remove\n            int numToTrimFromFront = Math.min(foundFromResources.size(), pageParams.getFrom());\n            for (int i = 0; i < numToTrimFromFront; i++) {\n                allFoundIds.remove(allFoundIds.first());\n            }\n        }\n\n        // trim down to size removing from the rear\n        while (allFoundIds.size() > pageParams.getSize()) {\n            allFoundIds.remove(allFoundIds.last());\n        }\n\n        return allFoundIds;\n    }", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r372979209", "createdAt": "2020-01-30T14:27:32Z", "author": {"login": "davidkyle"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            } else {\n+                // If we have removed all items belonging on the previous page, but are still over sized, this means we should\n+                // remove items that belong on the next page.\n+                allFoundIds.remove(allFoundIds.last());\n+            }\n+        }\n+        return allFoundIds;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ5ODIxNg=="}, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzA1OTgyNA==", "bodyText": "@davidkyle wrote some more test cases, and this solution works great. I modified my code :D", "url": "https://github.com/elastic/elasticsearch/pull/51573#discussion_r373059824", "createdAt": "2020-01-30T16:37:09Z", "author": {"login": "benwtrent"}, "path": "x-pack/plugin/ml/src/main/java/org/elasticsearch/xpack/ml/inference/persistence/TrainedModelProvider.java", "diffHunk": "@@ -406,47 +418,72 @@ public void expandIds(String idExpression,\n                 indicesOptions.expandWildcardsClosed(),\n                 indicesOptions))\n             .source(sourceBuilder);\n-        Set<String> foundResourceIds = new LinkedHashSet<>();\n-        if (tags.isEmpty()) {\n-            foundResourceIds.addAll(matchedResourceIds(tokens));\n-        } else {\n-            for(String resourceId : matchedResourceIds(tokens)) {\n-                // Does the model as a resource have all the tags?\n-                if (Sets.newHashSet(loadModelFromResource(resourceId, true).getTags()).containsAll(tags)) {\n-                    foundResourceIds.add(resourceId);\n-                }\n-            }\n-        }\n \n         executeAsyncWithOrigin(client.threadPool().getThreadContext(),\n             ML_ORIGIN,\n             searchRequest,\n             ActionListener.<SearchResponse>wrap(\n                 response -> {\n                     long totalHitCount = response.getHits().getTotalHits().value + foundResourceIds.size();\n+                    Set<String> foundFromDocs = new HashSet<>();\n                     for (SearchHit hit : response.getHits().getHits()) {\n                         Map<String, Object> docSource = hit.getSourceAsMap();\n                         if (docSource == null) {\n                             continue;\n                         }\n                         Object idValue = docSource.get(TrainedModelConfig.MODEL_ID.getPreferredName());\n                         if (idValue instanceof String) {\n-                            foundResourceIds.add(idValue.toString());\n+                            foundFromDocs.add(idValue.toString());\n                         }\n                     }\n+                    Set<String> allFoundIds = collectIds(pageParams, foundResourceIds, foundFromDocs, totalHitCount);\n                     ExpandedIdsMatcher requiredMatches = new ExpandedIdsMatcher(tokens, allowNoResources);\n-                    requiredMatches.filterMatchedIds(foundResourceIds);\n+                    requiredMatches.filterMatchedIds(allFoundIds);\n                     if (requiredMatches.hasUnmatchedIds()) {\n                         idsListener.onFailure(ExceptionsHelper.missingTrainedModel(requiredMatches.unmatchedIdsString()));\n                     } else {\n-                        idsListener.onResponse(Tuple.tuple(totalHitCount, foundResourceIds));\n+\n+                        idsListener.onResponse(Tuple.tuple(totalHitCount, allFoundIds));\n                     }\n                 },\n                 idsListener::onFailure\n             ),\n             client::search);\n     }\n \n+    static Set<String> collectIds(PageParams pageParams, Set<String> foundFromResources, Set<String> foundFromDocs, long totalMatchedIds) {\n+        TreeSet<String> allFoundIds = new TreeSet<>(foundFromDocs);\n+        allFoundIds.addAll(foundFromResources);\n+        int from = pageParams.getFrom();\n+        int bufferedFrom = Math.min(foundFromResources.size(), from);\n+\n+        // If size = 10_000 but there aren't that many total IDs, reduce the size here to make following logic simpler\n+        int sizeLimit = (int)Math.min(pageParams.getSize(), totalMatchedIds - from);\n+\n+        // Last page this means that if we \"buffered\" the from pagination due to resources we should clear that out\n+        // We only clear from the front as that would include buffered IDs that fall on the previous page\n+        if (from + sizeLimit >= totalMatchedIds) {\n+            while (bufferedFrom > 0 || allFoundIds.size() > sizeLimit) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            }\n+        }\n+\n+        // Systematically remove items while we are above the limit\n+        while (allFoundIds.size() > sizeLimit) {\n+            // If we are still over limit, and have buffered items, that means the first ids belong on the previous page\n+            if (bufferedFrom > 0) {\n+                allFoundIds.remove(allFoundIds.first());\n+                bufferedFrom--;\n+            } else {\n+                // If we have removed all items belonging on the previous page, but are still over sized, this means we should\n+                // remove items that belong on the next page.\n+                allFoundIds.remove(allFoundIds.last());\n+            }\n+        }\n+        return allFoundIds;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjQ5ODIxNg=="}, "originalCommit": {"oid": "e7e8249ac9c6787ed8625501519fd627d5f0a619"}, "originalPosition": 139}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 222, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}