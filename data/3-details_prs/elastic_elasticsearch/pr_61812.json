{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc3MjUwNDYx", "number": 61812, "title": "Faster Azure Blob InputStream", "bodyText": "Building our own that should perform better than the one in the SDK.\nAlso, as a result saving a HEAD call for each ranged read on Azure.", "createdAt": "2020-09-01T18:15:13Z", "url": "https://github.com/elastic/elasticsearch/pull/61812", "merged": true, "mergeCommit": {"oid": "953536b457ac0295d4b1e7fd46a29e03bb1cf407"}, "closed": true, "closedAt": "2020-09-15T14:27:11Z", "author": {"login": "original-brownbear"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdE3ViRAH2gAyNDc3MjUwNDYxOjdlMjBhZWNlNGFjMmVlOGE1MTNkMWU5NDFkOTE0OWZhZTlhOTliYWY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdF0pPdAH2gAyNDc3MjUwNDYxOjBjOTFkODcwOTdlNmIyNTgwNDIzM2M3Y2RjMjFmOGQzZjlmMzg3NGM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "7e20aece4ac2ee8a513d1e941d9149fae9a99baf", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/7e20aece4ac2ee8a513d1e941d9149fae9a99baf", "committedDate": "2020-09-02T08:01:14Z", "message": "Faster Azure Blob InputStream\n\nBuilding our own that should perform better than the one in the SDK."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "552374293d8ab8bfa7d5ad8aca9ce7732aff3a53", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/552374293d8ab8bfa7d5ad8aca9ce7732aff3a53", "committedDate": "2020-09-02T08:01:14Z", "message": "little smarter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38f516b6c3968eb1510a9835efdf03d44a5dd49d", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/38f516b6c3968eb1510a9835efdf03d44a5dd49d", "committedDate": "2020-09-02T08:01:14Z", "message": "16M"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c297734ad7b669ce72006f23f82ad699511bb21", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/9c297734ad7b669ce72006f23f82ad699511bb21", "committedDate": "2020-09-02T08:01:14Z", "message": "way faster"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cddef3dae9e54d445cd49bc0b710ef3282ab2da7", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/cddef3dae9e54d445cd49bc0b710ef3282ab2da7", "committedDate": "2020-09-02T08:01:14Z", "message": "fixed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "072a1cc42de97d850979d6550af09609271ee302", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/072a1cc42de97d850979d6550af09609271ee302", "committedDate": "2020-09-02T08:25:19Z", "message": "comment"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ea5d9afa6eda41e5f7065205313b14757051b52c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/ea5d9afa6eda41e5f7065205313b14757051b52c", "committedDate": "2020-09-01T19:06:35Z", "message": "fixed"}, "afterCommit": {"oid": "072a1cc42de97d850979d6550af09609271ee302", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/072a1cc42de97d850979d6550af09609271ee302", "committedDate": "2020-09-02T08:25:19Z", "message": "comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/2ad6dc5e5a3be555f8c1ad6357e982ad21c79427", "committedDate": "2020-09-02T08:51:47Z", "message": "bck"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNjE2ODcy", "url": "https://github.com/elastic/elasticsearch/pull/61812#pullrequestreview-480616872", "createdAt": "2020-09-02T08:54:03Z", "commit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODo1NDowM1rOHLlPcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODo1NDowM1rOHLlPcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwNjU0NA==", "bodyText": "The SDK wraps buffers in https://github.com/Azure/azure-storage-java/blob/legacy-master/microsoft-azure-storage/src/com/microsoft/azure/storage/core/WrappedByteArrayOutputStream.java#L24 which will do singly byte copies for every byte! Me and @hub-cap found this to take macroscopic time (~10% of the runtime when reading from the stream) so this hack is well worth it IMO.", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481906544", "createdAt": "2020-09-02T08:54:03Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,4 +386,97 @@ public int read(byte[] b, int off, int len) throws IOException {\n                 \"PUT_BLOCK\", putBlockOperations.get());\n         }\n     }\n+\n+    /**\n+     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n+     * because that stream is highly inefficient in both memory and CPU use.\n+     */\n+    private static class BlobInputStream extends InputStream {\n+\n+        /**\n+         * Maximum number of bytes to fetch per read request and thus to buffer on heap at a time.\n+         * Set to 4M because that's what {@link com.microsoft.azure.storage.blob.BlobInputStream} uses.\n+         */\n+        private static final int MAX_READ_CHUNK_SIZE = ByteSizeUnit.MB.toIntBytes(4);\n+\n+        /**\n+         * Using a {@link ByteArrayOutputStream} as a buffer instead of a byte array since the byte array APIs on the SDK are less\n+         * efficient.\n+         */\n+        private final ByteArrayOutputStream buffer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNjE3NDk0", "url": "https://github.com/elastic/elasticsearch/pull/61812#pullrequestreview-480617494", "createdAt": "2020-09-02T08:54:50Z", "commit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODo1NDo1MFrOHLlSRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODo1NDo1MFrOHLlSRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwNzI2OA==", "bodyText": "Still horrific to allocate 4M on heap but unlike the SDKs stream we at least reuse these bytes instead of allocating them over and over for larger blobs.", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481907268", "createdAt": "2020-09-02T08:54:50Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,4 +386,97 @@ public int read(byte[] b, int off, int len) throws IOException {\n                 \"PUT_BLOCK\", putBlockOperations.get());\n         }\n     }\n+\n+    /**\n+     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n+     * because that stream is highly inefficient in both memory and CPU use.\n+     */\n+    private static class BlobInputStream extends InputStream {\n+\n+        /**\n+         * Maximum number of bytes to fetch per read request and thus to buffer on heap at a time.\n+         * Set to 4M because that's what {@link com.microsoft.azure.storage.blob.BlobInputStream} uses.\n+         */\n+        private static final int MAX_READ_CHUNK_SIZE = ByteSizeUnit.MB.toIntBytes(4);\n+\n+        /**\n+         * Using a {@link ByteArrayOutputStream} as a buffer instead of a byte array since the byte array APIs on the SDK are less\n+         * efficient.\n+         */\n+        private final ByteArrayOutputStream buffer;\n+\n+        private final long limit;\n+\n+        private final CloudBlockBlob blockBlobReference;\n+\n+        private final long start;\n+\n+        private final OperationContext context;\n+\n+        // current read position on the byte array backing #buffer\n+        private int pos;\n+\n+        // current position up to which the contents of the blob where buffered\n+        private long offset;\n+\n+        BlobInputStream(long limit, CloudBlockBlob blockBlobReference, long start, OperationContext context) {\n+            this.limit = limit;\n+            this.blockBlobReference = blockBlobReference;\n+            this.start = start;\n+            this.context = context;\n+            buffer = new ByteArrayOutputStream(Math.min(MAX_READ_CHUNK_SIZE, Math.toIntExact(Math.min(limit, Integer.MAX_VALUE)))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "originalPosition": 115}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNjE4NDQx", "url": "https://github.com/elastic/elasticsearch/pull/61812#pullrequestreview-480618441", "createdAt": "2020-09-02T08:55:53Z", "commit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODo1NTo1M1rOHLlV1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODo1NTo1M1rOHLlV1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwODE4MA==", "bodyText": "Duplicating the exception handling here because we want the original exception unwrapped in the initial fill() call for ranged reads so that we correctly bubble up 404s.", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481908180", "createdAt": "2020-09-02T08:55:53Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,4 +386,97 @@ public int read(byte[] b, int off, int len) throws IOException {\n                 \"PUT_BLOCK\", putBlockOperations.get());\n         }\n     }\n+\n+    /**\n+     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n+     * because that stream is highly inefficient in both memory and CPU use.\n+     */\n+    private static class BlobInputStream extends InputStream {\n+\n+        /**\n+         * Maximum number of bytes to fetch per read request and thus to buffer on heap at a time.\n+         * Set to 4M because that's what {@link com.microsoft.azure.storage.blob.BlobInputStream} uses.\n+         */\n+        private static final int MAX_READ_CHUNK_SIZE = ByteSizeUnit.MB.toIntBytes(4);\n+\n+        /**\n+         * Using a {@link ByteArrayOutputStream} as a buffer instead of a byte array since the byte array APIs on the SDK are less\n+         * efficient.\n+         */\n+        private final ByteArrayOutputStream buffer;\n+\n+        private final long limit;\n+\n+        private final CloudBlockBlob blockBlobReference;\n+\n+        private final long start;\n+\n+        private final OperationContext context;\n+\n+        // current read position on the byte array backing #buffer\n+        private int pos;\n+\n+        // current position up to which the contents of the blob where buffered\n+        private long offset;\n+\n+        BlobInputStream(long limit, CloudBlockBlob blockBlobReference, long start, OperationContext context) {\n+            this.limit = limit;\n+            this.blockBlobReference = blockBlobReference;\n+            this.start = start;\n+            this.context = context;\n+            buffer = new ByteArrayOutputStream(Math.min(MAX_READ_CHUNK_SIZE, Math.toIntExact(Math.min(limit, Integer.MAX_VALUE)))) {\n+                @Override\n+                public byte[] toByteArray() {\n+                    return buf;\n+                }\n+            };\n+            pos = 0;\n+            offset = 0;\n+        }\n+\n+        @Override\n+        public int read() throws IOException {\n+            try {\n+                fill();\n+            } catch (StorageException | URISyntaxException ex) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "originalPosition": 129}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNjE4NzI4", "url": "https://github.com/elastic/elasticsearch/pull/61812#pullrequestreview-480618728", "createdAt": "2020-09-02T08:56:11Z", "commit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODo1NjoxMlrOHLlXeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwODo1NjoxMlrOHLlXeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkwODYwMA==", "bodyText": "This should be a neat win for searchable snapshots IMO.", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481908600", "createdAt": "2020-09-02T08:56:12Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-azure/src/test/java/org/elasticsearch/repositories/azure/AzureBlobContainerRetriesTests.java", "diffHunk": "@@ -217,20 +217,13 @@ public void testReadBlobWithRetries() throws Exception {\n \n     public void testReadRangeBlobWithRetries() throws Exception {\n         final int maxRetries = randomIntBetween(1, 5);\n-        final CountDown countDownHead = new CountDown(maxRetries);\n         final CountDown countDownGet = new CountDown(maxRetries);\n         final byte[] bytes = randomBlobContent();\n         httpServer.createContext(\"/container/read_range_blob_max_retries\", exchange -> {\n             try {\n                 Streams.readFully(exchange.getRequestBody());\n                 if (\"HEAD\".equals(exchange.getRequestMethod())) {\n-                    if (countDownHead.countDown()) {\n-                        exchange.getResponseHeaders().add(\"Content-Type\", \"application/octet-stream\");\n-                        exchange.getResponseHeaders().add(\"x-ms-blob-content-length\", String.valueOf(bytes.length));\n-                        exchange.getResponseHeaders().add(\"x-ms-blob-type\", \"blockblob\");\n-                        exchange.sendResponseHeaders(RestStatus.OK.getStatus(), -1);\n-                        return;\n-                    }\n+                    throw new AssertionError(\"Should not HEAD blob for ranged reads\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNjMzMTM5", "url": "https://github.com/elastic/elasticsearch/pull/61812#pullrequestreview-480633139", "createdAt": "2020-09-02T09:14:29Z", "commit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwOToxNDoyOVrOHLmS4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQwOToxNDoyOVrOHLmS4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTkyMzgwOA==", "bodyText": "Technically speaking, we could avoid this whole dance and make the Azure download as fast as those in GCS and S3 by offering a different blob container API that consumes an OutputStream, that would do away with the chunked reading and make things fast (far as I can tell, haven't tried it yet). An annoying hack for sure, but probably less effort+risk than upgrading to SDK v12. For now this is the best I could come up with from the profiling @hub-cap did.", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r481923808", "createdAt": "2020-09-02T09:14:29Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -391,4 +386,97 @@ public int read(byte[] b, int off, int len) throws IOException {\n                 \"PUT_BLOCK\", putBlockOperations.get());\n         }\n     }\n+\n+    /**\n+     * Building our own input stream instead of using the SDK's {@link com.microsoft.azure.storage.blob.BlobInputStream}\n+     * because that stream is highly inefficient in both memory and CPU use.\n+     */\n+    private static class BlobInputStream extends InputStream {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwNzAyOTI0", "url": "https://github.com/elastic/elasticsearch/pull/61812#pullrequestreview-480702924", "createdAt": "2020-09-02T10:51:54Z", "commit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgyNjY2MjQ2", "url": "https://github.com/elastic/elasticsearch/pull/61812#pullrequestreview-482666246", "createdAt": "2020-09-04T13:47:38Z", "commit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMzo0NzozOFrOHNONwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMzo1Mzo1N1rOHNOcYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYyNjQzMw==", "bodyText": "space missing", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r483626433", "createdAt": "2020-09-04T13:47:38Z", "author": {"login": "ywelsch"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -245,9 +246,22 @@ public InputStream getInputStream(String blob, long position, @Nullable Long len\n         final OperationContext context = hookMetricCollector(client.v2().get(), getMetricsCollector);\n         final CloudBlockBlob blockBlobReference = client.v1().getContainerReference(container).getBlockBlobReference(blob);\n         logger.trace(() -> new ParameterizedMessage(\"reading container [{}], blob [{}]\", container, blob));\n-        final BlobInputStream is = SocketAccess.doPrivilegedException(() ->\n-            blockBlobReference.openInputStream(position, length, null, null, context));\n-        return giveSocketPermissionsToStream(is);\n+        final long limit;\n+        if (length == null){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYzMDE3Nw==", "bodyText": "This still executes a HEAD request?\nHow is length used afterwards?\nCould we always specify Long.MAX_VALUE here to avoid this call?", "url": "https://github.com/elastic/elasticsearch/pull/61812#discussion_r483630177", "createdAt": "2020-09-04T13:53:57Z", "author": {"login": "ywelsch"}, "path": "plugins/repository-azure/src/main/java/org/elasticsearch/repositories/azure/AzureBlobStore.java", "diffHunk": "@@ -245,9 +246,22 @@ public InputStream getInputStream(String blob, long position, @Nullable Long len\n         final OperationContext context = hookMetricCollector(client.v2().get(), getMetricsCollector);\n         final CloudBlockBlob blockBlobReference = client.v1().getContainerReference(container).getBlockBlobReference(blob);\n         logger.trace(() -> new ParameterizedMessage(\"reading container [{}], blob [{}]\", container, blob));\n-        final BlobInputStream is = SocketAccess.doPrivilegedException(() ->\n-            blockBlobReference.openInputStream(position, length, null, null, context));\n-        return giveSocketPermissionsToStream(is);\n+        final long limit;\n+        if (length == null){\n+            // Loading the blob attributes so we can get its length\n+            SocketAccess.doPrivilegedVoidException(() -> blockBlobReference.downloadAttributes(null, null, context));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2ad6dc5e5a3be555f8c1ad6357e982ad21c79427"}, "originalPosition": 31}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a20c0b67261538be59b367f36ccc5647d711bd7", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/0a20c0b67261538be59b367f36ccc5647d711bd7", "committedDate": "2020-09-04T14:58:16Z", "message": "Merge remote-tracking branch 'elastic/master' into faster-azure-reads-maybe"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "876d2de332fc6b215fb3eb64667d5d4674bf32da", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/876d2de332fc6b215fb3eb64667d5d4674bf32da", "committedDate": "2020-09-04T14:58:40Z", "message": "CR: whitespace"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c91d87097e6b25804233c7cdc21f8d3f9f3874c", "author": {"user": {"login": "original-brownbear", "name": "Armin Braun"}}, "url": "https://github.com/elastic/elasticsearch/commit/0c91d87097e6b25804233c7cdc21f8d3f9f3874c", "committedDate": "2020-09-05T07:26:58Z", "message": "Merge remote-tracking branch 'elastic/master' into faster-azure-reads-maybe"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4969, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}