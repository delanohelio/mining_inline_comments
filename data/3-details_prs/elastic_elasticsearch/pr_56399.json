{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE1MDIwMDgy", "number": 56399, "title": "Add Normalize Pipeline Aggregation", "bodyText": "This aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\nThe aggregations supports the following normalizations\n\nrescale 0-1\nrescale 0-100\npercentage of sum\nmean normalization\nz-score normalization\nsoftmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's normalizer field.\nFor example:\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n\nCloses #51005.", "createdAt": "2020-05-08T03:06:19Z", "url": "https://github.com/elastic/elasticsearch/pull/56399", "merged": true, "mergeCommit": {"oid": "79367e43da8eb875cb464403c4b31db80ce5c5d5"}, "closed": true, "closedAt": "2020-05-14T20:32:42Z", "author": {"login": "talevy"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcfJAbigBqjMzMTU0MDA3MTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABchS2XDgH2gAyNDE1MDIwMDgyOmU2ZGIwZjllOTQzNmY3NTYwYWQ4YTg4MWI4NDYxNDgwZWUzZDBkZDc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "17e0108d79ca1e314e9f198680bf5f923ef1dbf7", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/17e0108d79ca1e314e9f198680bf5f923ef1dbf7", "committedDate": "2020-05-08T03:05:22Z", "message": "Add Normalize Pipeline Aggregation\n\nThis aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\n\nThe aggregations supports the following normalizations\n\n- rescale 0-1\n- rescale 0-100\n- percentage of sum\n- mean normalization\n- z-score normalization\n- softmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's `normalizer` field.\n\nFor example:\n\n```\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n```\n\nCloses #51005."}, "afterCommit": {"oid": "7e8510de333fd45f4dd4c82be12a275a6b7f6274", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/7e8510de333fd45f4dd4c82be12a275a6b7f6274", "committedDate": "2020-05-08T03:07:13Z", "message": "Add Normalize Pipeline Aggregation\n\nThis aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\n\nThe aggregations supports the following normalizations\n\n- rescale 0-1\n- rescale 0-100\n- percentage of sum\n- mean normalization\n- z-score normalization\n- softmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's `normalizer` field.\n\nFor example:\n\n```\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n```\n\nCloses #51005."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/5427899ada2304388686d28ce8f3c1cd435dee88", "committedDate": "2020-05-08T03:15:16Z", "message": "Add Normalize Pipeline Aggregation\n\nThis aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\n\nThe aggregations supports the following normalizations\n\n- rescale 0-1\n- rescale 0-100\n- percentage of sum\n- mean normalization\n- z-score normalization\n- softmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's `normalizer` field.\n\nFor example:\n\n```\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n```\n\nCloses #51005."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7e8510de333fd45f4dd4c82be12a275a6b7f6274", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/7e8510de333fd45f4dd4c82be12a275a6b7f6274", "committedDate": "2020-05-08T03:07:13Z", "message": "Add Normalize Pipeline Aggregation\n\nThis aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\n\nThe aggregations supports the following normalizations\n\n- rescale 0-1\n- rescale 0-100\n- percentage of sum\n- mean normalization\n- z-score normalization\n- softmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's `normalizer` field.\n\nFor example:\n\n```\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n```\n\nCloses #51005."}, "afterCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/5427899ada2304388686d28ce8f3c1cd435dee88", "committedDate": "2020-05-08T03:15:16Z", "message": "Add Normalize Pipeline Aggregation\n\nThis aggregation will perform normalizations of metrics\nfor a given series of data in the form of bucket values.\n\nThe aggregations supports the following normalizations\n\n- rescale 0-1\n- rescale 0-100\n- percentage of sum\n- mean normalization\n- z-score normalization\n- softmax normalization\n\nTo specify which normalization is to be used, it can be specified\nin the normalize agg's `normalizer` field.\n\nFor example:\n\n```\n{\n  \"normalize\": {\n    \"buckets_path\": <>,\n    \"normalizer\": \"percent\"\n  }\n}\n```\n\nCloses #51005."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA4MTk0Mjgx", "url": "https://github.com/elastic/elasticsearch/pull/56399#pullrequestreview-408194281", "createdAt": "2020-05-08T12:11:27Z", "commit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoxMToyOFrOGSjbNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQxMjoyMjoyOVrOGSjsFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwNzk1Ng==", "bodyText": "This'll want [role=\"xpack\"] or so it'll get the little xpack bug.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422107956", "createdAt": "2020-05-08T12:11:28Z", "author": {"login": "nik9000"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,119 @@\n+[[search-aggregations-pipeline-normalize-aggregation]]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODIxOQ==", "bodyText": "Check out InstantiatingObjectParser!", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422108219", "createdAt": "2020-05-08T12:12:09Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwODc0OQ==", "bodyText": "The normalizer should probably be in hashCode and equals, right?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422108749", "createdAt": "2020-05-08T12:13:34Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizer.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(\n+        NAME, false, (args, name) -> new NormalizePipelineAggregationBuilder(name, (String) args[0],\n+        (String) args[1], (List<String>) args[2]));\n+\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), FORMAT);\n+        PARSER.declareString(constructorArg(), NORMALIZER_FIELD);\n+        PARSER.declareStringArray(constructorArg(), BUCKETS_PATH_FIELD);\n+    }\n+\n+    static final Map<String, Function<List<Double>, NormalizePipelineNormalizer>> NAME_MAP = Map.of(\n+        RescaleZeroToOne.NAME, RescaleZeroToOne::new,\n+        RescaleZeroToOneHundred.NAME, RescaleZeroToOneHundred::new,\n+        Mean.NAME, Mean::new,\n+        ZScore.NAME, ZScore::new,\n+        Percent.NAME, Percent::new,\n+        Softmax.NAME, Softmax::new\n+    );\n+\n+    static String validateNormalizerName(String name) {\n+        if (NAME_MAP.containsKey(name)) {\n+            return name;\n+        }\n+\n+        throw new IllegalArgumentException(\"invalid normalizer [\" + name + \"]\");\n+    }\n+\n+    private final String format;\n+    private final String normalizer;\n+\n+\n+    NormalizePipelineAggregationBuilder(String name, String format, String normalizer, List<String> bucketsPath) {\n+        super(name, NAME, bucketsPath.toArray(new String[0]));\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    NormalizePipelineAggregationBuilder(String name, String format, String normalizer, String bucketsPath) {\n+        super(name, NAME, new String[] { bucketsPath });\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    /**\n+     * Read from a stream.\n+     */\n+    public NormalizePipelineAggregationBuilder(StreamInput in) throws IOException {\n+        super(in, NAME);\n+        format = in.readOptionalString();\n+        normalizer = in.readString();\n+    }\n+\n+    @Override\n+    protected final void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeOptionalString(format);\n+        out.writeString(normalizer);\n+    }\n+\n+    /**\n+     * Gets the format to use on the output of this aggregation.\n+     */\n+    public String format() {\n+        return format;\n+    }\n+\n+    protected DocValueFormat formatter() {\n+        if (format != null) {\n+            return new DocValueFormat.Decimal(format);\n+        } else {\n+            return DocValueFormat.RAW;\n+        }\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metadata) {\n+        return new NormalizePipelineAggregator(name, bucketsPaths, formatter(), NAME_MAP.get(normalizer), metadata);\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        if (bucketsPaths.length != 1) {\n+            context.addBucketPathValidationError(\"must contain a single entry for aggregation [\" + name + \"]\");\n+        }\n+        context.validateParentAggSequentiallyOrdered(NAME, name);\n+    }\n+\n+    @Override\n+    protected final XContentBuilder internalXContent(XContentBuilder builder, Params params) throws IOException {\n+        if (format != null) {\n+            builder.field(BucketMetricsParser.FORMAT.getPreferredName(), format);\n+        }\n+        builder.field(NORMALIZER_FIELD.getPreferredName(), normalizer);\n+        return builder;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(super.hashCode(), format);\n+    }\n+\n+    @Override\n+    public boolean equals(Object obj) {\n+        if (this == obj) return true;\n+        if (obj == null || getClass() != obj.getClass()) return false;\n+        if (super.equals(obj) == false) return false;\n+        NormalizePipelineAggregationBuilder other = (NormalizePipelineAggregationBuilder) obj;\n+        return Objects.equals(format, other.format);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjEwOTU3NA==", "bodyText": "bucket.getAggregations().copyResults() does this without so much boiler plate.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422109574", "createdAt": "2020-05-08T12:15:34Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<List<Double>, NormalizePipelineNormalizer> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;\n+        List<Bucket> newBuckets = new ArrayList<>(buckets.size());\n+\n+        List<Double> values = buckets.stream().map(bucket -> resolveBucketValue(histo, bucket, bucketsPaths()[0], GapPolicy.SKIP))\n+            .collect(Collectors.toList());\n+\n+        NormalizePipelineNormalizer normalizer = normalizerSupplier.apply(values);\n+\n+        for (int i = 0; i < buckets.size(); i++) {\n+            InternalMultiBucketAggregation.InternalBucket bucket = buckets.get(i);\n+            Double thisBucketValue = values.get(i);\n+\n+            final double normalizedBucketValue;\n+\n+            // Only account for finite values\n+            if (thisBucketValue.isNaN()) {\n+                normalizedBucketValue = Double.NaN;\n+            } else {\n+                normalizedBucketValue = normalizer.normalize(thisBucketValue);\n+            }\n+\n+            List<InternalAggregation> aggs = StreamSupport.stream(bucket.getAggregations().spliterator(), false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ==", "bodyText": "Could this take a DoubleStream instead?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110209", "createdAt": "2020-05-08T12:17:18Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);\n+\n+    abstract static class SinglePassSimpleStatisticsNormalizer extends NormalizePipelineNormalizer {\n+        protected final double max;\n+        protected final double min;\n+        protected final double sum;\n+        protected final double mean;\n+        protected final int count;\n+\n+        SinglePassSimpleStatisticsNormalizer(List<Double> values) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDQ5MA==", "bodyText": "You'd only get a single pass but you wouldn't need to make Double objects which is kind of nice.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110490", "createdAt": "2020-05-08T12:18:08Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);\n+\n+    abstract static class SinglePassSimpleStatisticsNormalizer extends NormalizePipelineNormalizer {\n+        protected final double max;\n+        protected final double min;\n+        protected final double sum;\n+        protected final double mean;\n+        protected final int count;\n+\n+        SinglePassSimpleStatisticsNormalizer(List<Double> values) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDYxNA==", "bodyText": "Not that pipeline aggs are all that efficient here, but I feel compelled to save autoboxing when I can.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422110614", "createdAt": "2020-05-08T12:18:25Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);\n+\n+    abstract static class SinglePassSimpleStatisticsNormalizer extends NormalizePipelineNormalizer {\n+        protected final double max;\n+        protected final double min;\n+        protected final double sum;\n+        protected final double mean;\n+        protected final int count;\n+\n+        SinglePassSimpleStatisticsNormalizer(List<Double> values) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMDIwOQ=="}, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMTE4Mw==", "bodyText": "I wonder if we could have these normalizers all implement DoubleUnaryOperator instead of making an abstract class for them.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422111183", "createdAt": "2020-05-08T12:19:50Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineNormalizer.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+\n+import java.util.List;\n+\n+abstract class NormalizePipelineNormalizer {\n+\n+    static class RescaleZeroToOne extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_1\";\n+\n+        RescaleZeroToOne(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class RescaleZeroToOneHundred extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"rescale_0_100\";\n+\n+        RescaleZeroToOneHundred(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return 100 * (value - min) / (max - min);\n+        }\n+    }\n+\n+    static class Mean extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"mean\";\n+\n+        Mean(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / (max - min);\n+        }\n+    }\n+\n+    static class Percent extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"percent\";\n+\n+        Percent(List<Double> values) {\n+            super(values);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return value / sum;\n+        }\n+    }\n+\n+    static class ZScore extends SinglePassSimpleStatisticsNormalizer {\n+        static final String NAME = \"z-score\";\n+\n+        private final double stdev;\n+\n+        ZScore(List<Double> values) {\n+            super(values);\n+            double variance = 0.0;\n+            for (Double value : values) {\n+                if (value.isNaN() == false) {\n+                    variance += Math.pow(value - mean, 2);\n+                }\n+            }\n+            this.stdev = Math.sqrt(variance / count);\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return (value - mean) / stdev;\n+        }\n+    }\n+\n+    static class Softmax extends NormalizePipelineNormalizer {\n+        static final String NAME = \"softmax\";\n+\n+        private double sumExp;\n+\n+        Softmax(List<Double> values) {\n+            double sumExp = 0.0;\n+            for (Double value :  values) {\n+                if (value.isNaN() == false) {\n+                    sumExp += Math.exp(value);\n+                }\n+            }\n+\n+            this.sumExp = sumExp;\n+        }\n+\n+        @Override\n+        double normalize(double value) {\n+            return Math.exp(value) / sumExp;\n+        }\n+    }\n+\n+    abstract double normalize(double value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMTg1Ng==", "bodyText": "Do we need this bit? I don't remember needing it for top_metrics.", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422111856", "createdAt": "2020-05-08T12:21:32Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/analytics/normalize.yml", "diffHunk": "@@ -0,0 +1,86 @@\n+setup:\n+  - skip:\n+      features: headers\n+  - do:\n+      indices.create:\n+        index: foo\n+        body:\n+          mappings:\n+            properties:\n+              timestamp:\n+                type: date\n+              user:\n+                type: keyword\n+\n+\n+  - do:\n+      headers:\n+        Authorization: \"Basic eF9wYWNrX3Jlc3RfdXNlcjp4LXBhY2stdGVzdC1wYXNzd29yZA==\" # run as x_pack_rest_user, i.e. the test setup superuser", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjExMjI3Nw==", "bodyText": "You'll switch this to normalizer, right?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r422112277", "createdAt": "2020-05-08T12:22:29Z", "author": {"login": "nik9000"}, "path": "x-pack/plugin/src/test/resources/rest-api-spec/test/analytics/normalize.yml", "diffHunk": "@@ -0,0 +1,86 @@\n+setup:\n+  - skip:\n+      features: headers\n+  - do:\n+      indices.create:\n+        index: foo\n+        body:\n+          mappings:\n+            properties:\n+              timestamp:\n+                type: date\n+              user:\n+                type: keyword\n+\n+\n+  - do:\n+      headers:\n+        Authorization: \"Basic eF9wYWNrX3Jlc3RfdXNlcjp4LXBhY2stdGVzdC1wYXNzd29yZA==\" # run as x_pack_rest_user, i.e. the test setup superuser\n+      bulk:\n+        refresh: true\n+        body:\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-01T05:00:00Z\"\n+            user: \"a\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-01T05:00:00Z\"\n+            user: \"b\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-01T05:00:00Z\"\n+            user: \"c\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-02T05:00:00Z\"\n+            user: \"a\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-02T05:00:00Z\"\n+            user: \"b\"\n+\n+          - index:\n+              _index: \"foo\"\n+          - timestamp: \"2017-01-03T05:00:00Z\"\n+            user: \"d\"\n+\n+---\n+\"Basic Search\":\n+\n+  - do:\n+      search:\n+        index: \"foo\"\n+        body:\n+          size: 0\n+          aggs:\n+            histo:\n+              date_histogram:\n+                field: \"timestamp\"\n+                calendar_interval: \"day\"\n+              aggs:\n+                sum_users:\n+                  sum:\n+                    field: \"user\"\n+                total_users:\n+                  cumulative_cardinality:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5427899ada2304388686d28ce8f3c1cd435dee88"}, "originalPosition": 70}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15e5ca8ca63ab302f75f2945849a64d406e7b444", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/15e5ca8ca63ab302f75f2945849a64d406e7b444", "committedDate": "2020-05-08T18:28:57Z", "message": "Merge remote-tracking branch 'elastic/master' into normalize"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9782c2a7c583d075733799eedcae5e5c1faac9f4", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/9782c2a7c583d075733799eedcae5e5c1faac9f4", "committedDate": "2020-05-09T02:39:15Z", "message": "moar"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2dae977ba99d64e0617c43394fbc82005c0cd686", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/2dae977ba99d64e0617c43394fbc82005c0cd686", "committedDate": "2020-05-12T00:05:23Z", "message": "respond to rev"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cd088be0113fd0222feb925ade74a0b989c0fb2", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/8cd088be0113fd0222feb925ade74a0b989c0fb2", "committedDate": "2020-05-12T01:54:38Z", "message": "Merge remote-tracking branch 'elastic/master' into normalize"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9718e01a400b15db41b59bb17d9ee924d5ee0ec1", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/9718e01a400b15db41b59bb17d9ee924d5ee0ec1", "committedDate": "2020-05-12T05:29:20Z", "message": "revert change"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA5NzA4Nzg4", "url": "https://github.com/elastic/elasticsearch/pull/56399#pullrequestreview-409708788", "createdAt": "2020-05-12T05:30:41Z", "commit": {"oid": "9718e01a400b15db41b59bb17d9ee924d5ee0ec1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNTozMDo0MlrOGT2mxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwNTozMDo0MlrOGT2mxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQ3MDc4OA==", "bodyText": "I still need to update docs to discuss all the different normalizers", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423470788", "createdAt": "2020-05-12T05:30:42Z", "author": {"login": "talevy"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,121 @@\n+[role=\"xpack\"]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9718e01a400b15db41b59bb17d9ee924d5ee0ec1"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMDA2MjEx", "url": "https://github.com/elastic/elasticsearch/pull/56399#pullrequestreview-410006211", "createdAt": "2020-05-12T12:50:38Z", "commit": {"oid": "9718e01a400b15db41b59bb17d9ee924d5ee0ec1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/9c64e3613621bc3ca9e5187ce605e39ff4009167", "committedDate": "2020-05-12T15:40:56Z", "message": "Merge remote-tracking branch 'elastic/master' into normalize"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMjc3NjU1", "url": "https://github.com/elastic/elasticsearch/pull/56399#pullrequestreview-410277655", "createdAt": "2020-05-12T17:43:22Z", "commit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo0MzoyMlrOGUR5Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQxNzo1NTo0OFrOGUSYXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxNzg5NQ==", "bodyText": "Should we make a note somewhere that this pipeline always uses a skip gap policy?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423917895", "createdAt": "2020-05-12T17:43:22Z", "author": {"login": "polyfractal"}, "path": "docs/reference/aggregations/pipeline/normalize-aggregation.asciidoc", "diffHunk": "@@ -0,0 +1,121 @@\n+[role=\"xpack\"]\n+[testenv=\"basic\"]\n+[[search-aggregations-pipeline-normalize-aggregation]]\n+=== Normalize Aggregation\n+\n+A parent pipeline aggregation which calculates the specific normalized/rescaled value for a specific bucket value.\n+\n+==== Syntax\n+\n+A `normalize` aggregation looks like this in isolation:\n+\n+[source,js]\n+--------------------------------------------------\n+{\n+    \"normalize\": {\n+        \"buckets_path\": \"normalized\",\n+        \"normalizer\": \"percent_of_sum\"\n+    }\n+}\n+--------------------------------------------------\n+// NOTCONSOLE\n+\n+[[normalizer_pipeline-params]]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkxOTMzMA==", "bodyText": "Fine with normalizer, but wanted to also suggest method as a potential param name.  No strong opinion though :)", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423919330", "createdAt": "2020-05-12T17:45:40Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyMjA2Ng==", "bodyText": "Should we also check context.validateHasParent() to make sure this isn't at the top level?", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423922066", "createdAt": "2020-05-12T17:50:12Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregationBuilder.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.pipeline.AbstractPipelineAggregationBuilder;\n+import org.elasticsearch.search.aggregations.pipeline.BucketMetricsParser;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.constructorArg;\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+import static org.elasticsearch.search.aggregations.pipeline.PipelineAggregator.Parser.FORMAT;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Mean;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Percent;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOne;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.RescaleZeroToOneHundred;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.Softmax;\n+import static org.elasticsearch.xpack.analytics.normalize.NormalizePipelineNormalizers.ZScore;\n+\n+public class NormalizePipelineAggregationBuilder extends AbstractPipelineAggregationBuilder<NormalizePipelineAggregationBuilder> {\n+    public static final String NAME = \"normalize\";\n+    static final ParseField NORMALIZER_FIELD = new ParseField(\"normalizer\");\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConstructingObjectParser<NormalizePipelineAggregationBuilder, String> PARSER = new ConstructingObjectParser<>(\n+        NAME, false, (args, name) -> new NormalizePipelineAggregationBuilder(name, (String) args[0],\n+        (String) args[1], (List<String>) args[2]));\n+\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), FORMAT);\n+        PARSER.declareString(constructorArg(), NORMALIZER_FIELD);\n+        PARSER.declareStringArray(constructorArg(), BUCKETS_PATH_FIELD);\n+    }\n+\n+    static final Map<String, Function<double[], DoubleUnaryOperator>> NAME_MAP = Map.of(\n+        RescaleZeroToOne.NAME, RescaleZeroToOne::new,\n+        RescaleZeroToOneHundred.NAME, RescaleZeroToOneHundred::new,\n+        Mean.NAME, Mean::new,\n+        ZScore.NAME, ZScore::new,\n+        Percent.NAME, Percent::new,\n+        Softmax.NAME, Softmax::new\n+    );\n+\n+    static String validateNormalizerName(String name) {\n+        if (NAME_MAP.containsKey(name)) {\n+            return name;\n+        }\n+\n+        throw new IllegalArgumentException(\"invalid normalizer [\" + name + \"]\");\n+    }\n+\n+    private final String format;\n+    private final String normalizer;\n+\n+\n+    public NormalizePipelineAggregationBuilder(String name, String format, String normalizer, List<String> bucketsPath) {\n+        super(name, NAME, bucketsPath.toArray(new String[0]));\n+        this.format = format;\n+        this.normalizer = validateNormalizerName(normalizer);\n+    }\n+\n+    /**\n+     * Read from a stream.\n+     */\n+    public NormalizePipelineAggregationBuilder(StreamInput in) throws IOException {\n+        super(in, NAME);\n+        format = in.readOptionalString();\n+        normalizer = in.readString();\n+    }\n+\n+    @Override\n+    protected final void doWriteTo(StreamOutput out) throws IOException {\n+        out.writeOptionalString(format);\n+        out.writeString(normalizer);\n+    }\n+\n+    /**\n+     * Gets the format to use on the output of this aggregation.\n+     */\n+    public String format() {\n+        return format;\n+    }\n+\n+    protected DocValueFormat formatter() {\n+        if (format != null) {\n+            return new DocValueFormat.Decimal(format);\n+        } else {\n+            return DocValueFormat.RAW;\n+        }\n+    }\n+\n+    @Override\n+    protected PipelineAggregator createInternal(Map<String, Object> metadata) {\n+        return new NormalizePipelineAggregator(name, bucketsPaths, formatter(), NAME_MAP.get(normalizer), metadata);\n+    }\n+\n+    @Override\n+    protected void validate(ValidationContext context) {\n+        if (bucketsPaths.length != 1) {\n+            context.addBucketPathValidationError(\"must contain a single entry for aggregation [\" + name + \"]\");\n+        }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkyNTg1Mg==", "bodyText": "Do we know if this works with a terms agg as the parent?  It feels like it should (e.g. it doesn't require any specific ordering of the buckets, unlike something like a moving avg which needs an ordering).\nIf we think it should work with terms we should tweak this to not use a HistogramFactory directly.  BucketScriptPipelineAggregator has an example of how to generically build buckets from any InternalMultiBucketAggregation (the internal agg can create buckets too, not just the factory).", "url": "https://github.com/elastic/elasticsearch/pull/56399#discussion_r423925852", "createdAt": "2020-05-12T17:55:48Z", "author": {"login": "polyfractal"}, "path": "x-pack/plugin/analytics/src/main/java/org/elasticsearch/xpack/analytics/normalize/NormalizePipelineAggregator.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.analytics.normalize;\n+\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalAggregation.ReduceContext;\n+import org.elasticsearch.search.aggregations.InternalAggregations;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.bucket.MultiBucketsAggregation.Bucket;\n+import org.elasticsearch.search.aggregations.bucket.histogram.HistogramFactory;\n+import org.elasticsearch.search.aggregations.pipeline.BucketHelpers.GapPolicy;\n+import org.elasticsearch.search.aggregations.pipeline.InternalSimpleValue;\n+import org.elasticsearch.search.aggregations.pipeline.PipelineAggregator;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.DoubleUnaryOperator;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+\n+import static org.elasticsearch.search.aggregations.pipeline.BucketHelpers.resolveBucketValue;\n+\n+public class NormalizePipelineAggregator extends PipelineAggregator {\n+    private final DocValueFormat formatter;\n+    private final Function<double[], DoubleUnaryOperator> normalizerSupplier;\n+\n+    NormalizePipelineAggregator(String name, String[] bucketsPaths, DocValueFormat formatter,\n+                                Function<double[], DoubleUnaryOperator> normalizerSupplier,\n+                                Map<String, Object> metadata) {\n+        super(name, bucketsPaths, metadata);\n+        this.formatter = formatter;\n+        this.normalizerSupplier = normalizerSupplier;\n+    }\n+\n+    @Override\n+    public InternalAggregation reduce(InternalAggregation aggregation, ReduceContext reduceContext) {\n+        InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends InternalMultiBucketAggregation.InternalBucket>\n+                histo = (InternalMultiBucketAggregation<? extends InternalMultiBucketAggregation, ? extends\n+                InternalMultiBucketAggregation.InternalBucket>) aggregation;\n+        List<? extends InternalMultiBucketAggregation.InternalBucket> buckets = histo.getBuckets();\n+        HistogramFactory factory = (HistogramFactory) histo;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c64e3613621bc3ca9e5187ce605e39ff4009167"}, "originalPosition": 48}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "023c34fab289abb5bce094db492a726308ca8a3b", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/023c34fab289abb5bce094db492a726308ca8a3b", "committedDate": "2020-05-13T22:43:38Z", "message": "respond to changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e76dbd9fb6e094015d7599b39ad1e0c972b7a67", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/4e76dbd9fb6e094015d7599b39ad1e0c972b7a67", "committedDate": "2020-05-13T22:43:56Z", "message": "Merge remote-tracking branch 'elastic/master' into normalize"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8da4960ed6e3adb4279f391168fe9f25a219067f", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/8da4960ed6e3adb4279f391168fe9f25a219067f", "committedDate": "2020-05-13T23:59:39Z", "message": "update docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d4737c044ab9ac6802ea2ecab6990fca072b050", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/5d4737c044ab9ac6802ea2ecab6990fca072b050", "committedDate": "2020-05-14T00:09:06Z", "message": "format"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa9eebca37427677f615c36e58a456d0c60d46dc", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/aa9eebca37427677f615c36e58a456d0c60d46dc", "committedDate": "2020-05-14T16:17:26Z", "message": "touch up"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExOTk5Njgw", "url": "https://github.com/elastic/elasticsearch/pull/56399#pullrequestreview-411999680", "createdAt": "2020-05-14T16:53:18Z", "commit": {"oid": "aa9eebca37427677f615c36e58a456d0c60d46dc"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72f29e71fec4421793887ad9e7a967fac53ce301", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/72f29e71fec4421793887ad9e7a967fac53ce301", "committedDate": "2020-05-14T17:08:21Z", "message": "use format in example"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e9f01532080b402d65ace333dbbfe23d8a7ec7c", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/1e9f01532080b402d65ace333dbbfe23d8a7ec7c", "committedDate": "2020-05-14T18:55:58Z", "message": "comma"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6db0f9e9436f7560ad8a881b8461480ee3d0dd7", "author": {"user": {"login": "talevy", "name": "Tal Levy"}}, "url": "https://github.com/elastic/elasticsearch/commit/e6db0f9e9436f7560ad8a881b8461480ee3d0dd7", "committedDate": "2020-05-14T19:43:15Z", "message": "final fix"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 255, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}