{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM5MDMzMjky", "number": 66237, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyMToxNVrOFFe5gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0MDo0NFrOFFkSBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMjkzNDQwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/internalClusterTest/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocationIntegTests.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyMToxNVrOIGA3Tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyMToxNVrOIGA3Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE3NjUyNg==", "bodyText": "Obviously this could use a few more tests (especially around various mixes of multiple shards in the cache and exception handling), I'm happy to add those in a follow-up. I think today it's tricky to get exhaustive testing up and running though + it makes this even longer to review.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543176526", "createdAt": "2020-12-15T09:21:15Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/internalClusterTest/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocationIntegTests.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots;\n+\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.common.util.CollectionUtils;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.snapshots.mockstore.MockRepository;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.util.Collection;\n+import java.util.List;\n+\n+import static org.elasticsearch.index.IndexSettings.INDEX_SOFT_DELETES_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class SearchableSnapshotAllocationIntegTests extends BaseSearchableSnapshotsIntegTestCase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d764e3d5f31885757a7a46b9b6437fbb094d021d"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMjk1MzA0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyNTowNlrOIGBB-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyNTowNlrOIGBB-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE3OTI1Nw==", "bodyText": "The whole implementation here is intentionally kept close to how replica and primary shard allocators work today code-wise. I think there's lots of room for drying things up here (especially if/when we want to tackle the enhancement TODOs in this that would duplicate a lot of logic in those allocators also) this way compared to going for the shortest+most specific possible implementation.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543179257", "createdAt": "2020-12-15T09:25:06Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "diffHunk": "@@ -90,8 +150,60 @@ private AllocateUnassignedDecision decideAllocation(RoutingAllocation allocation\n             return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null);\n         }\n \n-        // let BalancedShardsAllocator take care of allocating this shard\n-        // TODO: once we have persistent cache, choose a node that has existing data\n+        final AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchedCacheData = fetchData(shardRouting, allocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2922b2259b5ca6d3a117cf31c33014b4f18b4eb1"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMjk2MDAyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyNjozMlrOIGBGPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyNjozMlrOIGBGPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE4MDM0OQ==", "bodyText": "There might be better (as in more efficient) ways (certainly are) to build the synchronization here but I decided to keep it as simple as possible in the interest of saving some time today.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543180349", "createdAt": "2020-12-15T09:26:32Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "diffHunk": "@@ -103,16 +215,239 @@ public AllocateUnassignedDecision explainUnassignedShardAllocation(ShardRouting\n     }\n \n     @Override\n-    public void cleanCaches() {}\n+    public void cleanCaches() {\n+        asyncFetchStore.clear();\n+    }\n \n     @Override\n-    public void applyStartedShards(List<ShardRouting> startedShards, RoutingAllocation allocation) {}\n+    public void applyStartedShards(List<ShardRouting> startedShards, RoutingAllocation allocation) {\n+        for (ShardRouting startedShard : startedShards) {\n+            asyncFetchStore.remove(startedShard.shardId());\n+        }\n+    }\n \n     @Override\n-    public void applyFailedShards(List<FailedShard> failedShards, RoutingAllocation allocation) {}\n+    public void applyFailedShards(List<FailedShard> failedShards, RoutingAllocation allocation) {\n+        for (FailedShard failedShard : failedShards) {\n+            asyncFetchStore.remove(failedShard.getRoutingEntry().shardId());\n+        }\n+    }\n \n     @Override\n     public int getNumberOfInFlightFetches() {\n-        return 0;\n+        int count = 0;\n+        for (AsyncCacheStatusFetch fetch : asyncFetchStore.values()) {\n+            count += fetch.numberOfInFlightFetches();\n+        }\n+        return count;\n+    }\n+\n+    private AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchData(ShardRouting shard, RoutingAllocation allocation) {\n+        final ShardId shardId = shard.shardId();\n+        final Settings indexSettings = allocation.metadata().index(shard.index()).getSettings();\n+        final SnapshotId snapshotId = new SnapshotId(\n+            SNAPSHOT_SNAPSHOT_NAME_SETTING.get(indexSettings),\n+            SNAPSHOT_SNAPSHOT_ID_SETTING.get(indexSettings)\n+        );\n+        final DiscoveryNodes nodes = allocation.nodes();\n+        final AsyncCacheStatusFetch asyncFetch = asyncFetchStore.computeIfAbsent(shardId, sid -> new AsyncCacheStatusFetch());\n+        final DiscoveryNode[] dataNodes = asyncFetch.addFetches(nodes.getDataNodes().values().toArray(DiscoveryNode.class));\n+        if (dataNodes.length > 0) {\n+            client.execute(\n+                TransportSearchableSnapshotCacheStoresAction.TYPE,\n+                new TransportSearchableSnapshotCacheStoresAction.Request(snapshotId, shardId, dataNodes),\n+                ActionListener.runAfter(new ActionListener<>() {\n+                    @Override\n+                    public void onResponse(NodesCacheFilesMetadata nodesCacheFilesMetadata) {\n+                        final Map<DiscoveryNode, NodeCacheFilesMetadata> res = new HashMap<>(nodesCacheFilesMetadata.getNodesMap().size());\n+                        for (Map.Entry<String, NodeCacheFilesMetadata> entry : nodesCacheFilesMetadata.getNodesMap().entrySet()) {\n+                            res.put(nodes.get(entry.getKey()), entry.getValue());\n+                        }\n+                        asyncFetch.addData(res);\n+                    }\n+\n+                    @Override\n+                    public void onFailure(Exception e) {\n+                        logger.warn(\"Failure when trying to fetch existing cache sizes\", e);\n+                        final Map<DiscoveryNode, NodeCacheFilesMetadata> res = new HashMap<>(dataNodes.length);\n+                        for (DiscoveryNode dataNode : dataNodes) {\n+                            res.put(dataNode, new NodeCacheFilesMetadata(dataNode, 0L));\n+                        }\n+                        asyncFetch.addData(res);\n+                    }\n+                }, () -> client.admin().cluster().prepareReroute().execute(REROUTE_LISTENER))\n+            );\n+        }\n+        return new AsyncShardFetch.FetchResult<>(shardId, asyncFetch.data(), Collections.emptySet());\n+    }\n+\n+    /**\n+     * Takes the store info for nodes that have a shard store and adds them to the node decisions,\n+     * leaving the node explanations untouched for those nodes that do not have any store information.\n+     */\n+    private static List<NodeAllocationResult> augmentExplanationsWithStoreInfo(\n+        Map<String, NodeAllocationResult> nodeDecisions,\n+        Map<String, NodeAllocationResult> withShardStores\n+    ) {\n+        if (nodeDecisions == null || withShardStores == null) {\n+            return null;\n+        }\n+        List<NodeAllocationResult> augmented = new ArrayList<>();\n+        for (Map.Entry<String, NodeAllocationResult> entry : nodeDecisions.entrySet()) {\n+            if (withShardStores.containsKey(entry.getKey())) {\n+                augmented.add(withShardStores.get(entry.getKey()));\n+            } else {\n+                augmented.add(entry.getValue());\n+            }\n+        }\n+        return augmented;\n+    }\n+\n+    /**\n+     * Determines if the shard can be allocated on at least one node based on the allocation deciders.\n+     *\n+     * Returns the best allocation decision for allocating the shard on any node (i.e. YES if at least one\n+     * node decided YES, THROTTLE if at least one node decided THROTTLE, and NO if none of the nodes decided\n+     * YES or THROTTLE).  If in explain mode, also returns the node-level explanations as the second element\n+     * in the returned tuple.\n+     * TODO: dry this method up against ReplicaShardAllocator\n+     */\n+    private static Tuple<Decision, Map<String, NodeAllocationResult>> canBeAllocatedToAtLeastOneNode(\n+        ShardRouting shard,\n+        RoutingAllocation allocation\n+    ) {\n+        Decision madeDecision = Decision.NO;\n+        final boolean explain = allocation.debugDecision();\n+        Map<String, NodeAllocationResult> nodeDecisions = explain ? new HashMap<>() : null;\n+        for (ObjectCursor<DiscoveryNode> cursor : allocation.nodes().getDataNodes().values()) {\n+            RoutingNode node = allocation.routingNodes().node(cursor.value.getId());\n+            if (node == null) {\n+                continue;\n+            }\n+            // if we can't allocate it on a node, ignore it\n+            Decision decision = allocation.deciders().canAllocate(shard, node, allocation);\n+            if (decision.type() == Decision.Type.YES && madeDecision.type() != Decision.Type.YES) {\n+                if (explain) {\n+                    madeDecision = decision;\n+                } else {\n+                    return Tuple.tuple(decision, null);\n+                }\n+            } else if (madeDecision.type() == Decision.Type.NO && decision.type() == Decision.Type.THROTTLE) {\n+                madeDecision = decision;\n+            }\n+            if (explain) {\n+                nodeDecisions.put(node.nodeId(), new NodeAllocationResult(node.node(), null, decision));\n+            }\n+        }\n+        return Tuple.tuple(madeDecision, nodeDecisions);\n+    }\n+\n+    private MatchingNodes findMatchingNodes(\n+        ShardRouting shard,\n+        RoutingAllocation allocation,\n+        AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> data,\n+        boolean explain\n+    ) {\n+        final Map<DiscoveryNode, Long> matchingNodesCacheSizes = new HashMap<>();\n+        final Map<String, NodeAllocationResult> nodeDecisionsDebug = explain ? new HashMap<>() : null;\n+        for (Map.Entry<DiscoveryNode, NodeCacheFilesMetadata> nodeStoreEntry : data.getData().entrySet()) {\n+            DiscoveryNode discoNode = nodeStoreEntry.getKey();\n+            NodeCacheFilesMetadata nodeCacheFilesMetadata = nodeStoreEntry.getValue();\n+            // we don't have any existing cached bytes at all\n+            if (nodeCacheFilesMetadata.bytesCached() == 0L) {\n+                continue;\n+            }\n+\n+            RoutingNode node = allocation.routingNodes().node(discoNode.getId());\n+            if (node == null) {\n+                continue;\n+            }\n+\n+            // check if we can allocate on the node\n+            Decision decision = allocation.deciders().canAllocate(shard, node, allocation);\n+            Long matchingBytes = null;\n+            if (explain) {\n+                matchingBytes = nodeCacheFilesMetadata.bytesCached();\n+                NodeAllocationResult.ShardStoreInfo shardStoreInfo = new NodeAllocationResult.ShardStoreInfo(matchingBytes);\n+                nodeDecisionsDebug.put(node.nodeId(), new NodeAllocationResult(discoNode, shardStoreInfo, decision));\n+            }\n+\n+            if (decision.type() == Decision.Type.NO) {\n+                continue;\n+            }\n+\n+            if (matchingBytes == null) {\n+                matchingBytes = nodeCacheFilesMetadata.bytesCached();\n+            }\n+            matchingNodesCacheSizes.put(discoNode, matchingBytes);\n+            if (logger.isTraceEnabled()) {\n+                logger.trace(\n+                    \"{}: node [{}] has [{}/{}] bytes of re-usable cache data\",\n+                    shard,\n+                    discoNode.getName(),\n+                    new ByteSizeValue(matchingBytes),\n+                    matchingBytes\n+                );\n+            }\n+        }\n+\n+        return new MatchingNodes(matchingNodesCacheSizes, nodeDecisionsDebug);\n+    }\n+\n+    private static final class AsyncCacheStatusFetch {\n+\n+        private final Set<DiscoveryNode> fetchingDataNodes = new HashSet<>();\n+\n+        private final Map<DiscoveryNode, NodeCacheFilesMetadata> data = new HashMap<>();\n+\n+        AsyncCacheStatusFetch() {}\n+\n+        synchronized DiscoveryNode[] addFetches(DiscoveryNode[] nodes) {\n+            final Collection<DiscoveryNode> nodesToFetch = new ArrayList<>();\n+            for (DiscoveryNode node : nodes) {\n+                if (data.containsKey(node) == false && fetchingDataNodes.add(node)) {\n+                    nodesToFetch.add(node);\n+                }\n+            }\n+            return nodesToFetch.toArray(new DiscoveryNode[0]);\n+        }\n+\n+        synchronized void addData(Map<DiscoveryNode, NodeCacheFilesMetadata> newData) {\n+            data.putAll(newData);\n+            fetchingDataNodes.removeAll(newData.keySet());\n+        }\n+\n+        @Nullable\n+        synchronized Map<DiscoveryNode, NodeCacheFilesMetadata> data() {\n+            return fetchingDataNodes.size() > 0 ? null : Map.copyOf(data);\n+        }\n+\n+        synchronized int numberOfInFlightFetches() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2922b2259b5ca6d3a117cf31c33014b4f18b4eb1"}, "originalPosition": 387}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMjk2OTc3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshots.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyODoyN1rOIGBMBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOToyODoyN1rOIGBMBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE4MTgzMA==", "bodyText": "Best solution I could think of for passing the cache to the transport action (which is instantiated on master and data nodes but only ever handles the fan-out request on data nodes) on master- and data-nodes without instantiating the cache on master as well.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543181830", "createdAt": "2020-12-15T09:28:27Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshots.java", "diffHunk": "@@ -364,4 +370,19 @@ protected XPackLicenseState getLicenseState() {\n                 CACHE_PREWARMING_THREAD_POOL_SETTING\n             ) };\n     }\n+\n+    public static final class CacheServiceSupplier implements Supplier<CacheService> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2922b2259b5ca6d3a117cf31c33014b4f18b4eb1"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMjk5OTM5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocatorTests.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOTozNDozNFrOIGBdDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwOTozNDozNFrOIGBdDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE4NjE4OQ==", "bodyText": "Just like the IT lots of tests that could still be added here obviously that I'd push to a follow-up.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543186189", "createdAt": "2020-12-15T09:34:34Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocatorTests.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots;\n+\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRequest;\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteAction;\n+import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteResponse;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ESAllocationTestCase;\n+import org.elasticsearch.cluster.coordination.DeterministicTaskQueue;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.routing.RoutingNodes;\n+import org.elasticsearch.cluster.routing.RoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.ExistingShardsAllocator;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.RoutingExplanations;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.collect.ImmutableOpenMap;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.Snapshot;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotShardSizeInfo;\n+import org.elasticsearch.test.client.NoOpNodeClient;\n+import org.elasticsearch.xpack.searchablesnapshots.action.cache.TransportSearchableSnapshotCacheStoresAction;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.node.Node.NODE_NAME_SETTING;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class SearchableSnapshotAllocatorTests extends ESAllocationTestCase {\n+\n+    public void testAllocateToNodeWithLargestCache() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzE4NTExOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMDoxMzo0MVrOIGDL0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjoyODoxNlrOIGIcDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzIxNDU0NQ==", "bodyText": "I think a part of the purpose here is to not fetch data unnecessarily and that this should go before fetchData above? Looks like that is the case in ReplicaShardAllocator too.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543214545", "createdAt": "2020-12-15T10:13:41Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "diffHunk": "@@ -90,8 +150,60 @@ private AllocateUnassignedDecision decideAllocation(RoutingAllocation allocation\n             return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null);\n         }\n \n-        // let BalancedShardsAllocator take care of allocating this shard\n-        // TODO: once we have persistent cache, choose a node that has existing data\n+        final AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchedCacheData = fetchData(shardRouting, allocation);\n+        if (fetchedCacheData.hasData() == false) {\n+            return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null);\n+        }\n+\n+        final boolean explain = allocation.debugDecision();\n+        final MatchingNodes matchingNodes = findMatchingNodes(shardRouting, allocation, fetchedCacheData, explain);\n+        assert explain == false || matchingNodes.nodeDecisions != null : \"in explain mode, we must have individual node decisions\";\n+\n+        // pre-check if it can be allocated to any node that currently exists, so we won't list the cache sizes for it for nothing\n+        // TODO: in the following logic, we do not account for existing cache size when handling disk space checks, should and can we\n+        // reliably do this in a world of concurrent cache evictions or are we ok with the cache size just being a best effort hint\n+        // here?\n+        Tuple<Decision, Map<String, NodeAllocationResult>> result = canBeAllocatedToAtLeastOneNode(shardRouting, allocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwMDYyMA==", "bodyText": "++ sorry about that oversight", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543300620", "createdAt": "2020-12-15T12:28:16Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "diffHunk": "@@ -90,8 +150,60 @@ private AllocateUnassignedDecision decideAllocation(RoutingAllocation allocation\n             return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null);\n         }\n \n-        // let BalancedShardsAllocator take care of allocating this shard\n-        // TODO: once we have persistent cache, choose a node that has existing data\n+        final AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchedCacheData = fetchData(shardRouting, allocation);\n+        if (fetchedCacheData.hasData() == false) {\n+            return AllocateUnassignedDecision.no(UnassignedInfo.AllocationStatus.FETCHING_SHARD_DATA, null);\n+        }\n+\n+        final boolean explain = allocation.debugDecision();\n+        final MatchingNodes matchingNodes = findMatchingNodes(shardRouting, allocation, fetchedCacheData, explain);\n+        assert explain == false || matchingNodes.nodeDecisions != null : \"in explain mode, we must have individual node decisions\";\n+\n+        // pre-check if it can be allocated to any node that currently exists, so we won't list the cache sizes for it for nothing\n+        // TODO: in the following logic, we do not account for existing cache size when handling disk space checks, should and can we\n+        // reliably do this in a world of concurrent cache evictions or are we ok with the cache size just being a best effort hint\n+        // here?\n+        Tuple<Decision, Map<String, NodeAllocationResult>> result = canBeAllocatedToAtLeastOneNode(shardRouting, allocation);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzIxNDU0NQ=="}, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzI5NjY1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMDozNzoxOFrOIGEMig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjoxNTowOVrOIGH8xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzIzMTExNA==", "bodyText": "I am curious why we are not using the existing AsyncShardFetch for this? No need to change anything, I did not spot any issues, so purely a question to figure out if we need a follow-up later.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543231114", "createdAt": "2020-12-15T10:37:18Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "diffHunk": "@@ -103,16 +215,239 @@ public AllocateUnassignedDecision explainUnassignedShardAllocation(ShardRouting\n     }\n \n     @Override\n-    public void cleanCaches() {}\n+    public void cleanCaches() {\n+        asyncFetchStore.clear();\n+    }\n \n     @Override\n-    public void applyStartedShards(List<ShardRouting> startedShards, RoutingAllocation allocation) {}\n+    public void applyStartedShards(List<ShardRouting> startedShards, RoutingAllocation allocation) {\n+        for (ShardRouting startedShard : startedShards) {\n+            asyncFetchStore.remove(startedShard.shardId());\n+        }\n+    }\n \n     @Override\n-    public void applyFailedShards(List<FailedShard> failedShards, RoutingAllocation allocation) {}\n+    public void applyFailedShards(List<FailedShard> failedShards, RoutingAllocation allocation) {\n+        for (FailedShard failedShard : failedShards) {\n+            asyncFetchStore.remove(failedShard.getRoutingEntry().shardId());\n+        }\n+    }\n \n     @Override\n     public int getNumberOfInFlightFetches() {\n-        return 0;\n+        int count = 0;\n+        for (AsyncCacheStatusFetch fetch : asyncFetchStore.values()) {\n+            count += fetch.numberOfInFlightFetches();\n+        }\n+        return count;\n+    }\n+\n+    private AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchData(ShardRouting shard, RoutingAllocation allocation) {\n+        final ShardId shardId = shard.shardId();\n+        final Settings indexSettings = allocation.metadata().index(shard.index()).getSettings();\n+        final SnapshotId snapshotId = new SnapshotId(\n+            SNAPSHOT_SNAPSHOT_NAME_SETTING.get(indexSettings),\n+            SNAPSHOT_SNAPSHOT_ID_SETTING.get(indexSettings)\n+        );\n+        final DiscoveryNodes nodes = allocation.nodes();\n+        final AsyncCacheStatusFetch asyncFetch = asyncFetchStore.computeIfAbsent(shardId, sid -> new AsyncCacheStatusFetch());\n+        final DiscoveryNode[] dataNodes = asyncFetch.addFetches(nodes.getDataNodes().values().toArray(DiscoveryNode.class));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI5MjYxMw==", "bodyText": "Yea we certainly could now. An earlier version of this worked a little differently and that made using the existing AsyncShardFetch not a great fit, but with the way it works now I think we can simplify this in a follow-up for sure.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543292613", "createdAt": "2020-12-15T12:15:09Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "diffHunk": "@@ -103,16 +215,239 @@ public AllocateUnassignedDecision explainUnassignedShardAllocation(ShardRouting\n     }\n \n     @Override\n-    public void cleanCaches() {}\n+    public void cleanCaches() {\n+        asyncFetchStore.clear();\n+    }\n \n     @Override\n-    public void applyStartedShards(List<ShardRouting> startedShards, RoutingAllocation allocation) {}\n+    public void applyStartedShards(List<ShardRouting> startedShards, RoutingAllocation allocation) {\n+        for (ShardRouting startedShard : startedShards) {\n+            asyncFetchStore.remove(startedShard.shardId());\n+        }\n+    }\n \n     @Override\n-    public void applyFailedShards(List<FailedShard> failedShards, RoutingAllocation allocation) {}\n+    public void applyFailedShards(List<FailedShard> failedShards, RoutingAllocation allocation) {\n+        for (FailedShard failedShard : failedShards) {\n+            asyncFetchStore.remove(failedShard.getRoutingEntry().shardId());\n+        }\n+    }\n \n     @Override\n     public int getNumberOfInFlightFetches() {\n-        return 0;\n+        int count = 0;\n+        for (AsyncCacheStatusFetch fetch : asyncFetchStore.values()) {\n+            count += fetch.numberOfInFlightFetches();\n+        }\n+        return count;\n+    }\n+\n+    private AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchData(ShardRouting shard, RoutingAllocation allocation) {\n+        final ShardId shardId = shard.shardId();\n+        final Settings indexSettings = allocation.metadata().index(shard.index()).getSettings();\n+        final SnapshotId snapshotId = new SnapshotId(\n+            SNAPSHOT_SNAPSHOT_NAME_SETTING.get(indexSettings),\n+            SNAPSHOT_SNAPSHOT_ID_SETTING.get(indexSettings)\n+        );\n+        final DiscoveryNodes nodes = allocation.nodes();\n+        final AsyncCacheStatusFetch asyncFetch = asyncFetchStore.computeIfAbsent(shardId, sid -> new AsyncCacheStatusFetch());\n+        final DiscoveryNode[] dataNodes = asyncFetch.addFetches(nodes.getDataNodes().values().toArray(DiscoveryNode.class));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzIzMTExNA=="}, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 216}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzM4ODkzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/internalClusterTest/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocationIntegTests.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMDo1NzoxM1rOIGFCFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMDo1NzoxM1rOIGFCFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI0NDgyMw==", "bodyText": "Can we randomly use NEW_PRIMARIES too?", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543244823", "createdAt": "2020-12-15T10:57:13Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/internalClusterTest/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocationIntegTests.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots;\n+\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.xpack.searchablesnapshots.cache.CacheService;\n+\n+import java.util.List;\n+\n+import static org.elasticsearch.index.IndexSettings.INDEX_SOFT_DELETES_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class SearchableSnapshotAllocationIntegTests extends BaseSearchableSnapshotsIntegTestCase {\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        return Settings.builder()\n+            .put(super.nodeSettings(nodeOrdinal))\n+            // ensure the cache is definitely used\n+            .put(CacheService.SNAPSHOT_CACHE_SIZE_SETTING.getKey(), new ByteSizeValue(1L, ByteSizeUnit.GB))\n+            .build();\n+    }\n+\n+    public void testAllocatesToBestAvailableNodeOnRestart() throws Exception {\n+        internalCluster().startMasterOnlyNode();\n+        final String firstDataNode = internalCluster().startDataOnlyNode();\n+        final String index = \"test-idx\";\n+        createIndexWithContent(index, indexSettingsNoReplicas(1).put(INDEX_SOFT_DELETES_SETTING.getKey(), true).build());\n+        final String repoName = \"test-repo\";\n+        createRepository(repoName, \"fs\");\n+        final String snapshotName = \"test-snapshot\";\n+        createSnapshot(repoName, snapshotName, List.of(index));\n+        assertAcked(client().admin().indices().prepareDelete(index));\n+        final String restoredIndex = mountSnapshot(repoName, snapshotName, index, Settings.EMPTY);\n+        ensureGreen(restoredIndex);\n+        internalCluster().startDataOnlyNodes(randomIntBetween(1, 4));\n+\n+        setAllocation(EnableAllocationDecider.Allocation.NONE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzQ0NTc3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocatorTests.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMToxMDoxOFrOIGFi3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMToxMDoxOFrOIGFi3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI1MzIxMg==", "bodyText": "Let us follow-up with either a test or an integration test validation that this all works for replicas too.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543253212", "createdAt": "2020-12-15T11:10:18Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocatorTests.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots;\n+\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRequest;\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteAction;\n+import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteResponse;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ESAllocationTestCase;\n+import org.elasticsearch.cluster.coordination.DeterministicTaskQueue;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.routing.RoutingNodes;\n+import org.elasticsearch.cluster.routing.RoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.ExistingShardsAllocator;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.RoutingExplanations;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.collect.ImmutableOpenMap;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.Snapshot;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotShardSizeInfo;\n+import org.elasticsearch.test.client.NoOpNodeClient;\n+import org.elasticsearch.xpack.searchablesnapshots.action.cache.TransportSearchableSnapshotCacheStoresAction;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.node.Node.NODE_NAME_SETTING;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class SearchableSnapshotAllocatorTests extends ESAllocationTestCase {\n+\n+    public void testAllocateToNodeWithLargestCache() {\n+        final ShardId shardId = new ShardId(\"test\", \"_na_\", 0);\n+        final List<DiscoveryNode> nodes = randomList(1, 10, () -> newNode(\"node-\" + UUIDs.randomBase64UUID(random())));\n+        final DiscoveryNode localNode = randomFrom(nodes);\n+        final Settings localNodeSettings = Settings.builder().put(NODE_NAME_SETTING.getKey(), localNode.getName()).build();\n+\n+        final ClusterName clusterName = org.elasticsearch.cluster.ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY);\n+\n+        final DeterministicTaskQueue deterministicTaskQueue = new DeterministicTaskQueue(localNodeSettings, random());\n+\n+        final Metadata metadata = Metadata.builder()\n+            .put(\n+                IndexMetadata.builder(shardId.getIndexName())\n+                    .settings(\n+                        settings(Version.CURRENT).put(\n+                            ExistingShardsAllocator.EXISTING_SHARDS_ALLOCATOR_SETTING.getKey(),\n+                            SearchableSnapshotAllocator.ALLOCATOR_NAME\n+                        )\n+                    )\n+                    .numberOfShards(1)\n+                    .numberOfReplicas(0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzQ3NzAzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocatorTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMToxNzoyNlrOIGF0ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMToxNzoyNlrOIGF0ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI1Nzc2Mw==", "bodyText": "Can we improve the selection of sizes here to more frequently pick 0 and also sometimes pick 0 for all nodes?", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543257763", "createdAt": "2020-12-15T11:17:26Z", "author": {"login": "henningandersen"}, "path": "x-pack/plugin/searchable-snapshots/src/test/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocatorTests.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.searchablesnapshots;\n+\n+import org.elasticsearch.Version;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRequest;\n+import org.elasticsearch.action.ActionResponse;\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteAction;\n+import org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteResponse;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.cluster.ClusterName;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.ESAllocationTestCase;\n+import org.elasticsearch.cluster.coordination.DeterministicTaskQueue;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.metadata.Metadata;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.RecoverySource;\n+import org.elasticsearch.cluster.routing.RoutingNodes;\n+import org.elasticsearch.cluster.routing.RoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.allocation.ExistingShardsAllocator;\n+import org.elasticsearch.cluster.routing.allocation.RoutingAllocation;\n+import org.elasticsearch.cluster.routing.allocation.RoutingExplanations;\n+import org.elasticsearch.common.UUIDs;\n+import org.elasticsearch.common.collect.ImmutableOpenMap;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.repositories.IndexId;\n+import org.elasticsearch.snapshots.Snapshot;\n+import org.elasticsearch.snapshots.SnapshotId;\n+import org.elasticsearch.snapshots.SnapshotShardSizeInfo;\n+import org.elasticsearch.test.client.NoOpNodeClient;\n+import org.elasticsearch.xpack.searchablesnapshots.action.cache.TransportSearchableSnapshotCacheStoresAction;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.node.Node.NODE_NAME_SETTING;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class SearchableSnapshotAllocatorTests extends ESAllocationTestCase {\n+\n+    public void testAllocateToNodeWithLargestCache() {\n+        final ShardId shardId = new ShardId(\"test\", \"_na_\", 0);\n+        final List<DiscoveryNode> nodes = randomList(1, 10, () -> newNode(\"node-\" + UUIDs.randomBase64UUID(random())));\n+        final DiscoveryNode localNode = randomFrom(nodes);\n+        final Settings localNodeSettings = Settings.builder().put(NODE_NAME_SETTING.getKey(), localNode.getName()).build();\n+\n+        final ClusterName clusterName = org.elasticsearch.cluster.ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY);\n+\n+        final DeterministicTaskQueue deterministicTaskQueue = new DeterministicTaskQueue(localNodeSettings, random());\n+\n+        final Metadata metadata = Metadata.builder()\n+            .put(\n+                IndexMetadata.builder(shardId.getIndexName())\n+                    .settings(\n+                        settings(Version.CURRENT).put(\n+                            ExistingShardsAllocator.EXISTING_SHARDS_ALLOCATOR_SETTING.getKey(),\n+                            SearchableSnapshotAllocator.ALLOCATOR_NAME\n+                        )\n+                    )\n+                    .numberOfShards(1)\n+                    .numberOfReplicas(0)\n+                    .putInSyncAllocationIds(shardId.id(), Collections.emptySet())\n+            )\n+            .build();\n+        final RoutingTable.Builder routingTableBuilder = RoutingTable.builder();\n+        routingTableBuilder.addAsRestore(metadata.index(shardId.getIndex()), randomSnapshotSource(shardId));\n+\n+        final DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder();\n+        for (DiscoveryNode node : nodes) {\n+            nodesBuilder.add(node);\n+        }\n+        final DiscoveryNodes discoveryNodes = nodesBuilder.build();\n+        final ClusterState state = ClusterState.builder(clusterName)\n+            .metadata(metadata)\n+            .routingTable(routingTableBuilder.build())\n+            .nodes(discoveryNodes)\n+            .build();\n+        final long shardSize = randomNonNegativeLong();\n+        final RoutingAllocation allocation = new RoutingAllocation(\n+            yesAllocationDeciders(),\n+            new RoutingNodes(state, false),\n+            state,\n+            null,\n+            new SnapshotShardSizeInfo(ImmutableOpenMap.of()) {\n+                @Override\n+                public Long getShardSize(ShardRouting shardRouting) {\n+                    return shardSize;\n+                }\n+            },\n+            TimeUnit.MILLISECONDS.toNanos(deterministicTaskQueue.getCurrentTimeMillis())\n+        );\n+\n+        final AtomicInteger reroutesTriggered = new AtomicInteger(0);\n+\n+        final Map<DiscoveryNode, Long> existingCacheSizes = nodes.stream()\n+            .collect(Collectors.toUnmodifiableMap(Function.identity(), k -> randomLongBetween(0, shardSize)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzY2NDk3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjowMzozMFrOIGHhSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjowMzozMFrOIGHhSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI4NTU3Ng==", "bodyText": "should we assert SearchableSnapshotsConstants.isSearchableSnapshotStore(indexSettings), just in case?", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543285576", "createdAt": "2020-12-15T12:03:30Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/SearchableSnapshotAllocator.java", "diffHunk": "@@ -103,16 +215,239 @@ public AllocateUnassignedDecision explainUnassignedShardAllocation(ShardRouting\n     }\n \n     @Override\n-    public void cleanCaches() {}\n+    public void cleanCaches() {\n+        asyncFetchStore.clear();\n+    }\n \n     @Override\n-    public void applyStartedShards(List<ShardRouting> startedShards, RoutingAllocation allocation) {}\n+    public void applyStartedShards(List<ShardRouting> startedShards, RoutingAllocation allocation) {\n+        for (ShardRouting startedShard : startedShards) {\n+            asyncFetchStore.remove(startedShard.shardId());\n+        }\n+    }\n \n     @Override\n-    public void applyFailedShards(List<FailedShard> failedShards, RoutingAllocation allocation) {}\n+    public void applyFailedShards(List<FailedShard> failedShards, RoutingAllocation allocation) {\n+        for (FailedShard failedShard : failedShards) {\n+            asyncFetchStore.remove(failedShard.getRoutingEntry().shardId());\n+        }\n+    }\n \n     @Override\n     public int getNumberOfInFlightFetches() {\n-        return 0;\n+        int count = 0;\n+        for (AsyncCacheStatusFetch fetch : asyncFetchStore.values()) {\n+            count += fetch.numberOfInFlightFetches();\n+        }\n+        return count;\n+    }\n+\n+    private AsyncShardFetch.FetchResult<NodeCacheFilesMetadata> fetchData(ShardRouting shard, RoutingAllocation allocation) {\n+        final ShardId shardId = shard.shardId();\n+        final Settings indexSettings = allocation.metadata().index(shard.index()).getSettings();\n+        final SnapshotId snapshotId = new SnapshotId(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzcwNDcyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjoxMzoxNVrOIGH4NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjoxMzoxNVrOIGH4NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzI5MTQ0NQ==", "bodyText": "I think we should check that the shard does not belong to a shard that has just been deleted (see ShardEviction and evictedShards) but that would also mean to transport the snapshot index name", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543291445", "createdAt": "2020-12-15T12:13:15Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/CacheService.java", "diffHunk": "@@ -283,6 +283,16 @@ public CacheFile get(final CacheKey cacheKey, final long fileLength, final Path\n         });\n     }\n \n+    /**\n+     * Get the number of bytes cached for the given shard id in the given snapshot id.\n+     * @param shardId    shard id\n+     * @param snapshotId snapshot id\n+     * @return number of bytes cached\n+     */\n+    public long getCachedSize(ShardId shardId, SnapshotId snapshotId) {\n+        return persistentCache.getCacheSize(shardId, snapshotId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMzgxNjM4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0MDo0NFrOIGI5wQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQxMjo0Mzo0NFrOIGJBeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwODIyNQ==", "bodyText": "There is a persistent cache Lucene index per node data path and a data node can have more than one data paths. A shard is assigned to one of the data path and its cache files will be indexed into the persistent cache Lucene index that exist on that node path.\nWhat you can do here is iterating over CacheIndexWriter writers and stop if the search query returns at least a document.", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543308225", "createdAt": "2020-12-15T12:40:44Z", "author": {"login": "tlrx"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -127,6 +136,43 @@ public void removeCacheFile(CacheFile cacheFile) throws IOException {\n         getWriter(cacheFile).deleteCacheFile(cacheFile);\n     }\n \n+    public long getCacheSize(ShardId shardId, SnapshotId snapshotId) {\n+        long aggregateSize = 0L;\n+        final CacheIndexWriter writer = writers.get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMxMDIwMQ==", "bodyText": "\ud83e\udd26 right that makes more sense, thanks for spotting!", "url": "https://github.com/elastic/elasticsearch/pull/66237#discussion_r543310201", "createdAt": "2020-12-15T12:43:44Z", "author": {"login": "original-brownbear"}, "path": "x-pack/plugin/searchable-snapshots/src/main/java/org/elasticsearch/xpack/searchablesnapshots/cache/PersistentCache.java", "diffHunk": "@@ -127,6 +136,43 @@ public void removeCacheFile(CacheFile cacheFile) throws IOException {\n         getWriter(cacheFile).deleteCacheFile(cacheFile);\n     }\n \n+    public long getCacheSize(ShardId shardId, SnapshotId snapshotId) {\n+        long aggregateSize = 0L;\n+        final CacheIndexWriter writer = writers.get(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzMwODIyNQ=="}, "originalCommit": {"oid": "f3e2ba1cab9a1155e87f09ea8b9575489d5f4017"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4658, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}