{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE4MjE0NjY4", "number": 56789, "title": "Save memory on numeric sig terms when not top", "bodyText": "This saves memory when running numeric significant terms which are not\nat the top level by merging its collection into numeric terms and relying\non the optimization that we made in #55873.", "createdAt": "2020-05-14T19:55:19Z", "url": "https://github.com/elastic/elasticsearch/pull/56789", "merged": true, "mergeCommit": {"oid": "9aaab6efddbfbc721990a4249811ed83ffed9e4c"}, "closed": true, "closedAt": "2020-05-27T14:53:10Z", "author": {"login": "nik9000"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABchS9q1AH2gAyNDE4MjE0NjY4OjE4Mzg3YjYxM2RmYmYwMGE2MjFiM2U3OGE4YzU2MWNlNWQxNWI2Zjg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcjLiVCgH2gAyNDE4MjE0NjY4OjM2MGM0MjkyOTUwZjgzN2U3NzFiNTg5YTc5YTdkZDhjYjY3MjJkZjQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "18387b613dfbf00a621b3e78a8c561ce5d15b6f8", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/18387b613dfbf00a621b3e78a8c561ce5d15b6f8", "committedDate": "2020-05-14T19:51:14Z", "message": "Save memory on numeric sig terms when not top\n\nThis saves memory when running numeric significant terms which are not\nat the top level by merging its collection into numeric terms and relying\non the optimization that we made in #55873."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f61504a168fcd94cf0bd6315bc766a4bcf41f265", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/f61504a168fcd94cf0bd6315bc766a4bcf41f265", "committedDate": "2020-05-14T21:20:04Z", "message": "Code formating"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ecb22d89f1ecf544daf380b2a3a0524fa900713e", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/ecb22d89f1ecf544daf380b2a3a0524fa900713e", "committedDate": "2020-05-14T21:47:19Z", "message": "Sneaky tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91e1c83982f4e1868abe9c66fd7d02b72fa9f870", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/91e1c83982f4e1868abe9c66fd7d02b72fa9f870", "committedDate": "2020-05-15T12:46:30Z", "message": "More normal"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5dc77f6a22fbf5a7d2215dc408f9c9c8d66be0b1", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/5dc77f6a22fbf5a7d2215dc408f9c9c8d66be0b1", "committedDate": "2020-05-15T17:41:42Z", "message": "Go fast plz k thnx"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a5d90c6b545cfd4a954f42a7014dc983a90d5f6", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/5a5d90c6b545cfd4a954f42a7014dc983a90d5f6", "committedDate": "2020-05-16T18:49:03Z", "message": "Merge branch 'master' into sig_terms_cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7426a8ef50e9768944967e8300504a7186f9d2d1", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/7426a8ef50e9768944967e8300504a7186f9d2d1", "committedDate": "2020-05-16T19:02:01Z", "message": "Drop cache at top level"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzOTE3MzU3", "url": "https://github.com/elastic/elasticsearch/pull/56789#pullrequestreview-413917357", "createdAt": "2020-05-18T20:30:11Z", "commit": {"oid": "7426a8ef50e9768944967e8300504a7186f9d2d1"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMDo1NDoxMFrOGXyPiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQyMToxMjo1MFrOGXy04A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU5MzYxMQ==", "bodyText": "Could we slap a TODO/warning on this?  I put a warning on the old updateScore() method because it violates the bucket immutability that pretty much all other aggs have, and can lead to some really subtle bugs if you're not careful.  And the new setup means it could affect any of the terms aggs, not just sigterms, so the surface area of accidental misuse is a lot larger.\nOld warning: https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalSignificantTerms.java#L107", "url": "https://github.com/elastic/elasticsearch/pull/56789#discussion_r427593611", "createdAt": "2020-05-19T20:54:10Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/NumericTermsAggregator.java", "diffHunk": "@@ -0,0 +1,609 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.search.aggregations.bucket.terms;\n+\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SortedNumericDocValues;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.util.NumericUtils;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.util.BigArrays;\n+import org.elasticsearch.common.util.LongArray;\n+import org.elasticsearch.common.util.LongHash;\n+import org.elasticsearch.index.fielddata.FieldData;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.Aggregator;\n+import org.elasticsearch.search.aggregations.AggregatorFactories;\n+import org.elasticsearch.search.aggregations.BucketOrder;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.InternalOrder;\n+import org.elasticsearch.search.aggregations.LeafBucketCollector;\n+import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;\n+import org.elasticsearch.search.aggregations.bucket.terms.IncludeExclude.LongFilter;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds.BucketOrdsEnum;\n+import org.elasticsearch.search.aggregations.bucket.terms.heuristic.SignificanceHeuristic;\n+import org.elasticsearch.search.aggregations.support.ValuesSource;\n+import org.elasticsearch.search.internal.ContextIndexSearcher;\n+import org.elasticsearch.search.internal.SearchContext;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+import static java.util.Collections.emptyList;\n+\n+public class NumericTermsAggregator extends TermsAggregator {\n+    private final ResultStrategy<?, ?> resultStrategy;\n+    private final ValuesSource.Numeric valuesSource;\n+    private final LongKeyedBucketOrds bucketOrds;\n+    private final LongFilter longFilter;\n+\n+    public NumericTermsAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        Function<NumericTermsAggregator, ResultStrategy<?, ?>> resultStrategy,\n+        ValuesSource.Numeric valuesSource,\n+        DocValueFormat format,\n+        BucketOrder order,\n+        BucketCountThresholds bucketCountThresholds,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        SubAggCollectionMode subAggCollectMode,\n+        IncludeExclude.LongFilter longFilter,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    )\n+        throws IOException {\n+        super(name, factories, aggregationContext, parent, bucketCountThresholds, order, format, subAggCollectMode, metadata);\n+        this.resultStrategy = resultStrategy.apply(this);\n+        this.valuesSource = valuesSource;\n+        this.longFilter = longFilter;\n+        bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+    }\n+\n+    @Override\n+    public ScoreMode scoreMode() {\n+        if (valuesSource != null && valuesSource.needsScores()) {\n+            return ScoreMode.COMPLETE;\n+        }\n+        return super.scoreMode();\n+    }\n+\n+    @Override\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+        SortedNumericDocValues values = resultStrategy.getValues(ctx);\n+        return resultStrategy.wrapCollector(new LeafBucketCollectorBase(sub, values) {\n+            @Override\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n+                if (values.advanceExact(doc)) {\n+                    int valuesCount = values.docValueCount();\n+\n+                    long previous = Long.MAX_VALUE;\n+                    for (int i = 0; i < valuesCount; ++i) {\n+                        long val = values.nextValue();\n+                        if (previous != val || i == 0) {\n+                            if ((longFilter == null) || (longFilter.accept(val))) {\n+                                long bucketOrdinal = bucketOrds.add(owningBucketOrd, val);\n+                                if (bucketOrdinal < 0) { // already seen\n+                                    bucketOrdinal = -1 - bucketOrdinal;\n+                                    collectExistingBucket(sub, doc, bucketOrdinal);\n+                                } else {\n+                                    collectBucket(sub, doc, bucketOrdinal);\n+                                }\n+                            }\n+\n+                            previous = val;\n+                        }\n+                    }\n+                }\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        return resultStrategy.buildAggregations(owningBucketOrds);\n+    }\n+\n+    @Override\n+    public InternalAggregation buildEmptyAggregation() {\n+        return resultStrategy.buildEmptyResult();\n+    }\n+\n+    @Override\n+    public void doClose() {\n+        Releasables.close(super::doClose, bucketOrds, resultStrategy);\n+    }\n+\n+    @Override\n+    public void collectDebugInfo(BiConsumer<String, Object> add) {\n+        super.collectDebugInfo(add);\n+        add.accept(\"result_strategy\", resultStrategy.describe());\n+        add.accept(\"total_buckets\", bucketOrds.size());\n+    }\n+\n+    /**\n+     * Strategy for building results.\n+     */\n+    abstract class ResultStrategy<R extends InternalAggregation, B extends InternalMultiBucketAggregation.InternalBucket>\n+        implements\n+            Releasable {\n+        private InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            B[][] topBucketsPerOrd = buildTopBucketsPerOrd(owningBucketOrds.length);\n+            long[] otherDocCounts = new long[owningBucketOrds.length];\n+            for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) {\n+                collectZeroDocEntriesIfNeeded(owningBucketOrds[ordIdx]);\n+                long bucketsInOrd = bucketOrds.bucketsInOrd(owningBucketOrds[ordIdx]);\n+\n+                int size = (int) Math.min(bucketsInOrd, bucketCountThresholds.getShardSize());\n+                PriorityQueue<B> ordered = buildPriorityQueue(size);\n+                B spare = null;\n+                BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrds[ordIdx]);\n+                Supplier<B> emptyBucketBuilder = emptyBucketBuilder(owningBucketOrds[ordIdx]);\n+                while (ordsEnum.next()) {\n+                    if (spare == null) {\n+                        spare = emptyBucketBuilder.get();\n+                    }\n+                    long docCount = bucketDocCount(ordsEnum.ord());\n+                    otherDocCounts[ordIdx] += docCount;\n+                    if (bucketCountThresholds.getShardMinDocCount() <= docCount) {\n+                        updateBucket(spare, ordsEnum, docCount);\n+                        spare = ordered.insertWithOverflow(spare);\n+                        if (spare == null) {\n+                            consumeBucketsAndMaybeBreak(1);\n+                        }\n+                    }\n+                }\n+\n+                // Get the top buckets\n+                B[] bucketsForOrd = buildBuckets(ordered.size());\n+                topBucketsPerOrd[ordIdx] = bucketsForOrd;\n+                for (int b = ordered.size() - 1; b >= 0; --b) {\n+                    topBucketsPerOrd[ordIdx][b] = ordered.pop();\n+                    otherDocCounts[ordIdx] -= topBucketsPerOrd[ordIdx][b].getDocCount();\n+                }\n+            }\n+\n+            buildSubAggs(topBucketsPerOrd);\n+\n+            InternalAggregation[] result = new InternalAggregation[owningBucketOrds.length];\n+            for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) {\n+                result[ordIdx] = buildResult(owningBucketOrds[ordIdx], otherDocCounts[ordIdx], topBucketsPerOrd[ordIdx]);\n+            }\n+            return result;\n+        }\n+\n+        abstract String describe();\n+\n+        abstract SortedNumericDocValues getValues(LeafReaderContext ctx) throws IOException;\n+\n+        abstract LeafBucketCollector wrapCollector(LeafBucketCollector primary);\n+\n+        abstract B[][] buildTopBucketsPerOrd(int size);\n+\n+        abstract B[] buildBuckets(int size);\n+\n+        abstract Supplier<B> emptyBucketBuilder(long owningBucketOrd);\n+\n+        abstract void updateBucket(B spare, BucketOrdsEnum ordsEnum, long docCount) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7426a8ef50e9768944967e8300504a7186f9d2d1"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU5NTk2Nw==", "bodyText": "Could we add some light javadocs to a few of these abstract methods?  Still working through them so perhaps they are self-explanatory in code (or by method name).  buildResult / buildBuckets buildSubAggs / buildResult / buildEmptyResult, etc", "url": "https://github.com/elastic/elasticsearch/pull/56789#discussion_r427595967", "createdAt": "2020-05-19T20:58:33Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/NumericTermsAggregator.java", "diffHunk": "@@ -0,0 +1,609 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.search.aggregations.bucket.terms;\n+\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SortedNumericDocValues;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.util.NumericUtils;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.util.BigArrays;\n+import org.elasticsearch.common.util.LongArray;\n+import org.elasticsearch.common.util.LongHash;\n+import org.elasticsearch.index.fielddata.FieldData;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.Aggregator;\n+import org.elasticsearch.search.aggregations.AggregatorFactories;\n+import org.elasticsearch.search.aggregations.BucketOrder;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.InternalOrder;\n+import org.elasticsearch.search.aggregations.LeafBucketCollector;\n+import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;\n+import org.elasticsearch.search.aggregations.bucket.terms.IncludeExclude.LongFilter;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds.BucketOrdsEnum;\n+import org.elasticsearch.search.aggregations.bucket.terms.heuristic.SignificanceHeuristic;\n+import org.elasticsearch.search.aggregations.support.ValuesSource;\n+import org.elasticsearch.search.internal.ContextIndexSearcher;\n+import org.elasticsearch.search.internal.SearchContext;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+import static java.util.Collections.emptyList;\n+\n+public class NumericTermsAggregator extends TermsAggregator {\n+    private final ResultStrategy<?, ?> resultStrategy;\n+    private final ValuesSource.Numeric valuesSource;\n+    private final LongKeyedBucketOrds bucketOrds;\n+    private final LongFilter longFilter;\n+\n+    public NumericTermsAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        Function<NumericTermsAggregator, ResultStrategy<?, ?>> resultStrategy,\n+        ValuesSource.Numeric valuesSource,\n+        DocValueFormat format,\n+        BucketOrder order,\n+        BucketCountThresholds bucketCountThresholds,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        SubAggCollectionMode subAggCollectMode,\n+        IncludeExclude.LongFilter longFilter,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    )\n+        throws IOException {\n+        super(name, factories, aggregationContext, parent, bucketCountThresholds, order, format, subAggCollectMode, metadata);\n+        this.resultStrategy = resultStrategy.apply(this);\n+        this.valuesSource = valuesSource;\n+        this.longFilter = longFilter;\n+        bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+    }\n+\n+    @Override\n+    public ScoreMode scoreMode() {\n+        if (valuesSource != null && valuesSource.needsScores()) {\n+            return ScoreMode.COMPLETE;\n+        }\n+        return super.scoreMode();\n+    }\n+\n+    @Override\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+        SortedNumericDocValues values = resultStrategy.getValues(ctx);\n+        return resultStrategy.wrapCollector(new LeafBucketCollectorBase(sub, values) {\n+            @Override\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n+                if (values.advanceExact(doc)) {\n+                    int valuesCount = values.docValueCount();\n+\n+                    long previous = Long.MAX_VALUE;\n+                    for (int i = 0; i < valuesCount; ++i) {\n+                        long val = values.nextValue();\n+                        if (previous != val || i == 0) {\n+                            if ((longFilter == null) || (longFilter.accept(val))) {\n+                                long bucketOrdinal = bucketOrds.add(owningBucketOrd, val);\n+                                if (bucketOrdinal < 0) { // already seen\n+                                    bucketOrdinal = -1 - bucketOrdinal;\n+                                    collectExistingBucket(sub, doc, bucketOrdinal);\n+                                } else {\n+                                    collectBucket(sub, doc, bucketOrdinal);\n+                                }\n+                            }\n+\n+                            previous = val;\n+                        }\n+                    }\n+                }\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        return resultStrategy.buildAggregations(owningBucketOrds);\n+    }\n+\n+    @Override\n+    public InternalAggregation buildEmptyAggregation() {\n+        return resultStrategy.buildEmptyResult();\n+    }\n+\n+    @Override\n+    public void doClose() {\n+        Releasables.close(super::doClose, bucketOrds, resultStrategy);\n+    }\n+\n+    @Override\n+    public void collectDebugInfo(BiConsumer<String, Object> add) {\n+        super.collectDebugInfo(add);\n+        add.accept(\"result_strategy\", resultStrategy.describe());\n+        add.accept(\"total_buckets\", bucketOrds.size());\n+    }\n+\n+    /**\n+     * Strategy for building results.\n+     */\n+    abstract class ResultStrategy<R extends InternalAggregation, B extends InternalMultiBucketAggregation.InternalBucket>\n+        implements\n+            Releasable {\n+        private InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            B[][] topBucketsPerOrd = buildTopBucketsPerOrd(owningBucketOrds.length);\n+            long[] otherDocCounts = new long[owningBucketOrds.length];\n+            for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) {\n+                collectZeroDocEntriesIfNeeded(owningBucketOrds[ordIdx]);\n+                long bucketsInOrd = bucketOrds.bucketsInOrd(owningBucketOrds[ordIdx]);\n+\n+                int size = (int) Math.min(bucketsInOrd, bucketCountThresholds.getShardSize());\n+                PriorityQueue<B> ordered = buildPriorityQueue(size);\n+                B spare = null;\n+                BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrds[ordIdx]);\n+                Supplier<B> emptyBucketBuilder = emptyBucketBuilder(owningBucketOrds[ordIdx]);\n+                while (ordsEnum.next()) {\n+                    if (spare == null) {\n+                        spare = emptyBucketBuilder.get();\n+                    }\n+                    long docCount = bucketDocCount(ordsEnum.ord());\n+                    otherDocCounts[ordIdx] += docCount;\n+                    if (bucketCountThresholds.getShardMinDocCount() <= docCount) {\n+                        updateBucket(spare, ordsEnum, docCount);\n+                        spare = ordered.insertWithOverflow(spare);\n+                        if (spare == null) {\n+                            consumeBucketsAndMaybeBreak(1);\n+                        }\n+                    }\n+                }\n+\n+                // Get the top buckets\n+                B[] bucketsForOrd = buildBuckets(ordered.size());\n+                topBucketsPerOrd[ordIdx] = bucketsForOrd;\n+                for (int b = ordered.size() - 1; b >= 0; --b) {\n+                    topBucketsPerOrd[ordIdx][b] = ordered.pop();\n+                    otherDocCounts[ordIdx] -= topBucketsPerOrd[ordIdx][b].getDocCount();\n+                }\n+            }\n+\n+            buildSubAggs(topBucketsPerOrd);\n+\n+            InternalAggregation[] result = new InternalAggregation[owningBucketOrds.length];\n+            for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) {\n+                result[ordIdx] = buildResult(owningBucketOrds[ordIdx], otherDocCounts[ordIdx], topBucketsPerOrd[ordIdx]);\n+            }\n+            return result;\n+        }\n+\n+        abstract String describe();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7426a8ef50e9768944967e8300504a7186f9d2d1"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzU5ODY2MA==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/elastic/elasticsearch/pull/56789#discussion_r427598660", "createdAt": "2020-05-19T21:03:42Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/NumericTermsAggregator.java", "diffHunk": "@@ -0,0 +1,609 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.elasticsearch.search.aggregations.bucket.terms;\n+\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.LeafReaderContext;\n+import org.apache.lucene.index.SortedNumericDocValues;\n+import org.apache.lucene.search.ScoreMode;\n+import org.apache.lucene.util.NumericUtils;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.elasticsearch.common.lease.Releasable;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.util.BigArrays;\n+import org.elasticsearch.common.util.LongArray;\n+import org.elasticsearch.common.util.LongHash;\n+import org.elasticsearch.index.fielddata.FieldData;\n+import org.elasticsearch.search.DocValueFormat;\n+import org.elasticsearch.search.aggregations.Aggregator;\n+import org.elasticsearch.search.aggregations.AggregatorFactories;\n+import org.elasticsearch.search.aggregations.BucketOrder;\n+import org.elasticsearch.search.aggregations.InternalAggregation;\n+import org.elasticsearch.search.aggregations.InternalMultiBucketAggregation;\n+import org.elasticsearch.search.aggregations.InternalOrder;\n+import org.elasticsearch.search.aggregations.LeafBucketCollector;\n+import org.elasticsearch.search.aggregations.LeafBucketCollectorBase;\n+import org.elasticsearch.search.aggregations.bucket.terms.IncludeExclude.LongFilter;\n+import org.elasticsearch.search.aggregations.bucket.terms.LongKeyedBucketOrds.BucketOrdsEnum;\n+import org.elasticsearch.search.aggregations.bucket.terms.heuristic.SignificanceHeuristic;\n+import org.elasticsearch.search.aggregations.support.ValuesSource;\n+import org.elasticsearch.search.internal.ContextIndexSearcher;\n+import org.elasticsearch.search.internal.SearchContext;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.BiConsumer;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+\n+import static java.util.Collections.emptyList;\n+\n+public class NumericTermsAggregator extends TermsAggregator {\n+    private final ResultStrategy<?, ?> resultStrategy;\n+    private final ValuesSource.Numeric valuesSource;\n+    private final LongKeyedBucketOrds bucketOrds;\n+    private final LongFilter longFilter;\n+\n+    public NumericTermsAggregator(\n+        String name,\n+        AggregatorFactories factories,\n+        Function<NumericTermsAggregator, ResultStrategy<?, ?>> resultStrategy,\n+        ValuesSource.Numeric valuesSource,\n+        DocValueFormat format,\n+        BucketOrder order,\n+        BucketCountThresholds bucketCountThresholds,\n+        SearchContext aggregationContext,\n+        Aggregator parent,\n+        SubAggCollectionMode subAggCollectMode,\n+        IncludeExclude.LongFilter longFilter,\n+        boolean collectsFromSingleBucket,\n+        Map<String, Object> metadata\n+    )\n+        throws IOException {\n+        super(name, factories, aggregationContext, parent, bucketCountThresholds, order, format, subAggCollectMode, metadata);\n+        this.resultStrategy = resultStrategy.apply(this);\n+        this.valuesSource = valuesSource;\n+        this.longFilter = longFilter;\n+        bucketOrds = LongKeyedBucketOrds.build(context.bigArrays(), collectsFromSingleBucket);\n+    }\n+\n+    @Override\n+    public ScoreMode scoreMode() {\n+        if (valuesSource != null && valuesSource.needsScores()) {\n+            return ScoreMode.COMPLETE;\n+        }\n+        return super.scoreMode();\n+    }\n+\n+    @Override\n+    public LeafBucketCollector getLeafCollector(LeafReaderContext ctx, LeafBucketCollector sub) throws IOException {\n+        SortedNumericDocValues values = resultStrategy.getValues(ctx);\n+        return resultStrategy.wrapCollector(new LeafBucketCollectorBase(sub, values) {\n+            @Override\n+            public void collect(int doc, long owningBucketOrd) throws IOException {\n+                if (values.advanceExact(doc)) {\n+                    int valuesCount = values.docValueCount();\n+\n+                    long previous = Long.MAX_VALUE;\n+                    for (int i = 0; i < valuesCount; ++i) {\n+                        long val = values.nextValue();\n+                        if (previous != val || i == 0) {\n+                            if ((longFilter == null) || (longFilter.accept(val))) {\n+                                long bucketOrdinal = bucketOrds.add(owningBucketOrd, val);\n+                                if (bucketOrdinal < 0) { // already seen\n+                                    bucketOrdinal = -1 - bucketOrdinal;\n+                                    collectExistingBucket(sub, doc, bucketOrdinal);\n+                                } else {\n+                                    collectBucket(sub, doc, bucketOrdinal);\n+                                }\n+                            }\n+\n+                            previous = val;\n+                        }\n+                    }\n+                }\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+        return resultStrategy.buildAggregations(owningBucketOrds);\n+    }\n+\n+    @Override\n+    public InternalAggregation buildEmptyAggregation() {\n+        return resultStrategy.buildEmptyResult();\n+    }\n+\n+    @Override\n+    public void doClose() {\n+        Releasables.close(super::doClose, bucketOrds, resultStrategy);\n+    }\n+\n+    @Override\n+    public void collectDebugInfo(BiConsumer<String, Object> add) {\n+        super.collectDebugInfo(add);\n+        add.accept(\"result_strategy\", resultStrategy.describe());\n+        add.accept(\"total_buckets\", bucketOrds.size());\n+    }\n+\n+    /**\n+     * Strategy for building results.\n+     */\n+    abstract class ResultStrategy<R extends InternalAggregation, B extends InternalMultiBucketAggregation.InternalBucket>\n+        implements\n+            Releasable {\n+        private InternalAggregation[] buildAggregations(long[] owningBucketOrds) throws IOException {\n+            B[][] topBucketsPerOrd = buildTopBucketsPerOrd(owningBucketOrds.length);\n+            long[] otherDocCounts = new long[owningBucketOrds.length];\n+            for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) {\n+                collectZeroDocEntriesIfNeeded(owningBucketOrds[ordIdx]);\n+                long bucketsInOrd = bucketOrds.bucketsInOrd(owningBucketOrds[ordIdx]);\n+\n+                int size = (int) Math.min(bucketsInOrd, bucketCountThresholds.getShardSize());\n+                PriorityQueue<B> ordered = buildPriorityQueue(size);\n+                B spare = null;\n+                BucketOrdsEnum ordsEnum = bucketOrds.ordsEnum(owningBucketOrds[ordIdx]);\n+                Supplier<B> emptyBucketBuilder = emptyBucketBuilder(owningBucketOrds[ordIdx]);\n+                while (ordsEnum.next()) {\n+                    if (spare == null) {\n+                        spare = emptyBucketBuilder.get();\n+                    }\n+                    long docCount = bucketDocCount(ordsEnum.ord());\n+                    otherDocCounts[ordIdx] += docCount;\n+                    if (bucketCountThresholds.getShardMinDocCount() <= docCount) {\n+                        updateBucket(spare, ordsEnum, docCount);\n+                        spare = ordered.insertWithOverflow(spare);\n+                        if (spare == null) {\n+                            consumeBucketsAndMaybeBreak(1);\n+                        }\n+                    }\n+                }\n+\n+                // Get the top buckets\n+                B[] bucketsForOrd = buildBuckets(ordered.size());\n+                topBucketsPerOrd[ordIdx] = bucketsForOrd;\n+                for (int b = ordered.size() - 1; b >= 0; --b) {\n+                    topBucketsPerOrd[ordIdx][b] = ordered.pop();\n+                    otherDocCounts[ordIdx] -= topBucketsPerOrd[ordIdx][b].getDocCount();\n+                }\n+            }\n+\n+            buildSubAggs(topBucketsPerOrd);\n+\n+            InternalAggregation[] result = new InternalAggregation[owningBucketOrds.length];\n+            for (int ordIdx = 0; ordIdx < owningBucketOrds.length; ordIdx++) {\n+                result[ordIdx] = buildResult(owningBucketOrds[ordIdx], otherDocCounts[ordIdx], topBucketsPerOrd[ordIdx]);\n+            }\n+            return result;\n+        }\n+\n+        abstract String describe();\n+\n+        abstract SortedNumericDocValues getValues(LeafReaderContext ctx) throws IOException;\n+\n+        abstract LeafBucketCollector wrapCollector(LeafBucketCollector primary);\n+\n+        abstract B[][] buildTopBucketsPerOrd(int size);\n+\n+        abstract B[] buildBuckets(int size);\n+\n+        abstract Supplier<B> emptyBucketBuilder(long owningBucketOrd);\n+\n+        abstract void updateBucket(B spare, BucketOrdsEnum ordsEnum, long docCount) throws IOException;\n+\n+        abstract PriorityQueue<B> buildPriorityQueue(int size);\n+\n+        abstract void buildSubAggs(B[][] topBucketsPerOrd) throws IOException;\n+\n+        abstract void collectZeroDocEntriesIfNeeded(long ord) throws IOException;\n+\n+        abstract R buildResult(long owningBucketOrd, long otherDocCounts, B[] topBuckets);\n+\n+        abstract R buildEmptyResult();\n+    }\n+\n+    abstract class StandardTermsResultStrategy<R extends InternalMappedTerms<R, B>, B extends InternalTerms.Bucket<B>> extends\n+        ResultStrategy<R, B> {\n+        protected final boolean showTermDocCountError;\n+\n+        StandardTermsResultStrategy(boolean showTermDocCountError) {\n+            this.showTermDocCountError = showTermDocCountError;\n+        }\n+\n+        @Override\n+        final LeafBucketCollector wrapCollector(LeafBucketCollector primary) {\n+            return primary;\n+        }\n+\n+        @Override\n+        final PriorityQueue<B> buildPriorityQueue(int size) {\n+            return new BucketPriorityQueue<>(size, partiallyBuiltBucketComparator);\n+        }\n+\n+        @Override\n+        final void buildSubAggs(B[][] topBucketsPerOrd) throws IOException {\n+            buildSubAggsForAllBuckets(topBucketsPerOrd, b -> b.bucketOrd, (b, aggs) -> b.aggregations = aggs);\n+        }\n+\n+        @Override\n+        Supplier<B> emptyBucketBuilder(long owningBucketOrd) {\n+            return this::buildEmptyBucket;\n+        }\n+\n+        abstract B buildEmptyBucket();\n+\n+        @Override\n+        final void collectZeroDocEntriesIfNeeded(long ord) throws IOException {\n+            if (bucketCountThresholds.getMinDocCount() != 0) {\n+                return;\n+            }\n+            if (InternalOrder.isCountDesc(order) && bucketOrds.bucketsInOrd(ord) >= bucketCountThresholds.getRequiredSize()) {\n+                return;\n+            }\n+            // we need to fill-in the blanks\n+            for (LeafReaderContext ctx : context.searcher().getTopReaderContext().leaves()) {\n+                SortedNumericDocValues values = getValues(ctx);\n+                for (int docId = 0; docId < ctx.reader().maxDoc(); ++docId) {\n+                    if (values.advanceExact(docId)) {\n+                        int valueCount = values.docValueCount();\n+                        for (int v = 0; v < valueCount; ++v) {\n+                            long value = values.nextValue();\n+                            if (longFilter == null || longFilter.accept(value)) {\n+                                bucketOrds.add(ord, value);\n+                            }\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        @Override\n+        public final void close() {}\n+    }\n+\n+    class LongTermsResults extends StandardTermsResultStrategy<LongTerms, LongTerms.Bucket> {\n+        LongTermsResults(boolean showTermDocCountError) {\n+            super(showTermDocCountError);\n+        }\n+\n+        @Override\n+        String describe() {\n+            return \"long_terms\";\n+        }\n+\n+        @Override\n+        SortedNumericDocValues getValues(LeafReaderContext ctx) throws IOException {\n+            return valuesSource.longValues(ctx);\n+        }\n+\n+        @Override\n+        LongTerms.Bucket[][] buildTopBucketsPerOrd(int size) {\n+            return new LongTerms.Bucket[size][];\n+        }\n+\n+        @Override\n+        LongTerms.Bucket[] buildBuckets(int size) {\n+            return new LongTerms.Bucket[size];\n+        }\n+\n+        @Override\n+        LongTerms.Bucket buildEmptyBucket() {\n+            return new LongTerms.Bucket(0, 0, null, showTermDocCountError, 0, format);\n+        }\n+\n+        @Override\n+        void updateBucket(LongTerms.Bucket spare, BucketOrdsEnum ordsEnum, long docCount) {\n+            spare.term = ordsEnum.value();\n+            spare.docCount = docCount;\n+            spare.bucketOrd = ordsEnum.ord();\n+        }\n+\n+        @Override\n+        LongTerms buildResult(long owningBucketOrd, long otherDocCount, LongTerms.Bucket[] topBuckets) {\n+            return new LongTerms(\n+                name,\n+                order,\n+                bucketCountThresholds.getRequiredSize(),\n+                bucketCountThresholds.getMinDocCount(),\n+                metadata(),\n+                format,\n+                bucketCountThresholds.getShardSize(),\n+                showTermDocCountError,\n+                otherDocCount,\n+                List.of(topBuckets),\n+                0\n+            );\n+        }\n+\n+        @Override\n+        LongTerms buildEmptyResult() {\n+            return new LongTerms(\n+                name,\n+                order,\n+                bucketCountThresholds.getRequiredSize(),\n+                bucketCountThresholds.getMinDocCount(),\n+                metadata(),\n+                format,\n+                bucketCountThresholds.getShardSize(),\n+                showTermDocCountError,\n+                0,\n+                emptyList(),\n+                0\n+            );\n+        }\n+    }\n+\n+    class DoubleTermsResults extends StandardTermsResultStrategy<DoubleTerms, DoubleTerms.Bucket> {\n+        DoubleTermsResults(boolean showTermDocCountError) {\n+            super(showTermDocCountError);\n+        }\n+\n+        @Override\n+        String describe() {\n+            return \"double_terms\";\n+        }\n+\n+        @Override\n+        SortedNumericDocValues getValues(LeafReaderContext ctx) throws IOException {\n+            return FieldData.toSortableLongBits(valuesSource.doubleValues(ctx));\n+        }\n+\n+        @Override\n+        DoubleTerms.Bucket[][] buildTopBucketsPerOrd(int size) {\n+            return new DoubleTerms.Bucket[size][];\n+        }\n+\n+        @Override\n+        DoubleTerms.Bucket[] buildBuckets(int size) {\n+            return new DoubleTerms.Bucket[size];\n+        }\n+\n+        @Override\n+        DoubleTerms.Bucket buildEmptyBucket() {\n+            return new DoubleTerms.Bucket(0, 0, null, showTermDocCountError, 0, format);\n+        }\n+\n+        @Override\n+        void updateBucket(DoubleTerms.Bucket spare, BucketOrdsEnum ordsEnum, long docCount) {\n+            spare.term = NumericUtils.sortableLongToDouble(ordsEnum.value());\n+            spare.docCount = docCount;\n+            spare.bucketOrd = ordsEnum.ord();\n+        }\n+\n+        @Override\n+        DoubleTerms buildResult(long owningBucketOrd, long otherDocCount, DoubleTerms.Bucket[] topBuckets) {\n+            return new DoubleTerms(\n+                name,\n+                order,\n+                bucketCountThresholds.getRequiredSize(),\n+                bucketCountThresholds.getMinDocCount(),\n+                metadata(),\n+                format,\n+                bucketCountThresholds.getShardSize(),\n+                showTermDocCountError,\n+                otherDocCount,\n+                List.of(topBuckets),\n+                0\n+            );\n+        }\n+\n+        @Override\n+        DoubleTerms buildEmptyResult() {\n+            return new DoubleTerms(\n+                name,\n+                order,\n+                bucketCountThresholds.getRequiredSize(),\n+                bucketCountThresholds.getMinDocCount(),\n+                metadata(),\n+                format,\n+                bucketCountThresholds.getShardSize(),\n+                showTermDocCountError,\n+                0,\n+                emptyList(),\n+                0\n+            );\n+        }\n+    }\n+\n+    class SignificantLongTermsResults extends ResultStrategy<SignificantLongTerms, SignificantLongTerms.Bucket> {\n+        private final BackgroundFrequencies backgroundFrequencies;\n+        private final long supersetSize;\n+        private final SignificanceHeuristic significanceHeuristic;\n+        private LongArray subsetSizes;\n+\n+        SignificantLongTermsResults(\n+            SignificantTermsAggregatorFactory termsAggFactory,\n+            SignificanceHeuristic significanceHeuristic,\n+            boolean collectsFromSingleBucket\n+        ) {\n+            LookupBackgroundFrequencies lookup = new LookupBackgroundFrequencies(termsAggFactory);\n+            backgroundFrequencies = collectsFromSingleBucket ? lookup : new CacheBackgroundFrequencies(lookup, context.bigArrays());\n+            supersetSize = termsAggFactory.getSupersetNumDocs();\n+            this.significanceHeuristic = significanceHeuristic;\n+            subsetSizes = context.bigArrays().newLongArray(1, true);\n+        }\n+\n+        @Override\n+        SortedNumericDocValues getValues(LeafReaderContext ctx) throws IOException {\n+            return valuesSource.longValues(ctx);\n+        }\n+\n+        @Override\n+        String describe() {\n+            return \"significant_terms\";\n+        }\n+\n+        @Override\n+        LeafBucketCollector wrapCollector(LeafBucketCollector primary) {\n+            return new LeafBucketCollectorBase(primary, null) {\n+                @Override\n+                public void collect(int doc, long owningBucketOrd) throws IOException {\n+                    super.collect(doc, owningBucketOrd);\n+                    subsetSizes = context.bigArrays().grow(subsetSizes, owningBucketOrd + 1);\n+                    subsetSizes.increment(owningBucketOrd, 1);\n+                }\n+            };\n+        }\n+\n+        @Override\n+        SignificantLongTerms.Bucket[][] buildTopBucketsPerOrd(int size) {\n+            return new SignificantLongTerms.Bucket[size][];\n+        }\n+\n+        @Override\n+        SignificantLongTerms.Bucket[] buildBuckets(int size) {\n+            return new SignificantLongTerms.Bucket[size];\n+        }\n+\n+        @Override\n+        Supplier<SignificantLongTerms.Bucket> emptyBucketBuilder(long owningBucketOrd) {\n+            long subsetSize = subsetSizes.get(owningBucketOrd);\n+            return () -> new SignificantLongTerms.Bucket(0, subsetSize, 0, supersetSize, 0, null, format, 0);\n+        }\n+\n+        @Override\n+        void updateBucket(SignificantLongTerms.Bucket spare, BucketOrdsEnum ordsEnum, long docCount) throws IOException {\n+            spare.term = ordsEnum.value();\n+            spare.subsetDf = docCount;\n+            spare.supersetDf = backgroundFrequencies.freq(spare.term);\n+            spare.bucketOrd = ordsEnum.ord();\n+            // During shard-local down-selection we use subset/superset stats that are for this shard only\n+            // Back at the central reducer these properties will be updated with global stats\n+            spare.updateScore(significanceHeuristic);\n+        }\n+\n+        @Override\n+        PriorityQueue<SignificantLongTerms.Bucket> buildPriorityQueue(int size) {\n+            return new BucketSignificancePriorityQueue<>(size);\n+        }\n+\n+        @Override\n+        void buildSubAggs(SignificantLongTerms.Bucket[][] topBucketsPerOrd) throws IOException {\n+            buildSubAggsForAllBuckets(topBucketsPerOrd, b -> b.bucketOrd, (b, aggs) -> b.aggregations = aggs);\n+        }\n+\n+        @Override\n+        void collectZeroDocEntriesIfNeeded(long ord) throws IOException {}\n+\n+        @Override\n+        SignificantLongTerms buildResult(long owningBucketOrd, long otherDocCounts, SignificantLongTerms.Bucket[] topBuckets) {\n+            return new SignificantLongTerms(\n+                name,\n+                bucketCountThresholds.getRequiredSize(),\n+                bucketCountThresholds.getMinDocCount(),\n+                metadata(),\n+                format,\n+                subsetSizes.get(owningBucketOrd),\n+                supersetSize,\n+                significanceHeuristic,\n+                List.of(topBuckets)\n+            );\n+        }\n+\n+        @Override\n+        SignificantLongTerms buildEmptyResult() {\n+            // We need to account for the significance of a miss in our global stats - provide corpus size as context\n+            ContextIndexSearcher searcher = context.searcher();\n+            IndexReader topReader = searcher.getIndexReader();\n+            int supersetSize = topReader.numDocs();\n+            return new SignificantLongTerms(\n+                name,\n+                bucketCountThresholds.getRequiredSize(),\n+                bucketCountThresholds.getMinDocCount(),\n+                metadata(),\n+                format,\n+                0,\n+                supersetSize,\n+                significanceHeuristic,\n+                emptyList()\n+            );\n+        }\n+\n+        @Override\n+        public void close() {\n+            Releasables.close(backgroundFrequencies, subsetSizes);\n+        }\n+    }\n+\n+    /**\n+     * Lookup frequencies for terms.\n+     */\n+    private interface BackgroundFrequencies extends Releasable {\n+        long freq(long term) throws IOException;\n+    }\n+\n+    /**\n+     * Lookup frequencies for terms.\n+     */\n+    private static class LookupBackgroundFrequencies implements BackgroundFrequencies {\n+        // TODO a reference to the factory is weird - probably should be reference to what we need from it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7426a8ef50e9768944967e8300504a7186f9d2d1"}, "originalPosition": 558}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzYwMzE2OA==", "bodyText": "Should we just throw an exception here?  I'm assuming relatively bad things happen if this condition is violated and the user will get garbage results (or a different exception)?", "url": "https://github.com/elastic/elasticsearch/pull/56789#discussion_r427603168", "createdAt": "2020-05-19T21:12:50Z", "author": {"login": "polyfractal"}, "path": "server/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/SignificantTermsAggregatorFactory.java", "diffHunk": "@@ -103,8 +103,10 @@ public Aggregator build(String name,\n                                     Aggregator parent,\n                                     SignificanceHeuristic significanceHeuristic,\n                                     SignificantTermsAggregatorFactory sigTermsFactory,\n+                                    boolean collectsFromSingleBucket,\n                                     Map<String, Object> metadata) throws IOException {\n \n+                assert collectsFromSingleBucket;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7426a8ef50e9768944967e8300504a7186f9d2d1"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0ODUwOTcw", "url": "https://github.com/elastic/elasticsearch/pull/56789#pullrequestreview-414850970", "createdAt": "2020-05-19T21:53:48Z", "commit": {"oid": "7426a8ef50e9768944967e8300504a7186f9d2d1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4915c9c391e05b27ed17c800c3e1a9dbaed9dec0", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/4915c9c391e05b27ed17c800c3e1a9dbaed9dec0", "committedDate": "2020-05-20T16:09:01Z", "message": "Merge branch 'master' into sig_terms_cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "360c4292950f837e771b589a79a7dd8cb6722df4", "author": {"user": {"login": "nik9000", "name": "Nik Everett"}}, "url": "https://github.com/elastic/elasticsearch/commit/360c4292950f837e771b589a79a7dd8cb6722df4", "committedDate": "2020-05-20T16:19:53Z", "message": "Javadoc"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4992, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}