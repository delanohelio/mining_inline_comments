{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxMjc0NDI4", "number": 66452, "reviewThreads": {"totalCount": 55, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMTo0NToxNVrOFGaUog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNS0wNlQwNzoxMTozNVrOF68WCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMjY3MDQyOnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMTo0NToxNVrOIHaf7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMjo0ODowNlrOINwidw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY0NTEwMA==", "bodyText": "I wonder why the API is <index>/_terms/list, why not just <index>/_terms?  Are we expecting to add more to _terms besides list?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544645100", "createdAt": "2020-12-16T21:45:15Z", "author": {"login": "mayya-sharipova"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms/list", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk3NDY4MQ==", "bodyText": "No plans but I thought it might make sense to leave things open to that", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544974681", "createdAt": "2020-12-17T10:22:11Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms/list", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY0NTEwMA=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTI5NzY1NQ==", "bodyText": "+1 for _terms only. We want to limit the number of options here so we don't need to plan for future extension imo. If we need another option, it should be added in the main API.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551297655", "createdAt": "2021-01-04T12:48:06Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms/list", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY0NTEwMA=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMjc4MjA0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMjoxNDo1NVrOIHbgEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMjoxNDo1NVrOIHbgEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2MTUyMQ==", "bodyText": "since getDocCount() returns a primitive type, may be for comparison we can just do: getDocCount() == other.gertDocCount()?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544661521", "createdAt": "2020-12-16T22:14:55Z", "author": {"login": "mayya-sharipova"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {\n+        this.term = term;\n+        this.docCount = count;\n+    }\n+\n+    public String getTerm() {\n+        return this.term;\n+    }\n+\n+    public long getDocCount() {\n+        return this.docCount;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeString(term);\n+        out.writeLong(docCount);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.field(TERM_FIELD, getTerm());\n+        builder.field(DOC_COUNT_FIELD, getDocCount());\n+        return builder;\n+    }\n+\n+    public static TermCount fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        TermCount other = (TermCount) o;\n+        return Objects.equals(getTerm(), other.getTerm()) && Objects.equals(getDocCount(), other.getDocCount());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMjgwMDE5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumResponse.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMjoxOTo1NVrOIHbqdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QxMDowNzowOVrOJQSweQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NDE4MQ==", "bodyText": "what does SKIPPED_FIELD is responsible for ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544664181", "createdAt": "2020-12-16T22:19:55Z", "author": {"login": "mayya-sharipova"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumResponse.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+/**\n+ * The response of the termenum/list action.\n+ */\n+public class TermEnumResponse extends BroadcastResponse {\n+\n+    public static final String TERMS_FIELD = \"terms\";\n+    public static final String TIMED_OUT_FIELD = \"timed_out\";\n+    public static final String SKIPPED_FIELD = \"skipped\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk3NjI1MA==", "bodyText": "Like the search API - it's what shards failed the canMatch phase.\nIf you supply an index_filter it does a canMatch on each shard before considering the terms in it. (Currently the test fails for use of index filters after I moved the code to xpack. Something about QueryBuilders not being available).", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544976250", "createdAt": "2020-12-17T10:24:35Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumResponse.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+/**\n+ * The response of the termenum/list action.\n+ */\n+public class TermEnumResponse extends BroadcastResponse {\n+\n+    public static final String TERMS_FIELD = \"terms\";\n+    public static final String TIMED_OUT_FIELD = \"timed_out\";\n+    public static final String SKIPPED_FIELD = \"skipped\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NDE4MQ=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA2NDMxMw==", "bodyText": "Removed", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621064313", "createdAt": "2021-04-27T10:07:09Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumResponse.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+/**\n+ * The response of the termenum/list action.\n+ */\n+public class TermEnumResponse extends BroadcastResponse {\n+\n+    public static final String TERMS_FIELD = \"terms\";\n+    public static final String TIMED_OUT_FIELD = \"timed_out\";\n+    public static final String SKIPPED_FIELD = \"skipped\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NDE4MQ=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyMjgxNDcyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMjoyNDowMlrOIHbzEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDo1MTo0MlrOJH-MSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw==", "bodyText": "May be too early to talk about this, but when security is enabled, who can access to this endpoint (those who can read the index))?  what about when FLS and DLS is enabled?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544666387", "createdAt": "2020-12-16T22:24:02Z", "author": {"login": "mayya-sharipova"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {\n+\n+    public static final TermEnumAction INSTANCE = new TermEnumAction();\n+//    public static final String NAME = \"indices:admin/termsenum/list\";\n+    public static final String NAME = \"indices:data/read/xpack/termsenum/list\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk3NjYyOQ==", "bodyText": "DLS and FLS is not part of this PR currently. A TBD.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544976629", "createdAt": "2020-12-17T10:25:07Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {\n+\n+    public static final TermEnumAction INSTANCE = new TermEnumAction();\n+//    public static final String NAME = \"indices:admin/termsenum/list\";\n+    public static final String NAME = \"indices:data/read/xpack/termsenum/list\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM2ODU1OA==", "bodyText": "@markharwood Thanks for the answers, Mark. Is this PR is ready for a more thorough review? or you wanted to add more/ do more modifications?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r545368558", "createdAt": "2020-12-17T20:03:12Z", "author": {"login": "mayya-sharipova"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {\n+\n+    public static final TermEnumAction INSTANCE = new TermEnumAction();\n+//    public static final String NAME = \"indices:admin/termsenum/list\";\n+    public static final String NAME = \"indices:data/read/xpack/termsenum/list\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwNzc2NA==", "bodyText": "Thanks for the review Mayya! There's still some stuff I want to add just yet. (Sorry, I should have added the WIP label).\nI need to\n\nadd logic and tests to filter slower indices based on tiers (we ignore frozen indices and searchable snapshots).\nUse a separate thread pool to the search one.\nAdd HLRC support.\n\nI'll leave DLS and FLS for a subsequent PR.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r545707764", "createdAt": "2020-12-18T09:29:26Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {\n+\n+    public static final TermEnumAction INSTANCE = new TermEnumAction();\n+//    public static final String NAME = \"indices:admin/termsenum/list\";\n+    public static final String NAME = \"indices:data/read/xpack/termsenum/list\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzODc2MA==", "bodyText": "Should it be indices:data/read/terms ? No need to add xpack reference here.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612338760", "createdAt": "2021-04-13T10:51:42Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {\n+\n+    public static final TermEnumAction INSTANCE = new TermEnumAction();\n+//    public static final String NAME = \"indices:admin/termsenum/list\";\n+    public static final String NAME = \"indices:data/read/xpack/termsenum/list\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2OTUyODcyOnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMjo0NTowNVrOINwcjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMjo0NTowNVrOINwcjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTI5NjE0MQ==", "bodyText": "I don't think we should mix the two use cases. This API is useful to list the terms that appear in a field. Allowing a prefix-based completion seems acceptable but anything beyond that would suffer at large scale. Regexes and patterns in general are costly and serve a different purpose so they shouldn't be allowed here.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551296141", "createdAt": "2021-01-04T12:45:05Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0d98c2a5895775c1b75b689bca8edd9f4518de6"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2OTU1MzM1OnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMjo1MzoyMVrOINwrAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxNDo0MzowMlrOIN0KYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTI5OTg0Mw==", "bodyText": "There are way too many options imo. The main use case is to retrieve the full list of terms from low cardinality fields so I think we should limit our options to the minimum.\nI propose the following list which is already big ;):\n\nfield\ntimeout\nsize\nprefix\nindex_filter\nsort_by_popularity\n\nWe can also handle case_insensitive if needed but that should be enough for the initial API.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551299843", "createdAt": "2021-01-04T12:53:21Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms/list\n+{\n+    \"field\" : \"tags\",\n+    \"pattern\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    {\n+      \"term\": \"kibana\",\n+      \"doc_count\": 8\n+    }\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms/list`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that match\n+a pattern. By default it looks for terms that begin with the provided\n+pattern but more complex pattern matching can be used by setting the\n+appropriate flags that control matching.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0d98c2a5895775c1b75b689bca8edd9f4518de6"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTM1NzAyNg==", "bodyText": "The design doc from @giladgal  had a stated objective of infix searching (perhaps for a later phase):\n\n\"The use case to keep in mind for this scenario would be URLs and a user that types the domain name without the \u2018http://www\u2019 prefix or the .co suffix (perhaps not knowing the exact prefix and suffix).\"\n\nI started with the search string called prefix but changed it to pattern to accommodate these plans for other search modes.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551357026", "createdAt": "2021-01-04T14:43:02Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms/list\n+{\n+    \"field\" : \"tags\",\n+    \"pattern\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    {\n+      \"term\": \"kibana\",\n+      \"doc_count\": 8\n+    }\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms/list`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that match\n+a pattern. By default it looks for terms that begin with the provided\n+pattern but more complex pattern matching can be used by setting the\n+appropriate flags that control matching.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTI5OTg0Mw=="}, "originalCommit": {"oid": "c0d98c2a5895775c1b75b689bca8edd9f4518de6"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ2OTU2NDgzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMjo1NzowMFrOINwxjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMjo1NzowMFrOINwxjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTMwMTUxOA==", "bodyText": "We shouldn't return the doc count in the final response. It's an approximation that cannot be used without an error bound so I'd rather remove it from the response.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551301518", "createdAt": "2021-01-04T12:57:00Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0d98c2a5895775c1b75b689bca8edd9f4518de6"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODgxMDYyOnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzowNDoxMFrOISAjvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xMlQxNjowNjo1MVrOJHbEYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1NDQyOQ==", "bodyText": "Should we forbid searching all data ? Plain * or omitting the pattern could return an error ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555754429", "createdAt": "2021-01-12T13:04:10Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMTc2MzI5Ng==", "bodyText": "We have the tier filtering now so this so searching all may be more viable", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r611763296", "createdAt": "2021-04-12T16:06:51Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1NDQyOQ=="}, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODgzNDQ0OnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzoxMDoyNlrOISAyDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xM1QxMToyMDo0MFrOISq3ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1ODA5NQ==", "bodyText": "We also need to know if the result is complete or not ? For small cardinality field it can be useful to know that the response contains all the possible values for that input.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555758095", "createdAt": "2021-01-12T13:10:26Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjQ0NzY4Mw==", "bodyText": "\"complete = true\" will have some caveats e.g. if we skip indices on cold nodes", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r556447683", "createdAt": "2021-01-13T11:20:40Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1ODA5NQ=="}, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODgzNjE2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/ShardTermEnumRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzoxMTowMlrOISAzNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzoxMTowMlrOISAzNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1ODM5MA==", "bodyText": "You need to change the name here too (string)  ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555758390", "createdAt": "2021-01-12T13:11:02Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/ShardTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.support.broadcast.BroadcastShardRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.search.internal.AliasFilter;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+/**\n+ * Internal termenum request executed directly against a specific index shard.\n+ */\n+public class ShardTermEnumRequest extends BroadcastShardRequest {\n+\n+    private String field;\n+    private String pattern;\n+    private long taskStartedTimeMillis;\n+    private long shardStartedTimeMillis;\n+    private AliasFilter filteringAliases;\n+    private boolean caseInsensitive;\n+    private boolean sortByPopularity;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    \n+\n+    public ShardTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a shard.\n+        shardStartedTimeMillis = System.currentTimeMillis();\n+\n+        filteringAliases = new AliasFilter(in);\n+        field = in.readString();\n+        pattern = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        sortByPopularity = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+    }\n+\n+    public ShardTermEnumRequest(ShardId shardId, AliasFilter filteringAliases, TermEnumRequest request) {\n+        super(shardId, request);\n+        this.field = request.field();\n+        this.pattern = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.sortByPopularity = request.sortByPopularity();\n+        this.filteringAliases = Objects.requireNonNull(filteringAliases, \"filteringAliases must not be null\");\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String pattern() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODg0NTIwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzoxMzoyNVrOISA4mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzoxMzoyNVrOISA4mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1OTc3MQ==", "bodyText": "Is the TODO still relevant ? The threadpool is there from what I can see.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555759771", "createdAt": "2021-01-12T13:13:25Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODg2ODY2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzoxOToyNFrOISBGeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzoxOToyNFrOISBGeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc2MzMyMQ==", "bodyText": "We should rely on node's data-tier when selecting indices and not care about index settings. The goal is to   restrict the API to some data-tiers, we can add more flexibility in the future but I'd prefer that we start simple. Only the hot and warm tier should be queried.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555763321", "createdAt": "2021-01-12T13:19:24Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODg3NTkyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzoyMToyMVrOISBKzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxNDoxNDo1MFrOISDSpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc2NDQzMQ==", "bodyText": "we should never catch an AssertionError, did you mean Exception ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555764431", "createdAt": "2021-01-12T13:21:21Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);\n+        try {\n+            IndexReader reader = searchContext.getQueryShardContext().searcher().getTopReaderContext().reader();\n+            Terms terms = MultiTerms.getTerms(reader, request.field());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+            }\n+            Automaton a = request.caseInsensitive()\n+                    ? AutomatonQueries.caseInsensitivePrefix(request.pattern())\n+                    : Automata.makeString(request.pattern());\n+            a = Operations.concatenate(a, Automata.makeAnyString());\n+            a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+\n+            // TODO make this a param and scale up based on num shards like we do with terms aggs?\n+            int shard_size = request.size();\n+\n+            CompiledAutomaton automaton = new CompiledAutomaton(a);\n+            TermsEnum te = automaton.getTermsEnum(terms);\n+\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            if (request.sortByPopularity()) {\n+                // Collect most popular matches\n+                TermCountPriorityQueue pq = new TermCountPriorityQueue(shard_size);\n+                TermCount spare = null;\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            // Gather what we have collected so far\n+                            while (pq.size() > 0) {\n+                                termsList.add(pq.pop());\n+                            }\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+\n+                    if (spare == null) {\n+                        spare = new TermCount(bytes.utf8ToString(), df);\n+                    } else {\n+                        spare.setTerm(bytes.utf8ToString());\n+                        spare.setDocCount(df);\n+                    }\n+                    spare = pq.insertWithOverflow(spare);\n+                }\n+                while (pq.size() > 0) {\n+                    termsList.add(pq.pop());\n+                }\n+            } else {\n+                // Collect in alphabetical order\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+                    termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                    if (termsList.size() >= shard_size) {\n+                        break;\n+                    }\n+                }\n+            }\n+\n+        } catch (AssertionError e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 335}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc5OTIwNg==", "bodyText": "Yep - a copy and paste from TransportValidateQueryAction", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555799206", "createdAt": "2021-01-12T14:14:50Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);\n+        try {\n+            IndexReader reader = searchContext.getQueryShardContext().searcher().getTopReaderContext().reader();\n+            Terms terms = MultiTerms.getTerms(reader, request.field());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+            }\n+            Automaton a = request.caseInsensitive()\n+                    ? AutomatonQueries.caseInsensitivePrefix(request.pattern())\n+                    : Automata.makeString(request.pattern());\n+            a = Operations.concatenate(a, Automata.makeAnyString());\n+            a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+\n+            // TODO make this a param and scale up based on num shards like we do with terms aggs?\n+            int shard_size = request.size();\n+\n+            CompiledAutomaton automaton = new CompiledAutomaton(a);\n+            TermsEnum te = automaton.getTermsEnum(terms);\n+\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            if (request.sortByPopularity()) {\n+                // Collect most popular matches\n+                TermCountPriorityQueue pq = new TermCountPriorityQueue(shard_size);\n+                TermCount spare = null;\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            // Gather what we have collected so far\n+                            while (pq.size() > 0) {\n+                                termsList.add(pq.pop());\n+                            }\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+\n+                    if (spare == null) {\n+                        spare = new TermCount(bytes.utf8ToString(), df);\n+                    } else {\n+                        spare.setTerm(bytes.utf8ToString());\n+                        spare.setDocCount(df);\n+                    }\n+                    spare = pq.insertWithOverflow(spare);\n+                }\n+                while (pq.size() > 0) {\n+                    termsList.add(pq.pop());\n+                }\n+            } else {\n+                // Collect in alphabetical order\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+                    termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                    if (termsList.size() >= shard_size) {\n+                        break;\n+                    }\n+                }\n+            }\n+\n+        } catch (AssertionError e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc2NDQzMQ=="}, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 335}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODkzMDk4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzozNTowOVrOISBsQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjozMjoxNFrOJICESw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ==", "bodyText": "In the interest of limiting the number of threads, I wonder if we can group the shard requests per node ?\nIf there are 30 shards to lookup on a single node, I think it would be better to do it in a single call and adapt the strategy globally. With the current configuration, running the request on these 30 shards would raise 18 errors out of 30 (2 can run and 10 are queued, the rest are refused).", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555772995", "createdAt": "2021-01-12T13:35:09Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "diffHunk": "@@ -171,6 +172,7 @@ public ThreadPool(final Settings settings, final ExecutorBuilder<?>... customBui\n         builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000, false));\n         builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16, false));\n         builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(allocatedProcessors), 1000, true));\n+        builders.put(Names.AUTO_COMPLETE, new FixedExecutorBuilder(settings, Names.AUTO_COMPLETE, 2, 10, true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTgwMzA3Ng==", "bodyText": "Would it best to leave unbundled, then admins can play with degrees of parallelism they want to throw at the problem by tweaking num threads and queue size settings?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555803076", "createdAt": "2021-01-12T14:18:46Z", "author": {"login": "markharwood"}, "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "diffHunk": "@@ -171,6 +172,7 @@ public ThreadPool(final Settings settings, final ExecutorBuilder<?>... customBui\n         builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000, false));\n         builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16, false));\n         builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(allocatedProcessors), 1000, true));\n+        builders.put(Names.AUTO_COMPLETE, new FixedExecutorBuilder(settings, Names.AUTO_COMPLETE, 2, 10, true));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ=="}, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI4MDM2OA==", "bodyText": "I wonder if we should start with a bigger queue size ? I'd prefer that we start with a more conservative value like 100. We also said that we'll try to use the search thread pool when the queue is empty. Can you add the idea to the meta issue ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612280368", "createdAt": "2021-04-13T09:22:14Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "diffHunk": "@@ -171,6 +172,7 @@ public ThreadPool(final Settings settings, final ExecutorBuilder<?>... customBui\n         builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000, false));\n         builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16, false));\n         builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(allocatedProcessors), 1000, true));\n+        builders.put(Names.AUTO_COMPLETE, new FixedExecutorBuilder(settings, Names.AUTO_COMPLETE, 2, 10, true));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ=="}, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQwMjI1MQ==", "bodyText": "My gut feeling is that we should have a slightly larger queue size as well, 100 sounds better to me too.\nShould we also scale the number of threads based on the number of processors? 2 threads feels too much for a single-core machine, maybe something like max(allocatedProcessors/4, 1)?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612402251", "createdAt": "2021-04-13T12:32:14Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "diffHunk": "@@ -171,6 +172,7 @@ public ThreadPool(final Settings settings, final ExecutorBuilder<?>... customBui\n         builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000, false));\n         builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16, false));\n         builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(allocatedProcessors), 1000, true));\n+        builders.put(Names.AUTO_COMPLETE, new FixedExecutorBuilder(settings, Names.AUTO_COMPLETE, 2, 10, true));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ=="}, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODk2MDc1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzo0MjoyMVrOISB-Pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzo0MjoyMVrOISB-Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3NzU5OA==", "bodyText": "You don't need a SearchContex here, you should use aQueryShardContext instead like we do in the can match phase. Also note that you could use the same QueryShardContext for the canMatchShard above, SearchService#queryStillMatchesAfterRewrite was added for this purpose.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555777598", "createdAt": "2021-01-12T13:42:21Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 259}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ5ODk2OTg2OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzo0NDozOFrOISCD4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzo0NDozOFrOISCD4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3OTA0MQ==", "bodyText": "The timeout could be checked here to save some round trip and could avoid accumulating requests on the node's thread pool ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555779041", "createdAt": "2021-01-12T13:44:38Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);\n+        try {\n+            IndexReader reader = searchContext.getQueryShardContext().searcher().getTopReaderContext().reader();\n+            Terms terms = MultiTerms.getTerms(reader, request.field());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+            }\n+            Automaton a = request.caseInsensitive()\n+                    ? AutomatonQueries.caseInsensitivePrefix(request.pattern())\n+                    : Automata.makeString(request.pattern());\n+            a = Operations.concatenate(a, Automata.makeAnyString());\n+            a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+\n+            // TODO make this a param and scale up based on num shards like we do with terms aggs?\n+            int shard_size = request.size();\n+\n+            CompiledAutomaton automaton = new CompiledAutomaton(a);\n+            TermsEnum te = automaton.getTermsEnum(terms);\n+\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            if (request.sortByPopularity()) {\n+                // Collect most popular matches\n+                TermCountPriorityQueue pq = new TermCountPriorityQueue(shard_size);\n+                TermCount spare = null;\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            // Gather what we have collected so far\n+                            while (pq.size() > 0) {\n+                                termsList.add(pq.pop());\n+                            }\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+\n+                    if (spare == null) {\n+                        spare = new TermCount(bytes.utf8ToString(), df);\n+                    } else {\n+                        spare.setTerm(bytes.utf8ToString());\n+                        spare.setDocCount(df);\n+                    }\n+                    spare = pq.insertWithOverflow(spare);\n+                }\n+                while (pq.size() > 0) {\n+                    termsList.add(pq.pop());\n+                }\n+            } else {\n+                // Collect in alphabetical order\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+                    termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                    if (termsList.size() >= shard_size) {\n+                        break;\n+                    }\n+                }\n+            }\n+\n+        } catch (AssertionError e) {\n+            error = e.getMessage();\n+        } finally {\n+            Releasables.close(searchContext);\n+        }\n+\n+        return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+    }\n+    \n+    \n+    private boolean canMatchShard(ShardTermEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(req.shardId(), req.taskStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }    \n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermEnumRequest request;\n+        private ActionListener<TermEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final GroupShardsIterator<ShardIterator> shardsIts;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray shardsResponses;\n+\n+        protected AsyncBroadcastAction(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            shardsIts = shards(clusterState, request, concreteIndices);\n+            expectedOps = shardsIts.size();\n+\n+            shardsResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (shardsIts.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray(0), clusterState, false));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int shardIndex = -1;\n+            for (final ShardIterator shardIt : shardsIts) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                shardIndex++;\n+                final ShardRouting shard = shardIt.nextOrNull();\n+                if (shard != null) {\n+                    performOperation(shardIt, shard, shardIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onOperation(null, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+                }\n+            }\n+        }\n+        \n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ( (now - task.getStartTime()) > request.timeout().getMillis() ) {\n+                finishHim(true);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final ShardIterator shardIt, final ShardRouting shard, final int shardIndex) {\n+            if (shard == null) {\n+                // no more active shards... (we should not really get here, just safety)\n+                onOperation(null, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+            } else {\n+                try {\n+                    //TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final ShardTermEnumRequest shardRequest = newShardRequest(shardIt.size(), shard, request);\n+                    shardRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(shard.currentNodeId());\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onOperation(shard, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+                    } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 444}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNDEyMDQ0OnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQxNjo0Mzo0NVrOIVw1ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQxNjo1MTo0MVrOIVxHBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTY5MTIwMw==", "bodyText": "should this be string?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r559691203", "createdAt": "2021-01-18T16:43:45Z", "author": {"login": "mayya-sharipova"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`field`::", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdac4edee0dc42dd1468a9c248fe0f790eff90e"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTY5NTYyMw==", "bodyText": "Good catch!", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r559695623", "createdAt": "2021-01-18T16:51:41Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`field`::", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTY5MTIwMw=="}, "originalCommit": {"oid": "ecdac4edee0dc42dd1468a9c248fe0f790eff90e"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU5MDk5Mzk1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQwMTo1Mjo1MVrOIffILg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wOFQxMTo0MTowMFrOIhe7aQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng==", "bodyText": "I realize this is WIP, but had one high-level thought that might affect code structure/ naming. It'd be great to encapsulate this term loading logic into MappedFieldType instead of interacting with Lucene data structures directly here. We have been trying to do this consistently, and now delegate query creation, doc value loading, and other data fetching tasks to MappedFieldType. Some benefits:\n\nField types can decide to handle the call differently. For example constant_keyword could support this functionality (even though it doesn't write them in the index).\nField aliases will be handled correctly, since we resolve them when looking up MappedFieldType\nIf there's any change to logic because of an index version change, we can consolidate that in one place", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r569886766", "createdAt": "2021-02-04T01:52:51Z", "author": {"login": "jtibshirani"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDA4NDI1Nw==", "bodyText": "Discussed with @jimczi and we agreed to add a new option to MappedFieldType to retrieve a Terms enumerator given some search criteria e.g.\nTerms getTerms(String prefix, boolean caseSensitive, int timeoutMs)\n\nThe default impl in MappedFieldType could be to return null or some empty enumerator if not supported.\nThe caller (the autocomplete service) would have timer checks as it iterated across the result to make sure it hadn't timed out so the Terms implementation returned shouldn't block for lengthy periods on individual Terms.next() calls. The mapped field type shouldn't spend too long in constructing the getTerms(...) response either and the timeoutMs is provided as an advisory notice to maybe take some shortcuts e.g. using sampling to achieve the required response time. With the proposed interface above it's not clear to me how the MappedFieldType would report back that shortcuts were made in the Terms object construction. This would be important information given the autocomplete service ultimately aims to report back if the the results were exhaustive or not. Presumably we'd need to return something other than a Terms object in this API to convey this completeness?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570084257", "createdAt": "2021-02-04T09:48:04Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDE2MzkxNA==", "bodyText": "+1 to add the logic to return Terms or TermsEnum in the MappedFiedlType. Mark we've discussed this already and I remember saying that it was important in order to handle constant_keyword and flattened correctly ;).\nIt could return an empty or null terms by default to make it clear that the API is not mandatory for each field type.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570163914", "createdAt": "2021-02-04T11:52:25Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDI0Mjk3NQ==", "bodyText": "What's your feeling on the timer question in my (edited) comment above?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570242975", "createdAt": "2021-02-04T13:58:19Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDI3MDQwOQ==", "bodyText": "I don't understand the relation with completeness ? We're talking of a terms enum that can only be used to search by prefix so the proposal is something like:\nTermsEnum getTerms(String prefix, boolean caseSensitive) or Terms getTerms() where we'd leave more room for the caller. So there's no need for each field type to implement anything regarding early termination or timeout. It's the terms enum API that wraps all these TermsEnum in a MultiTermsEnum and knows where to stop and if the result is complete or not.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570270409", "createdAt": "2021-02-04T14:33:47Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDMyMDMyMg==", "bodyText": "It's allowing for anything that might be expensive e.g. runtime fields or wildcard fields that choose to attempt something within the provided time constraints (timeout being something you said should be a parameter in this API).\nThey'd have to gather a sample of docs before returning a TermsEnum which enumerates over the discovered terms.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570320322", "createdAt": "2021-02-04T15:34:32Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDMyMzY2OQ==", "bodyText": "I think runtime fields should just return an empty TermsEnum here? Probably the same with wildcard fields tbh, they don't have a sensible terms dictionary to respond from.  Or wildcard fields could return a MultiTerms over the term dictionaries for all their doc value leaf dictionaries, which will be fast to construct even if its slower to iterate over", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570323669", "createdAt": "2021-02-04T15:38:35Z", "author": {"login": "romseygeek"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM1NzE3NQ==", "bodyText": "I don't think wildcard could do anything trustworthy whether it chooses to block on the construction or the logic that determines what to return first from terms.next();\nThat's kind of the point here - we want to communicate to users if there's the possibility of any false negatives in the results being presented. A false negative is bad because we don't want users to walk away without searching because the autocomplete results have (falsely) suggested to them there's nothing to be found.\nWe have several policy options:\n\nKibana teaches users to distrust all suggestions\nUsers generally trust results but Kibana flags any incomplete suggestions. Field types inform incompleteness by either:\na) returning all-or-nothing. A null response means users are told there's stuff missing. A non-null response means the set is assumed to be complete\nb) field types can return a subset of  terms but somehow communicate the completeness of this set.\n\nIf we assume communicating completeness is useful (and that was a design goal) we need to pick either 2a or 2b for this API contract", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570357175", "createdAt": "2021-02-04T16:19:39Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM2MTY3MA==", "bodyText": "It's allowing for anything that might be expensive e.g. runtime fields or wildcard fields that choose to attempt something within the provided time constraints (timeout being something you said should be a parameter in this API).\nThey'd have to gather a sample of docs before returning a TermsEnum which enumerates over the discovered terms.\n\nPlease no, we discussed that already and we said that it's not something we want to provide. If you have a wildcard or a runtime field then you're not eligible to the terms enum API and we return an empty enum or object. That's the part that should be clear in the javadocs of the method, return the terms enum only if you can provide the feature efficiently.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570361670", "createdAt": "2021-02-04T16:25:35Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM4OTIyOQ==", "bodyText": "OK. What confused me was the idea that timeout should be a parameter passed to this field type API and how that would be interpreted/used", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r570389229", "createdAt": "2021-02-04T17:00:35Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MTkzMjc3OA==", "bodyText": "The MappedFieldType interface may need some more thought.\nFor efficiency's sake the current PR uses a MultiReader on a node to provide a single view across potentially many shards. While quick, there are several issues here:\n\nInteger doc count limit - if we ever want to sort by popularity, the TermsEnum.docFreq() returns an integer which will potentially overflow if we try use a single view across multiple shards. Using this Lucene-based abstraction over multiple Lucene indices is not going to scale (TODO - fix in Lucene or copy Lucene's MultiTermsEnum ?)\nResource management the current one-size-fits-all logic  has a try-finally block to close readers used while iterating across Terms - the proposed new MappedFieldType API above has not considered how to close any underlying resources accessed by the returned Terms object.\nSome, all or no fusion delegation? - an open question is if the underlying field types are expected to perform any fusion of data across shards in the view offered by the returned TermsEnum. How do they know which Lucene  index or indices they should access? Are they passed one shardID or multiple? How would the caller determine that 2 field types e.g. the keyword field for host in index logs08-02-2021 and the same field in index logs09-02-2021 were merge-able by one of the field types? We would, of course, assume that MappedFieldTypes of different classes would have different implementations and so their TermsEnums would have to be merged by the caller doing the collection on a node. How would the caller fuse multiple TermEnums? The Lucene MultiTermsEnum class is constructed using ReaderSlice objects.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r571932778", "createdAt": "2021-02-08T10:25:39Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MTk4MDY0OQ==", "bodyText": "Discussed with Jim. We'll go with\nTermsEnum getTerms(String prefix, boolean caseSensitive, SearchExecutionContext context)\n\nThe caller calls this on the relevant field type for every shard and merges the TermsEnum objects using a new MultiTermsEnum class that differs from the Lucene one in that it:\n\nTakes a number of TermsEnum objects rather than ReaderSlice objects\nUses longs not ints for summing doc frequencies\nDoesn't implement TermsEnum, so doesn't offer all its methods like ord, postings etc Instead it has next(), term() and docFreq() only", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r571980649", "createdAt": "2021-02-08T11:41:00Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng=="}, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzQ3NTQ3OnYy", "diffSide": "RIGHT", "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/RequestConverters.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTowODo0NVrOJH6DIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTowODo0NVrOJH6DIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MDg4MQ==", "bodyText": "nit: remove", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612270881", "createdAt": "2021-04-13T09:08:45Z", "author": {"login": "jimczi"}, "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/RequestConverters.java", "diffHunk": "@@ -534,7 +534,7 @@ static Request rankEval(RankEvalRequest rankEvalRequest) throws IOException {\n         request.setEntity(createEntity(rankEvalRequest.getRankEvalSpec(), REQUEST_BODY_CONTENT_TYPE));\n         return request;\n     }\n-\n+    ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzQ3NzY3OnYy", "diffSide": "RIGHT", "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/RestHighLevelClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTowOToxNFrOJH6Egg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTowOToxNFrOJH6Egg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MTIzNA==", "bodyText": "These changes are not needed anymore", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612271234", "createdAt": "2021-04-13T09:09:14Z", "author": {"login": "jimczi"}, "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/RestHighLevelClient.java", "diffHunk": "@@ -120,18 +120,18 @@\n import org.elasticsearch.search.aggregations.bucket.range.RangeAggregationBuilder;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzQ4NTM3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOToxMDo1MlrOJH6JGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMTowNjoxM1rOJH-taA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MjQwOQ==", "bodyText": "doc_freq is not required either ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612272409", "createdAt": "2021-04-13T09:10:52Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java", "diffHunk": "@@ -399,4 +400,20 @@ public TextSearchInfo getTextSearchInfo() {\n         KEYWORD,\n         NUMERIC\n     }\n+\n+    /**\n+     * This method is used to support auto-complete services and implementations\n+     * are expected to find terms beginning with the provided string very quickly.\n+     * If fields cannot look up matching terms quickly they should return null.  \n+     * The returned TermEnum should implement next(), term() and doc_freq() methods", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM0NzI0MA==", "bodyText": "I kept it in for now because 1) it's cheap to get from the index and 2) Even if we don't use it for popularity sorting we might want to use it later to filter low-frequency items e.g. user tags that occur only once", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612347240", "createdAt": "2021-04-13T11:06:13Z", "author": {"login": "markharwood"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java", "diffHunk": "@@ -399,4 +400,20 @@ public TextSearchInfo getTextSearchInfo() {\n         KEYWORD,\n         NUMERIC\n     }\n+\n+    /**\n+     * This method is used to support auto-complete services and implementations\n+     * are expected to find terms beginning with the provided string very quickly.\n+     * If fields cannot look up matching terms quickly they should return null.  \n+     * The returned TermEnum should implement next(), term() and doc_freq() methods", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MjQwOQ=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3Mzg5MTQwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDo0NzowNVrOJH-Bdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzo1ODoxN1rOJIGY3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNTk5MQ==", "bodyText": "Is this still accurate. I thought that we would try to rewrite the role query and if it results in a match_all then we'd accept the request ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612335991", "createdAt": "2021-04-13T10:47:05Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - see https://github.com/elastic/elasticsearch/issues/70221", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 332}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ3MzA1NQ==", "bodyText": "The TODO should perhaps be about moving any security filtering logic like this to a custom \"Interceptor\" class which wraps the core implementation and rewrites requests rather than having security+impl in the same class.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612473055", "createdAt": "2021-04-13T13:58:17Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - see https://github.com/elastic/elasticsearch/issues/70221", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNTk5MQ=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 332}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3Mzg5NjMwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDo0ODoyMFrOJH-EZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDo0ODoyMFrOJH-EZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNjc0MA==", "bodyText": "We could also use the search thread pool if the queue is empty. Happy to do it in a follow up.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612336740", "createdAt": "2021-04-13T10:48:20Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - see https://github.com/elastic/elasticsearch/issues/70221\n+    private boolean canAccess(String indexName, String fieldName, XPackLicenseState frozenLicenseState, ThreadContext threadContext) {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(indexName);\n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.shardStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermEnumRequest request;\n+        private ActionListener<TermEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final NodeTermEnumRequest nodeRequest = newNodeRequest(nodeId, shardIds, request);\n+                    nodeRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(nodeId);\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onNoOperation(nodeId);\n+                    } else if (checkForEarlyFinish() == false) {\n+                        transportService.sendRequest(\n+                            node,\n+                            transportShardAction,\n+                            nodeRequest,\n+                            new TransportResponseHandler<NodeTermEnumResponse>() {\n+                                @Override\n+                                public NodeTermEnumResponse read(StreamInput in) throws IOException {\n+                                    return readShardResponse(in);\n+                                }\n+\n+                                @Override\n+                                public void handleResponse(NodeTermEnumResponse response) {\n+                                    onOperation(nodeId, nodeIndex, response);\n+                                }\n+\n+                                @Override\n+                                public void handleException(TransportException e) {\n+                                    onNoOperation(nodeId);\n+                                }\n+                            }\n+                        );\n+                    }\n+                } catch (Exception e) {\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        protected void onOperation(String nodeId, int nodeIndex, NodeTermEnumResponse response) {\n+            logger.trace(\"received response for node {}\", nodeId);\n+            nodesResponses.set(nodeIndex, response);\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            } else {\n+                checkForEarlyFinish();\n+            }\n+        }\n+\n+        void onNoOperation(String nodeId) {\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            }\n+        }\n+\n+        // Can be called multiple times - either for early time-outs or for fully-completed collections.\n+        protected synchronized void finishHim(boolean complete) {\n+            if (listener == null) {\n+                return;\n+            }\n+            try {\n+                listener.onResponse(newResponse(request, nodesResponses, complete, nodeBundles));\n+            } catch (Exception e) {\n+                listener.onFailure(e);\n+            } finally {\n+                listener = null;\n+            }\n+        }\n+    }\n+\n+    class NodeTransportHandler implements TransportRequestHandler<NodeTermEnumRequest> {\n+\n+        @Override\n+        public void messageReceived(NodeTermEnumRequest request, TransportChannel channel, Task task) throws Exception {\n+            asyncNodeOperation(request, task, ActionListener.wrap(channel::sendResponse, e -> {\n+                try {\n+                    channel.sendResponse(e);\n+                } catch (Exception e1) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\n+                            \"Failed to send error response for action [{}] and request [{}]\",\n+                            actionName,\n+                            request\n+                        ),\n+                        e1\n+                    );\n+                }\n+            }));\n+        }\n+    }\n+\n+    private void asyncNodeOperation(NodeTermEnumRequest request, Task task, ActionListener<NodeTermEnumResponse> listener)\n+        throws IOException {\n+        // DLS/FLS check copied from ResizeRequestInterceptor - check permissions and\n+        // any index_filter canMatch checks on network thread before allocating work\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+        for (ShardId shardId : request.shardIds().toArray(new ShardId[0])) {\n+            if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) == false || canMatchShard(\n+                shardId,\n+                request\n+            ) == false) {\n+                // Permission denied or can't match, remove shardID from request\n+                request.remove(shardId);\n+            }\n+        }\n+        if (request.shardIds().size() == 0) {\n+            listener.onResponse(new NodeTermEnumResponse(request.nodeId(), Collections.emptyList(), null, true));\n+        } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 546}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3Mzg5NzUxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/rest/RestTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDo0ODozNlrOJH-FEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDo0ODozNlrOJH-FEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNjkxMw==", "bodyText": "nit: extra lines", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612336913", "createdAt": "2021-04-13T10:48:36Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/rest/RestTermEnumAction.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.rest;\n+\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.rest.BaseRestHandler;\n+import org.elasticsearch.rest.RestRequest;\n+import org.elasticsearch.rest.action.RestToXContentListener;\n+import org.elasticsearch.xpack.core.termenum.action.TermEnumAction;\n+import org.elasticsearch.xpack.core.termenum.action.TermEnumRequest;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.elasticsearch.rest.RestRequest.Method.GET;\n+import static org.elasticsearch.rest.RestRequest.Method.POST;\n+\n+public class RestTermEnumAction extends BaseRestHandler {\n+\n+    @Override\n+    public List<Route> routes() {\n+        return List.of(\n+            new Route(GET, \"/{index}/_terms\"),\n+            new Route(POST, \"/{index}/_terms\"));\n+    }\n+\n+    @Override\n+    public String getName() {\n+        return \"term_enum_action\";\n+    }\n+\n+    @Override\n+    public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException {\n+        try (XContentParser parser = request.contentOrSourceParamParser()) {\n+            TermEnumRequest termEnumRequest = TermEnumAction.fromXContent(parser, \n+                Strings.splitStringByCommaToArray(request.param(\"index\")));\n+            return channel ->\n+            client.execute(TermEnumAction.INSTANCE, termEnumRequest, new RestToXContentListener<>(channel));\n+        }        \n+        ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzkxMjQ0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDo1MjozNFrOJH-OLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QwOToxNjo1MlrOJQQUSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzOTI0NA==", "bodyText": "Should we use the plural (Terms) instead ? That would be consistent with the name of the API (_terms). Same for all the other class names (TermEnumResponse, ...).", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612339244", "createdAt": "2021-04-13T10:52:34Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMzA3OTA0OA==", "bodyText": "I don't know if \"Terms\" will be too short. We already have a lot of o.e.Term* classes to navigate and users looking for help with the terms aggregation.\nA verb would be a good addition e.g. termFinder", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r613079048", "createdAt": "2021-04-14T09:15:16Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzOTI0NA=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTAyNDMyOA==", "bodyText": "Now have REST endpoint _terms_enum and TermsEnumXxxx class names", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621024328", "createdAt": "2021-04-27T09:16:52Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzOTI0NA=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDE5OTkyOnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjowNTowOVrOJIA6sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjowNTowOVrOJIA6sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4MzQwOA==", "bodyText": "I wonder if we should be more intentional.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            a partial string. This is used for features like auto-complete:\n          \n          \n            \n            a partial string. This is used for auto-complete:", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612383408", "createdAt": "2021-04-13T12:05:09Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDIwNDcxOnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjowNjowNlrOJIA9aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzozNDozOFrOJIFGyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4NDEwNw==", "bodyText": "I'm confused, there is no \"8\" in the response?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612384107", "createdAt": "2021-04-13T12:06:06Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ1MjA0MQ==", "bodyText": "copy/paste relic I suspect", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612452041", "createdAt": "2021-04-13T13:34:38Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4NDEwNw=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDIxNDIwOnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjowODoyM1rOJIBDFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QwOToxNzozMlrOJQQW6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4NTU1Nw==", "bodyText": "Can we have more consistency between the name of the API and the name of the endpoint, ie. either calling it the terms API or changing the endpoint name from terms to term_enum? I have a preference for the former, but I haven't thought much about it. Or maybe the API should even be called _auto_complete to be more discoverable?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612385557", "createdAt": "2021-04-13T12:08:23Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ1MTY1Nw==", "bodyText": "I lose count myself of how many ways there are to do autocomplete. Maybe we need more docs to be updated e.g. https://www.elastic.co/guide/en/elasticsearch/reference/7.x/search-as-you-type.html", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612451657", "createdAt": "2021-04-13T13:34:10Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4NTU1Nw=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTAyNTAwMQ==", "bodyText": "Now have _terms_enum REST endpoint and TermsEnumxxx classnames", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621025001", "createdAt": "2021-04-27T09:17:32Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4NTU1Nw=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDIzNzk3OnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoxNDoxOFrOJIBR2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxNDo0MTowNVrOJIIt2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4OTMzNg==", "bodyText": "I could imagine complete to mean to users that all possible terms have been included in the response, while I think that what you are saying is slightly different. E.g. if size is 10 and there are 100 terms in the response, complete would be true if we looked at all shards even though not all possible terms are included in the response?\nMy gut feeling is that the semantics users would need is knowing whether all possible terms are in the response? (I haven't thought much about it)", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612389336", "createdAt": "2021-04-13T12:14:18Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ1NjY0OA==", "bodyText": "When this PR had a sort-by-popularity mode we had a space constraint of only considering the first 10k terms+counts with the prefix. Whatever modes and constraints we place in future the one thing end users need to know is \"is it worth finishing typing the word in my head if it's not shown in the suggestions so far?\". The reason(s) why we might not have shown them what's in their head don't matter too much.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612456648", "createdAt": "2021-04-13T13:39:53Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4OTMzNg=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ4NjYyOQ==", "bodyText": "That makes me think that we should also add an option to search after a value. That would allow to paginate values that match a prefix efficiently ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612486629", "createdAt": "2021-04-13T14:14:12Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4OTMzNg=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjUxMTE5Mg==", "bodyText": "Yep. A search_after would make sense. I added that to the meta issue for a follow-up PR", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612511192", "createdAt": "2021-04-13T14:41:05Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4OTMzNg=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDI0MzUxOnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoxNTozMlrOJIBVJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzo0Mjo1NFrOJIFjHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MDE4Mw==", "bodyText": "Should we rename it prefix to be clearer about how the matching works?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612390183", "createdAt": "2021-04-13T12:15:32Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`string`::\n+(Mandatory, string)\n+The string to match at the start of indexed terms", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ1OTI5NQ==", "bodyText": "We opted for string to allow a future \"match_mode\" style flag to change behaviour e.g. infix matching", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612459295", "createdAt": "2021-04-13T13:42:54Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`string`::\n+(Mandatory, string)\n+The string to match at the start of indexed terms", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MDE4Mw=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDI0NzA5OnYy", "diffSide": "RIGHT", "path": "docs/reference/search/term-enum.asciidoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoxNjoxM1rOJIBXOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QwOToxMzo1NlrOJQQI1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MDcxNQ==", "bodyText": "Should we take a time value rather than a number, ie. 1s instead of 1000?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612390715", "createdAt": "2021-04-13T12:16:13Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`string`::\n+(Mandatory, string)\n+The string to match at the start of indexed terms\n+\n+[[term-enum-size-param]]\n+`size`::\n+(Optional, integer)\n+How many matching terms to return. Defaults to 10\n+\n+[[term-enum-timeout-param]]\n+`timeout`::\n+(Optional, integer)\n+The maximum length of time in milliseconds to spend collecting results. Defaults to 1000.\n+If the timeout is exceeded the `complete` flag set to false in the response and the results may\n+be partial or empty.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTAyMTM5OQ==", "bodyText": "Will add a max limit e.g. 1 minute", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621021399", "createdAt": "2021-04-27T09:13:56Z", "author": {"login": "markharwood"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`string`::\n+(Mandatory, string)\n+The string to match at the start of indexed terms\n+\n+[[term-enum-size-param]]\n+`size`::\n+(Optional, integer)\n+How many matching terms to return. Defaults to 10\n+\n+[[term-enum-timeout-param]]\n+`timeout`::\n+(Optional, integer)\n+The maximum length of time in milliseconds to spend collecting results. Defaults to 1000.\n+If the timeout is exceeded the `complete` flag set to false in the response and the results may\n+be partial or empty.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MDcxNQ=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDI2NzA5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldParser.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyMDoxNlrOJIBi-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyMDoxNlrOJIBi-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MzcyMA==", "bodyText": "can you add a newline between the two functions?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612393720", "createdAt": "2021-04-13T12:20:16Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldParser.java", "diffHunk": "@@ -166,4 +166,14 @@ static BytesRef extractKey(BytesRef keyedValue) {\n         }\n         return new BytesRef(keyedValue.bytes, keyedValue.offset, length);\n     }\n+    static BytesRef extractValue(BytesRef keyedValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDI3OTU2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyMzoxNFrOJIBqwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyMzoxNFrOJIBqwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTcxMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if(result != null) {\n          \n          \n            \n                        if (result != null) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395713", "createdAt": "2021-04-13T12:23:14Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{\n+        TermsEnum delegate;\n+\n+        TranslatingTermsEnum(TermsEnum delegate) {\n+            this.delegate = delegate;\n+        }\n+        \n+        @Override\n+        public BytesRef next() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.next();\n+            if(result != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDI4MDExOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyMzoyMVrOJIBrFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyMzoyMVrOJIBrFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTc5OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if(result != null) {\n          \n          \n            \n                        if (result != null) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395798", "createdAt": "2021-04-13T12:23:21Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{\n+        TermsEnum delegate;\n+\n+        TranslatingTermsEnum(TermsEnum delegate) {\n+            this.delegate = delegate;\n+        }\n+        \n+        @Override\n+        public BytesRef next() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.next();\n+            if(result != null) {\n+                result = FlattenedFieldParser.extractValue(result);\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public BytesRef term() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.term();\n+            if(result != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDI4MTA5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyMzozNlrOJIBrug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyMzozNlrOJIBrug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTk2Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                static class TranslatingTermsEnum extends TermsEnum{\n          \n          \n            \n                static class TranslatingTermsEnum extends TermsEnum {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395962", "createdAt": "2021-04-13T12:23:36Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDI5NjQ3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyNjo0NVrOJIB1DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjoyNjo0NVrOJIB1DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5ODM0OQ==", "bodyText": "I don't think that this is correct as this will do case-insensitive search on the field name too. E.g. if the flattened object contains {\"foo\": \"bar\", \"Foo\": \"quux\"} we should only consider the right foo/Foo even when case insensitivity is enabled.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612398349", "createdAt": "2021-04-13T12:26:45Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -239,6 +252,26 @@ public Query wildcardQuery(String value,\n         public Query termQueryCaseInsensitive(Object value, SearchExecutionContext context) {\n             return AutomatonQueries.caseInsensitiveTermQuery(new Term(name(), indexedValueForSearch(value)));\n         }\n+        \n+        @Override\n+        public TermsEnum getTerms(boolean caseInsensitive, String string, SearchExecutionContext queryShardContext) throws IOException {\n+            IndexReader reader = queryShardContext.searcher().getTopReaderContext().reader();\n+            String searchString = FlattenedFieldParser.createKeyedValue(key, string);\n+            Terms terms = MultiTerms.getTerms(reader, name());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return null;\n+            }\n+            Automaton a = caseInsensitive\n+                ? AutomatonQueries.caseInsensitivePrefix(searchString)\n+                : Automata.makeString(searchString);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDUwMDA3OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzowODo0NFrOJIDxmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0zMFQxMzowOToyOFrOJS9pqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg==", "bodyText": "it feels inconsistent to set it when deserializing, could we require callers to call shardStartedTimeMillis instead?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612430232", "createdAt": "2021-04-13T13:08:44Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA1Mjk4Ng==", "bodyText": "Can we rely on caller and data nodes' clocks not drifting by the small amounts we measure here?\nWe subtract the time-already-spent on caller node from the available max time-to-spend on the data node to account for caller delays in dispatch but I wanted to avoid any clock-drift between caller and data node to be a part of the calculations on the data node", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621052986", "createdAt": "2021-04-27T09:51:42Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzgwMjI2OQ==", "bodyText": "I think what Adrien meant is that we shouldn't set the timer when deserializing the data. That feels fragile and inconsistent. If that represents the time when the action started on a data node then we should be able to set it once in TransportTermsEnumAction#asyncNodeOperation ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r623802269", "createdAt": "2021-04-30T11:19:07Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzg2MTE0Mg==", "bodyText": "My goal was to start the timer at the earliest opportunity on the data node (ideally prior to any queuing etc) and given the NodeTermEnumRequest was passed to TransportTermsEnumAction#asyncNodeOperation then the deserialization would have occurred before this point?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r623861142", "createdAt": "2021-04-30T13:04:42Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzg2NDIzNA==", "bodyText": "I don't think this is needed. We call asyncNodeOperation early enough to not worry too much about what happens before.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r623864234", "createdAt": "2021-04-30T13:09:28Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDUwNTQ1OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzowOTo0N1rOJID0wA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QwOToyMzoyNlrOJQQt4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMTA0MA==", "bodyText": "I'm confused why the method is called shardStartedXXX while the field is called nodeStartedXXX.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612431040", "createdAt": "2021-04-13T13:09:47Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTAzMDg4Mg==", "bodyText": "A relic from when we did per-shard requests rather than bundling into a per-node request", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621030882", "createdAt": "2021-04-27T09:23:26Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMTA0MA=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDUxNjc4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxMjowNFrOJID71A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxMjowNFrOJID71A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMjg1Mg==", "bodyText": "I don't think that this assert is legal. The delta could be negative if we spent a long time on the above line for instance.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612432852", "createdAt": "2021-04-13T13:12:04Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    \n+    \n+    public Set<ShardId> shardIds() {\n+        return Collections.unmodifiableSet(shardIds);\n+    }\n+\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    public int size() {\n+        return size;\n+    }\n+\n+    public long timeout() {\n+        return timeout;\n+    }\n+    public String nodeId() {\n+        return nodeId;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        // Adjust the amount of permitted time the shard has remaining to gather terms. \n+        long timeSpentSoFarInCoordinatingNode = System.currentTimeMillis() - taskStartedTimeMillis;\n+        assert timeSpentSoFarInCoordinatingNode >= 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDUyMTQwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxMjo0N1rOJID-fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxMjo0N1rOJID-fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMzUzNA==", "bodyText": "why do we need to cast to an int?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612433534", "createdAt": "2021-04-13T13:12:47Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    \n+    \n+    public Set<ShardId> shardIds() {\n+        return Collections.unmodifiableSet(shardIds);\n+    }\n+\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    public int size() {\n+        return size;\n+    }\n+\n+    public long timeout() {\n+        return timeout;\n+    }\n+    public String nodeId() {\n+        return nodeId;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        // Adjust the amount of permitted time the shard has remaining to gather terms. \n+        long timeSpentSoFarInCoordinatingNode = System.currentTimeMillis() - taskStartedTimeMillis;\n+        assert timeSpentSoFarInCoordinatingNode >= 0;\n+        int remainingTimeForShardToUse = (int) (timeout - timeSpentSoFarInCoordinatingNode);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDUzNzk5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxNjowOFrOJIEJAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxNjowOFrOJIEJAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNjIyNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class SimpleTermCountEnum extends TermsEnum{\n          \n          \n            \n            public class SimpleTermCountEnum extends TermsEnum {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612436224", "createdAt": "2021-04-13T13:16:08Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU0MDg5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxNjo0M1rOJIEK1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxNjo0M1rOJIEK1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNjY5NQ==", "bodyText": "could you copy the input array instead of modifying it in-place?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612436695", "createdAt": "2021-04-13T13:16:43Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{\n+    int index =-1;\n+    TermCount[] sortedTerms;\n+    TermCount current = null;\n+    \n+    public SimpleTermCountEnum(TermCount[] terms) {\n+        sortedTerms = terms;\n+        Arrays.sort(sortedTerms, Comparator.comparing(TermCount::getTerm));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU0MzYzOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxNzoxMlrOJIEMgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxNzoxMlrOJIEMgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNzEyMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if(index >= sortedTerms.length) {\n          \n          \n            \n                    if (index >= sortedTerms.length) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612437123", "createdAt": "2021-04-13T13:17:12Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{\n+    int index =-1;\n+    TermCount[] sortedTerms;\n+    TermCount current = null;\n+    \n+    public SimpleTermCountEnum(TermCount[] terms) {\n+        sortedTerms = terms;\n+        Arrays.sort(sortedTerms, Comparator.comparing(TermCount::getTerm));\n+    }\n+    \n+    public SimpleTermCountEnum(TermCount termCount) {\n+        sortedTerms = new TermCount[1];\n+        sortedTerms[0] = termCount;\n+    }\n+\n+    @Override\n+    public BytesRef term() throws IOException {\n+        if (current == null) {\n+            return null;\n+        }\n+        return new BytesRef(current.getTerm());\n+    }    \n+\n+    @Override\n+    public BytesRef next() throws IOException {\n+        index++;\n+        if(index >= sortedTerms.length) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU1MTYyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxODo0MFrOJIERWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxODo0MFrOJIERWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzODM2Mg==", "bodyText": "Should we be using constructorArg() rather than optionalConstructorArg()?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612438362", "createdAt": "2021-04-13T13:18:40Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU1NzMwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoxOTo0NVrOJIEU0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QwOTozODozNVrOJQRcAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzOTI0OA==", "bodyText": "I'm not seeing where these setters are called, are they needed? If not can we make term and docCount final?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612439248", "createdAt": "2021-04-13T13:19:45Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {\n+        this.term = term;\n+        this.docCount = count;\n+    }\n+\n+    public String getTerm() {\n+        return this.term;\n+    }\n+\n+    public long getDocCount() {\n+        return this.docCount;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeString(term);\n+        out.writeLong(docCount);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.field(TERM_FIELD, getTerm());\n+        builder.field(DOC_COUNT_FIELD, getDocCount());\n+        return builder;\n+    }\n+\n+    public static TermCount fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        TermCount other = (TermCount) o;\n+        return Objects.equals(getTerm(), other.getTerm()) && Objects.equals(getDocCount(), other.getDocCount());\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(getTerm(), getDocCount());\n+    }\n+\n+    void addToDocCount(long extra) {\n+        docCount += extra;\n+    }\n+\n+    void setTerm(String term) {\n+        this.term = term;\n+    }\n+\n+    void setDocCount(long docCount) {\n+        this.docCount = docCount;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA0MjY4OA==", "bodyText": "addToDocCount(int) is used to combine counts from shards but the setDocCount isn't. Removed.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621042688", "createdAt": "2021-04-27T09:38:35Z", "author": {"login": "markharwood"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {\n+        this.term = term;\n+        this.docCount = count;\n+    }\n+\n+    public String getTerm() {\n+        return this.term;\n+    }\n+\n+    public long getDocCount() {\n+        return this.docCount;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeString(term);\n+        out.writeLong(docCount);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.field(TERM_FIELD, getTerm());\n+        builder.field(DOC_COUNT_FIELD, getDocCount());\n+        return builder;\n+    }\n+\n+    public static TermCount fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        TermCount other = (TermCount) o;\n+        return Objects.equals(getTerm(), other.getTerm()) && Objects.equals(getDocCount(), other.getDocCount());\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(getTerm(), getDocCount());\n+    }\n+\n+    void addToDocCount(long extra) {\n+        docCount += extra;\n+    }\n+\n+    void setTerm(String term) {\n+        this.term = term;\n+    }\n+\n+    void setDocCount(long docCount) {\n+        this.docCount = docCount;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzOTI0OA=="}, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU2NjkwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumRequest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyMTo0MVrOJIEa4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyMTo0MVrOJIEa4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MDgwMg==", "bodyText": "let's write it as a time value?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612440802", "createdAt": "2021-04-13T13:21:41Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumRequest.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.ValidateActions;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.action.support.broadcast.BroadcastRequest;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+/**\n+ * A request to gather terms for a given field matching a string prefix\n+ */\n+public class TermEnumRequest extends BroadcastRequest<TermEnumRequest> implements ToXContentObject {\n+\n+    public static int DEFAULT_SIZE = 10;\n+    public static int DEFAULT_TIMEOUT_MILLIS = 1000;\n+\n+    private String field;\n+    private String string;\n+    private int size = DEFAULT_SIZE;\n+    private boolean caseInsensitive;\n+    long taskStartTimeMillis;\n+    private QueryBuilder indexFilter;\n+\n+    public TermEnumRequest() {\n+        this(Strings.EMPTY_ARRAY);\n+    }\n+\n+    public TermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+    }\n+\n+    /**\n+     * Constructs a new term enum request against the provided indices. No indices provided means it will\n+     * run against all indices.\n+     */\n+    public TermEnumRequest(String... indices) {\n+        super(indices);\n+        indicesOptions(IndicesOptions.fromOptions(false, false, true, false));\n+        timeout(new TimeValue(DEFAULT_TIMEOUT_MILLIS));\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        ActionRequestValidationException validationException = super.validate();\n+        if (field == null) {\n+            validationException = ValidateActions.addValidationError(\"field cannot be null\", validationException);\n+        }\n+        return validationException;\n+    }\n+\n+    /**\n+     * The field to look inside for values\n+     */\n+    public void field(String field) {\n+        this.field = field;\n+    }\n+\n+    /**\n+     * Indicates if detailed information about query is requested\n+     */\n+    public String field() {\n+        return field;\n+    }\n+\n+    /**\n+     * The string required in matching field values\n+     */\n+    public void string(String string) {\n+        this.string = string;\n+    }\n+\n+    /**\n+     * The string required in matching field values\n+     */\n+    public String string() {\n+        return string;\n+    }\n+\n+    /**\n+     *  The number of terms to return\n+     */\n+    public int size() {\n+        return size;\n+    }\n+\n+    /**\n+     * The number of terms to return\n+     */\n+    public void size(int size) {\n+        this.size = size;\n+    }\n+\n+    /**\n+     * TThe max time in milliseconds to spend gathering terms\n+     */\n+    public void timeoutInMillis(int timeout) {\n+        timeout(new TimeValue(timeout));\n+    }\n+\n+    /**\n+     * If case insensitive matching is required\n+     */\n+    public void caseInsensitive(boolean caseInsensitive) {\n+        this.caseInsensitive = caseInsensitive;\n+    }\n+\n+    /**\n+     * If case insensitive matching is required\n+     */\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    /**\n+     * Allows to filter shards if the provided {@link QueryBuilder} rewrites to `match_none`.\n+     */\n+    public void indexFilter(QueryBuilder indexFilter) {\n+        this.indexFilter = indexFilter;\n+    }    \n+    \n+    public QueryBuilder indexFilter() {\n+        return indexFilter;\n+    }    \n+    \n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        out.writeOptionalNamedWriteable(indexFilter);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"[\" + Arrays.toString(indices) + \"] field[\" + field + \"], string[\" + string + \"] \"  + \" size=\" + size + \" timeout=\"\n+            + timeout().getMillis() + \" case_insensitive=\"\n+            + caseInsensitive + \" indexFilter = \"+ indexFilter;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.field(\"field\", field);\n+        builder.field(\"string\", string);\n+        builder.field(\"size\", size);\n+        builder.field(\"timeout\", timeout().getMillis());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 168}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU3NTYwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyMzoyNlrOJIEgmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyMzoyNlrOJIEgmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjI2Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                if(randomBoolean()) {\n          \n          \n            \n                                if (randomBoolean()) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442267", "createdAt": "2021-04-13T13:23:26Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU3NjMyOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyMzozNVrOJIEhFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyMzozNVrOJIEhFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjM4OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    while(te.next()!=null) {\n          \n          \n            \n                                    while (te.next()!=null) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442388", "createdAt": "2021-04-13T13:23:35Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU3NzQxOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyMzo0NVrOJIEhrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyMzo0NVrOJIEhrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjU0MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                if(term.startsWith(searchPrefix)) {\n          \n          \n            \n                                if (term.startsWith(searchPrefix)) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442540", "createdAt": "2021-04-13T13:23:45Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {\n+                            termCounts.add(new TermCount(te.term().utf8ToString(), te.docFreq()));\n+                        }\n+                        SimpleTermCountEnum simpleEnum = new SimpleTermCountEnum(termCounts.toArray(new TermCount[0]));\n+                        termsEnums.add(simpleEnum);\n+                    } else {\n+                        termsEnums.add(te);\n+                    }\n+                }\n+                MultiShardTermsEnum mte = new MultiShardTermsEnum(termsEnums.toArray(new TermsEnum[0]));\n+                HashMap<String, Integer> expecteds = new HashMap<>();\n+\n+                for (String term : globalTermCounts.keySet()) {\n+                    if(term.startsWith(searchPrefix)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU4MjgwOnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyNDozM1rOJIEkuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyNDozM1rOJIEkuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MzMyMQ==", "bodyText": "nit: can you iterate over entries rathen than keys since you need values too?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612443321", "createdAt": "2021-04-13T13:24:33Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {\n+                            termCounts.add(new TermCount(te.term().utf8ToString(), te.docFreq()));\n+                        }\n+                        SimpleTermCountEnum simpleEnum = new SimpleTermCountEnum(termCounts.toArray(new TermCount[0]));\n+                        termsEnums.add(simpleEnum);\n+                    } else {\n+                        termsEnums.add(te);\n+                    }\n+                }\n+                MultiShardTermsEnum mte = new MultiShardTermsEnum(termsEnums.toArray(new TermsEnum[0]));\n+                HashMap<String, Integer> expecteds = new HashMap<>();\n+\n+                for (String term : globalTermCounts.keySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDU4OTc5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/mapper-constant-keyword/src/main/java/org/elasticsearch/xpack/constantkeyword/mapper/ConstantKeywordFieldMapper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyNTo0MVrOJIEo4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyNTo0MVrOJIEo4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0NDM4Ng==", "bodyText": "should we use maxDoc() to be consistent with keyword fields, which don't ignore deletes?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612444386", "createdAt": "2021-04-13T13:25:41Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/mapper-constant-keyword/src/main/java/org/elasticsearch/xpack/constantkeyword/mapper/ConstantKeywordFieldMapper.java", "diffHunk": "@@ -140,6 +144,20 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n                 ? lookup -> List.of()\n                 : lookup -> List.of(value);\n         }\n+        \n+        \n+\n+        @Override\n+        public TermsEnum getTerms(boolean caseInsensitive, String string, SearchExecutionContext queryShardContext) throws IOException {\n+            boolean matches = caseInsensitive ? \n+                value.toLowerCase(Locale.ROOT).startsWith(string.toLowerCase(Locale.ROOT)) : \n+                value.startsWith(string);\n+            if (matches == false) {\n+                return null;\n+            }\n+            int docCount = queryShardContext.searcher().getIndexReader().numDocs();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3NDYwOTE0OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyOTowOVrOJIE0_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyOTowOVrOJIE0_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0NzQ4Ng==", "bodyText": "I usually don't like losing stack traces, which can be very useful for debugging purposes.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612447486", "createdAt": "2021-04-13T13:29:09Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 325}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzkzMTUxMTM5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/index/mapper/KeywordFieldMapper.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QwODowMDo0NlrOJQMcyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QwODowMDo0NlrOJQMcyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMDk2MDk3MQ==", "bodyText": "nit: extra lines ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r620960971", "createdAt": "2021-04-27T08:00:46Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/KeywordFieldMapper.java", "diffHunk": "@@ -247,6 +257,27 @@ public KeywordFieldType(String name, NamedAnalyzer analyzer) {\n             this.eagerGlobalOrdinals = false;\n             this.scriptValues = null;\n         }\n+        \n+        \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e43475f856f7dca4ff6bd92598f2d64dda5cf921"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzkzMjMwNjM5OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QxMDoyNzo1MlrOJQTn-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QxMDoyNzo1MlrOJQTn-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA3ODUyMQ==", "bodyText": "We should use the search threadpool here if the queue is empty.\nSomething like:\n        assert transportService.getThreadPool().executor(ThreadPool.Names.SEARCH) instanceof EsThreadPoolExecutor\n            : \"SEARCH threadpool must be an instance of ThreadPoolExecutor\";\n        EsThreadPoolExecutor ex = (EsThreadPoolExecutor) transportService.getThreadPool().executor(ThreadPool.Names.SEARCH);\n        final String executorName = ex.getQueue().size() == 0 ? ThreadPool.Names.SEARCH : shardExecutor;", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621078521", "createdAt": "2021-04-27T10:27:52Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termsenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.query.Rewriteable;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.script.ScriptService;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.SecurityContext;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+import org.elasticsearch.xpack.core.security.authz.support.DLSRoleQueryValidator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermsEnumAction extends HandledTransportAction<TermsEnumRequest, TermsEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    private final ScriptService scriptService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermsEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ScriptService scriptService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermsEnumAction.NAME, transportService, actionFilters, TermsEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.scriptService = scriptService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermsEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermsEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermsEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermsEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermsEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermsEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermsEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermsEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermsEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermsEnumResponse newResponse(\n+        TermsEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermsEnumResponse str = (NodeTermsEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermsEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermsEnumResponse dataNodeOperation(NodeTermsEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = ExceptionsHelper.stackTrace(e);\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - write a separate Interceptor class to \n+    // rewrite requests according to security rules \n+    private boolean canAccess(\n+        ShardId shardId,\n+        NodeTermsEnumRequest request,\n+        XPackLicenseState frozenLicenseState,\n+        ThreadContext threadContext        \n+    ) throws IOException {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(shardId.getIndexName());\n+\n+         \n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    // Check to see if any of the roles defined for the current user rewrite to match_all \n+                    \n+                    SecurityContext securityContext = new SecurityContext(clusterService.getSettings(), threadContext);\n+                    final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        null,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+\n+                    // Current user has potentially many roles and therefore potentially many queries\n+                    // defining sets of docs accessible\n+                    Set<BytesReference> queries = indexAccessControl.getDocumentPermissions().getQueries();\n+                    for (BytesReference querySource : queries) {\n+                        QueryBuilder queryBuilder = DLSRoleQueryValidator.evaluateAndVerifyRoleQuery(\n+                            querySource,\n+                            scriptService,\n+                            queryShardContext.getXContentRegistry(),\n+                            securityContext.getUser()\n+                        );\n+                        QueryBuilder rewrittenQueryBuilder = Rewriteable.rewrite(queryBuilder, queryShardContext);\n+                        if (rewrittenQueryBuilder instanceof MatchAllQueryBuilder) {\n+                            // One of the roles assigned has \"all\" permissions - allow unfettered access to termsDict\n+                            return true;\n+                        }\n+                    }\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermsEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.shardStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermsEnumRequest request;\n+        private ActionListener<TermsEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermsEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final NodeTermsEnumRequest nodeRequest = newNodeRequest(nodeId, shardIds, request);\n+                    nodeRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(nodeId);\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onNoOperation(nodeId);\n+                    } else if (checkForEarlyFinish() == false) {\n+                        transportService.sendRequest(\n+                            node,\n+                            transportShardAction,\n+                            nodeRequest,\n+                            new TransportResponseHandler<NodeTermsEnumResponse>() {\n+                                @Override\n+                                public NodeTermsEnumResponse read(StreamInput in) throws IOException {\n+                                    return readShardResponse(in);\n+                                }\n+\n+                                @Override\n+                                public void handleResponse(NodeTermsEnumResponse response) {\n+                                    onOperation(nodeId, nodeIndex, response);\n+                                }\n+\n+                                @Override\n+                                public void handleException(TransportException e) {\n+                                    onNoOperation(nodeId);\n+                                }\n+                            }\n+                        );\n+                    }\n+                } catch (Exception e) {\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        protected void onOperation(String nodeId, int nodeIndex, NodeTermsEnumResponse response) {\n+            logger.trace(\"received response for node {}\", nodeId);\n+            nodesResponses.set(nodeIndex, response);\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            } else {\n+                checkForEarlyFinish();\n+            }\n+        }\n+\n+        void onNoOperation(String nodeId) {\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            }\n+        }\n+\n+        // Can be called multiple times - either for early time-outs or for fully-completed collections.\n+        protected synchronized void finishHim(boolean complete) {\n+            if (listener == null) {\n+                return;\n+            }\n+            try {\n+                listener.onResponse(newResponse(request, nodesResponses, complete, nodeBundles));\n+            } catch (Exception e) {\n+                listener.onFailure(e);\n+            } finally {\n+                listener = null;\n+            }\n+        }\n+    }\n+\n+    class NodeTransportHandler implements TransportRequestHandler<NodeTermsEnumRequest> {\n+\n+        @Override\n+        public void messageReceived(NodeTermsEnumRequest request, TransportChannel channel, Task task) throws Exception {\n+            asyncNodeOperation(request, task, ActionListener.wrap(channel::sendResponse, e -> {\n+                try {\n+                    channel.sendResponse(e);\n+                } catch (Exception e1) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\n+                            \"Failed to send error response for action [{}] and request [{}]\",\n+                            actionName,\n+                            request\n+                        ),\n+                        e1\n+                    );\n+                }\n+            }));\n+        }\n+    }\n+\n+    private void asyncNodeOperation(NodeTermsEnumRequest request, Task task, ActionListener<NodeTermsEnumResponse> listener)\n+        throws IOException {\n+        // DLS/FLS check copied from ResizeRequestInterceptor - check permissions and\n+        // any index_filter canMatch checks on network thread before allocating work\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+        for (ShardId shardId : request.shardIds().toArray(new ShardId[0])) {\n+            if (canAccess(shardId, request, frozenLicenseState, threadContext) == false || canMatchShard(\n+                shardId,\n+                request\n+            ) == false) {\n+                // Permission denied or can't match, remove shardID from request\n+                request.remove(shardId);\n+            }\n+        }\n+        if (request.shardIds().size() == 0) {\n+            listener.onResponse(new NodeTermsEnumResponse(request.nodeId(), Collections.emptyList(), null, true));\n+        } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e43475f856f7dca4ff6bd92598f2d64dda5cf921"}, "originalPosition": 593}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzk3MzUwNDA4OnYy", "diffSide": "RIGHT", "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNS0wNlQwNzoxMTozNVrOJWFreg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNS0wNlQwNzoxMTozNVrOJWFreg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNzE0MTQ5OA==", "bodyText": "Is the TODO still needed ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r627141498", "createdAt": "2021-05-06T07:11:35Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java", "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termsenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.query.Rewriteable;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.script.ScriptService;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.SecurityContext;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+import org.elasticsearch.xpack.core.security.authz.support.DLSRoleQueryValidator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermsEnumAction extends HandledTransportAction<TermsEnumRequest, TermsEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    private final ScriptService scriptService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermsEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ScriptService scriptService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermsEnumAction.NAME, transportService, actionFilters, TermsEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.scriptService = scriptService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermsEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermsEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermsEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermsEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermsEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermsEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermsEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermsEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermsEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermsEnumResponse newResponse(\n+        TermsEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermsEnumResponse str = (NodeTermsEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermsEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermsEnumResponse dataNodeOperation(NodeTermsEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.nodeStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::nodeStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = ExceptionsHelper.stackTrace(e);\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - write a separate Interceptor class to \n+    // rewrite requests according to security rules \n+    private boolean canAccess(\n+        ShardId shardId,\n+        NodeTermsEnumRequest request,\n+        XPackLicenseState frozenLicenseState,\n+        ThreadContext threadContext        \n+    ) throws IOException {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(shardId.getIndexName());\n+\n+         \n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    // Check to see if any of the roles defined for the current user rewrite to match_all \n+                    \n+                    SecurityContext securityContext = new SecurityContext(clusterService.getSettings(), threadContext);\n+                    final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        null,\n+                        request::nodeStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+\n+                    // Current user has potentially many roles and therefore potentially many queries\n+                    // defining sets of docs accessible\n+                    Set<BytesReference> queries = indexAccessControl.getDocumentPermissions().getQueries();\n+                    for (BytesReference querySource : queries) {\n+                        QueryBuilder queryBuilder = DLSRoleQueryValidator.evaluateAndVerifyRoleQuery(\n+                            querySource,\n+                            scriptService,\n+                            queryShardContext.getXContentRegistry(),\n+                            securityContext.getUser()\n+                        );\n+                        QueryBuilder rewrittenQueryBuilder = Rewriteable.rewrite(queryBuilder, queryShardContext);\n+                        if (rewrittenQueryBuilder instanceof MatchAllQueryBuilder) {\n+                            // One of the roles assigned has \"all\" permissions - allow unfettered access to termsDict\n+                            return true;\n+                        }\n+                    }\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermsEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.nodeStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermsEnumRequest request;\n+        private ActionListener<TermsEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermsEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcf76a4c62533664747506bc1c876a5b2a1fbf2c"}, "originalPosition": 489}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4530, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}