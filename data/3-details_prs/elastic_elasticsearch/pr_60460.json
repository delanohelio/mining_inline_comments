{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5MjUwMTA5", "number": 60460, "title": "Introduce integ tests for high disk watermark", "bodyText": "An important goal of the disk threshold decider is to ensure that nodes\nuse less disk space than the high watermark, and to take action if a\nnode ever exceeds this watermark. Today we do not have any\nintegration-style tests of this high-level behaviour. This commit\nintroduces a small test harness that can adjust the apparent size of the\ndisk and verify that the disk threshold decider moves shards around in\nresponse.", "createdAt": "2020-07-30T14:38:34Z", "url": "https://github.com/elastic/elasticsearch/pull/60460", "merged": true, "mergeCommit": {"oid": "5f8c1f04845d495744de5ddb0b49a9e0e5c73b4b"}, "closed": true, "closedAt": "2020-09-07T12:03:03Z", "author": {"login": "DaveCTurner"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc6AmyDgH2gAyNDU5MjUwMTA5OmQzNmFiNzg0ZGM0YTY2ZmM0ODg4MGEyNDBiNWY3ZGJlMTQxMjkyNjM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdGgxuDgFqTQ4MzQ2MDQ0OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "d36ab784dc4a66fc48880a240b5f7dbe14129263", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/d36ab784dc4a66fc48880a240b5f7dbe14129263", "committedDate": "2020-07-30T14:36:03Z", "message": "Introduce integ tests for high disk watermark\n\nAn important goal of the disk threshold decider is to ensure that nodes\nuse less disk space than the high watermark, and to take action if a\nnode ever exceeds this watermark. Today we do not have any\nintegration-style tests of this high-level behaviour. This commit\nintroduces a small test harness that can adjust the apparent size of the\ndisk and verify that the disk threshold decider moves shards around in\nresponse."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "02ce50039ef32f6cd591daaa50a79ed8ce32b2ae", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/02ce50039ef32f6cd591daaa50a79ed8ce32b2ae", "committedDate": "2020-07-30T15:43:15Z", "message": "Defeat the is-spinning-disk check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff6b96b64218a1b6aa9d0ac78ffb85509cc05653", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/ff6b96b64218a1b6aa9d0ac78ffb85509cc05653", "committedDate": "2020-07-30T15:54:05Z", "message": "More hacker"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dab520506715d7b5a085233c9a40761e17322530", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/dab520506715d7b5a085233c9a40761e17322530", "committedDate": "2020-08-05T08:42:13Z", "message": "Mock filesystem at suite level, don't replace it every test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53194c9f10eb367856617f4c046e23ad35d28ad5", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/53194c9f10eb367856617f4c046e23ad35d28ad5", "committedDate": "2020-08-05T08:43:22Z", "message": "Merge branch 'master' into 2020-07-30-high-watermark-integ-tests-WIP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967", "author": {"user": {"login": "DaveCTurner", "name": "David Turner"}}, "url": "https://github.com/elastic/elasticsearch/commit/946e2b6e54f0148af499fa495965e99282f69967", "committedDate": "2020-08-05T08:59:26Z", "message": "Wrong IOUtils"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMjYwNzcy", "url": "https://github.com/elastic/elasticsearch/pull/60460#pullrequestreview-462260772", "createdAt": "2020-08-06T07:39:52Z", "commit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzozOTo1MlrOG8m-9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzo0NToxMlrOG8nJ5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwNjQ1Mg==", "bodyText": "why the force-merge?", "url": "https://github.com/elastic/elasticsearch/pull/60460#discussion_r466206452", "createdAt": "2020-08-06T07:39:52Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.cluster.routing.allocation.decider;\n+\n+import org.apache.lucene.mockfile.FilterFileStore;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.apache.lucene.mockfile.FilterPath;\n+import org.apache.lucene.util.Constants;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.cluster.ClusterInfoService;\n+import org.elasticsearch.cluster.InternalClusterInfoService;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.monitor.fs.FsService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.FileStore;\n+import java.nio.file.FileSystem;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.NotDirectoryException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;\n+import static org.elasticsearch.index.store.Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.anyOf;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class DiskThresholdDeciderIT extends ESIntegTestCase {\n+\n+    private static TestFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFilesystemProvider() {\n+        assertNull(fileSystemProvider);\n+        fileSystemProvider = new TestFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFilesystemProvider() {\n+        fileSystemProvider = null;\n+        // ESIntegTestCase takes care of tearing down the mock file system\n+    }\n+\n+    @Before\n+    public void clearTrackedPaths() throws IOException {\n+        fileSystemProvider.clearTrackedPaths();\n+    }\n+\n+    private static final long WATERMARK_BYTES = new ByteSizeValue(10, ByteSizeUnit.KB).getBytes();\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        final Path dataPath = fileSystemProvider.getRootDir().resolve(\"node-\" + nodeOrdinal);\n+        try {\n+            Files.createDirectories(dataPath);\n+        } catch (IOException e) {\n+            throw new AssertionError(\"unexpected\", e);\n+        }\n+        fileSystemProvider.addTrackedPath(dataPath);\n+        return Settings.builder()\n+                .put(super.nodeSettings(nodeOrdinal))\n+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)\n+                .put(FsService.ALWAYS_REFRESH_SETTING.getKey(), true)\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), \"0b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build();\n+    }\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return List.of(InternalSettingsPlugin.class);\n+    }\n+\n+    public void testHighWatermarkNotExceeded() throws InterruptedException {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String dataNodeName = internalCluster().startDataOnlyNode();\n+\n+        final InternalClusterInfoService clusterInfoService\n+                = (InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class);\n+        internalCluster().getMasterNodeInstance(ClusterService.class).addListener(event -> clusterInfoService.refresh());\n+\n+        final String dataNode0Id = internalCluster().getInstance(NodeEnvironment.class, dataNodeName).nodeId();\n+        final Path dataNode0Path = internalCluster().getInstance(Environment.class, dataNodeName).dataFiles()[0];\n+\n+        createIndex(\"test\", Settings.builder()\n+                .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+                .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 6)\n+                .put(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build());\n+        final long minShardSize = createReasonableSizedShards();\n+\n+        // reduce disk size of node 0 so that no shards fit below the high watermark, forcing all shards onto the other data node\n+        // (subtract the translog size since the disk threshold decider ignores this and may therefore move the shard back again)\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES - 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), empty());\n+\n+        // increase disk size of node 0 to allow just enough room for one shard, and check that it's rebalanced back\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES + 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), hasSize(1));\n+    }\n+\n+    private Set<ShardRouting> getShardRoutings(String nodeId) {\n+        final Set<ShardRouting> shardRoutings = new HashSet<>();\n+        for (IndexShardRoutingTable indexShardRoutingTable : client().admin().cluster().prepareState().clear().setRoutingTable(true)\n+                .get().getState().getRoutingTable().index(\"test\")) {\n+            for (ShardRouting shard : indexShardRoutingTable.shards()) {\n+                assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\n+                if (shard.currentNodeId().equals(nodeId)) {\n+                    shardRoutings.add(shard);\n+                }\n+            }\n+        }\n+        return shardRoutings;\n+    }\n+\n+    /**\n+     * Index documents until all the shards are at least WATERMARK_BYTES in size, and return the size of the smallest shard\n+     */\n+    private long createReasonableSizedShards() throws InterruptedException {\n+        while (true) {\n+            final IndexRequestBuilder[] indexRequestBuilders = new IndexRequestBuilder[scaledRandomIntBetween(100, 10000)];\n+            for (int i = 0; i < indexRequestBuilders.length; i++) {\n+                indexRequestBuilders[i] = client().prepareIndex(\"test\").setSource(\"field\", randomAlphaOfLength(10));\n+            }\n+            indexRandom(true, indexRequestBuilders);\n+            forceMerge();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwNzEyNg==", "bodyText": "can we somehow only conditionally do this reroute here, i.e.  if all nodes under low watermark?", "url": "https://github.com/elastic/elasticsearch/pull/60460#discussion_r466207126", "createdAt": "2020-08-06T07:41:03Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.cluster.routing.allocation.decider;\n+\n+import org.apache.lucene.mockfile.FilterFileStore;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.apache.lucene.mockfile.FilterPath;\n+import org.apache.lucene.util.Constants;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.cluster.ClusterInfoService;\n+import org.elasticsearch.cluster.InternalClusterInfoService;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.monitor.fs.FsService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.FileStore;\n+import java.nio.file.FileSystem;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.NotDirectoryException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;\n+import static org.elasticsearch.index.store.Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.anyOf;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class DiskThresholdDeciderIT extends ESIntegTestCase {\n+\n+    private static TestFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFilesystemProvider() {\n+        assertNull(fileSystemProvider);\n+        fileSystemProvider = new TestFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFilesystemProvider() {\n+        fileSystemProvider = null;\n+        // ESIntegTestCase takes care of tearing down the mock file system\n+    }\n+\n+    @Before\n+    public void clearTrackedPaths() throws IOException {\n+        fileSystemProvider.clearTrackedPaths();\n+    }\n+\n+    private static final long WATERMARK_BYTES = new ByteSizeValue(10, ByteSizeUnit.KB).getBytes();\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        final Path dataPath = fileSystemProvider.getRootDir().resolve(\"node-\" + nodeOrdinal);\n+        try {\n+            Files.createDirectories(dataPath);\n+        } catch (IOException e) {\n+            throw new AssertionError(\"unexpected\", e);\n+        }\n+        fileSystemProvider.addTrackedPath(dataPath);\n+        return Settings.builder()\n+                .put(super.nodeSettings(nodeOrdinal))\n+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)\n+                .put(FsService.ALWAYS_REFRESH_SETTING.getKey(), true)\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), \"0b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build();\n+    }\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return List.of(InternalSettingsPlugin.class);\n+    }\n+\n+    public void testHighWatermarkNotExceeded() throws InterruptedException {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String dataNodeName = internalCluster().startDataOnlyNode();\n+\n+        final InternalClusterInfoService clusterInfoService\n+                = (InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class);\n+        internalCluster().getMasterNodeInstance(ClusterService.class).addListener(event -> clusterInfoService.refresh());\n+\n+        final String dataNode0Id = internalCluster().getInstance(NodeEnvironment.class, dataNodeName).nodeId();\n+        final Path dataNode0Path = internalCluster().getInstance(Environment.class, dataNodeName).dataFiles()[0];\n+\n+        createIndex(\"test\", Settings.builder()\n+                .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+                .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 6)\n+                .put(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build());\n+        final long minShardSize = createReasonableSizedShards();\n+\n+        // reduce disk size of node 0 so that no shards fit below the high watermark, forcing all shards onto the other data node\n+        // (subtract the translog size since the disk threshold decider ignores this and may therefore move the shard back again)\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES - 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), empty());\n+\n+        // increase disk size of node 0 to allow just enough room for one shard, and check that it's rebalanced back\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES + 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), hasSize(1));\n+    }\n+\n+    private Set<ShardRouting> getShardRoutings(String nodeId) {\n+        final Set<ShardRouting> shardRoutings = new HashSet<>();\n+        for (IndexShardRoutingTable indexShardRoutingTable : client().admin().cluster().prepareState().clear().setRoutingTable(true)\n+                .get().getState().getRoutingTable().index(\"test\")) {\n+            for (ShardRouting shard : indexShardRoutingTable.shards()) {\n+                assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\n+                if (shard.currentNodeId().equals(nodeId)) {\n+                    shardRoutings.add(shard);\n+                }\n+            }\n+        }\n+        return shardRoutings;\n+    }\n+\n+    /**\n+     * Index documents until all the shards are at least WATERMARK_BYTES in size, and return the size of the smallest shard\n+     */\n+    private long createReasonableSizedShards() throws InterruptedException {\n+        while (true) {\n+            final IndexRequestBuilder[] indexRequestBuilders = new IndexRequestBuilder[scaledRandomIntBetween(100, 10000)];\n+            for (int i = 0; i < indexRequestBuilders.length; i++) {\n+                indexRequestBuilders[i] = client().prepareIndex(\"test\").setSource(\"field\", randomAlphaOfLength(10));\n+            }\n+            indexRandom(true, indexRequestBuilders);\n+            forceMerge();\n+            refresh();\n+\n+            final ShardStats[] shardStatses = client().admin().indices().prepareStats(\"test\")\n+                    .clear().setStore(true).setTranslog(true).get().getShards();\n+            final long[] shardSizes = new long[shardStatses.length];\n+            for (ShardStats shardStats : shardStatses) {\n+                shardSizes[shardStats.getShardRouting().id()] = shardStats.getStats().getStore().sizeInBytes();\n+            }\n+\n+            final long minShardSize = Arrays.stream(shardSizes).min().orElseThrow(() -> new AssertionError(\"no shards\"));\n+            if (minShardSize > WATERMARK_BYTES) {\n+                return minShardSize;\n+            }\n+        }\n+    }\n+\n+    private void refreshDiskUsage() {\n+        ((InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class)).refresh();\n+        // if the nodes were all under the low watermark already (but unbalanced) then a change in the disk usage doesn't trigger a reroute\n+        // even though it's now possible to achieve better balance, so we have to do an explicit reroute. TODO fix this?\n+        assertAcked(client().admin().cluster().prepareReroute());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwOTI1Mg==", "bodyText": "Instead of excluding _state and  translog path, should we select the index folder explicitly? Or  assert that  there are  no other paths?", "url": "https://github.com/elastic/elasticsearch/pull/60460#discussion_r466209252", "createdAt": "2020-08-06T07:45:12Z", "author": {"login": "ywelsch"}, "path": "server/src/internalClusterTest/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderIT.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.cluster.routing.allocation.decider;\n+\n+import org.apache.lucene.mockfile.FilterFileStore;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.apache.lucene.mockfile.FilterPath;\n+import org.apache.lucene.util.Constants;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.index.IndexRequestBuilder;\n+import org.elasticsearch.cluster.ClusterInfoService;\n+import org.elasticsearch.cluster.InternalClusterInfoService;\n+import org.elasticsearch.cluster.metadata.IndexMetadata;\n+import org.elasticsearch.cluster.routing.IndexShardRoutingTable;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.routing.ShardRoutingState;\n+import org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.io.PathUtils;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.unit.ByteSizeUnit;\n+import org.elasticsearch.common.unit.ByteSizeValue;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.env.Environment;\n+import org.elasticsearch.env.NodeEnvironment;\n+import org.elasticsearch.monitor.fs.FsService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.FileStore;\n+import java.nio.file.FileSystem;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.NotDirectoryException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMap;\n+import static org.elasticsearch.index.store.Store.INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING;\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.Matchers.anyOf;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.greaterThan;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class DiskThresholdDeciderIT extends ESIntegTestCase {\n+\n+    private static TestFileSystemProvider fileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installFilesystemProvider() {\n+        assertNull(fileSystemProvider);\n+        fileSystemProvider = new TestFileSystemProvider(PathUtils.getDefaultFileSystem(), createTempDir());\n+        PathUtilsForTesting.installMock(fileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeFilesystemProvider() {\n+        fileSystemProvider = null;\n+        // ESIntegTestCase takes care of tearing down the mock file system\n+    }\n+\n+    @Before\n+    public void clearTrackedPaths() throws IOException {\n+        fileSystemProvider.clearTrackedPaths();\n+    }\n+\n+    private static final long WATERMARK_BYTES = new ByteSizeValue(10, ByteSizeUnit.KB).getBytes();\n+\n+    @Override\n+    protected Settings nodeSettings(int nodeOrdinal) {\n+        final Path dataPath = fileSystemProvider.getRootDir().resolve(\"node-\" + nodeOrdinal);\n+        try {\n+            Files.createDirectories(dataPath);\n+        } catch (IOException e) {\n+            throw new AssertionError(\"unexpected\", e);\n+        }\n+        fileSystemProvider.addTrackedPath(dataPath);\n+        return Settings.builder()\n+                .put(super.nodeSettings(nodeOrdinal))\n+                .put(Environment.PATH_DATA_SETTING.getKey(), dataPath)\n+                .put(FsService.ALWAYS_REFRESH_SETTING.getKey(), true)\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_LOW_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_HIGH_DISK_WATERMARK_SETTING.getKey(), WATERMARK_BYTES + \"b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_DISK_FLOOD_STAGE_WATERMARK_SETTING.getKey(), \"0b\")\n+                .put(DiskThresholdSettings.CLUSTER_ROUTING_ALLOCATION_REROUTE_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build();\n+    }\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        return List.of(InternalSettingsPlugin.class);\n+    }\n+\n+    public void testHighWatermarkNotExceeded() throws InterruptedException {\n+        internalCluster().startMasterOnlyNode();\n+        internalCluster().startDataOnlyNode();\n+        final String dataNodeName = internalCluster().startDataOnlyNode();\n+\n+        final InternalClusterInfoService clusterInfoService\n+                = (InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class);\n+        internalCluster().getMasterNodeInstance(ClusterService.class).addListener(event -> clusterInfoService.refresh());\n+\n+        final String dataNode0Id = internalCluster().getInstance(NodeEnvironment.class, dataNodeName).nodeId();\n+        final Path dataNode0Path = internalCluster().getInstance(Environment.class, dataNodeName).dataFiles()[0];\n+\n+        createIndex(\"test\", Settings.builder()\n+                .put(IndexMetadata.SETTING_NUMBER_OF_REPLICAS, 0)\n+                .put(IndexMetadata.SETTING_NUMBER_OF_SHARDS, 6)\n+                .put(INDEX_STORE_STATS_REFRESH_INTERVAL_SETTING.getKey(), \"0ms\")\n+                .build());\n+        final long minShardSize = createReasonableSizedShards();\n+\n+        // reduce disk size of node 0 so that no shards fit below the high watermark, forcing all shards onto the other data node\n+        // (subtract the translog size since the disk threshold decider ignores this and may therefore move the shard back again)\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES - 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), empty());\n+\n+        // increase disk size of node 0 to allow just enough room for one shard, and check that it's rebalanced back\n+        fileSystemProvider.getTestFileStore(dataNode0Path).setTotalSpace(minShardSize + WATERMARK_BYTES + 1L);\n+        refreshDiskUsage();\n+        assertThat(getShardRoutings(dataNode0Id), hasSize(1));\n+    }\n+\n+    private Set<ShardRouting> getShardRoutings(String nodeId) {\n+        final Set<ShardRouting> shardRoutings = new HashSet<>();\n+        for (IndexShardRoutingTable indexShardRoutingTable : client().admin().cluster().prepareState().clear().setRoutingTable(true)\n+                .get().getState().getRoutingTable().index(\"test\")) {\n+            for (ShardRouting shard : indexShardRoutingTable.shards()) {\n+                assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\n+                if (shard.currentNodeId().equals(nodeId)) {\n+                    shardRoutings.add(shard);\n+                }\n+            }\n+        }\n+        return shardRoutings;\n+    }\n+\n+    /**\n+     * Index documents until all the shards are at least WATERMARK_BYTES in size, and return the size of the smallest shard\n+     */\n+    private long createReasonableSizedShards() throws InterruptedException {\n+        while (true) {\n+            final IndexRequestBuilder[] indexRequestBuilders = new IndexRequestBuilder[scaledRandomIntBetween(100, 10000)];\n+            for (int i = 0; i < indexRequestBuilders.length; i++) {\n+                indexRequestBuilders[i] = client().prepareIndex(\"test\").setSource(\"field\", randomAlphaOfLength(10));\n+            }\n+            indexRandom(true, indexRequestBuilders);\n+            forceMerge();\n+            refresh();\n+\n+            final ShardStats[] shardStatses = client().admin().indices().prepareStats(\"test\")\n+                    .clear().setStore(true).setTranslog(true).get().getShards();\n+            final long[] shardSizes = new long[shardStatses.length];\n+            for (ShardStats shardStats : shardStatses) {\n+                shardSizes[shardStats.getShardRouting().id()] = shardStats.getStats().getStore().sizeInBytes();\n+            }\n+\n+            final long minShardSize = Arrays.stream(shardSizes).min().orElseThrow(() -> new AssertionError(\"no shards\"));\n+            if (minShardSize > WATERMARK_BYTES) {\n+                return minShardSize;\n+            }\n+        }\n+    }\n+\n+    private void refreshDiskUsage() {\n+        ((InternalClusterInfoService) internalCluster().getMasterNodeInstance(ClusterInfoService.class)).refresh();\n+        // if the nodes were all under the low watermark already (but unbalanced) then a change in the disk usage doesn't trigger a reroute\n+        // even though it's now possible to achieve better balance, so we have to do an explicit reroute. TODO fix this?\n+        assertAcked(client().admin().cluster().prepareReroute());\n+        assertFalse(client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID)\n+                .setWaitForNoRelocatingShards(true)\n+                .setWaitForNoInitializingShards(true).get().isTimedOut());\n+    }\n+\n+    private static class TestFileStore extends FilterFileStore {\n+\n+        private final Path path;\n+\n+        private volatile long totalSpace = -1;\n+\n+        TestFileStore(FileStore delegate, String scheme, Path path) {\n+            super(delegate, scheme);\n+            this.path = path;\n+        }\n+\n+        @Override\n+        public String name() {\n+            return \"fake\"; // Lucene's is-spinning-disk check expects the device name here\n+        }\n+\n+        @Override\n+        public long getTotalSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getTotalSpace();\n+            } else {\n+                return totalSpace;\n+            }\n+        }\n+\n+        public void setTotalSpace(long totalSpace) {\n+            assertThat(totalSpace, anyOf(is(-1L), greaterThan(0L)));\n+            this.totalSpace = totalSpace;\n+        }\n+\n+        @Override\n+        public long getUsableSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getUsableSpace();\n+            } else {\n+                return Math.max(0L, totalSpace - getTotalFileSize(path));\n+            }\n+        }\n+\n+        @Override\n+        public long getUnallocatedSpace() throws IOException {\n+            final long totalSpace = this.totalSpace;\n+            if (totalSpace == -1) {\n+                return super.getUnallocatedSpace();\n+            } else {\n+                return Math.max(0L, totalSpace - getTotalFileSize(path));\n+            }\n+        }\n+\n+        private static long getTotalFileSize(Path path) throws IOException {\n+            if (Files.isRegularFile(path)) {\n+                try {\n+                    return Files.size(path);\n+                } catch (NoSuchFileException | FileNotFoundException e) {\n+                    // probably removed\n+                    return 0L;\n+                }\n+            } else if (path.getFileName().toString().equals(\"_state\") || path.getFileName().toString().equals(\"translog\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946e2b6e54f0148af499fa495965e99282f69967"}, "originalPosition": 271}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd4bc074fbaa667c05bf6ccd2c5236f3659a6a0e", "author": {"user": {"login": "ywelsch", "name": "Yannick Welsch"}}, "url": "https://github.com/elastic/elasticsearch/commit/cd4bc074fbaa667c05bf6ccd2c5236f3659a6a0e", "committedDate": "2020-09-03T15:13:30Z", "message": "Merge remote-tracking branch 'elastic/master' into high-disk-tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a40d5af10418fa3d58b805a2c7fb4fcca7d2ae5", "author": {"user": {"login": "ywelsch", "name": "Yannick Welsch"}}, "url": "https://github.com/elastic/elasticsearch/commit/4a40d5af10418fa3d58b805a2c7fb4fcca7d2ae5", "committedDate": "2020-09-03T15:25:39Z", "message": "Fix issue with overriding filesystem\n\nWhen tests were run in a loop, e.g. using\n\n./gradlew ':server:internalClusterTest' --tests \"org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDeciderIT.testHighWatermarkNotExceeded\" -Dtests.seed=CBF5914690DC5CAC -Dtests.security.manager=true -Dtests.jvms=4 -Dtests.locale=de-CH -Dtests.timezone=Asia/Chungking -Dtests.iters=2 -Dtests.failfast=true\n\nthey would fail due to the way filesystem are overridden, interfering with the filesystem setup done by ESTestCase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "14a8147df637da53d3f88ba2a235e84aacc1a174", "author": {"user": {"login": "ywelsch", "name": "Yannick Welsch"}}, "url": "https://github.com/elastic/elasticsearch/commit/14a8147df637da53d3f88ba2a235e84aacc1a174", "committedDate": "2020-09-04T12:04:28Z", "message": "conditionally reroute"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ca2b00a67096139fd17b0aa561b43fa11eb4655", "author": {"user": {"login": "ywelsch", "name": "Yannick Welsch"}}, "url": "https://github.com/elastic/elasticsearch/commit/6ca2b00a67096139fd17b0aa561b43fa11eb4655", "committedDate": "2020-09-04T12:12:48Z", "message": "Merge remote-tracking branch 'elastic/master' into high-disk-tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzNDYwNDQ4", "url": "https://github.com/elastic/elasticsearch/pull/60460#pullrequestreview-483460448", "createdAt": "2020-09-07T10:52:03Z", "commit": {"oid": "6ca2b00a67096139fd17b0aa561b43fa11eb4655"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3664, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}