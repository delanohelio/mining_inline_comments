{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyODUzNTQ0", "number": 62167, "title": "Abort non-fully consumed S3 input streams instead of draining", "bodyText": "Today when an S3RetryingInputStream is closed the remaining bytes that were not consumed are drained right before closing the underlying stream. In some contexts it might be more efficient to not consume the remaining bytes and just drop the connection.\nThis is for example the case with snapshot backed indices prewarming, where there is not point in reading potentially large blobs if we know the cache file we want to write the content of the blob as already been evicted. Draining all bytes here takes a slot in the prewarming thread pool for nothing.\nRegular snapshot restores could also benefit from dropping connection instead of draining bytes in the case the restore is aborted. As of today, the restoring of the file continues even if the restore was aborted and takes a slot in the snapshot thread pool. By throwing an appropriate exception and aborting the S3 input stream we could quickly stop the download and free up the slot in the snapshot thread pool (could be done in a follow up PR).", "createdAt": "2020-09-09T13:40:56Z", "url": "https://github.com/elastic/elasticsearch/pull/62167", "merged": true, "mergeCommit": {"oid": "92fb003566df04364d5fd74956818f275b067b8b"}, "closed": true, "closedAt": "2020-09-15T10:52:42Z", "author": {"login": "tlrx"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdHMN_jgH2gAyNDgyODUzNTQ0OjgyNGU4ZjlmZjA5YjAxNzljMGNiNWZjNjcwNDc2Njk4ZGY5OGRmMzY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdJC4YkgH2gAyNDgyODUzNTQ0OjAzZjVmMzc0MzkwMGM1M2Q4MjMyYTUyNDQ2YWMyMjZlOGY2NzA2Nzc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36", "author": {"user": {"login": "tlrx", "name": "Tanguy Leroux"}}, "url": "https://github.com/elastic/elasticsearch/commit/824e8f9ff09b0179c0cb5fc670476698df98df36", "committedDate": "2020-09-09T13:28:51Z", "message": "Abort non-fully consumed S3 input stream"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3NTU1Njc5", "url": "https://github.com/elastic/elasticsearch/pull/62167#pullrequestreview-487555679", "createdAt": "2020-09-14T09:00:57Z", "commit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOTowODozMVrOHRKtfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwOTozNTo1MlrOHRLs2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc2MzMyNg==", "bodyText": "See my comment below, I think this isn't necessary potentially.", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487763326", "createdAt": "2020-09-14T09:08:31Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -53,12 +54,14 @@\n     private final long start;\n     private final long end;\n     private final int maxAttempts;\n+    private final List<IOException> failures;\n \n-    private InputStream currentStream;\n+    private S3ObjectInputStream currentStream;\n+    private long currentStreamLastOffset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3MTIwMQ==", "bodyText": "Same here, maybe we should just use our own end and start offsets or use metadata.getContentLength() if we don't have an end instead of going through the indirection of the SDH header parsing here? That seems a lot more straight forward to me and doesn't require us to be scared of random exceptions from SDK misbehavior?\nAlso, then we could just make our life real easy. If eof is set to true or start + currentOffset == currentStreamLastOffset -> close, else abort. No need to even get the length from the metadata because any open ended stream of unknown length we'd read till EOF anyway?", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487771201", "createdAt": "2020-09-14T09:21:33Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -101,12 +106,35 @@ private InputStream openStream() throws IOException {\n         }\n     }\n \n+    private long getStreamLength(final S3Object object) {\n+        final ObjectMetadata metadata = object.getObjectMetadata();\n+        try {\n+            // Returns the content range of the object if response contains the Content-Range header.\n+            final Long[] range = metadata.getContentRange();\n+            if (range != null) {\n+                assert range[1] >= range[0] : range[1] + \" vs \" + range[0];\n+                assert range[0] == start + currentOffset :\n+                    \"Content-Range start value [\" + range[0] + \"] exceeds start [\" + start + \"] + current offset [\" + currentOffset + ']';\n+                assert range[1] == end : \"Content-Range end value [\" + range[1] + \"] exceeds end [\" + end + ']';\n+                return range[1] - range[0] + 1L;\n+            }\n+            return metadata.getContentLength();\n+        } catch (Exception e) {\n+            assert false : e;\n+            return Long.MAX_VALUE - 1L; // assume a large stream so that the underlying stream is aborted on closing, unless eof is reached", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3OTU0NA==", "bodyText": "if (eof || start + currentOffset == currentStreamLastOffset) { and drop the conditional from the try { since both cases mean the same to us?", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487779544", "createdAt": "2020-09-14T09:35:52Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -151,24 +180,36 @@ private void reopenStreamOrFail(IOException e) throws IOException {\n         if (failures.size() < MAX_SUPPRESSED_EXCEPTIONS) {\n             failures.add(e);\n         }\n-        try {\n-            Streams.consumeFully(currentStream);\n-        } catch (Exception e2) {\n-            logger.trace(\"Failed to fully consume stream on close\", e);\n-        }\n+        maybeAbort(currentStream);\n         IOUtils.closeWhileHandlingException(currentStream);\n-        currentStream = openStream();\n+        openStream();\n     }\n \n     @Override\n     public void close() throws IOException {\n+        maybeAbort(currentStream);\n         try {\n-            Streams.consumeFully(currentStream);\n+            currentStream.close();\n+        } finally {\n+            closed = true;\n+        }\n+    }\n+\n+    /**\n+     * Abort the {@link S3ObjectInputStream} if it wasn't read completely at the time this method is called,\n+     * suppressing all thrown exceptions.\n+     */\n+    private void maybeAbort(S3ObjectInputStream stream) {\n+        if (eof) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3NjQ1NzEz", "url": "https://github.com/elastic/elasticsearch/pull/62167#pullrequestreview-487645713", "createdAt": "2020-09-14T11:04:41Z", "commit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxMTowNDo0MVrOHROtZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxMTowNDo0MVrOHROtZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgyODgzOQ==", "bodyText": "Fair point, I guess I still don't really like that we redundantly store the lengths and offsets here to some degree, but I also just noticed we we do the same in the GCS stream as well. I suppose this approach is the safest for now :) => let's go with it then.", "url": "https://github.com/elastic/elasticsearch/pull/62167#discussion_r487828839", "createdAt": "2020-09-14T11:04:41Z", "author": {"login": "original-brownbear"}, "path": "plugins/repository-s3/src/main/java/org/elasticsearch/repositories/s3/S3RetryingInputStream.java", "diffHunk": "@@ -101,12 +106,35 @@ private InputStream openStream() throws IOException {\n         }\n     }\n \n+    private long getStreamLength(final S3Object object) {\n+        final ObjectMetadata metadata = object.getObjectMetadata();\n+        try {\n+            // Returns the content range of the object if response contains the Content-Range header.\n+            final Long[] range = metadata.getContentRange();\n+            if (range != null) {\n+                assert range[1] >= range[0] : range[1] + \" vs \" + range[0];\n+                assert range[0] == start + currentOffset :\n+                    \"Content-Range start value [\" + range[0] + \"] exceeds start [\" + start + \"] + current offset [\" + currentOffset + ']';\n+                assert range[1] == end : \"Content-Range end value [\" + range[1] + \"] exceeds end [\" + end + ']';\n+                return range[1] - range[0] + 1L;\n+            }\n+            return metadata.getContentLength();\n+        } catch (Exception e) {\n+            assert false : e;\n+            return Long.MAX_VALUE - 1L; // assume a large stream so that the underlying stream is aborted on closing, unless eof is reached", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc3MTIwMQ=="}, "originalCommit": {"oid": "824e8f9ff09b0179c0cb5fc670476698df98df36"}, "originalPosition": 77}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03f5f3743900c53d8232a52446ac226e8f670677", "author": {"user": {"login": "elasticmachine", "name": "Elastic Machine"}}, "url": "https://github.com/elastic/elasticsearch/commit/03f5f3743900c53d8232a52446ac226e8f670677", "committedDate": "2020-09-15T07:43:57Z", "message": "Merge branch 'master' into abort-s3-retrying-input-streams"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4647, "cost": 1, "resetAt": "2021-10-28T19:08:13Z"}}}