{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEyMjkxMjE4", "number": 56072, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMToxODoyOFrOD4fMZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMToxODoyOFrOD4fMZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNTU3OTI1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMToxODoyOFrOGPWADQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxMjo1Mzo1NVrOGPjBjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc0MjI4NQ==", "bodyText": "NIT: I guess technically if lastChunk == true we could just not add the buffer back to the queue and just use () -> {} here? (not sure it matters much but for 500k buffers it might be worth it?)", "url": "https://github.com/elastic/elasticsearch/pull/56072#discussion_r418742285", "createdAt": "2020-05-01T21:18:28Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java", "diffHunk": "@@ -868,12 +878,14 @@ public void close() throws IOException {\n                 protected FileChunk nextChunkRequest(StoreFileMetadata md) throws IOException {\n                     assert Transports.assertNotTransportThread(\"read file chunk\");\n                     cancellableThreads.checkForCancel();\n+                    final byte[] buffer = Objects.requireNonNullElseGet(buffers.pollFirst(), () -> new byte[chunkSizeInBytes]);\n                     final int bytesRead = currentInput.read(buffer);\n                     if (bytesRead == -1) {\n                         throw new CorruptIndexException(\"file truncated; length=\" + md.length() + \" offset=\" + offset, md.name());\n                     }\n                     final boolean lastChunk = offset + bytesRead == md.length();\n-                    final FileChunk chunk = new FileChunk(md, new BytesArray(buffer, 0, bytesRead), offset, lastChunk);\n+                    final FileChunk chunk = new FileChunk(md, new BytesArray(buffer, 0, bytesRead), offset, lastChunk,\n+                        () -> buffers.addFirst(buffer));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d793485b9e374feb523c99d4e07240f5934b3ef2"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2MDM2OQ==", "bodyText": "I think we still need to add the buffer back because the flag lastChunk is for each file while the buffers are used for multiple files.", "url": "https://github.com/elastic/elasticsearch/pull/56072#discussion_r418760369", "createdAt": "2020-05-01T22:15:07Z", "author": {"login": "dnhatn"}, "path": "server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java", "diffHunk": "@@ -868,12 +878,14 @@ public void close() throws IOException {\n                 protected FileChunk nextChunkRequest(StoreFileMetadata md) throws IOException {\n                     assert Transports.assertNotTransportThread(\"read file chunk\");\n                     cancellableThreads.checkForCancel();\n+                    final byte[] buffer = Objects.requireNonNullElseGet(buffers.pollFirst(), () -> new byte[chunkSizeInBytes]);\n                     final int bytesRead = currentInput.read(buffer);\n                     if (bytesRead == -1) {\n                         throw new CorruptIndexException(\"file truncated; length=\" + md.length() + \" offset=\" + offset, md.name());\n                     }\n                     final boolean lastChunk = offset + bytesRead == md.length();\n-                    final FileChunk chunk = new FileChunk(md, new BytesArray(buffer, 0, bytesRead), offset, lastChunk);\n+                    final FileChunk chunk = new FileChunk(md, new BytesArray(buffer, 0, bytesRead), offset, lastChunk,\n+                        () -> buffers.addFirst(buffer));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc0MjI4NQ=="}, "originalCommit": {"oid": "d793485b9e374feb523c99d4e07240f5934b3ef2"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk1NTY2MQ==", "bodyText": "Right \ud83e\udd26 => nevermind :)", "url": "https://github.com/elastic/elasticsearch/pull/56072#discussion_r418955661", "createdAt": "2020-05-02T12:53:55Z", "author": {"login": "original-brownbear"}, "path": "server/src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java", "diffHunk": "@@ -868,12 +878,14 @@ public void close() throws IOException {\n                 protected FileChunk nextChunkRequest(StoreFileMetadata md) throws IOException {\n                     assert Transports.assertNotTransportThread(\"read file chunk\");\n                     cancellableThreads.checkForCancel();\n+                    final byte[] buffer = Objects.requireNonNullElseGet(buffers.pollFirst(), () -> new byte[chunkSizeInBytes]);\n                     final int bytesRead = currentInput.read(buffer);\n                     if (bytesRead == -1) {\n                         throw new CorruptIndexException(\"file truncated; length=\" + md.length() + \" offset=\" + offset, md.name());\n                     }\n                     final boolean lastChunk = offset + bytesRead == md.length();\n-                    final FileChunk chunk = new FileChunk(md, new BytesArray(buffer, 0, bytesRead), offset, lastChunk);\n+                    final FileChunk chunk = new FileChunk(md, new BytesArray(buffer, 0, bytesRead), offset, lastChunk,\n+                        () -> buffers.addFirst(buffer));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc0MjI4NQ=="}, "originalCommit": {"oid": "d793485b9e374feb523c99d4e07240f5934b3ef2"}, "originalPosition": 67}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2442, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}