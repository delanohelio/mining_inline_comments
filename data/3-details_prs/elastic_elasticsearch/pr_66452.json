{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxMjc0NDI4", "number": 66452, "title": "New terms_enum API for discovering terms in the index.", "bodyText": "A search string is supplied which is used as prefix for matching terms found in a given field in the index.\nA timeout can limit the amount of time spent looking for matches.\nDesigned for use in Kibana auto-complete use cases.\nKibana requests for this API would typically look like this:\nlocalhost:9200/myindex/_terms_enum\n{\n\"field\" : \"myfield\",\n\"string\" : \"Microsof\",\n\"index_filter\": {\n    \"bool\":{\n        \"must\":[\n           { \"range\": \"my Kibana time-picker range\"},\n           { \"terms\": {\"_tier\": [\"data_warm\", \"data_hot\"]}\n         ]\n}}\n\nThe time range would avoid any indices that fall outside of the range but does not filter any doc values in overlapping indices. The tier clause would avoid hitting frozen/cold indices.\nAn optional timeout time value can also be passed  (default is \"1s\", one second).\nThe response looks like this:\n{\n  \"_shards\": {\n    \"total\": 1,\n    \"successful\": 1,\n    \"failed\": 0\n  },\n  \"terms\": [\n    \"Microsoft\",\n    \"Microsoft Windows\"\n  ],\n  \"complete\": true\n}\n\nAny requests that hit the timeout setting will return \"complete\":false", "createdAt": "2020-12-16T16:08:32Z", "url": "https://github.com/elastic/elasticsearch/pull/66452", "merged": true, "mergeCommit": {"oid": "73e0662f091809226fe3c3d9374f63b1b96bb2ce"}, "closed": true, "closedAt": "2021-05-06T09:45:37Z", "author": {"login": "markharwood"}, "timelineItems": {"totalCount": 75, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdm2oTPgFqTU1NDEwOTczNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABeUDWFvgBqjQ2OTM3MDgxNjk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MTA5NzM0", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-554109734", "createdAt": "2020-12-16T21:45:14Z", "commit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMTo0NToxNVrOIHaf7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQyMjoyNDowMlrOIHbzEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY0NTEwMA==", "bodyText": "I wonder why the API is <index>/_terms/list, why not just <index>/_terms?  Are we expecting to add more to _terms besides list?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544645100", "createdAt": "2020-12-16T21:45:15Z", "author": {"login": "mayya-sharipova"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms/list", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2MTUyMQ==", "bodyText": "since getDocCount() returns a primitive type, may be for comparison we can just do: getDocCount() == other.gertDocCount()?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544661521", "createdAt": "2020-12-16T22:14:55Z", "author": {"login": "mayya-sharipova"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {\n+        this.term = term;\n+        this.docCount = count;\n+    }\n+\n+    public String getTerm() {\n+        return this.term;\n+    }\n+\n+    public long getDocCount() {\n+        return this.docCount;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeString(term);\n+        out.writeLong(docCount);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.field(TERM_FIELD, getTerm());\n+        builder.field(DOC_COUNT_FIELD, getDocCount());\n+        return builder;\n+    }\n+\n+    public static TermCount fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        TermCount other = (TermCount) o;\n+        return Objects.equals(getTerm(), other.getTerm()) && Objects.equals(getDocCount(), other.getDocCount());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NDE4MQ==", "bodyText": "what does SKIPPED_FIELD is responsible for ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544664181", "createdAt": "2020-12-16T22:19:55Z", "author": {"login": "mayya-sharipova"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumResponse.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.broadcast.BroadcastResponse;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+/**\n+ * The response of the termenum/list action.\n+ */\n+public class TermEnumResponse extends BroadcastResponse {\n+\n+    public static final String TERMS_FIELD = \"terms\";\n+    public static final String TIMED_OUT_FIELD = \"timed_out\";\n+    public static final String SKIPPED_FIELD = \"skipped\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw==", "bodyText": "May be too early to talk about this, but when security is enabled, who can access to this endpoint (those who can read the index))?  what about when FLS and DLS is enabled?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r544666387", "createdAt": "2020-12-16T22:24:02Z", "author": {"login": "mayya-sharipova"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {\n+\n+    public static final TermEnumAction INSTANCE = new TermEnumAction();\n+//    public static final String NAME = \"indices:admin/termsenum/list\";\n+    public static final String NAME = \"indices:data/read/xpack/termsenum/list\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxMDAyMTcz", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-561002173", "createdAt": "2021-01-04T12:45:05Z", "commit": {"oid": "c0d98c2a5895775c1b75b689bca8edd9f4518de6"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMjo0NTowNVrOINwcjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMjo1NzowMFrOINwxjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTI5NjE0MQ==", "bodyText": "I don't think we should mix the two use cases. This API is useful to list the terms that appear in a field. Allowing a prefix-based completion seems acceptable but anything beyond that would suffer at large scale. Regexes and patterns in general are costly and serve a different purpose so they shouldn't be allowed here.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551296141", "createdAt": "2021-01-04T12:45:05Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0d98c2a5895775c1b75b689bca8edd9f4518de6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTI5NzY1NQ==", "bodyText": "+1 for _terms only. We want to limit the number of options here so we don't need to plan for future extension imo. If we need another option, it should be added in the main API.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551297655", "createdAt": "2021-01-04T12:48:06Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms/list", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY0NTEwMA=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTI5OTg0Mw==", "bodyText": "There are way too many options imo. The main use case is to retrieve the full list of terms from low cardinality fields so I think we should limit our options to the minimum.\nI propose the following list which is already big ;):\n\nfield\ntimeout\nsize\nprefix\nindex_filter\nsort_by_popularity\n\nWe can also handle case_insensitive if needed but that should be enough for the initial API.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551299843", "createdAt": "2021-01-04T12:53:21Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,130 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a pattern. This can be useful for features like auto-complete or regexp authoring tools:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms/list\n+{\n+    \"field\" : \"tags\",\n+    \"pattern\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    {\n+      \"term\": \"kibana\",\n+      \"doc_count\": 8\n+    }\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms/list`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that match\n+a pattern. By default it looks for terms that begin with the provided\n+pattern but more complex pattern matching can be used by setting the\n+appropriate flags that control matching.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0d98c2a5895775c1b75b689bca8edd9f4518de6"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTMwMTUxOA==", "bodyText": "We shouldn't return the doc count in the final response. It's an approximation that cannot be used without an error bound so I'd rather remove it from the response.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r551301518", "createdAt": "2021-01-04T12:57:00Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0d98c2a5895775c1b75b689bca8edd9f4518de6"}, "originalPosition": 47}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "68cb22ec790ca40510584f5fba567fa5023a1b93", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/68cb22ec790ca40510584f5fba567fa5023a1b93", "committedDate": "2021-01-05T17:42:27Z", "message": "The way TermEnums are obtained has changed."}, "afterCommit": {"oid": "2a1ac2dd39787de3a6f150c6dbb04ba927645525", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/2a1ac2dd39787de3a6f150c6dbb04ba927645525", "committedDate": "2021-01-05T17:53:54Z", "message": "The way TermEnums are obtained has changed."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0a1a2d3a42a480bb546746413cee75edac6f0fce", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/0a1a2d3a42a480bb546746413cee75edac6f0fce", "committedDate": "2021-01-07T10:31:44Z", "message": "Removed counts from response and related tests"}, "afterCommit": {"oid": "b48a4730e2ac8335766dc214ebfe8a5577f588cc", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/b48a4730e2ac8335766dc214ebfe8a5577f588cc", "committedDate": "2021-01-07T12:19:35Z", "message": "Removed counts from response and related tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY2MjQ2NDcy", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-566246472", "createdAt": "2021-01-12T13:04:10Z", "commit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzowNDoxMFrOISAjvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMlQxMzo0NDozOFrOISCD4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1NDQyOQ==", "bodyText": "Should we forbid searching all data ? Plain * or omitting the pattern could return an error ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555754429", "createdAt": "2021-01-12T13:04:10Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1ODA5NQ==", "bodyText": "We also need to know if the result is complete or not ? For small cardinality field it can be useful to know that the response contains all the possible values for that input.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555758095", "createdAt": "2021-01-12T13:10:26Z", "author": {"login": "jimczi"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1ODM5MA==", "bodyText": "You need to change the name here too (string)  ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555758390", "createdAt": "2021-01-12T13:11:02Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/ShardTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.support.broadcast.BroadcastShardRequest;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.search.internal.AliasFilter;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+/**\n+ * Internal termenum request executed directly against a specific index shard.\n+ */\n+public class ShardTermEnumRequest extends BroadcastShardRequest {\n+\n+    private String field;\n+    private String pattern;\n+    private long taskStartedTimeMillis;\n+    private long shardStartedTimeMillis;\n+    private AliasFilter filteringAliases;\n+    private boolean caseInsensitive;\n+    private boolean sortByPopularity;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    \n+\n+    public ShardTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a shard.\n+        shardStartedTimeMillis = System.currentTimeMillis();\n+\n+        filteringAliases = new AliasFilter(in);\n+        field = in.readString();\n+        pattern = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        sortByPopularity = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+    }\n+\n+    public ShardTermEnumRequest(ShardId shardId, AliasFilter filteringAliases, TermEnumRequest request) {\n+        super(shardId, request);\n+        this.field = request.field();\n+        this.pattern = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.sortByPopularity = request.sortByPopularity();\n+        this.filteringAliases = Objects.requireNonNull(filteringAliases, \"filteringAliases must not be null\");\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String pattern() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc1OTc3MQ==", "bodyText": "Is the TODO still relevant ? The threadpool is there from what I can see.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555759771", "createdAt": "2021-01-12T13:13:25Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc2MzMyMQ==", "bodyText": "We should rely on node's data-tier when selecting indices and not care about index settings. The goal is to   restrict the API to some data-tiers, we can add more flexibility in the future but I'd prefer that we start simple. Only the hot and warm tier should be queried.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555763321", "createdAt": "2021-01-12T13:19:24Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc2NDQzMQ==", "bodyText": "we should never catch an AssertionError, did you mean Exception ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555764431", "createdAt": "2021-01-12T13:21:21Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);\n+        try {\n+            IndexReader reader = searchContext.getQueryShardContext().searcher().getTopReaderContext().reader();\n+            Terms terms = MultiTerms.getTerms(reader, request.field());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+            }\n+            Automaton a = request.caseInsensitive()\n+                    ? AutomatonQueries.caseInsensitivePrefix(request.pattern())\n+                    : Automata.makeString(request.pattern());\n+            a = Operations.concatenate(a, Automata.makeAnyString());\n+            a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+\n+            // TODO make this a param and scale up based on num shards like we do with terms aggs?\n+            int shard_size = request.size();\n+\n+            CompiledAutomaton automaton = new CompiledAutomaton(a);\n+            TermsEnum te = automaton.getTermsEnum(terms);\n+\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            if (request.sortByPopularity()) {\n+                // Collect most popular matches\n+                TermCountPriorityQueue pq = new TermCountPriorityQueue(shard_size);\n+                TermCount spare = null;\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            // Gather what we have collected so far\n+                            while (pq.size() > 0) {\n+                                termsList.add(pq.pop());\n+                            }\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+\n+                    if (spare == null) {\n+                        spare = new TermCount(bytes.utf8ToString(), df);\n+                    } else {\n+                        spare.setTerm(bytes.utf8ToString());\n+                        spare.setDocCount(df);\n+                    }\n+                    spare = pq.insertWithOverflow(spare);\n+                }\n+                while (pq.size() > 0) {\n+                    termsList.add(pq.pop());\n+                }\n+            } else {\n+                // Collect in alphabetical order\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+                    termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                    if (termsList.size() >= shard_size) {\n+                        break;\n+                    }\n+                }\n+            }\n+\n+        } catch (AssertionError e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 335}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ==", "bodyText": "In the interest of limiting the number of threads, I wonder if we can group the shard requests per node ?\nIf there are 30 shards to lookup on a single node, I think it would be better to do it in a single call and adapt the strategy globally. With the current configuration, running the request on these 30 shards would raise 18 errors out of 30 (2 can run and 10 are queued, the rest are refused).", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555772995", "createdAt": "2021-01-12T13:35:09Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "diffHunk": "@@ -171,6 +172,7 @@ public ThreadPool(final Settings settings, final ExecutorBuilder<?>... customBui\n         builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000, false));\n         builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16, false));\n         builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(allocatedProcessors), 1000, true));\n+        builders.put(Names.AUTO_COMPLETE, new FixedExecutorBuilder(settings, Names.AUTO_COMPLETE, 2, 10, true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3NzU5OA==", "bodyText": "You don't need a SearchContex here, you should use aQueryShardContext instead like we do in the can match phase. Also note that you could use the same QueryShardContext for the canMatchShard above, SearchService#queryStillMatchesAfterRewrite was added for this purpose.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555777598", "createdAt": "2021-01-12T13:42:21Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3OTA0MQ==", "bodyText": "The timeout could be checked here to save some round trip and could avoid accumulating requests on the node's thread pool ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r555779041", "createdAt": "2021-01-12T13:44:38Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,595 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.NoShardAvailableActionException;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.TransportActions;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Nullable;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lease.Releasables;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.SearchContext;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+import java.util.stream.Collectors;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private SearchService searchService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        SearchService searchService,\n+        ActionFilters actionFilters,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        // TODO new threadpool\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.searchService = searchService;\n+        // this.client = client;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            ShardTermEnumRequest::new,\n+            new ShardTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected ShardTermEnumRequest newShardRequest(int numShards, ShardRouting shard, TermEnumRequest request) {\n+        final ClusterState clusterState = clusterService.state();\n+        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new ShardTermEnumRequest(shard.shardId(), aliasFilter, request);\n+    }\n+\n+    protected ShardTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new ShardTermEnumResponse(in);\n+    }\n+\n+    protected GroupShardsIterator shards(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        final String routing = null;\n+\n+        // Remove any cold or frozen indices from the set of indices to be searched.\n+        ArrayList<TermCount> fastIndices = new ArrayList<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();            \n+            long indexCreationDate = clusterState.metadata().index(indexName).getCreationDate();\n+            // Search-throttled indices (which include frozen indices) are slow and should be ignored\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings) == false) {\n+                fastIndices.add(new TermCount(indexName, indexCreationDate));\n+            }\n+        }\n+        \n+        // Create list of fast indices sorted by newest-created first. We want to prioritise gathering new content\n+        fastIndices\n+            .sort((o1,o2)-> Long.compare(o2.getDocCount(), o1.getDocCount()));\n+        List<String> indexNames = fastIndices.stream()\n+            .map(object -> object.getTerm())\n+            .collect(Collectors.toList());         \n+        \n+        \n+        Map<String, Set<String>> routingMap = indexNameExpressionResolver.resolveSearchRouting(clusterState, routing, request.indices());\n+\n+        return clusterService.operationRouting().searchShards(clusterState, indexNames.toArray(new String[0]), routingMap, \"_local\");\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray shardsResponses,\n+        ClusterState clusterState, boolean timedOut) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < shardsResponses.length(); i++) {\n+            Object shardResponse = shardsResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                ShardTermEnumResponse str = (ShardTermEnumResponse) shardResponse;\n+                if (str.getTimedOut()) {\n+                    timedOut = true;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+//        long timeTook = System.currentTimeMillis() - request.taskStartTimeMillis;\n+//        System.err.println(\"Took \"+timeTook+\" ms\");\n+        return new TermEnumResponse(terms, shardsResponses.length(), successfulShards, failedShards, shardFailures, timedOut);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+\n+    protected ShardTermEnumResponse shardOperation(ShardTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        // Check we haven't just arrived on a shard and time is up already.\n+        if (System.currentTimeMillis() > scheduledEnd) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+        }        \n+        \n+        // Like TransportFieldCapabilitiesAction - fail fast if the filter excludes this index.\n+        if (canMatchShard(request) == false) {\n+            return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+        }\n+        ShardSearchRequest shardSearchLocalRequest = new ShardSearchRequest(\n+            request.shardId(),\n+            request.taskStartedTimeMillis(),\n+            request.filteringAliases()\n+        );\n+        SearchContext searchContext = searchService.createSearchContext(shardSearchLocalRequest, SearchService.NO_TIMEOUT);\n+        try {\n+            IndexReader reader = searchContext.getQueryShardContext().searcher().getTopReaderContext().reader();\n+            Terms terms = MultiTerms.getTerms(reader, request.field());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+            }\n+            Automaton a = request.caseInsensitive()\n+                    ? AutomatonQueries.caseInsensitivePrefix(request.pattern())\n+                    : Automata.makeString(request.pattern());\n+            a = Operations.concatenate(a, Automata.makeAnyString());\n+            a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+\n+            // TODO make this a param and scale up based on num shards like we do with terms aggs?\n+            int shard_size = request.size();\n+\n+            CompiledAutomaton automaton = new CompiledAutomaton(a);\n+            TermsEnum te = automaton.getTermsEnum(terms);\n+\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            if (request.sortByPopularity()) {\n+                // Collect most popular matches\n+                TermCountPriorityQueue pq = new TermCountPriorityQueue(shard_size);\n+                TermCount spare = null;\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            // Gather what we have collected so far\n+                            while (pq.size() > 0) {\n+                                termsList.add(pq.pop());\n+                            }\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+\n+                    if (spare == null) {\n+                        spare = new TermCount(bytes.utf8ToString(), df);\n+                    } else {\n+                        spare.setTerm(bytes.utf8ToString());\n+                        spare.setDocCount(df);\n+                    }\n+                    spare = pq.insertWithOverflow(spare);\n+                }\n+                while (pq.size() > 0) {\n+                    termsList.add(pq.pop());\n+                }\n+            } else {\n+                // Collect in alphabetical order\n+                while (te.next() != null) {\n+                    termCount++;\n+                    if (termCount > numTermsBetweenClockChecks) {\n+                        if (System.currentTimeMillis() > scheduledEnd) {\n+                            return new ShardTermEnumResponse(request.shardId(), termsList, error, true);\n+                        }\n+                        termCount = 0;\n+                    }\n+                    int df = te.docFreq();\n+                    BytesRef bytes = te.term();\n+                    termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                    if (termsList.size() >= shard_size) {\n+                        break;\n+                    }\n+                }\n+            }\n+\n+        } catch (AssertionError e) {\n+            error = e.getMessage();\n+        } finally {\n+            Releasables.close(searchContext);\n+        }\n+\n+        return new ShardTermEnumResponse(request.shardId(), termsList, error, false);\n+    }\n+    \n+    \n+    private boolean canMatchShard(ShardTermEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(req.shardId(), req.taskStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }    \n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermEnumRequest request;\n+        private ActionListener<TermEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final GroupShardsIterator<ShardIterator> shardsIts;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray shardsResponses;\n+\n+        protected AsyncBroadcastAction(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            shardsIts = shards(clusterState, request, concreteIndices);\n+            expectedOps = shardsIts.size();\n+\n+            shardsResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (shardsIts.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray(0), clusterState, false));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int shardIndex = -1;\n+            for (final ShardIterator shardIt : shardsIts) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                shardIndex++;\n+                final ShardRouting shard = shardIt.nextOrNull();\n+                if (shard != null) {\n+                    performOperation(shardIt, shard, shardIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onOperation(null, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+                }\n+            }\n+        }\n+        \n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ( (now - task.getStartTime()) > request.timeout().getMillis() ) {\n+                finishHim(true);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final ShardIterator shardIt, final ShardRouting shard, final int shardIndex) {\n+            if (shard == null) {\n+                // no more active shards... (we should not really get here, just safety)\n+                onOperation(null, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+            } else {\n+                try {\n+                    //TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final ShardTermEnumRequest shardRequest = newShardRequest(shardIt.size(), shard, request);\n+                    shardRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(shard.currentNodeId());\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onOperation(shard, shardIt, shardIndex, new NoShardAvailableActionException(shardIt.shardId()));\n+                    } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 444}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8ed989cbb14eb2fa5f413590c060e35b5f2896ef", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/8ed989cbb14eb2fa5f413590c060e35b5f2896ef", "committedDate": "2021-01-12T16:15:53Z", "message": "Addressing some review comments - change to QueryShardContext, remove TODO, change pattern to string, stop catching assertion error, add timer check before calling each shard"}, "afterCommit": {"oid": "e9fe715ff98eec5d1d529a2845b7c3b678bce71f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/e9fe715ff98eec5d1d529a2845b7c3b678bce71f", "committedDate": "2021-01-12T16:26:44Z", "message": "Addressing some review comments - change to QueryShardContext, remove TODO, change pattern to string, stop catching assertion error, add timer check before calling each shard"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e9fe715ff98eec5d1d529a2845b7c3b678bce71f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/e9fe715ff98eec5d1d529a2845b7c3b678bce71f", "committedDate": "2021-01-12T16:26:44Z", "message": "Addressing some review comments - change to QueryShardContext, remove TODO, change pattern to string, stop catching assertion error, add timer check before calling each shard"}, "afterCommit": {"oid": "3e2d905ca14cf29acf95e111ce9f3fa97ac32865", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/3e2d905ca14cf29acf95e111ce9f3fa97ac32865", "committedDate": "2021-01-12T17:16:24Z", "message": "Addressing some review comments - change to QueryShardContext, remove TODO, change pattern to string, stop catching assertion error, add timer check before calling each shard"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a099d1a1792ef2b431ed7998fb78b2293124e491", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/a099d1a1792ef2b431ed7998fb78b2293124e491", "committedDate": "2021-01-13T12:21:32Z", "message": "Doh."}, "afterCommit": {"oid": "9c78360a7fdcc49051ddc1bf08f0145f2eec67b8", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/9c78360a7fdcc49051ddc1bf08f0145f2eec67b8", "committedDate": "2021-01-13T14:29:03Z", "message": "Doh."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9c78360a7fdcc49051ddc1bf08f0145f2eec67b8", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/9c78360a7fdcc49051ddc1bf08f0145f2eec67b8", "committedDate": "2021-01-13T14:29:03Z", "message": "Doh."}, "afterCommit": {"oid": "601ab8cfa90ed8f28dcbdbf0540b3261519dd1f3", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/601ab8cfa90ed8f28dcbdbf0540b3261519dd1f3", "committedDate": "2021-01-13T17:12:20Z", "message": "Doh."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "601ab8cfa90ed8f28dcbdbf0540b3261519dd1f3", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/601ab8cfa90ed8f28dcbdbf0540b3261519dd1f3", "committedDate": "2021-01-13T17:12:20Z", "message": "Doh."}, "afterCommit": {"oid": "29f92da0c426e2905ceadd3e5a584c60200b95aa", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/29f92da0c426e2905ceadd3e5a584c60200b95aa", "committedDate": "2021-01-14T10:35:35Z", "message": "Doh."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "29f92da0c426e2905ceadd3e5a584c60200b95aa", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/29f92da0c426e2905ceadd3e5a584c60200b95aa", "committedDate": "2021-01-14T10:35:35Z", "message": "Doh."}, "afterCommit": {"oid": "6916077afcff83997d1ed08f074af65401518a25", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/6916077afcff83997d1ed08f074af65401518a25", "committedDate": "2021-01-18T11:56:01Z", "message": "Doh."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fa6eaf1f939a4e8464f8b95f319f1d38ce6e424d", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/fa6eaf1f939a4e8464f8b95f319f1d38ce6e424d", "committedDate": "2021-01-18T14:02:59Z", "message": "Compile fix after QueryShardContext rename"}, "afterCommit": {"oid": "7e2ce352f0ff69e79cc2189943fef4a153f981e2", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/7e2ce352f0ff69e79cc2189943fef4a153f981e2", "committedDate": "2021-01-18T14:31:39Z", "message": "Compile fix after QueryShardContext rename"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcwNjUyMDkz", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-570652093", "createdAt": "2021-01-18T16:43:45Z", "commit": {"oid": "ecdac4edee0dc42dd1468a9c248fe0f790eff90e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQxNjo0Mzo0NVrOIVw1ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQxNjo0Mzo0NVrOIVw1ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTY5MTIwMw==", "bodyText": "should this be string?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r559691203", "createdAt": "2021-01-18T16:43:45Z", "author": {"login": "mayya-sharipova"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,101 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"timed_out\" : false\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`field`::", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ecdac4edee0dc42dd1468a9c248fe0f790eff90e"}, "originalPosition": 69}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "eb50f69dba7d28f672df9e5194ffae177b8acceb", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/eb50f69dba7d28f672df9e5194ffae177b8acceb", "committedDate": "2021-01-25T14:40:53Z", "message": "Change IOUtils implies due to forbidden APIs"}, "afterCommit": {"oid": "9d955e1c15797f6a8f9f4863cc21478b983524b6", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/9d955e1c15797f6a8f9f4863cc21478b983524b6", "committedDate": "2021-01-25T17:06:42Z", "message": "Change IOUtils implies due to forbidden APIs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTgyOTcwNTM5", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-582970539", "createdAt": "2021-02-04T01:52:51Z", "commit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQwMTo1Mjo1MVrOIffILg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQwMTo1Mjo1MVrOIffILg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTg4Njc2Ng==", "bodyText": "I realize this is WIP, but had one high-level thought that might affect code structure/ naming. It'd be great to encapsulate this term loading logic into MappedFieldType instead of interacting with Lucene data structures directly here. We have been trying to do this consistently, and now delegate query creation, doc value loading, and other data fetching tasks to MappedFieldType. Some benefits:\n\nField types can decide to handle the call differently. For example constant_keyword could support this functionality (even though it doesn't write them in the index).\nField aliases will be handled correctly, since we resolve them when looking up MappedFieldType\nIf there's any change to logic because of an index version change, we can consolidate that in one place", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r569886766", "createdAt": "2021-02-04T01:52:51Z", "author": {"login": "jtibshirani"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,605 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.MultiReader;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.apache.lucene.util.PriorityQueue;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.IndexSettings;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.DataTier;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+    \n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+        \n+        this.clusterService = clusterService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService =  indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+//        final ClusterState clusterState = clusterService.state();\n+//        final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+//        final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+            Settings settings = clusterState.metadata().index(indexName).getSettings();\n+            if (IndexSettings.INDEX_SEARCH_THROTTLED.get(settings)) {\n+                // ignore slow throttled indices (this includes frozen)\n+                continue;\n+            }\n+            \n+            String[] singleIndex = {indexName};\n+            \n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+            assert (shards.size() == 1); // We are only considering a single concrete index\n+            ShardIterator shardsForIndex = shards.get(0);\n+            for (ShardRouting shardRouting : shardsForIndex.getShardRoutings()) {\n+                String nodeId = shardRouting.currentNodeId();\n+                \n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)){\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    DiscoveryNode node = clusterState.getNodes().getDataNodes().get(nodeId);\n+                    //Only consider hot and warm nodes\n+                    if (DataTier.isHotNode(node)  || DataTier.isWarmNode(node)) {\n+                        bundle = new HashSet<ShardId>();\n+                        fastNodeBundles.put(nodeId, bundle);\n+                    }\n+                }\n+                if (bundle != null) {\n+                    bundle.add(shardRouting.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(TermEnumRequest request, AtomicReferenceArray nodesResponses,\n+        ClusterState clusterState, boolean complete) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object shardResponse = nodesResponses.get(i);\n+            if (shardResponse == null) {\n+                // simply ignore non active shards\n+            } else if (shardResponse instanceof BroadcastShardOperationFailedException) {\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) shardResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) shardResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+                successfulShards++;\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        if (request.sortByPopularity()) {\n+            // Sort by doc count descending\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return Long.compare(t2.getDocCount(), t1.getDocCount());\n+                }\n+            });\n+        } else {\n+            // Sort alphabetically\n+            Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+                public int compare(TermCount t1, TermCount t2) {\n+                    return t1.getTerm().compareTo(t2.getTerm());\n+                }\n+            });\n+        }\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, nodesResponses.length(), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    static class TermCountPriorityQueue extends PriorityQueue<TermCount> {\n+\n+        TermCountPriorityQueue(int maxSize) {\n+            super(maxSize);\n+        }\n+\n+        @Override\n+        protected boolean lessThan(TermCount a, TermCount b) {\n+            return a.getDocCount() < b.getDocCount();\n+        }\n+\n+    }\n+    \n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+        \n+        // DLS/FLS check copied from ResizeRequestInterceptor\n+        // MH code - not sure this is the right context\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<IndexReader> shardReaders = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+    \n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.CAN_MATCH_SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        searcher,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+                    if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) && \n+                        canMatchShard(shardId, request, queryShardContext)) {\n+                        shardReaders.add(searcher.getTopReaderContext().reader());\n+                    }\n+                \n+            }\n+            MultiReader multiReader = new MultiReader(shardReaders.toArray(new IndexReader[0]), false);\n+\n+            Terms terms = MultiTerms.getTerms(multiReader, request.field());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "242a84c404a48148e463746b861c689a0eb2c363"}, "originalPosition": 306}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ecd478d2ec3f5ee3efb11d529a5bcc15cf714dd1", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/ecd478d2ec3f5ee3efb11d529a5bcc15cf714dd1", "committedDate": "2021-02-09T16:49:07Z", "message": "Added missing Locale"}, "afterCommit": {"oid": "1dc1510782ac00837d913586d1926aae5796501f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/1dc1510782ac00837d913586d1926aae5796501f", "committedDate": "2021-02-10T10:15:25Z", "message": "A TermsEnum API for discovering terms in the index.\nTerms matching a given prefix can be returned in alphabetical or by popularity,\nA timeout can limit the amount of time spent looking for matches.\n\nExpected to be useful in auto-complete or regex debugging use cases."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f051c866bc0ab47e16cfa879080790045f50174e", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/f051c866bc0ab47e16cfa879080790045f50174e", "committedDate": "2021-02-10T12:01:29Z", "message": "Renamed to naming convention for tests"}, "afterCommit": {"oid": "051ffd0064db8d3c51d2f8a7ad0f353d298f9ef0", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/051ffd0064db8d3c51d2f8a7ad0f353d298f9ef0", "committedDate": "2021-02-10T15:31:05Z", "message": "Renamed to naming convention for tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7061c7a5e37857e9ca70b105df1c6df7a6989fc9", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/7061c7a5e37857e9ca70b105df1c6df7a6989fc9", "committedDate": "2021-02-15T13:43:49Z", "message": "YAML fix"}, "afterCommit": {"oid": "8d528a9d07783e162203771ec69c58e7681cec0f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/8d528a9d07783e162203771ec69c58e7681cec0f", "committedDate": "2021-02-16T12:12:37Z", "message": "Fix bug where some indices don\u2019t have mapped field type"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8d528a9d07783e162203771ec69c58e7681cec0f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/8d528a9d07783e162203771ec69c58e7681cec0f", "committedDate": "2021-02-16T12:12:37Z", "message": "Fix bug where some indices don\u2019t have mapped field type"}, "afterCommit": {"oid": "6869b7f5e430d651eb3e737d00cd351296e40dfc", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/6869b7f5e430d651eb3e737d00cd351296e40dfc", "committedDate": "2021-02-16T15:14:10Z", "message": "A TermsEnum API for discovering terms in the index.\nTerms matching a given prefix can be returned in alphabetical or by popularity,\nA timeout can limit the amount of time spent looking for matches.\n\nExpected to be useful in auto-complete or regex debugging use cases."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d4811049be459ef9d6d214acb3650a47c24259e4", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/d4811049be459ef9d6d214acb3650a47c24259e4", "committedDate": "2021-02-17T18:35:16Z", "message": "License fix"}, "afterCommit": {"oid": "7bbf5b2abc391c333726a5c62022c3ca88821533", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/7bbf5b2abc391c333726a5c62022c3ca88821533", "committedDate": "2021-02-18T10:51:15Z", "message": "Remove HLRC code for now - requires less-than-ideal package names while we\u2019re unable to move main server implementation out of xpack (due to security dependency).\nDon\u2019t want clients to build dependencies on the wrong package names and the Java client changes are underway anyway."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7bbf5b2abc391c333726a5c62022c3ca88821533", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/7bbf5b2abc391c333726a5c62022c3ca88821533", "committedDate": "2021-02-18T10:51:15Z", "message": "Remove HLRC code for now - requires less-than-ideal package names while we\u2019re unable to move main server implementation out of xpack (due to security dependency).\nDon\u2019t want clients to build dependencies on the wrong package names and the Java client changes are underway anyway."}, "afterCommit": {"oid": "e5c48c1b3d0d2cd615fd539c141533bd823ce136", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/e5c48c1b3d0d2cd615fd539c141533bd823ce136", "committedDate": "2021-03-10T10:21:34Z", "message": "Remove HLRC code for now - requires less-than-ideal package names while we\u2019re unable to move main server implementation out of xpack (due to security dependency).\nDon\u2019t want clients to build dependencies on the wrong package names and the Java client changes are underway anyway."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3debca098122761e61ab9cecd4bad86c6913299e", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/3debca098122761e61ab9cecd4bad86c6913299e", "committedDate": "2021-03-11T16:36:31Z", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards"}, "afterCommit": {"oid": "6d1d1b7d1ab5017ee9c77d79f55b04aaffadc2a4", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/6d1d1b7d1ab5017ee9c77d79f55b04aaffadc2a4", "committedDate": "2021-03-11T16:53:23Z", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6d1d1b7d1ab5017ee9c77d79f55b04aaffadc2a4", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/6d1d1b7d1ab5017ee9c77d79f55b04aaffadc2a4", "committedDate": "2021-03-11T16:53:23Z", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards"}, "afterCommit": {"oid": "de5615884977a6dadd3e3b5fc5e14190e13e31db", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/de5615884977a6dadd3e3b5fc5e14190e13e31db", "committedDate": "2021-03-16T09:33:22Z", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "de5615884977a6dadd3e3b5fc5e14190e13e31db", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/de5615884977a6dadd3e3b5fc5e14190e13e31db", "committedDate": "2021-03-16T09:33:22Z", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards"}, "afterCommit": {"oid": "31a8844f55b4d3cc2e683c7b7714ee5a8eb60936", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/31a8844f55b4d3cc2e683c7b7714ee5a8eb60936", "committedDate": "2021-03-23T11:51:28Z", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7aa112ef2a521b4cdc4d6672996a142d8cef521e", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/7aa112ef2a521b4cdc4d6672996a142d8cef521e", "committedDate": "2021-03-23T15:32:04Z", "message": "Types warning"}, "afterCommit": {"oid": "b89016b61481ec9d8a9274db0c2d17ace1bd1c7f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/b89016b61481ec9d8a9274db0c2d17ace1bd1c7f", "committedDate": "2021-03-23T15:52:26Z", "message": "Types warning"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b89016b61481ec9d8a9274db0c2d17ace1bd1c7f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/b89016b61481ec9d8a9274db0c2d17ace1bd1c7f", "committedDate": "2021-03-23T15:52:26Z", "message": "Types warning"}, "afterCommit": {"oid": "81c8a734d5e76bcd0a4982aa3f99c40bdc7bbcaa", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/81c8a734d5e76bcd0a4982aa3f99c40bdc7bbcaa", "committedDate": "2021-04-06T17:34:54Z", "message": "Removed hot/warm tier tests (in anticipation of new queryable _tier field)\nMove canMatch logic to run on network thread\nInjected searchService so we can use its canMatch method"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1e04579257d48afa7ec50bdbb7ef2025d3661501", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/1e04579257d48afa7ec50bdbb7ef2025d3661501", "committedDate": "2021-04-08T15:16:12Z", "message": "Unused import"}, "afterCommit": {"oid": "15e2ca23ed5818e6bca66c58f7240be3b59a690e", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/15e2ca23ed5818e6bca66c58f7240be3b59a690e", "committedDate": "2021-04-08T15:30:35Z", "message": "Unused import"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "69949dcf3df4dd0cc07d017173ce5634d92a4ba3", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/69949dcf3df4dd0cc07d017173ce5634d92a4ba3", "committedDate": "2021-04-12T14:34:58Z", "message": "Removed sort by popularity option"}, "afterCommit": {"oid": "235af04eca795bc2957990335ceda6f69f275c51", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/235af04eca795bc2957990335ceda6f69f275c51", "committedDate": "2021-04-12T14:49:45Z", "message": "Removed sort by popularity option"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM0MzU0MTc5", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-634354179", "createdAt": "2021-04-13T09:08:44Z", "commit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTowODo0NVrOJH6DIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDo1MjozNFrOJH-OLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MDg4MQ==", "bodyText": "nit: remove", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612270881", "createdAt": "2021-04-13T09:08:45Z", "author": {"login": "jimczi"}, "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/RequestConverters.java", "diffHunk": "@@ -534,7 +534,7 @@ static Request rankEval(RankEvalRequest rankEvalRequest) throws IOException {\n         request.setEntity(createEntity(rankEvalRequest.getRankEvalSpec(), REQUEST_BODY_CONTENT_TYPE));\n         return request;\n     }\n-\n+    ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MTIzNA==", "bodyText": "These changes are not needed anymore", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612271234", "createdAt": "2021-04-13T09:09:14Z", "author": {"login": "jimczi"}, "path": "client/rest-high-level/src/main/java/org/elasticsearch/client/RestHighLevelClient.java", "diffHunk": "@@ -120,18 +120,18 @@\n import org.elasticsearch.search.aggregations.bucket.range.RangeAggregationBuilder;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI3MjQwOQ==", "bodyText": "doc_freq is not required either ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612272409", "createdAt": "2021-04-13T09:10:52Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/MappedFieldType.java", "diffHunk": "@@ -399,4 +400,20 @@ public TextSearchInfo getTextSearchInfo() {\n         KEYWORD,\n         NUMERIC\n     }\n+\n+    /**\n+     * This method is used to support auto-complete services and implementations\n+     * are expected to find terms beginning with the provided string very quickly.\n+     * If fields cannot look up matching terms quickly they should return null.  \n+     * The returned TermEnum should implement next(), term() and doc_freq() methods", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI4MDM2OA==", "bodyText": "I wonder if we should start with a bigger queue size ? I'd prefer that we start with a more conservative value like 100. We also said that we'll try to use the search thread pool when the queue is empty. Can you add the idea to the meta issue ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612280368", "createdAt": "2021-04-13T09:22:14Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "diffHunk": "@@ -171,6 +172,7 @@ public ThreadPool(final Settings settings, final ExecutorBuilder<?>... customBui\n         builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000, false));\n         builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16, false));\n         builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(allocatedProcessors), 1000, true));\n+        builders.put(Names.AUTO_COMPLETE, new FixedExecutorBuilder(settings, Names.AUTO_COMPLETE, 2, 10, true));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ=="}, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNTk5MQ==", "bodyText": "Is this still accurate. I thought that we would try to rewrite the role query and if it results in a match_all then we'd accept the request ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612335991", "createdAt": "2021-04-13T10:47:05Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - see https://github.com/elastic/elasticsearch/issues/70221", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 332}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNjc0MA==", "bodyText": "We could also use the search thread pool if the queue is empty. Happy to do it in a follow up.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612336740", "createdAt": "2021-04-13T10:48:20Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - see https://github.com/elastic/elasticsearch/issues/70221\n+    private boolean canAccess(String indexName, String fieldName, XPackLicenseState frozenLicenseState, ThreadContext threadContext) {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(indexName);\n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.shardStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermEnumRequest request;\n+        private ActionListener<TermEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final NodeTermEnumRequest nodeRequest = newNodeRequest(nodeId, shardIds, request);\n+                    nodeRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(nodeId);\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onNoOperation(nodeId);\n+                    } else if (checkForEarlyFinish() == false) {\n+                        transportService.sendRequest(\n+                            node,\n+                            transportShardAction,\n+                            nodeRequest,\n+                            new TransportResponseHandler<NodeTermEnumResponse>() {\n+                                @Override\n+                                public NodeTermEnumResponse read(StreamInput in) throws IOException {\n+                                    return readShardResponse(in);\n+                                }\n+\n+                                @Override\n+                                public void handleResponse(NodeTermEnumResponse response) {\n+                                    onOperation(nodeId, nodeIndex, response);\n+                                }\n+\n+                                @Override\n+                                public void handleException(TransportException e) {\n+                                    onNoOperation(nodeId);\n+                                }\n+                            }\n+                        );\n+                    }\n+                } catch (Exception e) {\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        protected void onOperation(String nodeId, int nodeIndex, NodeTermEnumResponse response) {\n+            logger.trace(\"received response for node {}\", nodeId);\n+            nodesResponses.set(nodeIndex, response);\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            } else {\n+                checkForEarlyFinish();\n+            }\n+        }\n+\n+        void onNoOperation(String nodeId) {\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            }\n+        }\n+\n+        // Can be called multiple times - either for early time-outs or for fully-completed collections.\n+        protected synchronized void finishHim(boolean complete) {\n+            if (listener == null) {\n+                return;\n+            }\n+            try {\n+                listener.onResponse(newResponse(request, nodesResponses, complete, nodeBundles));\n+            } catch (Exception e) {\n+                listener.onFailure(e);\n+            } finally {\n+                listener = null;\n+            }\n+        }\n+    }\n+\n+    class NodeTransportHandler implements TransportRequestHandler<NodeTermEnumRequest> {\n+\n+        @Override\n+        public void messageReceived(NodeTermEnumRequest request, TransportChannel channel, Task task) throws Exception {\n+            asyncNodeOperation(request, task, ActionListener.wrap(channel::sendResponse, e -> {\n+                try {\n+                    channel.sendResponse(e);\n+                } catch (Exception e1) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\n+                            \"Failed to send error response for action [{}] and request [{}]\",\n+                            actionName,\n+                            request\n+                        ),\n+                        e1\n+                    );\n+                }\n+            }));\n+        }\n+    }\n+\n+    private void asyncNodeOperation(NodeTermEnumRequest request, Task task, ActionListener<NodeTermEnumResponse> listener)\n+        throws IOException {\n+        // DLS/FLS check copied from ResizeRequestInterceptor - check permissions and\n+        // any index_filter canMatch checks on network thread before allocating work\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+        for (ShardId shardId : request.shardIds().toArray(new ShardId[0])) {\n+            if (canAccess(shardId.getIndexName(), request.field(), frozenLicenseState, threadContext) == false || canMatchShard(\n+                shardId,\n+                request\n+            ) == false) {\n+                // Permission denied or can't match, remove shardID from request\n+                request.remove(shardId);\n+            }\n+        }\n+        if (request.shardIds().size() == 0) {\n+            listener.onResponse(new NodeTermEnumResponse(request.nodeId(), Collections.emptyList(), null, true));\n+        } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 546}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNjkxMw==", "bodyText": "nit: extra lines", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612336913", "createdAt": "2021-04-13T10:48:36Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/rest/RestTermEnumAction.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.rest;\n+\n+import org.elasticsearch.client.node.NodeClient;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+import org.elasticsearch.rest.BaseRestHandler;\n+import org.elasticsearch.rest.RestRequest;\n+import org.elasticsearch.rest.action.RestToXContentListener;\n+import org.elasticsearch.xpack.core.termenum.action.TermEnumAction;\n+import org.elasticsearch.xpack.core.termenum.action.TermEnumRequest;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.elasticsearch.rest.RestRequest.Method.GET;\n+import static org.elasticsearch.rest.RestRequest.Method.POST;\n+\n+public class RestTermEnumAction extends BaseRestHandler {\n+\n+    @Override\n+    public List<Route> routes() {\n+        return List.of(\n+            new Route(GET, \"/{index}/_terms\"),\n+            new Route(POST, \"/{index}/_terms\"));\n+    }\n+\n+    @Override\n+    public String getName() {\n+        return \"term_enum_action\";\n+    }\n+\n+    @Override\n+    public RestChannelConsumer prepareRequest(final RestRequest request, final NodeClient client) throws IOException {\n+        try (XContentParser parser = request.contentOrSourceParamParser()) {\n+            TermEnumRequest termEnumRequest = TermEnumAction.fromXContent(parser, \n+                Strings.splitStringByCommaToArray(request.param(\"index\")));\n+            return channel ->\n+            client.execute(TermEnumAction.INSTANCE, termEnumRequest, new RestToXContentListener<>(channel));\n+        }        \n+        ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzODc2MA==", "bodyText": "Should it be indices:data/read/terms ? No need to add xpack reference here.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612338760", "createdAt": "2021-04-13T10:51:42Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License;\n+ * you may not use this file except in compliance with the Elastic License.\n+ */\n+\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {\n+\n+    public static final TermEnumAction INSTANCE = new TermEnumAction();\n+//    public static final String NAME = \"indices:admin/termsenum/list\";\n+    public static final String NAME = \"indices:data/read/xpack/termsenum/list\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDY2NjM4Nw=="}, "originalCommit": {"oid": "acff4942d0f38be6ad922f957692abf10e017d09"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzOTI0NA==", "bodyText": "Should we use the plural (Terms) instead ? That would be consistent with the name of the API (_terms). Same for all the other class names (TermEnumResponse, ...).", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612339244", "createdAt": "2021-04-13T10:52:34Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumAction.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionType;\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.xcontent.ObjectParser;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+\n+import static org.elasticsearch.index.query.AbstractQueryBuilder.parseInnerQueryBuilder;\n+\n+public class TermEnumAction extends ActionType<TermEnumResponse> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjM0NTA1Mzcy", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-634505372", "createdAt": "2021-04-13T12:05:09Z", "commit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "state": "COMMENTED", "comments": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMjowNTowOVrOJIA6sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMzoyOTowOVrOJIE0_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4MzQwOA==", "bodyText": "I wonder if we should be more intentional.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            a partial string. This is used for features like auto-complete:\n          \n          \n            \n            a partial string. This is used for auto-complete:", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612383408", "createdAt": "2021-04-13T12:05:09Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4NDEwNw==", "bodyText": "I'm confused, there is no \"8\" in the response?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612384107", "createdAt": "2021-04-13T12:06:06Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4NTU1Nw==", "bodyText": "Can we have more consistency between the name of the API and the name of the endpoint, ie. either calling it the terms API or changing the endpoint name from terms to term_enum? I have a preference for the former, but I haven't thought much about it. Or maybe the API should even be called _auto_complete to be more discoverable?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612385557", "createdAt": "2021-04-13T12:08:23Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 2}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM4OTMzNg==", "bodyText": "I could imagine complete to mean to users that all possible terms have been included in the response, while I think that what you are saying is slightly different. E.g. if size is 10 and there are 100 terms in the response, complete would be true if we looked at all shards even though not all possible terms are included in the response?\nMy gut feeling is that the semantics users would need is knowing whether all possible terms are in the response? (I haven't thought much about it)", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612389336", "createdAt": "2021-04-13T12:14:18Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MDE4Mw==", "bodyText": "Should we rename it prefix to be clearer about how the matching works?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612390183", "createdAt": "2021-04-13T12:15:32Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`string`::\n+(Mandatory, string)\n+The string to match at the start of indexed terms", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MDcxNQ==", "bodyText": "Should we take a time value rather than a number, ie. 1s instead of 1000?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612390715", "createdAt": "2021-04-13T12:16:13Z", "author": {"login": "jpountz"}, "path": "docs/reference/search/term-enum.asciidoc", "diffHunk": "@@ -0,0 +1,98 @@\n+[[search-term-enum]]\n+=== Term enum API\n+\n+The term enum API can be used to discover terms in the index that match\n+a partial string. This is used for features like auto-complete:\n+\n+[source,console]\n+--------------------------------------------------\n+POST stackoverflow/_terms\n+{\n+    \"field\" : \"tags\",\n+    \"string\" : \"kiba\"\n+}\n+--------------------------------------------------\n+// TEST[setup:stackoverflow]\n+\n+\n+The API returns the following response:\n+\n+[source,console-result]\n+--------------------------------------------------\n+{\n+  \"_shards\": {\n+    \"total\": 1,\n+    \"successful\": 1,\n+    \"failed\": 0\n+  },\n+  \"terms\": [\n+    \"kibana\"\n+  ],\n+  \"complete\" : true\n+}\n+--------------------------------------------------\n+// TESTRESPONSE[s/8/$body.terms.0.doc_count/]\n+\n+The \"complete\" flag is false if time or space constraints were met and the\n+set of terms examined was not the full set of available values.\n+\n+[[search-term-enum-api-request]]\n+==== {api-request-title}\n+\n+`GET /<target>/_terms`\n+\n+\n+[[search-term-enum-api-desc]]\n+==== {api-description-title}\n+\n+The termenum API  can be used to discover terms in the index that begin with the provided\n+string. It is designed for low-latency look-ups used in auto-complete scenarios.\n+\n+\n+[[search-term-enum-api-path-params]]\n+==== {api-path-parms-title}\n+\n+`<target>`::\n+(Mandatory, string)\n+Comma-separated list of data streams, indices, and index aliases to search.\n+Wildcard (`*`) expressions are supported.\n++\n+To search all data streams or indices in a cluster, omit this parameter or use\n+`_all` or `*`.\n+\n+[[search-term-enum-api-request-body]]\n+==== {api-request-body-title}\n+\n+[[term-enum-field-param]]\n+`field`::\n+(Mandatory, string)\n+Which field to match\n+\n+[[term-enum-string-param]]\n+`string`::\n+(Mandatory, string)\n+The string to match at the start of indexed terms\n+\n+[[term-enum-size-param]]\n+`size`::\n+(Optional, integer)\n+How many matching terms to return. Defaults to 10\n+\n+[[term-enum-timeout-param]]\n+`timeout`::\n+(Optional, integer)\n+The maximum length of time in milliseconds to spend collecting results. Defaults to 1000.\n+If the timeout is exceeded the `complete` flag set to false in the response and the results may\n+be partial or empty.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5MzcyMA==", "bodyText": "can you add a newline between the two functions?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612393720", "createdAt": "2021-04-13T12:20:16Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldParser.java", "diffHunk": "@@ -166,4 +166,14 @@ static BytesRef extractKey(BytesRef keyedValue) {\n         }\n         return new BytesRef(keyedValue.bytes, keyedValue.offset, length);\n     }\n+    static BytesRef extractValue(BytesRef keyedValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTcxMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if(result != null) {\n          \n          \n            \n                        if (result != null) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395713", "createdAt": "2021-04-13T12:23:14Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{\n+        TermsEnum delegate;\n+\n+        TranslatingTermsEnum(TermsEnum delegate) {\n+            this.delegate = delegate;\n+        }\n+        \n+        @Override\n+        public BytesRef next() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.next();\n+            if(result != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTc5OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if(result != null) {\n          \n          \n            \n                        if (result != null) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395798", "createdAt": "2021-04-13T12:23:21Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{\n+        TermsEnum delegate;\n+\n+        TranslatingTermsEnum(TermsEnum delegate) {\n+            this.delegate = delegate;\n+        }\n+        \n+        @Override\n+        public BytesRef next() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.next();\n+            if(result != null) {\n+                result = FlattenedFieldParser.extractValue(result);\n+            }\n+            return result;\n+        }\n+\n+        @Override\n+        public BytesRef term() throws IOException {\n+            // Strip the term of the fieldname value\n+            BytesRef result = delegate.term();\n+            if(result != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5NTk2Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                static class TranslatingTermsEnum extends TermsEnum{\n          \n          \n            \n                static class TranslatingTermsEnum extends TermsEnum {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612395962", "createdAt": "2021-04-13T12:23:36Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -265,6 +298,95 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n             return lookup -> List.of();\n         }\n     }\n+    \n+    \n+    // Wraps a raw Lucene TermsEnum to strip values of fieldnames\n+    static class TranslatingTermsEnum extends TermsEnum{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM5ODM0OQ==", "bodyText": "I don't think that this is correct as this will do case-insensitive search on the field name too. E.g. if the flattened object contains {\"foo\": \"bar\", \"Foo\": \"quux\"} we should only consider the right foo/Foo even when case insensitivity is enabled.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612398349", "createdAt": "2021-04-13T12:26:45Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/flattened/FlattenedFieldMapper.java", "diffHunk": "@@ -239,6 +252,26 @@ public Query wildcardQuery(String value,\n         public Query termQueryCaseInsensitive(Object value, SearchExecutionContext context) {\n             return AutomatonQueries.caseInsensitiveTermQuery(new Term(name(), indexedValueForSearch(value)));\n         }\n+        \n+        @Override\n+        public TermsEnum getTerms(boolean caseInsensitive, String string, SearchExecutionContext queryShardContext) throws IOException {\n+            IndexReader reader = queryShardContext.searcher().getTopReaderContext().reader();\n+            String searchString = FlattenedFieldParser.createKeyedValue(key, string);\n+            Terms terms = MultiTerms.getTerms(reader, name());\n+            if (terms == null) {\n+                // Field does not exist on this shard.\n+                return null;\n+            }\n+            Automaton a = caseInsensitive\n+                ? AutomatonQueries.caseInsensitivePrefix(searchString)\n+                : Automata.makeString(searchString);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQwMjI1MQ==", "bodyText": "My gut feeling is that we should have a slightly larger queue size as well, 100 sounds better to me too.\nShould we also scale the number of threads based on the number of processors? 2 threads feels too much for a single-core machine, maybe something like max(allocatedProcessors/4, 1)?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612402251", "createdAt": "2021-04-13T12:32:14Z", "author": {"login": "jpountz"}, "path": "server/src/main/java/org/elasticsearch/threadpool/ThreadPool.java", "diffHunk": "@@ -171,6 +172,7 @@ public ThreadPool(final Settings settings, final ExecutorBuilder<?>... customBui\n         builders.put(Names.GET, new FixedExecutorBuilder(settings, Names.GET, allocatedProcessors, 1000, false));\n         builders.put(Names.ANALYZE, new FixedExecutorBuilder(settings, Names.ANALYZE, 1, 16, false));\n         builders.put(Names.SEARCH, new FixedExecutorBuilder(settings, Names.SEARCH, searchThreadPoolSize(allocatedProcessors), 1000, true));\n+        builders.put(Names.AUTO_COMPLETE, new FixedExecutorBuilder(settings, Names.AUTO_COMPLETE, 2, 10, true));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc3Mjk5NQ=="}, "originalCommit": {"oid": "6d8b0a0337e2855a18925d67d51d4e365f9ab8b0"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMDIzMg==", "bodyText": "it feels inconsistent to set it when deserializing, could we require callers to call shardStartedTimeMillis instead?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612430232", "createdAt": "2021-04-13T13:08:44Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMTA0MA==", "bodyText": "I'm confused why the method is called shardStartedXXX while the field is called nodeStartedXXX.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612431040", "createdAt": "2021-04-13T13:09:47Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMjg1Mg==", "bodyText": "I don't think that this assert is legal. The delta could be negative if we spent a long time on the above line for instance.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612432852", "createdAt": "2021-04-13T13:12:04Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    \n+    \n+    public Set<ShardId> shardIds() {\n+        return Collections.unmodifiableSet(shardIds);\n+    }\n+\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    public int size() {\n+        return size;\n+    }\n+\n+    public long timeout() {\n+        return timeout;\n+    }\n+    public String nodeId() {\n+        return nodeId;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        // Adjust the amount of permitted time the shard has remaining to gather terms. \n+        long timeSpentSoFarInCoordinatingNode = System.currentTimeMillis() - taskStartedTimeMillis;\n+        assert timeSpentSoFarInCoordinatingNode >= 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzMzUzNA==", "bodyText": "why do we need to cast to an int?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612433534", "createdAt": "2021-04-13T13:12:47Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/NodeTermEnumRequest.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.IndicesRequest;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.transport.TransportRequest;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Internal termenum request executed directly against a specific node, querying potentially many \n+ * shards in one request\n+ */\n+public class NodeTermEnumRequest extends TransportRequest implements IndicesRequest {\n+\n+    private String field;\n+    private String string;\n+    private long taskStartedTimeMillis;\n+    private long nodeStartedTimeMillis;\n+    private boolean caseInsensitive;\n+    private int size;\n+    private long timeout;\n+    private final QueryBuilder indexFilter;\n+    private Set<ShardId> shardIds;\n+    private String nodeId;\n+    \n+\n+    public NodeTermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        // Set the clock running as soon as we appear on a node.\n+        nodeStartedTimeMillis = System.currentTimeMillis();\n+\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        timeout = in.readVLong();\n+        taskStartedTimeMillis = in.readVLong();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+        nodeId = in.readString();\n+        int numShards = in.readVInt();\n+        shardIds = new HashSet<>(numShards);\n+        for (int i = 0; i < numShards; i++) {\n+            shardIds.add(new ShardId(in));\n+        }\n+    }\n+\n+    public NodeTermEnumRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        this.field = request.field();\n+        this.string = request.string();\n+        this.caseInsensitive = request.caseInsensitive();\n+        this.size = request.size();\n+        this.timeout = request.timeout().getMillis();\n+        this.taskStartedTimeMillis = request.taskStartTimeMillis;\n+        this.indexFilter = request.indexFilter();\n+        this.nodeId = nodeId;\n+        this.shardIds = shardIds;        \n+        \n+        // TODO serialize shard ids\n+    }\n+\n+    public String field() {\n+        return field;\n+    }\n+\n+    public String string() {\n+        return string;\n+    }\n+\n+    public long taskStartedTimeMillis() {\n+        return this.taskStartedTimeMillis;\n+    }\n+    \n+    /** \n+     * The time this request was materialized on a shard\n+     * (defaults to \"now\" if serialization was not used e.g. a local request).\n+     */\n+    public long shardStartedTimeMillis() {\n+        if (nodeStartedTimeMillis == 0) {\n+            nodeStartedTimeMillis = System.currentTimeMillis();\n+        }\n+        return this.nodeStartedTimeMillis;\n+    }    \n+    \n+    public Set<ShardId> shardIds() {\n+        return Collections.unmodifiableSet(shardIds);\n+    }\n+\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    public int size() {\n+        return size;\n+    }\n+\n+    public long timeout() {\n+        return timeout;\n+    }\n+    public String nodeId() {\n+        return nodeId;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        // Adjust the amount of permitted time the shard has remaining to gather terms. \n+        long timeSpentSoFarInCoordinatingNode = System.currentTimeMillis() - taskStartedTimeMillis;\n+        assert timeSpentSoFarInCoordinatingNode >= 0;\n+        int remainingTimeForShardToUse = (int) (timeout - timeSpentSoFarInCoordinatingNode);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNjIyNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class SimpleTermCountEnum extends TermsEnum{\n          \n          \n            \n            public class SimpleTermCountEnum extends TermsEnum {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612436224", "createdAt": "2021-04-13T13:16:08Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNjY5NQ==", "bodyText": "could you copy the input array instead of modifying it in-place?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612436695", "createdAt": "2021-04-13T13:16:43Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{\n+    int index =-1;\n+    TermCount[] sortedTerms;\n+    TermCount current = null;\n+    \n+    public SimpleTermCountEnum(TermCount[] terms) {\n+        sortedTerms = terms;\n+        Arrays.sort(sortedTerms, Comparator.comparing(TermCount::getTerm));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzNzEyMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if(index >= sortedTerms.length) {\n          \n          \n            \n                    if (index >= sortedTerms.length) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612437123", "createdAt": "2021-04-13T13:17:12Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/SimpleTermCountEnum.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.lucene.index.ImpactsEnum;\n+import org.apache.lucene.index.PostingsEnum;\n+import org.apache.lucene.index.TermState;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.AttributeSource;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+/**\n+ * A utility class for fields that need to support autocomplete via\n+ * {@link MappedFieldType#getTerms(boolean, String, org.elasticsearch.index.query.SearchExecutionContext)}\n+ * but can't return a raw Lucene TermsEnum.\n+ */\n+public class SimpleTermCountEnum extends TermsEnum{\n+    int index =-1;\n+    TermCount[] sortedTerms;\n+    TermCount current = null;\n+    \n+    public SimpleTermCountEnum(TermCount[] terms) {\n+        sortedTerms = terms;\n+        Arrays.sort(sortedTerms, Comparator.comparing(TermCount::getTerm));\n+    }\n+    \n+    public SimpleTermCountEnum(TermCount termCount) {\n+        sortedTerms = new TermCount[1];\n+        sortedTerms[0] = termCount;\n+    }\n+\n+    @Override\n+    public BytesRef term() throws IOException {\n+        if (current == null) {\n+            return null;\n+        }\n+        return new BytesRef(current.getTerm());\n+    }    \n+\n+    @Override\n+    public BytesRef next() throws IOException {\n+        index++;\n+        if(index >= sortedTerms.length) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzODM2Mg==", "bodyText": "Should we be using constructorArg() rather than optionalConstructorArg()?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612438362", "createdAt": "2021-04-13T13:18:40Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQzOTI0OA==", "bodyText": "I'm not seeing where these setters are called, are they needed? If not can we make term and docCount final?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612439248", "createdAt": "2021-04-13T13:19:45Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermCount.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.common.ParseField;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.io.stream.Writeable;\n+import org.elasticsearch.common.xcontent.ConstructingObjectParser;\n+import org.elasticsearch.common.xcontent.ToXContentFragment;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.common.xcontent.XContentParser;\n+\n+import java.io.IOException;\n+import java.util.Objects;\n+\n+import static org.elasticsearch.common.xcontent.ConstructingObjectParser.optionalConstructorArg;\n+\n+public class TermCount implements Writeable, ToXContentFragment {\n+\n+    public static final String TERM_FIELD = \"term\";\n+    public static final String DOC_COUNT_FIELD = \"doc_count\";\n+\n+    static final ConstructingObjectParser<TermCount, Void> PARSER = new ConstructingObjectParser<>(\n+        \"term_count\",\n+        true,\n+        a -> { return new TermCount((String) a[0], (long) a[1]); }\n+    );\n+    static {\n+        PARSER.declareString(optionalConstructorArg(), new ParseField(TERM_FIELD));\n+        PARSER.declareLong(optionalConstructorArg(), new ParseField(DOC_COUNT_FIELD));\n+    }\n+\n+    private String term;\n+\n+    private long docCount;\n+\n+    public TermCount(StreamInput in) throws IOException {\n+        term = in.readString();\n+        docCount = in.readLong();\n+    }\n+\n+    public TermCount(String term, long count) {\n+        this.term = term;\n+        this.docCount = count;\n+    }\n+\n+    public String getTerm() {\n+        return this.term;\n+    }\n+\n+    public long getDocCount() {\n+        return this.docCount;\n+    }\n+\n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        out.writeString(term);\n+        out.writeLong(docCount);\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.field(TERM_FIELD, getTerm());\n+        builder.field(DOC_COUNT_FIELD, getDocCount());\n+        return builder;\n+    }\n+\n+    public static TermCount fromXContent(XContentParser parser) {\n+        return PARSER.apply(parser, null);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        TermCount other = (TermCount) o;\n+        return Objects.equals(getTerm(), other.getTerm()) && Objects.equals(getDocCount(), other.getDocCount());\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(getTerm(), getDocCount());\n+    }\n+\n+    void addToDocCount(long extra) {\n+        docCount += extra;\n+    }\n+\n+    void setTerm(String term) {\n+        this.term = term;\n+    }\n+\n+    void setDocCount(long docCount) {\n+        this.docCount = docCount;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MDgwMg==", "bodyText": "let's write it as a time value?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612440802", "createdAt": "2021-04-13T13:21:41Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TermEnumRequest.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.elasticsearch.action.ActionRequestValidationException;\n+import org.elasticsearch.action.ValidateActions;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.action.support.broadcast.BroadcastRequest;\n+import org.elasticsearch.common.Strings;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.io.stream.StreamOutput;\n+import org.elasticsearch.common.unit.TimeValue;\n+import org.elasticsearch.common.xcontent.ToXContentObject;\n+import org.elasticsearch.common.xcontent.XContentBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+/**\n+ * A request to gather terms for a given field matching a string prefix\n+ */\n+public class TermEnumRequest extends BroadcastRequest<TermEnumRequest> implements ToXContentObject {\n+\n+    public static int DEFAULT_SIZE = 10;\n+    public static int DEFAULT_TIMEOUT_MILLIS = 1000;\n+\n+    private String field;\n+    private String string;\n+    private int size = DEFAULT_SIZE;\n+    private boolean caseInsensitive;\n+    long taskStartTimeMillis;\n+    private QueryBuilder indexFilter;\n+\n+    public TermEnumRequest() {\n+        this(Strings.EMPTY_ARRAY);\n+    }\n+\n+    public TermEnumRequest(StreamInput in) throws IOException {\n+        super(in);\n+        field = in.readString();\n+        string = in.readString();\n+        caseInsensitive = in.readBoolean();\n+        size = in.readVInt();\n+        indexFilter = in.readOptionalNamedWriteable(QueryBuilder.class);\n+    }\n+\n+    /**\n+     * Constructs a new term enum request against the provided indices. No indices provided means it will\n+     * run against all indices.\n+     */\n+    public TermEnumRequest(String... indices) {\n+        super(indices);\n+        indicesOptions(IndicesOptions.fromOptions(false, false, true, false));\n+        timeout(new TimeValue(DEFAULT_TIMEOUT_MILLIS));\n+    }\n+\n+    @Override\n+    public ActionRequestValidationException validate() {\n+        ActionRequestValidationException validationException = super.validate();\n+        if (field == null) {\n+            validationException = ValidateActions.addValidationError(\"field cannot be null\", validationException);\n+        }\n+        return validationException;\n+    }\n+\n+    /**\n+     * The field to look inside for values\n+     */\n+    public void field(String field) {\n+        this.field = field;\n+    }\n+\n+    /**\n+     * Indicates if detailed information about query is requested\n+     */\n+    public String field() {\n+        return field;\n+    }\n+\n+    /**\n+     * The string required in matching field values\n+     */\n+    public void string(String string) {\n+        this.string = string;\n+    }\n+\n+    /**\n+     * The string required in matching field values\n+     */\n+    public String string() {\n+        return string;\n+    }\n+\n+    /**\n+     *  The number of terms to return\n+     */\n+    public int size() {\n+        return size;\n+    }\n+\n+    /**\n+     * The number of terms to return\n+     */\n+    public void size(int size) {\n+        this.size = size;\n+    }\n+\n+    /**\n+     * TThe max time in milliseconds to spend gathering terms\n+     */\n+    public void timeoutInMillis(int timeout) {\n+        timeout(new TimeValue(timeout));\n+    }\n+\n+    /**\n+     * If case insensitive matching is required\n+     */\n+    public void caseInsensitive(boolean caseInsensitive) {\n+        this.caseInsensitive = caseInsensitive;\n+    }\n+\n+    /**\n+     * If case insensitive matching is required\n+     */\n+    public boolean caseInsensitive() {\n+        return caseInsensitive;\n+    }\n+\n+    /**\n+     * Allows to filter shards if the provided {@link QueryBuilder} rewrites to `match_none`.\n+     */\n+    public void indexFilter(QueryBuilder indexFilter) {\n+        this.indexFilter = indexFilter;\n+    }    \n+    \n+    public QueryBuilder indexFilter() {\n+        return indexFilter;\n+    }    \n+    \n+    @Override\n+    public void writeTo(StreamOutput out) throws IOException {\n+        super.writeTo(out);\n+        out.writeString(field);\n+        out.writeString(string);\n+        out.writeBoolean(caseInsensitive);\n+        out.writeVInt(size);\n+        out.writeOptionalNamedWriteable(indexFilter);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"[\" + Arrays.toString(indices) + \"] field[\" + field + \"], string[\" + string + \"] \"  + \" size=\" + size + \" timeout=\"\n+            + timeout().getMillis() + \" case_insensitive=\"\n+            + caseInsensitive + \" indexFilter = \"+ indexFilter;\n+    }\n+\n+    @Override\n+    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n+        builder.startObject();\n+        builder.field(\"field\", field);\n+        builder.field(\"string\", string);\n+        builder.field(\"size\", size);\n+        builder.field(\"timeout\", timeout().getMillis());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjI2Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                if(randomBoolean()) {\n          \n          \n            \n                                if (randomBoolean()) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442267", "createdAt": "2021-04-13T13:23:26Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjM4OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    while(te.next()!=null) {\n          \n          \n            \n                                    while (te.next()!=null) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442388", "createdAt": "2021-04-13T13:23:35Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MjU0MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                if(term.startsWith(searchPrefix)) {\n          \n          \n            \n                                if (term.startsWith(searchPrefix)) {", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612442540", "createdAt": "2021-04-13T13:23:45Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {\n+                            termCounts.add(new TermCount(te.term().utf8ToString(), te.docFreq()));\n+                        }\n+                        SimpleTermCountEnum simpleEnum = new SimpleTermCountEnum(termCounts.toArray(new TermCount[0]));\n+                        termsEnums.add(simpleEnum);\n+                    } else {\n+                        termsEnums.add(te);\n+                    }\n+                }\n+                MultiShardTermsEnum mte = new MultiShardTermsEnum(termsEnums.toArray(new TermsEnum[0]));\n+                HashMap<String, Integer> expecteds = new HashMap<>();\n+\n+                for (String term : globalTermCounts.keySet()) {\n+                    if(term.startsWith(searchPrefix)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0MzMyMQ==", "bodyText": "nit: can you iterate over entries rathen than keys since you need values too?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612443321", "createdAt": "2021-04-13T13:24:33Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/test/java/org/elasticsearch/xpack/core/termenum/MultiShardTermsEnumTests.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum;\n+\n+import org.apache.lucene.analysis.MockAnalyzer;\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexWriterConfig;\n+import org.apache.lucene.index.MultiTerms;\n+import org.apache.lucene.index.Terms;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.store.ByteBuffersDirectory;\n+import org.apache.lucene.store.Directory;\n+import org.apache.lucene.util.automaton.Automata;\n+import org.apache.lucene.util.automaton.Automaton;\n+import org.apache.lucene.util.automaton.CompiledAutomaton;\n+import org.apache.lucene.util.automaton.MinimizationOperations;\n+import org.apache.lucene.util.automaton.Operations;\n+import org.elasticsearch.common.lucene.search.AutomatonQueries;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.test.ESTestCase;\n+import org.elasticsearch.xpack.core.termenum.action.MultiShardTermsEnum;\n+import org.elasticsearch.xpack.core.termenum.action.SimpleTermCountEnum;\n+import org.elasticsearch.xpack.core.termenum.action.TermCount;\n+\n+import java.io.Closeable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Locale;\n+import java.util.Map;\n+\n+public class MultiShardTermsEnumTests extends ESTestCase {\n+    \n+    public void testRandomIndexFusion() throws Exception {\n+        String fieldName = \"foo\";\n+        Map<String, Integer> globalTermCounts = new HashMap<>();\n+\n+        int numShards = randomIntBetween(2, 15);\n+\n+        ArrayList<Closeable> closeables = new ArrayList<>();\n+        ArrayList<DirectoryReader> readers = new ArrayList<>();\n+\n+        try {\n+            for (int s = 0; s < numShards; s++) {\n+                Directory directory = new ByteBuffersDirectory();\n+                IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(new MockAnalyzer(random())));\n+\n+                int numDocs = randomIntBetween(10,200);\n+                for (int i = 0; i < numDocs; i++) {\n+                    Document document = new Document();\n+                    String term = randomAlphaOfLengthBetween(1,3).toLowerCase(Locale.ROOT);\n+                    document.add(new StringField(fieldName, term, Field.Store.YES));\n+                    writer.addDocument(document);\n+                    int count = 0;\n+                    if (globalTermCounts.containsKey(term)) {\n+                        count = globalTermCounts.get(term);\n+                    }\n+                    count++;\n+                    globalTermCounts.put(term, count);\n+\n+                }\n+                DirectoryReader reader = DirectoryReader.open(writer);\n+                readers.add(reader);\n+                writer.close();\n+                closeables.add(reader);\n+                closeables.add(directory);\n+            }\n+\n+            int numSearches = 100;\n+            for (int q = 0; q < numSearches; q++) {\n+                String searchPrefix = randomAlphaOfLengthBetween(0, 3).toLowerCase(Locale.ROOT);\n+                Automaton a = AutomatonQueries.caseInsensitivePrefix(searchPrefix);\n+                a = Operations.concatenate(a, Automata.makeAnyString());\n+                a = MinimizationOperations.minimize(a, Integer.MAX_VALUE);\n+                CompiledAutomaton automaton = new CompiledAutomaton(a);\n+\n+                ArrayList<TermsEnum> termsEnums = new ArrayList<>();\n+                for (DirectoryReader reader : readers) {\n+                    Terms terms = MultiTerms.getTerms(reader, fieldName);\n+                    TermsEnum te = automaton.getTermsEnum(terms);\n+                    if(randomBoolean()) {\n+                        // Simulate fields like constant-keyword which use a SimpleTermCountEnum to present results\n+                        // rather than the raw TermsEnum from Lucene.\n+                        ArrayList<TermCount> termCounts = new ArrayList<>();\n+                        while(te.next()!=null) {\n+                            termCounts.add(new TermCount(te.term().utf8ToString(), te.docFreq()));\n+                        }\n+                        SimpleTermCountEnum simpleEnum = new SimpleTermCountEnum(termCounts.toArray(new TermCount[0]));\n+                        termsEnums.add(simpleEnum);\n+                    } else {\n+                        termsEnums.add(te);\n+                    }\n+                }\n+                MultiShardTermsEnum mte = new MultiShardTermsEnum(termsEnums.toArray(new TermsEnum[0]));\n+                HashMap<String, Integer> expecteds = new HashMap<>();\n+\n+                for (String term : globalTermCounts.keySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0NDM4Ng==", "bodyText": "should we use maxDoc() to be consistent with keyword fields, which don't ignore deletes?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612444386", "createdAt": "2021-04-13T13:25:41Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/mapper-constant-keyword/src/main/java/org/elasticsearch/xpack/constantkeyword/mapper/ConstantKeywordFieldMapper.java", "diffHunk": "@@ -140,6 +144,20 @@ public ValueFetcher valueFetcher(SearchExecutionContext context, String format)\n                 ? lookup -> List.of()\n                 : lookup -> List.of(value);\n         }\n+        \n+        \n+\n+        @Override\n+        public TermsEnum getTerms(boolean caseInsensitive, String string, SearchExecutionContext queryShardContext) throws IOException {\n+            boolean matches = caseInsensitive ? \n+                value.toLowerCase(Locale.ROOT).startsWith(string.toLowerCase(Locale.ROOT)) : \n+                value.startsWith(string);\n+            if (matches == false) {\n+                return null;\n+            }\n+            int docCount = queryShardContext.searcher().getIndexReader().numDocs();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjQ0NzQ4Ng==", "bodyText": "I usually don't like losing stack traces, which can be very useful for debugging purposes.", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r612447486", "createdAt": "2021-04-13T13:29:09Z", "author": {"login": "jpountz"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termenum/action/TransportTermEnumAction.java", "diffHunk": "@@ -0,0 +1,552 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermEnumAction extends HandledTransportAction<TermEnumRequest, TermEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermEnumAction.NAME, transportService, actionFilters, TermEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermEnumRequest request, ActionListener<TermEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermEnumResponse newResponse(\n+        TermEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermEnumResponse str = (NodeTermEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermEnumResponse dataNodeOperation(NodeTermEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = e.getMessage();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c8b4d1502b82e3f07069e3ceb40c8c4823daffbd"}, "originalPosition": 325}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "175e26a351cc5e1962c377a48e995057ca3d8dbf", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/175e26a351cc5e1962c377a48e995057ca3d8dbf", "committedDate": "2021-04-13T13:59:23Z", "message": "Addressing some review comments (thanks Jim/Adrien!)"}, "afterCommit": {"oid": "830f52b43a5ed1c94028a1b81ae726dfe6fb3f5c", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/830f52b43a5ed1c94028a1b81ae726dfe6fb3f5c", "committedDate": "2021-04-14T10:24:41Z", "message": "Docs tidy up"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d5e5384d03d75aaa570cc7a3ed92ca302f9cf698", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/d5e5384d03d75aaa570cc7a3ed92ca302f9cf698", "committedDate": "2021-04-14T15:50:09Z", "message": "Provide full stack traces for errors, change TODO comment"}, "afterCommit": {"oid": "2afb47beca8753746c2770e32ce763c72f7c3451", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/2afb47beca8753746c2770e32ce763c72f7c3451", "committedDate": "2021-04-14T16:49:29Z", "message": "Provide full stack traces for errors, change TODO comment"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2afb47beca8753746c2770e32ce763c72f7c3451", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/2afb47beca8753746c2770e32ce763c72f7c3451", "committedDate": "2021-04-14T16:49:29Z", "message": "Provide full stack traces for errors, change TODO comment"}, "afterCommit": {"oid": "ac097fece74dcfc02f3af6cacd1a64a50d785b8b", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/ac097fece74dcfc02f3af6cacd1a64a50d785b8b", "committedDate": "2021-04-19T08:28:18Z", "message": "Provide full stack traces for errors, change TODO comment"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "774e06d002e6ab00cc39f3336d77aaf986bcb9ac", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/774e06d002e6ab00cc39f3336d77aaf986bcb9ac", "committedDate": "2021-04-19T10:15:57Z", "message": "Move location of YAML test - was causing errors when seated alongside core/src/yamlRestTest"}, "afterCommit": {"oid": "5821ccb3253168ad870c07f7a18ec8dbf2d18949", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/5821ccb3253168ad870c07f7a18ec8dbf2d18949", "committedDate": "2021-04-19T11:34:52Z", "message": "Move location of YAML test - was causing errors when seated alongside core/src/yamlRestTest"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cd2f1857881e1ebe9b6699f2f5baa99eed650604", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/cd2f1857881e1ebe9b6699f2f5baa99eed650604", "committedDate": "2021-04-23T10:24:34Z", "message": "Remove acquisition of searcher from security check code"}, "afterCommit": {"oid": "24d73065943096b7174b216e8ec4fed76538a63c", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/24d73065943096b7174b216e8ec4fed76538a63c", "committedDate": "2021-04-23T12:38:01Z", "message": "Remove acquisition of searcher from security check code"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "24d73065943096b7174b216e8ec4fed76538a63c", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/24d73065943096b7174b216e8ec4fed76538a63c", "committedDate": "2021-04-23T12:38:01Z", "message": "Remove acquisition of searcher from security check code"}, "afterCommit": {"oid": "62782c3d62e5d967b32a13be373841297deac591", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/62782c3d62e5d967b32a13be373841297deac591", "committedDate": "2021-04-26T12:51:06Z", "message": "Remove acquisition of searcher from security check code"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "62782c3d62e5d967b32a13be373841297deac591", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/62782c3d62e5d967b32a13be373841297deac591", "committedDate": "2021-04-26T12:51:06Z", "message": "Remove acquisition of searcher from security check code"}, "afterCommit": {"oid": "fafdcef55087945712582ed0a3661d6d417478bd", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/fafdcef55087945712582ed0a3661d6d417478bd", "committedDate": "2021-04-26T13:20:47Z", "message": "Remove acquisition of searcher from security check code"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjQ1NTUzMDI0", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-645553024", "createdAt": "2021-04-27T08:00:45Z", "commit": {"oid": "e43475f856f7dca4ff6bd92598f2d64dda5cf921"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QwODowMDo0NlrOJQMcyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yN1QxMDoyNzo1MlrOJQTn-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMDk2MDk3MQ==", "bodyText": "nit: extra lines ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r620960971", "createdAt": "2021-04-27T08:00:46Z", "author": {"login": "jimczi"}, "path": "server/src/main/java/org/elasticsearch/index/mapper/KeywordFieldMapper.java", "diffHunk": "@@ -247,6 +257,27 @@ public KeywordFieldType(String name, NamedAnalyzer analyzer) {\n             this.eagerGlobalOrdinals = false;\n             this.scriptValues = null;\n         }\n+        \n+        \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e43475f856f7dca4ff6bd92598f2d64dda5cf921"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMTA3ODUyMQ==", "bodyText": "We should use the search threadpool here if the queue is empty.\nSomething like:\n        assert transportService.getThreadPool().executor(ThreadPool.Names.SEARCH) instanceof EsThreadPoolExecutor\n            : \"SEARCH threadpool must be an instance of ThreadPoolExecutor\";\n        EsThreadPoolExecutor ex = (EsThreadPoolExecutor) transportService.getThreadPool().executor(ThreadPool.Names.SEARCH);\n        final String executorName = ex.getQueue().size() == 0 ? ThreadPool.Names.SEARCH : shardExecutor;", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r621078521", "createdAt": "2021-04-27T10:27:52Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termsenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.query.Rewriteable;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.script.ScriptService;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.SecurityContext;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+import org.elasticsearch.xpack.core.security.authz.support.DLSRoleQueryValidator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermsEnumAction extends HandledTransportAction<TermsEnumRequest, TermsEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    private final ScriptService scriptService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermsEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ScriptService scriptService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermsEnumAction.NAME, transportService, actionFilters, TermsEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.scriptService = scriptService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermsEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermsEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermsEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermsEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermsEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermsEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermsEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermsEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermsEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermsEnumResponse newResponse(\n+        TermsEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermsEnumResponse str = (NodeTermsEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermsEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermsEnumResponse dataNodeOperation(NodeTermsEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.shardStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::shardStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = ExceptionsHelper.stackTrace(e);\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - write a separate Interceptor class to \n+    // rewrite requests according to security rules \n+    private boolean canAccess(\n+        ShardId shardId,\n+        NodeTermsEnumRequest request,\n+        XPackLicenseState frozenLicenseState,\n+        ThreadContext threadContext        \n+    ) throws IOException {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(shardId.getIndexName());\n+\n+         \n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    // Check to see if any of the roles defined for the current user rewrite to match_all \n+                    \n+                    SecurityContext securityContext = new SecurityContext(clusterService.getSettings(), threadContext);\n+                    final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        null,\n+                        request::shardStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+\n+                    // Current user has potentially many roles and therefore potentially many queries\n+                    // defining sets of docs accessible\n+                    Set<BytesReference> queries = indexAccessControl.getDocumentPermissions().getQueries();\n+                    for (BytesReference querySource : queries) {\n+                        QueryBuilder queryBuilder = DLSRoleQueryValidator.evaluateAndVerifyRoleQuery(\n+                            querySource,\n+                            scriptService,\n+                            queryShardContext.getXContentRegistry(),\n+                            securityContext.getUser()\n+                        );\n+                        QueryBuilder rewrittenQueryBuilder = Rewriteable.rewrite(queryBuilder, queryShardContext);\n+                        if (rewrittenQueryBuilder instanceof MatchAllQueryBuilder) {\n+                            // One of the roles assigned has \"all\" permissions - allow unfettered access to termsDict\n+                            return true;\n+                        }\n+                    }\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermsEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.shardStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermsEnumRequest request;\n+        private ActionListener<TermsEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermsEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.\n+                    final NodeTermsEnumRequest nodeRequest = newNodeRequest(nodeId, shardIds, request);\n+                    nodeRequest.setParentTask(clusterService.localNode().getId(), task.getId());\n+                    DiscoveryNode node = nodes.get(nodeId);\n+                    if (node == null) {\n+                        // no node connected, act as failure\n+                        onNoOperation(nodeId);\n+                    } else if (checkForEarlyFinish() == false) {\n+                        transportService.sendRequest(\n+                            node,\n+                            transportShardAction,\n+                            nodeRequest,\n+                            new TransportResponseHandler<NodeTermsEnumResponse>() {\n+                                @Override\n+                                public NodeTermsEnumResponse read(StreamInput in) throws IOException {\n+                                    return readShardResponse(in);\n+                                }\n+\n+                                @Override\n+                                public void handleResponse(NodeTermsEnumResponse response) {\n+                                    onOperation(nodeId, nodeIndex, response);\n+                                }\n+\n+                                @Override\n+                                public void handleException(TransportException e) {\n+                                    onNoOperation(nodeId);\n+                                }\n+                            }\n+                        );\n+                    }\n+                } catch (Exception e) {\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        protected void onOperation(String nodeId, int nodeIndex, NodeTermsEnumResponse response) {\n+            logger.trace(\"received response for node {}\", nodeId);\n+            nodesResponses.set(nodeIndex, response);\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            } else {\n+                checkForEarlyFinish();\n+            }\n+        }\n+\n+        void onNoOperation(String nodeId) {\n+            if (expectedOps == counterOps.incrementAndGet()) {\n+                finishHim(true);\n+            }\n+        }\n+\n+        // Can be called multiple times - either for early time-outs or for fully-completed collections.\n+        protected synchronized void finishHim(boolean complete) {\n+            if (listener == null) {\n+                return;\n+            }\n+            try {\n+                listener.onResponse(newResponse(request, nodesResponses, complete, nodeBundles));\n+            } catch (Exception e) {\n+                listener.onFailure(e);\n+            } finally {\n+                listener = null;\n+            }\n+        }\n+    }\n+\n+    class NodeTransportHandler implements TransportRequestHandler<NodeTermsEnumRequest> {\n+\n+        @Override\n+        public void messageReceived(NodeTermsEnumRequest request, TransportChannel channel, Task task) throws Exception {\n+            asyncNodeOperation(request, task, ActionListener.wrap(channel::sendResponse, e -> {\n+                try {\n+                    channel.sendResponse(e);\n+                } catch (Exception e1) {\n+                    logger.warn(\n+                        () -> new ParameterizedMessage(\n+                            \"Failed to send error response for action [{}] and request [{}]\",\n+                            actionName,\n+                            request\n+                        ),\n+                        e1\n+                    );\n+                }\n+            }));\n+        }\n+    }\n+\n+    private void asyncNodeOperation(NodeTermsEnumRequest request, Task task, ActionListener<NodeTermsEnumResponse> listener)\n+        throws IOException {\n+        // DLS/FLS check copied from ResizeRequestInterceptor - check permissions and\n+        // any index_filter canMatch checks on network thread before allocating work\n+        ThreadContext threadContext = transportService.getThreadPool().getThreadContext();\n+        final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();\n+        for (ShardId shardId : request.shardIds().toArray(new ShardId[0])) {\n+            if (canAccess(shardId, request, frozenLicenseState, threadContext) == false || canMatchShard(\n+                shardId,\n+                request\n+            ) == false) {\n+                // Permission denied or can't match, remove shardID from request\n+                request.remove(shardId);\n+            }\n+        }\n+        if (request.shardIds().size() == 0) {\n+            listener.onResponse(new NodeTermsEnumResponse(request.nodeId(), Collections.emptyList(), null, true));\n+        } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e43475f856f7dca4ff6bd92598f2d64dda5cf921"}, "originalPosition": 593}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2e68251d67c1defc7ea6e57b255ee264a4a37dd8", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/2e68251d67c1defc7ea6e57b255ee264a4a37dd8", "committedDate": "2021-04-27T16:02:10Z", "message": "Checkstyle fix"}, "afterCommit": {"oid": "73dda500944e011d198785cb0da7e444efb94527", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/73dda500944e011d198785cb0da7e444efb94527", "committedDate": "2021-04-27T16:27:05Z", "message": "Checkstyle fix"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e63c1db714c563e17df303f1240dd0944c7a57ac", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/e63c1db714c563e17df303f1240dd0944c7a57ac", "committedDate": "2021-04-30T09:30:08Z", "message": "In flattened fields make only the value (not the field name) subject to case insensitive search"}, "afterCommit": {"oid": "721b9315d36be5a43ae2ec272caa4315fb8339e5", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/721b9315d36be5a43ae2ec272caa4315fb8339e5", "committedDate": "2021-04-30T09:55:44Z", "message": "In flattened fields make only the value (not the field name) subject to case insensitive search"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bc408365c8449c1b76811fe20f5ffe260f8bba2d", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/bc408365c8449c1b76811fe20f5ffe260f8bba2d", "committedDate": "2021-04-30T13:30:35Z", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation"}, "afterCommit": {"oid": "900732e5b142282fcf0a921348973d214a97371f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/900732e5b142282fcf0a921348973d214a97371f", "committedDate": "2021-04-30T13:48:08Z", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "900732e5b142282fcf0a921348973d214a97371f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/900732e5b142282fcf0a921348973d214a97371f", "committedDate": "2021-04-30T13:48:08Z", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation"}, "afterCommit": {"oid": "bcf76a4c62533664747506bc1c876a5b2a1fbf2c", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/bcf76a4c62533664747506bc1c876a5b2a1fbf2c", "committedDate": "2021-05-04T13:26:48Z", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjUzMDM0NzEy", "url": "https://github.com/elastic/elasticsearch/pull/66452#pullrequestreview-653034712", "createdAt": "2021-05-06T07:11:35Z", "commit": {"oid": "bcf76a4c62533664747506bc1c876a5b2a1fbf2c"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNS0wNlQwNzoxMTozNVrOJWFreg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNS0wNlQwNzoxMTozNVrOJWFreg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNzE0MTQ5OA==", "bodyText": "Is the TODO still needed ?", "url": "https://github.com/elastic/elasticsearch/pull/66452#discussion_r627141498", "createdAt": "2021-05-06T07:11:35Z", "author": {"login": "jimczi"}, "path": "x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/termsenum/action/TransportTermsEnumAction.java", "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\n+ * or more contributor license agreements. Licensed under the Elastic License\n+ * 2.0; you may not use this file except in compliance with the Elastic License\n+ * 2.0.\n+ */\n+package org.elasticsearch.xpack.core.termsenum.action;\n+\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.lucene.index.TermsEnum;\n+import org.apache.lucene.util.BytesRef;\n+import org.elasticsearch.ExceptionsHelper;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.ActionRunnable;\n+import org.elasticsearch.action.support.ActionFilters;\n+import org.elasticsearch.action.support.DefaultShardOperationFailedException;\n+import org.elasticsearch.action.support.HandledTransportAction;\n+import org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException;\n+import org.elasticsearch.cluster.ClusterState;\n+import org.elasticsearch.cluster.block.ClusterBlockException;\n+import org.elasticsearch.cluster.block.ClusterBlockLevel;\n+import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.node.DiscoveryNodes;\n+import org.elasticsearch.cluster.routing.GroupShardsIterator;\n+import org.elasticsearch.cluster.routing.ShardIterator;\n+import org.elasticsearch.cluster.routing.ShardRouting;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.MemoizedSupplier;\n+import org.elasticsearch.common.bytes.BytesReference;\n+import org.elasticsearch.common.inject.Inject;\n+import org.elasticsearch.common.io.stream.StreamInput;\n+import org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n+import org.elasticsearch.core.internal.io.IOUtils;\n+import org.elasticsearch.index.IndexService;\n+import org.elasticsearch.index.engine.Engine;\n+import org.elasticsearch.index.mapper.MappedFieldType;\n+import org.elasticsearch.index.query.MatchAllQueryBuilder;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.query.Rewriteable;\n+import org.elasticsearch.index.query.SearchExecutionContext;\n+import org.elasticsearch.index.shard.IndexShard;\n+import org.elasticsearch.index.shard.ShardId;\n+import org.elasticsearch.indices.IndicesService;\n+import org.elasticsearch.license.XPackLicenseState;\n+import org.elasticsearch.license.XPackLicenseState.Feature;\n+import org.elasticsearch.script.ScriptService;\n+import org.elasticsearch.search.SearchService;\n+import org.elasticsearch.search.builder.SearchSourceBuilder;\n+import org.elasticsearch.search.internal.AliasFilter;\n+import org.elasticsearch.search.internal.ShardSearchRequest;\n+import org.elasticsearch.tasks.Task;\n+import org.elasticsearch.threadpool.ThreadPool;\n+import org.elasticsearch.transport.TransportChannel;\n+import org.elasticsearch.transport.TransportException;\n+import org.elasticsearch.transport.TransportRequestHandler;\n+import org.elasticsearch.transport.TransportResponseHandler;\n+import org.elasticsearch.transport.TransportService;\n+import org.elasticsearch.xpack.core.security.SecurityContext;\n+import org.elasticsearch.xpack.core.security.authz.AuthorizationServiceField;\n+import org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl;\n+import org.elasticsearch.xpack.core.security.authz.support.DLSRoleQueryValidator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReferenceArray;\n+\n+public class TransportTermsEnumAction extends HandledTransportAction<TermsEnumRequest, TermsEnumResponse> {\n+\n+    protected final ClusterService clusterService;\n+    protected final TransportService transportService;\n+    private final SearchService searchService;\n+    private final IndicesService indicesService;\n+    private final ScriptService scriptService;\n+    protected final IndexNameExpressionResolver indexNameExpressionResolver;\n+\n+    final String transportShardAction;\n+    private final String shardExecutor;\n+    private final XPackLicenseState licenseState;\n+\n+    @Inject\n+    public TransportTermsEnumAction(\n+        // NodeClient client,\n+        ClusterService clusterService,\n+        SearchService searchService,\n+        TransportService transportService,\n+        IndicesService indicesService,\n+        ScriptService scriptService,\n+        ActionFilters actionFilters,\n+        XPackLicenseState licenseState,\n+        IndexNameExpressionResolver indexNameExpressionResolver\n+    ) {\n+        super(TermsEnumAction.NAME, transportService, actionFilters, TermsEnumRequest::new);\n+\n+        this.clusterService = clusterService;\n+        this.searchService = searchService;\n+        this.transportService = transportService;\n+        this.indexNameExpressionResolver = indexNameExpressionResolver;\n+        this.transportShardAction = actionName + \"[s]\";\n+        this.shardExecutor = ThreadPool.Names.AUTO_COMPLETE;\n+        this.indicesService = indicesService;\n+        this.scriptService = scriptService;\n+        this.licenseState = licenseState;\n+\n+        transportService.registerRequestHandler(\n+            transportShardAction,\n+            ThreadPool.Names.SAME,\n+            NodeTermsEnumRequest::new,\n+            new NodeTransportHandler()\n+        );\n+\n+    }\n+\n+    @Override\n+    protected void doExecute(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+        request.taskStartTimeMillis = task.getStartTime();\n+        new AsyncBroadcastAction(task, request, listener).start();\n+    }\n+\n+    protected NodeTermsEnumRequest newNodeRequest(final String nodeId, final Set<ShardId> shardIds, TermsEnumRequest request) {\n+        // Given we look terms up in the terms dictionary alias filters is another aspect of search (like DLS) that we\n+        // currently do not support.\n+        // final ClusterState clusterState = clusterService.state();\n+        // final Set<String> indicesAndAliases = indexNameExpressionResolver.resolveExpressions(clusterState, request.indices());\n+        // final AliasFilter aliasFilter = searchService.buildAliasFilter(clusterState, shard.getIndexName(), indicesAndAliases);\n+        return new NodeTermsEnumRequest(nodeId, shardIds, request);\n+    }\n+\n+    protected NodeTermsEnumResponse readShardResponse(StreamInput in) throws IOException {\n+        return new NodeTermsEnumResponse(in);\n+    }\n+\n+    protected Map<String, Set<ShardId>> getNodeBundles(ClusterState clusterState, TermsEnumRequest request, String[] concreteIndices) {\n+        // Group targeted shards by nodeId\n+        Map<String, Set<ShardId>> fastNodeBundles = new HashMap<>();\n+        for (String indexName : concreteIndices) {\n+\n+            String[] singleIndex = { indexName };\n+\n+            GroupShardsIterator<ShardIterator> shards = clusterService.operationRouting()\n+                .searchShards(clusterState, singleIndex, null, null);\n+\n+            Iterator<ShardIterator> shardsForIndex = shards.iterator();\n+            while (shardsForIndex.hasNext()) {\n+                ShardIterator copiesOfShard = shardsForIndex.next();\n+                ShardRouting selectedCopyOfShard = null;\n+                for (ShardRouting copy : copiesOfShard) {\n+                    // Pick the first active node with a copy of the shard\n+                    if (copy.active() && copy.assignedToNode()) {\n+                        selectedCopyOfShard = copy;\n+                        break;\n+                    }\n+                }\n+                if (selectedCopyOfShard == null) {\n+                    break;\n+                }\n+                String nodeId = selectedCopyOfShard.currentNodeId();\n+                Set<ShardId> bundle = null;\n+                if (fastNodeBundles.containsKey(nodeId)) {\n+                    bundle = fastNodeBundles.get(nodeId);\n+                } else {\n+                    bundle = new HashSet<ShardId>();\n+                    fastNodeBundles.put(nodeId, bundle);\n+                }\n+                if (bundle != null) {\n+                    bundle.add(selectedCopyOfShard.shardId());\n+                }\n+            }\n+        }\n+        return fastNodeBundles;\n+    }\n+\n+    protected ClusterBlockException checkGlobalBlock(ClusterState state, TermsEnumRequest request) {\n+        return state.blocks().globalBlockedException(ClusterBlockLevel.READ);\n+    }\n+\n+    protected ClusterBlockException checkRequestBlock(ClusterState state, TermsEnumRequest countRequest, String[] concreteIndices) {\n+        return state.blocks().indicesBlockedException(ClusterBlockLevel.READ, concreteIndices);\n+    }\n+\n+    protected TermsEnumResponse newResponse(\n+        TermsEnumRequest request,\n+        AtomicReferenceArray<?> nodesResponses,\n+        boolean complete,\n+        Map<String, Set<ShardId>> nodeBundles\n+    ) {\n+        int successfulShards = 0;\n+        int failedShards = 0;\n+        List<DefaultShardOperationFailedException> shardFailures = null;\n+        Map<String, TermCount> combinedResults = new HashMap<String, TermCount>();\n+        for (int i = 0; i < nodesResponses.length(); i++) {\n+            Object nodeResponse = nodesResponses.get(i);\n+            if (nodeResponse == null) {\n+                // simply ignore non active shards\n+            } else if (nodeResponse instanceof BroadcastShardOperationFailedException) {\n+                complete = false;\n+                failedShards++;\n+                if (shardFailures == null) {\n+                    shardFailures = new ArrayList<>();\n+                }\n+                shardFailures.add(new DefaultShardOperationFailedException((BroadcastShardOperationFailedException) nodeResponse));\n+            } else {\n+                NodeTermsEnumResponse str = (NodeTermsEnumResponse) nodeResponse;\n+                // Only one node response has to be incomplete for the entire result to be labelled incomplete.\n+                if (str.getComplete() == false) {\n+                    complete = false;\n+                }\n+\n+                Set<ShardId> shards = nodeBundles.get(str.getNodeId());\n+                if (str.getError() != null) {\n+                    complete = false;\n+                    // A single reported error is assumed to be for all shards queried on that node.\n+                    // When reading we read from multiple Lucene indices in one unified view so any error is\n+                    // assumed to be all shards on that node.\n+                    failedShards += shards.size();\n+                    if (shardFailures == null) {\n+                        shardFailures = new ArrayList<>();\n+                    }\n+                    for (ShardId failedShard : shards) {\n+                        shardFailures.add(\n+                            new DefaultShardOperationFailedException(\n+                                new BroadcastShardOperationFailedException(failedShard, str.getError())\n+                            )\n+                        );\n+                    }\n+                } else {\n+                    successfulShards += shards.size();\n+                }\n+                for (TermCount term : str.terms()) {\n+                    TermCount existingTc = combinedResults.get(term.getTerm());\n+                    if (existingTc == null) {\n+                        combinedResults.put(term.getTerm(), term);\n+                    } else {\n+                        // add counts\n+                        existingTc.addToDocCount(term.getDocCount());\n+                    }\n+                }\n+            }\n+        }\n+        int size = Math.min(request.size(), combinedResults.size());\n+        List<String> terms = new ArrayList<>(size);\n+        TermCount[] sortedCombinedResults = combinedResults.values().toArray(new TermCount[0]);\n+        // Sort alphabetically\n+        Arrays.sort(sortedCombinedResults, new Comparator<TermCount>() {\n+            public int compare(TermCount t1, TermCount t2) {\n+                return t1.getTerm().compareTo(t2.getTerm());\n+            }\n+        });\n+\n+        for (TermCount term : sortedCombinedResults) {\n+            terms.add(term.getTerm());\n+            if (terms.size() == size) {\n+                break;\n+            }\n+        }\n+        return new TermsEnumResponse(terms, (failedShards + successfulShards), successfulShards, failedShards, shardFailures, complete);\n+    }\n+\n+    protected NodeTermsEnumResponse dataNodeOperation(NodeTermsEnumRequest request, Task task) throws IOException {\n+        List<TermCount> termsList = new ArrayList<>();\n+        String error = null;\n+\n+        long timeout_millis = request.timeout();\n+        long scheduledEnd = request.nodeStartedTimeMillis() + timeout_millis;\n+\n+        ArrayList<TermsEnum> shardTermsEnums = new ArrayList<>();\n+        ArrayList<Closeable> openedResources = new ArrayList<>();\n+        try {\n+            for (ShardId shardId : request.shardIds()) {\n+                // Check we haven't just arrived on a node and time is up already.\n+                if (System.currentTimeMillis() > scheduledEnd) {\n+                    return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+                }\n+                final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                final IndexShard indexShard = indexService.getShard(shardId.getId());\n+\n+                Engine.Searcher searcher = indexShard.acquireSearcher(Engine.SEARCH_SOURCE);\n+                openedResources.add(searcher);\n+                final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                    shardId.id(),\n+                    0,\n+                    searcher,\n+                    request::nodeStartedTimeMillis,\n+                    null,\n+                    Collections.emptyMap()\n+                );\n+                final MappedFieldType mappedFieldType = indexShard.mapperService().fieldType(request.field());\n+                if (mappedFieldType != null) {\n+                    TermsEnum terms = mappedFieldType.getTerms(request.caseInsensitive(), request.string(), queryShardContext);\n+                    if (terms != null) {\n+                        shardTermsEnums.add(terms);\n+                    }\n+                }\n+            }\n+            MultiShardTermsEnum te = new MultiShardTermsEnum(shardTermsEnums.toArray(new TermsEnum[0]));\n+\n+            int shard_size = request.size();\n+            // All the above prep might take a while - do a timer check now before we continue further.\n+            if (System.currentTimeMillis() > scheduledEnd) {\n+                return new NodeTermsEnumResponse(request.nodeId(), termsList, error, false);\n+            }\n+\n+            int numTermsBetweenClockChecks = 100;\n+            int termCount = 0;\n+            // Collect in alphabetical order\n+            while (te.next() != null) {\n+                termCount++;\n+                if (termCount > numTermsBetweenClockChecks) {\n+                    if (System.currentTimeMillis() > scheduledEnd) {\n+                        boolean complete = te.next() == null;\n+                        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, complete);\n+                    }\n+                    termCount = 0;\n+                }\n+                long df = te.docFreq();\n+                BytesRef bytes = te.term();\n+                termsList.add(new TermCount(bytes.utf8ToString(), df));\n+                if (termsList.size() >= shard_size) {\n+                    break;\n+                }\n+            }\n+\n+        } catch (Exception e) {\n+            error = ExceptionsHelper.stackTrace(e);\n+        } finally {\n+            IOUtils.close(openedResources);\n+        }\n+        return new NodeTermsEnumResponse(request.nodeId(), termsList, error, true);\n+    }\n+\n+    // TODO remove this so we can shift code to server module - write a separate Interceptor class to \n+    // rewrite requests according to security rules \n+    private boolean canAccess(\n+        ShardId shardId,\n+        NodeTermsEnumRequest request,\n+        XPackLicenseState frozenLicenseState,\n+        ThreadContext threadContext        \n+    ) throws IOException {\n+        if (frozenLicenseState.isSecurityEnabled()) {\n+            var licenseChecker = new MemoizedSupplier<>(() -> frozenLicenseState.checkFeature(Feature.SECURITY_DLS_FLS));\n+            IndicesAccessControl indicesAccessControl = threadContext.getTransient(AuthorizationServiceField.INDICES_PERMISSIONS_KEY);\n+            IndicesAccessControl.IndexAccessControl indexAccessControl = indicesAccessControl.getIndexPermissions(shardId.getIndexName());\n+\n+         \n+            if (indexAccessControl != null) {\n+                final boolean dls = indexAccessControl.getDocumentPermissions().hasDocumentLevelPermissions();\n+                if ( dls && licenseChecker.get()) {\n+                    // Check to see if any of the roles defined for the current user rewrite to match_all \n+                    \n+                    SecurityContext securityContext = new SecurityContext(clusterService.getSettings(), threadContext);\n+                    final IndexService indexService = indicesService.indexServiceSafe(shardId.getIndex());\n+                    final SearchExecutionContext queryShardContext = indexService.newSearchExecutionContext(\n+                        shardId.id(),\n+                        0,\n+                        null,\n+                        request::nodeStartedTimeMillis,\n+                        null,\n+                        Collections.emptyMap()\n+                    );\n+\n+                    // Current user has potentially many roles and therefore potentially many queries\n+                    // defining sets of docs accessible\n+                    Set<BytesReference> queries = indexAccessControl.getDocumentPermissions().getQueries();\n+                    for (BytesReference querySource : queries) {\n+                        QueryBuilder queryBuilder = DLSRoleQueryValidator.evaluateAndVerifyRoleQuery(\n+                            querySource,\n+                            scriptService,\n+                            queryShardContext.getXContentRegistry(),\n+                            securityContext.getUser()\n+                        );\n+                        QueryBuilder rewrittenQueryBuilder = Rewriteable.rewrite(queryBuilder, queryShardContext);\n+                        if (rewrittenQueryBuilder instanceof MatchAllQueryBuilder) {\n+                            // One of the roles assigned has \"all\" permissions - allow unfettered access to termsDict\n+                            return true;\n+                        }\n+                    }\n+                    return false;\n+                }\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private boolean canMatchShard(ShardId shardId, NodeTermsEnumRequest req) throws IOException {\n+        if (req.indexFilter() == null || req.indexFilter() instanceof MatchAllQueryBuilder) {\n+            return true;\n+        }\n+        ShardSearchRequest searchRequest = new ShardSearchRequest(shardId, req.nodeStartedTimeMillis(), AliasFilter.EMPTY);\n+        searchRequest.source(new SearchSourceBuilder().query(req.indexFilter()));\n+        return searchService.canMatch(searchRequest).canMatch();\n+    }\n+\n+    protected class AsyncBroadcastAction {\n+\n+        private final Task task;\n+        private final TermsEnumRequest request;\n+        private ActionListener<TermsEnumResponse> listener;\n+        private final ClusterState clusterState;\n+        private final DiscoveryNodes nodes;\n+        private final int expectedOps;\n+        private final AtomicInteger counterOps = new AtomicInteger();\n+        private final AtomicReferenceArray<NodeTermsEnumResponse> nodesResponses;\n+        private Map<String, Set<ShardId>> nodeBundles;\n+\n+        protected AsyncBroadcastAction(Task task, TermsEnumRequest request, ActionListener<TermsEnumResponse> listener) {\n+            this.task = task;\n+            this.request = request;\n+            this.listener = listener;\n+\n+            clusterState = clusterService.state();\n+\n+            ClusterBlockException blockException = checkGlobalBlock(clusterState, request);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+            // update to concrete indices\n+            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(clusterState, request);\n+            blockException = checkRequestBlock(clusterState, request, concreteIndices);\n+            if (blockException != null) {\n+                throw blockException;\n+            }\n+\n+            nodes = clusterState.nodes();\n+            logger.trace(\"resolving shards based on cluster state version [{}]\", clusterState.version());\n+            nodeBundles = getNodeBundles(clusterState, request, concreteIndices);\n+            expectedOps = nodeBundles.size();\n+\n+            nodesResponses = new AtomicReferenceArray<>(expectedOps);\n+        }\n+\n+        public void start() {\n+            if (nodeBundles.size() == 0) {\n+                // no shards\n+                try {\n+                    listener.onResponse(newResponse(request, new AtomicReferenceArray<>(0), true, nodeBundles));\n+                } catch (Exception e) {\n+                    listener.onFailure(e);\n+                }\n+                // TODO or remove above try and instead just call finishHim() here? Helps keep return logic consistent\n+                return;\n+            }\n+            // count the local operations, and perform the non local ones\n+            int nodeIndex = -1;\n+            for (final String nodeId : nodeBundles.keySet()) {\n+                if (checkForEarlyFinish()) {\n+                    return;\n+                }\n+                nodeIndex++;\n+                Set<ShardId> shardIds = nodeBundles.get(nodeId);\n+                if (shardIds.size() > 0) {\n+                    performOperation(nodeId, shardIds, nodeIndex);\n+                } else {\n+                    // really, no shards active in this group\n+                    onNoOperation(nodeId);\n+                }\n+            }\n+        }\n+\n+        // Returns true if we exited with a response to the caller.\n+        boolean checkForEarlyFinish() {\n+            long now = System.currentTimeMillis();\n+            if ((now - task.getStartTime()) > request.timeout().getMillis()) {\n+                finishHim(false);\n+                return true;\n+            }\n+            return false;\n+        }\n+\n+        protected void performOperation(final String nodeId, final Set<ShardId> shardIds, final int nodeIndex) {\n+            if (shardIds.size() == 0) {\n+                // no more active shards... (we should not really get here, just safety)\n+                // MH TODO somewhat arbitrarily returining firsy\n+                onNoOperation(nodeId);\n+            } else {\n+                try {\n+                    // TODO pass through a reduced timeout (the original time limit, minus whatever we may have\n+                    // spent already getting to this point.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bcf76a4c62533664747506bc1c876a5b2a1fbf2c"}, "originalPosition": 489}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ffbb03e37184c44c256a22a45e227032f554723", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/9ffbb03e37184c44c256a22a45e227032f554723", "committedDate": "2021-05-06T08:40:31Z", "message": "A TermsEnum API for discovering terms in the index.\nTerms matching a given prefix can be returned in alphabetical or by popularity,\nA timeout can limit the amount of time spent looking for matches.\n\nExpected to be useful in auto-complete or regex debugging use cases."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b95cf815b49ce78d42b51440ad983c025bb9c254", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/b95cf815b49ce78d42b51440ad983c025bb9c254", "committedDate": "2021-05-06T08:40:31Z", "message": "Added HLRC support and related integration test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab826b43205d0bfd257353fb46e234dc43103b3d", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/ab826b43205d0bfd257353fb46e234dc43103b3d", "committedDate": "2021-05-06T08:40:31Z", "message": "Added client classes for HLRC.\nAdded yaml test with index filter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a5e654f83912d0a27e87c0dca0b2ea89ede51d2", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/7a5e654f83912d0a27e87c0dca0b2ea89ede51d2", "committedDate": "2021-05-06T08:40:31Z", "message": "License fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "93dfe30fca29bffc372feda1bc44d3e317e6e973", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/93dfe30fca29bffc372feda1bc44d3e317e6e973", "committedDate": "2021-05-06T08:40:31Z", "message": "Remove HLRC code for now - requires less-than-ideal package names while we\u2019re unable to move main server implementation out of xpack (due to security dependency).\nDon\u2019t want clients to build dependencies on the wrong package names and the Java client changes are underway anyway."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "58bbf413044259897bd8c54e4013e6243c3eb630", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/58bbf413044259897bd8c54e4013e6243c3eb630", "committedDate": "2021-05-06T08:40:31Z", "message": "Return empty arrays when no results rather than no `terms` property at all"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7fb781d5f55fdf345242ac73ca9165ee998e4768", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/7fb781d5f55fdf345242ac73ca9165ee998e4768", "committedDate": "2021-05-06T08:40:31Z", "message": "Fix bundling of shardIds for nodes, add success/fail accounting of numbers of shards"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e9da3918ad693375203246f2bb73303007156f3", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/4e9da3918ad693375203246f2bb73303007156f3", "committedDate": "2021-05-06T08:40:31Z", "message": "Type fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cac1bb32d38a4481dd6812adb80790dbfa8784e9", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/cac1bb32d38a4481dd6812adb80790dbfa8784e9", "committedDate": "2021-05-06T08:40:31Z", "message": "Types warning"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "810d638656e36752cbe3f7c97a4dbdc6e8eeff1b", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/810d638656e36752cbe3f7c97a4dbdc6e8eeff1b", "committedDate": "2021-05-06T08:40:31Z", "message": "Removed hot/warm tier tests (in anticipation of new queryable _tier field)\nMove canMatch logic to run on network thread\nInjected searchService so we can use its canMatch method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "156302f3edfd1032c92b4e766eaacaaa3fe983a5", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/156302f3edfd1032c92b4e766eaacaaa3fe983a5", "committedDate": "2021-05-06T08:40:31Z", "message": "Move rest-api-spec and related YML test to new standard home for this stuff."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "62af758dc220f2a25d825c4e809403be73dfcb4d", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/62af758dc220f2a25d825c4e809403be73dfcb4d", "committedDate": "2021-05-06T08:40:31Z", "message": "Unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03f79deb3b10a9907699a8ca981a6b6c77b8bf75", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/03f79deb3b10a9907699a8ca981a6b6c77b8bf75", "committedDate": "2021-05-06T08:40:32Z", "message": "Move test to xpack"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53e12cdbbd0a1d884da6b44d19eaabf59eaaa35f", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/53e12cdbbd0a1d884da6b44d19eaabf59eaaa35f", "committedDate": "2021-05-06T08:40:32Z", "message": "Return early on network thread if can\u2019t match any shards.\nRemove FLS logic now we use the right searcher."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b36f477f5901ed4f5eaeb1686fbe9b2676a88e3d", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/b36f477f5901ed4f5eaeb1686fbe9b2676a88e3d", "committedDate": "2021-05-06T08:40:32Z", "message": "Removed sort by popularity option"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b1b6d935a51a4940e6ea7d05d97e4b3d87b1614", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/3b1b6d935a51a4940e6ea7d05d97e4b3d87b1614", "committedDate": "2021-05-06T08:40:32Z", "message": "Unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5250cc7e524b1019121feadbde24b80167a8a0da", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/5250cc7e524b1019121feadbde24b80167a8a0da", "committedDate": "2021-05-06T08:40:32Z", "message": "Addressing some review comments (thanks Jim/Adrien!)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3288641ae857d9aae7096f704ade6fdd9eac53e8", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/3288641ae857d9aae7096f704ade6fdd9eac53e8", "committedDate": "2021-05-06T08:40:32Z", "message": "Docs tidy up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c0096859600a7e8a8a3f36a92ab5a205dcd8dd2", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/2c0096859600a7e8a8a3f36a92ab5a205dcd8dd2", "committedDate": "2021-05-06T08:40:32Z", "message": "Provide full stack traces for errors, change TODO comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a68b70631cf226f2130069d4a369b50f3c7f01a", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/6a68b70631cf226f2130069d4a369b50f3c7f01a", "committedDate": "2021-05-06T08:40:32Z", "message": "Move location of YAML test - was causing errors when seated alongside core/src/yamlRestTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1fe0a11b41924d69a2fbea9e2d1b19c0cd3ff8b6", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/1fe0a11b41924d69a2fbea9e2d1b19c0cd3ff8b6", "committedDate": "2021-05-06T08:40:32Z", "message": "Security enhancement - allow access where DLS rewrites to match_all. Added security tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f598607ac112be5158e0c8654810fbdff006045", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/2f598607ac112be5158e0c8654810fbdff006045", "committedDate": "2021-05-06T08:40:32Z", "message": "Remove acquisition of searcher from security check code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c38b78005934cbd8c6b83e4ba65d74b98c6fd62", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/4c38b78005934cbd8c6b83e4ba65d74b98c6fd62", "committedDate": "2021-05-06T08:40:32Z", "message": "Changed termenum to termsenum. REST endpoint is now _terms_enum"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c40a0dba4994b8146499d0e298eeca90e527ab04", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/c40a0dba4994b8146499d0e298eeca90e527ab04", "committedDate": "2021-05-06T08:40:32Z", "message": "Checkstyle fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b9f41c3e3c7fb613745361977d485fdbb456a19", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/6b9f41c3e3c7fb613745361977d485fdbb456a19", "committedDate": "2021-05-06T08:40:32Z", "message": "Addressing review comments - formatting, thread pool choices and more"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "814e45e9e0c5e4ae1d94c26aed5fdb091851690d", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/814e45e9e0c5e4ae1d94c26aed5fdb091851690d", "committedDate": "2021-05-06T08:40:32Z", "message": "Oops. Thought I\u2019d resolved this review comment but hadn\u2019t"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "989751883f134abeafdef6da3a747ac70203573a", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/989751883f134abeafdef6da3a747ac70203573a", "committedDate": "2021-05-06T08:40:32Z", "message": "Changed timeout setting to a TimeValue"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2cb91dfcc247cd1deb849fd8cf5403f7e7117f34", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/2cb91dfcc247cd1deb849fd8cf5403f7e7117f34", "committedDate": "2021-05-06T08:40:33Z", "message": "Checkstyle fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d55f99c93e4f61b20a537d6c8ad1c507f95a314", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/6d55f99c93e4f61b20a537d6c8ad1c507f95a314", "committedDate": "2021-05-06T08:40:33Z", "message": "In flattened fields make only the value (not the field name) subject to case insensitive search"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf7005361a9b3426685eb5911f36c6255c423f2e", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/cf7005361a9b3426685eb5911f36c6255c423f2e", "committedDate": "2021-05-06T08:40:33Z", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22312cf82527d80e4b330d56fec8108c9cdfd513", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/22312cf82527d80e4b330d56fec8108c9cdfd513", "committedDate": "2021-05-06T08:40:33Z", "message": "Remove outdated TODOs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bcf76a4c62533664747506bc1c876a5b2a1fbf2c", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/bcf76a4c62533664747506bc1c876a5b2a1fbf2c", "committedDate": "2021-05-04T13:26:48Z", "message": "Moved initialisation of data node timing of request from NodeTermsEnumRequest constructor to TransportTermsEnumAction#asyncNodeOperation"}, "afterCommit": {"oid": "22312cf82527d80e4b330d56fec8108c9cdfd513", "author": {"user": {"login": "markharwood", "name": null}}, "url": "https://github.com/elastic/elasticsearch/commit/22312cf82527d80e4b330d56fec8108c9cdfd513", "committedDate": "2021-05-06T08:40:33Z", "message": "Remove outdated TODOs"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4442, "cost": 1, "resetAt": "2021-10-28T18:54:27Z"}}}